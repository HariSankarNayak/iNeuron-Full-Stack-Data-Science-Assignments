{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f149cae",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c063f",
   "metadata": {},
   "source": [
    "## 1. In what modes should the PdfFileReader() and PdfFileWriter() File objects will be opened?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "For `PdfFileReader()` file objects should be opened in `rb` -> read binary mode,\n",
    "\n",
    "Whereas for `PdfFileWriter()` file objects should be opened in `wb` -> write binary mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd13eb",
   "metadata": {},
   "source": [
    "## 2. From a PdfFileReader object, how do you get a Page object for page 5?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "`PdfFileReader` class provides a method called `getPage(page_no)` to get a page object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "911bee10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Frame the Problem                                                                                                     39\\n Select a P erformance M easure                                                                                   42\\n Check the Assum ptions                                                                                              45\\n Get the Da ta                                                                                                                     45\\n Crea te the W orkspace                                                                                                 45\\n Download the Da ta                                                                                                     49\\n T ake a Quick Look a t the Da ta Structure                                                                 50\\n Crea te a T est Set                                                                                                           54\\n Discover and V isualize the Da ta to Gain Insigh ts                                                      58\\n V isualizing Geogra phical Da ta                                                                                  59\\n Looking for Correla tions                                                                                            62\\n Experimen ting with A ttribute Combina tions                                                         65\\n Prepare the Da ta for M achine Learning Algorithms                                                 66\\n Da ta Cleaning                                                                                                              67\\n H andling T ext and Ca tegorical A ttributes                                                               69\\n Custom T ransformers                                                                                                 71\\n F ea ture Scaling                                                                                                             72\\n T ransforma tion Pipelines                                                                                           73\\n Select and T rain a M odel                                                                                                75\\n T raining and E valua ting on the T raining Set                                                          75\\n B etter E valua tion U sing Cross-V alida tion                                                               76\\n Fine-T une Y our M odel                                                                                                   79\\n Grid Search                                                                                                                  79\\n R andomized Search                                                                                                    81\\n Ensemble M ethods                                                                                                      82\\n Analyze the B est M odels and Their Errors                                                              82\\n E valua te Y our System on the T est Set                                                                       83\\n La unch, M onitor , and M ain tain Y our System                                                             84\\n T r y I t Out!                                                                                                                        85\\n Exercises                                                                                                                           85\\n3.\\nClassi•cation\\n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   87\\n MNIST                                                                                                                              87\\n T raining a Binar y Classifier                                                                                           90\\n P erformance M easures                                                                                                   90\\n M easuring A ccuracy U sing Cross-V alida tion                                                         91\\n Confusion M a trix                                                                                                        92\\n Precision and Recall                                                                                                    94\\n Precision/Recall T radeoff                                                                                           95\\n The RO C Cur ve                                                                                                           99\\n M ulticlass Classifica tion                                                                                              102\\n Error Analysis                                                                                                               104\\n iv  |  Table of Contents\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Code:\n",
    "from PyPDF2 import PdfFileReader\n",
    "pdf_reader = PdfFileReader('HandsonMachine-Learning-with-Scikit-2E.pdf')\n",
    "pdf_reader.getPage(5).extractText()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7d5e9",
   "metadata": {},
   "source": [
    "## 3. What PdfFileReader variable stores the number of pages in the PDF document?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "`getNumPages()` method of `PdfFileReader` class returns the no pages in a PDF document and type of the variable is `int`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf52330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PyPDF2 import PdfFileReader\n",
    "pdf_reader = PdfFileReader('HandsonMachine-Learning-with-Scikit-2E.pdf')\n",
    "print(pdf_reader.getNumPages()) # Prints the no of pages\n",
    "print(type(pdf_reader.getNumPages())) # Prints the type of the no of pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750ecda",
   "metadata": {},
   "source": [
    "## 4. If a PdfFileReader object’s PDF is encrypted with the password `swordfish`, what must you do before you can obtain Page objects from it?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "If a `PdfFileReader` object’s PDF is encrypted with the password `swordfish` and you're not aware of it. first read the Pdf using the PdfFileReader Class. PdfFileReader class provides a attribute called `isEncrypted` to check whether a pdf is encrypted or not. the method returns true if a pdf is encrypted and vice versa.  \n",
    "if pdf is encrypted use the `decrypt()` method provided by PdfFileReader class first then try to read the contents/pages of the pdf, else PyPDF2 will raise the following error `PyPDF2.utils.PdfReadError: file has not been decrypted`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbf3e761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Au r • l ie n G • r o n\n",
      "Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      " C o n c e p ts, T oo l s, a n d T e c h n iq ues t o\n",
      " B u i ld I n t e l l i ge n t S y s t e m s\n",
      "SECOND EDITION\n",
      "Boston\n",
      "Farnham\n",
      "Sebastopol\n",
      "Tokyo\n",
      "Beijing\n",
      "Boston\n",
      "Farnham\n",
      "Sebastopol\n",
      "Tokyo\n",
      "Beijing\n",
      "\n",
      "978-1-492-03264-9\n",
      "[LSI]\n",
      "Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      " by A ur•lien G•ron\n",
      " Copyrigh t † 2019 A ur•lien G•ron. All righ ts reser ved.\n",
      " Prin ted in the U nited Sta tes of America.\n",
      " Published by O ‡Reilly M edia, Inc., 1005 Gra venstein High wa y N orth, Sebastopol, CA 95472.\n",
      " O ‡Reilly books ma y be purchased for educa tional, business, or sales promotional use. Online editions are\n",
      " also a vailable for most titles (\n",
      " h ttp://o r ei l l y .c o m\n",
      " ). F or more informa tion, con tact our corpora te/institutional\n",
      " sales departmen t: 800-998-9938 or \n",
      " c o r p o r a t e@o r ei l l y .c o m\n",
      ".\n",
      "Editor:\n",
      "  N icole T ache\n",
      "Interior Designer:\n",
      "  Da vid F uta to\n",
      "Cover Designer:\n",
      "  Karen M on tgomer y\n",
      "Illustrator:\n",
      " Rebecca Demarest\n",
      " J une 2019:\n",
      " Second Edition\n",
      "Revision History for the Early Release\n",
      " 2018-11-05:  First Release\n",
      " 2019-01-24:  Second Release\n",
      " 2019-03-07:  Third Release\n",
      " 2019-03-29:  F ourth Release\n",
      " 2019-04-22:  Fifth Release\n",
      "See \n",
      " h ttp://o r ei l l y .c o m/c a t a l og/er r a t a.cs p?i sb n=9781492032649\n",
      " for release details.\n",
      " The O ‡Reilly logo is a registered trademark of O ‡Reilly M edia, Inc. \n",
      " H a n d s-o n M a c h i n e L e a r n i n g w i t h\n",
      " S ci k i t-L e a r n, K er as, a n d T ens o rF l o w\n",
      " , the cover image, and rela ted trade dress are trademarks of O ‡Reilly\n",
      " M edia, Inc.\n",
      " While the publisher and the a uthor ha ve used good faith efforts to ensure tha t the informa tion and\n",
      " instructions con tained in this work are accura te, the publisher and the a uthor disclaim all responsibility\n",
      " for errors or omissions, including without limita tion responsibility for damages resulting from the use of\n",
      " or reliance on this work. U se of the informa tion and instructions con tained in this work is a t your own\n",
      " risk. If an y code sam ples or other technolog y this work con tains or describes is subject to open source\n",
      " licenses or the in tellectual property righ ts of others, it is your responsibility to ensure tha t your use\n",
      " thereof com plies with such licenses and/or righ ts.\n",
      "\n",
      "Table of Contents\n",
      " Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   xi\n",
      "Part I. \n",
      "The Fundamentals of Machine Learning\n",
      "1.\n",
      " The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3\n",
      " Wha t I s M achine Learning?                                                                                            4\n",
      " Wh y U se M achine Learning?                                                                                          4\n",
      " T ypes of M achine Learning Systems                                                                              8\n",
      " Super vised/U nsuper vised Learning                                                                            8\n",
      " Ba tch and Online Learning                                                                                        15\n",
      " Instance-Based V ersus M odel-Based Learning                                                       18\n",
      " M ain Challenges of M achine Learning                                                                        24\n",
      " Insufficien t Quan tity of T raining Da ta                                                                    24\n",
      " N onrepresen ta tive T raining Da ta                                                                             26\n",
      " P oor -Quality Da ta                                                                                                       27\n",
      " Irrelevan t F ea tures                                                                                                      27\n",
      " O verfitting the T raining Da ta                                                                                    28\n",
      " U nderfitting the T raining Da ta                                                                                 30\n",
      " Stepping Back                                                                                                              30\n",
      " T esting and V alida ting                                                                                                    31\n",
      " H yperparameter T uning and M odel Selection                                                        32\n",
      " Da ta Misma tch                                                                                                            33\n",
      " Exercises                                                                                                                           34\n",
      "2.\n",
      " End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   37\n",
      " W orking with Real Da ta                                                                                                 38\n",
      " Look a t the Big Picture                                                                                                   39\n",
      "iii\n",
      "\n",
      " Frame the Problem                                                                                                     39\n",
      " Select a P erformance M easure                                                                                   42\n",
      " Check the Assum ptions                                                                                              45\n",
      " Get the Da ta                                                                                                                     45\n",
      " Crea te the W orkspace                                                                                                 45\n",
      " Download the Da ta                                                                                                     49\n",
      " T ake a Quick Look a t the Da ta Structure                                                                 50\n",
      " Crea te a T est Set                                                                                                           54\n",
      " Discover and V isualize the Da ta to Gain Insigh ts                                                      58\n",
      " V isualizing Geogra phical Da ta                                                                                  59\n",
      " Looking for Correla tions                                                                                            62\n",
      " Experimen ting with A ttribute Combina tions                                                         65\n",
      " Prepare the Da ta for M achine Learning Algorithms                                                 66\n",
      " Da ta Cleaning                                                                                                              67\n",
      " H andling T ext and Ca tegorical A ttributes                                                               69\n",
      " Custom T ransformers                                                                                                 71\n",
      " F ea ture Scaling                                                                                                             72\n",
      " T ransforma tion Pipelines                                                                                           73\n",
      " Select and T rain a M odel                                                                                                75\n",
      " T raining and E valua ting on the T raining Set                                                          75\n",
      " B etter E valua tion U sing Cross-V alida tion                                                               76\n",
      " Fine-T une Y our M odel                                                                                                   79\n",
      " Grid Search                                                                                                                  79\n",
      " R andomized Search                                                                                                    81\n",
      " Ensemble M ethods                                                                                                      82\n",
      " Analyze the B est M odels and Their Errors                                                              82\n",
      " E valua te Y our System on the T est Set                                                                       83\n",
      " La unch, M onitor , and M ain tain Y our System                                                             84\n",
      " T r y I t Out!                                                                                                                        85\n",
      " Exercises                                                                                                                           85\n",
      "3.\n",
      "Classi•cation\n",
      " . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   87\n",
      " MNIST                                                                                                                              87\n",
      " T raining a Binar y Classifier                                                                                           90\n",
      " P erformance M easures                                                                                                   90\n",
      " M easuring A ccuracy U sing Cross-V alida tion                                                         91\n",
      " Confusion M a trix                                                                                                        92\n",
      " Precision and Recall                                                                                                    94\n",
      " Precision/Recall T radeoff                                                                                           95\n",
      " The RO C Cur ve                                                                                                           99\n",
      " M ulticlass Classifica tion                                                                                              102\n",
      " Error Analysis                                                                                                               104\n",
      " iv  |  Table of Contents\n",
      "\n",
      " M ultilabel Classifica tion                                                                                              108\n",
      " M ultioutput Classifica tion                                                                                           109\n",
      " Exercises                                                                                                                         110\n",
      "4.\n",
      " Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   113\n",
      " Linear Regression                                                                                                         114\n",
      " The N ormal Equa tion                                                                                               116\n",
      " Com puta tional Com plexity                                                                                     119\n",
      " Gradien t Descen t                                                                                                          119\n",
      " Ba tch Gradien t Descen t                                                                                            123\n",
      " Stochastic Gradien t Descen t                                                                                    126\n",
      " Mini-ba tch Gradien t Descen t                                                                                  129\n",
      " P olynomial Regression                                                                                                 130\n",
      " Learning Cur ves                                                                                                            132\n",
      " Regularized Linear M odels                                                                                          136\n",
      " Ridge Regression                                                                                                       137\n",
      " Lasso Regression                                                                                                       139\n",
      " Elastic N et                                                                                                                  142\n",
      " Early Stopping                                                                                                           142\n",
      " Logistic Regression                                                                                                       144\n",
      " Estima ting Probabilities                                                                                           144\n",
      " T raining and Cost F unction                                                                                    145\n",
      " Decision B oundaries                                                                                                 146\n",
      " Softmax Regression                                                                                                   149\n",
      " Exercises                                                                                                                         153\n",
      "5.\n",
      " Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   155\n",
      " Linear SVM Classifica tion                                                                                           155\n",
      " Soft M argin Classifica tion                                                                                        156\n",
      " N onlinear SVM Classifica tion                                                                                    159\n",
      " P olynomial K ernel                                                                                                    160\n",
      " A dding Similarity F ea tures                                                                                      161\n",
      " Ga ussian RBF K ernel                                                                                                162\n",
      " Com puta tional Com plexity                                                                                     163\n",
      " SVM Regression                                                                                                            164\n",
      " U nder the H ood                                                                                                            166\n",
      " Decision F unction and Predictions                                                                        166\n",
      " T raining Objective                                                                                                    167\n",
      " Quadra tic Programming                                                                                          169\n",
      " The Dual Problem                                                                                                     170\n",
      " K ernelized SVM                                                                                                        171\n",
      " Online SVMs                                                                                                             174\n",
      " Table of Contents  |  v\n",
      "\n",
      " Exercises                                                                                                                         175\n",
      "6.\n",
      " Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   177\n",
      " T raining and V isualizing a Decision T ree                                                                 177\n",
      " M aking Predictions                                                                                                      179\n",
      " Estima ting Class Probabilities                                                                                    181\n",
      " The CAR T T raining Algorithm                                                                                  182\n",
      " Com puta tional Com plexity                                                                                         183\n",
      " Gini Im purity or En tropy?                                                                                          183\n",
      " Regulariza tion H yperparameters                                                                               184\n",
      " Regression                                                                                                                      185\n",
      " Instability                                                                                                                       188\n",
      " Exercises                                                                                                                         189\n",
      "7.\n",
      " Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   191\n",
      " V oting Classifiers                                                                                                          192\n",
      " Bagging and P asting                                                                                                     195\n",
      " Bagging and P asting in Scikit-Learn                                                                      196\n",
      " Out-of-Bag E valua tion                                                                                             197\n",
      " R andom P a tches and R andom Subspaces                                                                 198\n",
      " R andom F orests                                                                                                            199\n",
      " Extra-T rees                                                                                                                 200\n",
      " F ea ture Im portance                                                                                                   200\n",
      " B oosting                                                                                                                         201\n",
      " A daB oost                                                                                                                    202\n",
      " Gradien t B oosting                                                                                                     205\n",
      " Stacking                                                                                                                          210\n",
      " Exercises                                                                                                                         213\n",
      "8.\n",
      " Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   215\n",
      " The Curse of Dimensionality                                                                                      216\n",
      " M ain A pproaches for Dimensionality Reduction                                                    218\n",
      " Projection                                                                                                                   218\n",
      " M anifold Learning                                                                                                    220\n",
      " PCA                                                                                                                                 222\n",
      " Preser ving the V ariance                                                                                           222\n",
      " Principal Com ponen ts                                                                                             223\n",
      "Projecting Down to \n",
      "d\n",
      "  Dimensions                                                                         224\n",
      " U sing Scikit-Learn                                                                                                     224\n",
      " Explained V ariance R a tio                                                                                         225\n",
      " Choosing the Righ t N umber of Dimensions                                                        225\n",
      " PCA for Com pression                                                                                              226\n",
      " vi  |  Table of Contents\n",
      "\n",
      " R andomized PCA                                                                                                     227\n",
      " Incremen tal PCA                                                                                                      227\n",
      " K ernel PCA                                                                                                                    228\n",
      " Selecting a K ernel and T uning H yperparameters                                                 229\n",
      " LLE                                                                                                                                  232\n",
      " Other Dimensionality Reduction T echniques                                                          234\n",
      " Exercises                                                                                                                         235\n",
      "9.\n",
      " Unsupervised Learning Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   237\n",
      " Clustering                                                                                                                       238\n",
      " K-M eans                                                                                                                     240\n",
      " Limits of K-M eans                                                                                                    250\n",
      " U sing clustering for image segmen ta tion                                                              251\n",
      " U sing Clustering for Preprocessing                                                                        252\n",
      " U sing Clustering for Semi-Super vised Learning                                                  254\n",
      " DBSCAN                                                                                                                    256\n",
      " Other Clustering Algorithms                                                                                  259\n",
      " Ga ussian Mixtures                                                                                                        260\n",
      " Anomaly Detection using Ga ussian Mixtures                                                      266\n",
      " Selecting the N umber of Clusters                                                                           267\n",
      " Ba yesian Ga ussian Mixture M odels                                                                       270\n",
      " Other Anomaly Detection and N ovelty Detection Algorithms                         274\n",
      "Part II. \n",
      "Neural Networks and Deep Learning\n",
      "10.\n",
      "Introduction to \n",
      "Arti•cial\n",
      "  Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . .   277\n",
      " From Biological to Artificial N eurons                                                                       278\n",
      " Biological N eurons                                                                                                    279\n",
      " Logical Com puta tions with N eurons                                                                     281\n",
      " The P erceptron                                                                                                          281\n",
      " M ulti-La yer P erceptron and Backpropaga tion                                                     286\n",
      " Regression MLP s                                                                                                       289\n",
      " Classifica tion MLP s                                                                                                  290\n",
      " Im plemen ting MLP s with K eras                                                                                 292\n",
      " Installing T ensorFlow 2                                                                                            293\n",
      " Building an Image Classifier U sing the Sequen tial API                                      294\n",
      " Building a Regression MLP U sing the Sequen tial API                                        303\n",
      " Building Com plex M odels U sing the F unctional API                                         304\n",
      " Building D ynamic M odels U sing the Subclassing API                                        309\n",
      " Sa ving and Restoring a M odel                                                                                 311\n",
      " U sing Callbacks                                                                                                         311\n",
      " Table of Contents  |  vii\n",
      "\n",
      " V isualiza tion U sing T ensorB oard                                                                           313\n",
      " Fine-T uning N eural N etwork H yperparameters                                                      315\n",
      " N umber of Hidden La yers                                                                                       319\n",
      " N umber of N eurons per Hidden La yer                                                                  320\n",
      " Learning R a te, Ba tch Size and Other H yperparameters                                      320\n",
      " Exercises                                                                                                                         322\n",
      "11.\n",
      " Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   325\n",
      " V anishing/Exploding Gradien ts Problems                                                               326\n",
      " Glorot and H e Initializa tion                                                                                    327\n",
      " N onsa tura ting A ctiva tion F unctions                                                                      329\n",
      " Ba tch N ormaliza tion                                                                                                 333\n",
      " Gradien t Clipping                                                                                                     338\n",
      " Reusing Pretrained La yers                                                                                           339\n",
      " T ransfer Learning W ith K eras                                                                                 341\n",
      " U nsuper vised Pretraining                                                                                        343\n",
      " Pretraining on an A uxiliar y T ask                                                                            344\n",
      " F aster Optimizers                                                                                                          344\n",
      " M omen tum Optimiza tion                                                                                       345\n",
      " N esterov A ccelera ted Gradien t                                                                               346\n",
      " A daGrad                                                                                                                     347\n",
      " RMSProp                                                                                                                    349\n",
      " A dam and N adam Optimiza tion                                                                            349\n",
      " Learning R a te Scheduling                                                                                        352\n",
      " A voiding O verfitting Through Regulariza tion                                                         356\n",
      "…\n",
      "1\n",
      " and …\n",
      "2\n",
      "  Regulariza tion                                                                                            356\n",
      " Dropout                                                                                                                      357\n",
      " M on te-Carlo (MC) Dropout                                                                                   360\n",
      " M ax-N orm Regulariza tion                                                                                       362\n",
      " Summar y and Practical Guidelines                                                                            363\n",
      " Exercises                                                                                                                         364\n",
      "12.\n",
      " Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   367\n",
      " A Quick T our of T ensorFlow                                                                                      368\n",
      " U sing T ensorFlow like N umPy                                                                                   371\n",
      " T ensors and Opera tions                                                                                           371\n",
      " T ensors and N umPy                                                                                                 373\n",
      " T ype Con versions                                                                                                      374\n",
      " V ariables                                                                                                                     374\n",
      " Other Da ta Structures                                                                                              375\n",
      " Customizing M odels and T raining Algorithms                                                       376\n",
      " Custom Loss F unctions                                                                                            376\n",
      " viii  |  Table of Contents\n",
      "\n",
      " Sa ving and Loading M odels Tha t Con tain Custom Com ponen ts                     377\n",
      " Custom A ctiva tion F unctions, Initializers, Regularizers, and Constrain ts      379\n",
      " Custom M etrics                                                                                                         380\n",
      " Custom La yers                                                                                                           383\n",
      " Custom M odels                                                                                                         386\n",
      " Losses and M etrics Based on M odel In ternals                                                      388\n",
      " Com puting Gradien ts U sing A utodiff                                                                    389\n",
      " Custom T raining Loops                                                                                           393\n",
      " T ensorFlow F unctions and Gra phs                                                                            396\n",
      " A utogra ph and T racing                                                                                            398\n",
      " TF F unction R ules                                                                                                     400\n",
      "13.\n",
      " Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . .   403\n",
      " The Da ta API                                                                                                                 404\n",
      " Chaining T ransforma tions                                                                                       405\n",
      " Sh uffling the Da ta                                                                                                     406\n",
      " Preprocessing the Da ta                                                                                             409\n",
      " Putting E ver ything T ogether                                                                                   410\n",
      " Prefetching                                                                                                                 411\n",
      " U sing the Da taset W ith tf.keras                                                                               413\n",
      " The TFRecord F orma t                                                                                                 414\n",
      " Com pressed TFRecord Files                                                                                    415\n",
      " A Brief In troduction to Protocol Buffers                                                               415\n",
      " T ensorFlow Protobufs                                                                                              416\n",
      " Loading and P arsing Exam ples                                                                               418\n",
      " H andling Lists of Lists U sing the \n",
      "SequenceExample\n",
      "  Protobuf                           419\n",
      " The F ea tures API                                                                                                          420\n",
      " Ca tegorical F ea tures                                                                                                  421\n",
      " Crossed Ca tegorical F ea tures                                                                                  421\n",
      " Encoding Ca tegorical F ea tures U sing One-H ot V ectors                                     422\n",
      " Encoding Ca tegorical F ea tures U sing Embeddings                                             423\n",
      " U sing F ea ture Columns for P arsing                                                                       426\n",
      " U sing F ea ture Columns in Y our M odels                                                                426\n",
      " TF T ransform                                                                                                                428\n",
      " The T ensorFlow Da tasets (TFDS) Project                                                                 429\n",
      "14.\n",
      " Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .   431\n",
      " The Architecture of the V isual Cortex                                                                      432\n",
      " Con volutional La yer                                                                                                     434\n",
      " Filters                                                                                                                          436\n",
      " Stacking M ultiple F ea ture M a ps                                                                              437\n",
      " T ensorFlow Im plemen ta tion                                                                                   439\n",
      " Table of Contents  |  ix\n",
      "\n",
      " M emor y Requiremen ts                                                                                            441\n",
      " P ooling La yer                                                                                                                 442\n",
      " T ensorFlow Im plemen ta tion                                                                                   444\n",
      " CNN Architectures                                                                                                       446\n",
      " LeN et-5                                                                                                                       449\n",
      " AlexN et                                                                                                                       450\n",
      " GoogLeN et                                                                                                                 452\n",
      " V GGN et                                                                                                                      456\n",
      " ResN et                                                                                                                         457\n",
      " Xception                                                                                                                     459\n",
      " SEN et                                                                                                                          461\n",
      " Im plemen ting a ResN et-34 CNN U sing K eras                                                         464\n",
      " U sing Pretrained M odels From K eras                                                                       465\n",
      " Pretrained M odels for T ransfer Learning                                                                  467\n",
      " Classifica tion and Localiza tion                                                                                   469\n",
      " Object Detection                                                                                                           471\n",
      " F ully Con volutional N etworks (FCN s)                                                                  473\n",
      " Y ou Only Look Once (Y OLO)                                                                                 475\n",
      " Seman tic Segmen ta tion                                                                                                478\n",
      " Exercises                                                                                                                         482\n",
      " x  |  Table of Contents\n",
      "\n",
      "1\n",
      " A vailable on Hin ton ‡ s home page a t \n",
      " h ttp://w w w .cs.t o r o n t o .e d u/~h i n t o n/\n",
      ".\n",
      "2\n",
      " Despite the fact tha t Y ann Lecun ‡ s deep con volutional neural networks had worked well for image recognition\n",
      "since the 1990s, although they were not as general purpose.\n",
      "Preface\n",
      "The Machine Learning Tsunami\n",
      " In 2006, Geoffrey Hin ton et al. published a pa per\n",
      "1\n",
      " \n",
      "showing how to train a deep neural\n",
      " network ca pable of recognizing handwritten digits with sta te-of-the-art precision\n",
      " (>98%). They branded this technique —Deep Learning. – T raining a deep neural net\n",
      " was widely considered im possible a t the time,\n",
      "2\n",
      " and most researchers had abandoned\n",
      " the idea since the 1990s. This pa per revived the in terest of the scien tific comm unity\n",
      " and before long man y new pa pers demonstra ted tha t Deep Learning was not only\n",
      " possible, but ca pable of mind-blowing achievemen ts tha t no other M achine Learning\n",
      " (ML) technique could hope to ma tch (with the help of tremendous com puting power\n",
      " and grea t amoun ts of da ta). This en th usiasm soon extended to man y other areas of\n",
      " M achine Learning.\n",
      " F ast-for ward 10 years and M achine Learning has conquered the industr y : it is now a t\n",
      " the heart of m uch of the magic in toda y‡ s high-tech products, ranking your web\n",
      " search results, powering your smartphone ‡ s speech recognition, recommending vidƒ\n",
      " eos, and bea ting the world cham pion a t the game of Go . B efore you know it, it will be\n",
      " driving your car .\n",
      "Machine Learning in Your Projects\n",
      " So na turally you are excited about M achine Learning and you would love to join the\n",
      "party!\n",
      " P erha ps you would like to give your homemade robot a brain of its own? M ake it recƒ\n",
      "ognize faces? Or learn to walk around?\n",
      "xi\n",
      "\n",
      " Or ma ybe your com pan y has tons of da ta (user logs, financial da ta, production da ta,\n",
      " machine sensor da ta, hotline sta ts, HR reports, etc.), and more than likely you could\n",
      " unearth some hidden gems if you just knew where to look; for exam ple:\n",
      "⁄\n",
      " Segmen t customers and find the best marketing stra teg y for each group\n",
      "⁄\n",
      " Recommend products for each clien t based on wha t similar clien ts bough t\n",
      "⁄\n",
      " Detect which transactions are likely to be fra udulen t\n",
      "⁄\n",
      " F orecast next year‡ s reven ue\n",
      "⁄\n",
      "And more\n",
      " Wha tever the reason, you ha ve decided to learn M achine Learning and im plemen t it\n",
      " in your projects. Grea t idea!\n",
      "Objective and Approach\n",
      " This book assumes tha t you know close to nothing about M achine Learning. I ts goal\n",
      " is to give you the concepts, the in tuitions, and the tools you need to actually im pleƒ\n",
      " men t programs ca pable of \n",
      " l e a r n i n g f r o m d a t a\n",
      ".\n",
      " W e will cover a large n umber of techniques, from the sim plest and most commonly\n",
      " used (such as linear regression) to some of the Deep Learning techniques tha t reguƒ\n",
      " larly win com petitions.\n",
      " R a ther than im plemen ting our own toy versions of each algorithm, we will be using\n",
      "actual production-ready Python frameworks:\n",
      "⁄\n",
      "Scikit-Learn\n",
      "  is ver y easy to use, yet it im plemen ts man y M achine Learning algoƒ\n",
      " rithms efficien tly , so it makes for a grea t en tr y poin t to learn M achine Learning.\n",
      "⁄\n",
      " T ensorFlow\n",
      "  is a more com plex librar y for distributed n umerical com puta tion. I t\n",
      " makes it possible to train and run ver y large neural networks efficien tly by disƒ\n",
      " tributing the com puta tions across poten tially h undreds of m ulti-GPU ser vers.\n",
      " T ensorFlow was crea ted a t Google and supports man y of their large-scale\n",
      " M achine Learning a pplica tions. I t was open sourced in N ovember 2015.\n",
      "⁄\n",
      " K eras\n",
      "  is a high level Deep Learning API tha t makes it ver y sim ple to train and\n",
      " run neural networks. I t can run on top of either T ensorFlow , Theano or Microƒ\n",
      " soft Cognitive T oolkit (formerly known as CNTK). T ensorFlow comes with its\n",
      " own im plemen ta tion of this API, called \n",
      " t f .k er as\n",
      ", which provides support for some\n",
      " advanced T ensorFlow fea tures (e.g., to efficien tly load da ta).\n",
      " The book fa vors a hands-on a pproach, growing an in tuitive understanding of\n",
      " M achine Learning through concrete working exam ples and just a little bit of theor y .\n",
      " While you can read this book without picking up your la ptop , we highly recommend\n",
      " xii  |  Preface\n",
      "\n",
      " you experimen t with the code exam ples a vailable online as J upyter notebooks a t\n",
      " h ttp s://g i t h u b .c o m/a ger o n/h a n d s o n-m l2\n",
      ".\n",
      "Prerequisites\n",
      " This book assumes tha t you ha ve some Python programming experience and tha t you\n",
      " are familiar with Python ‡ s main scien tific libraries, in particular \n",
      " N umPy\n",
      ", \n",
      " P andas\n",
      ", and\n",
      " M a tplotlib\n",
      ".\n",
      " Also , if you care about wha t ‡ s under the hood you should ha ve a reasonable underƒ\n",
      " standing of college-level ma th as well (calculus, linear algebra, probabilities, and staƒ\n",
      "tistics).\n",
      " If you don ‡ t know Python yet, \n",
      " h ttp://l e a r n p y t h o n.o r g/\n",
      " \n",
      " is a grea t place to start. The offiƒ\n",
      "cial tutorial on \n",
      "python.org\n",
      " is also quite good.\n",
      " If you ha ve never used J upyter , \n",
      " Cha pter 2\n",
      "  will guide you through installa tion and the\n",
      " basics: it is a grea t tool to ha ve in your toolbox.\n",
      " If you are not familiar with Python ‡ s scien tific libraries, the provided J upyter noteƒ\n",
      " books include a few tutorials. There is also a quick ma th tutorial for linear algebra.\n",
      "Roadmap\n",
      "This book is organized in two parts. \n",
      " P art I, \n",
      "•e\n",
      "  F u n d a m en t a l s o f M a c h i n e L e a r n i n g\n",
      ",\n",
      "covers the following topics:\n",
      "⁄\n",
      " Wha t is M achine Learning? Wha t problems does it tr y to solve? Wha t are the\n",
      " main ca tegories and fundamen tal concepts of M achine Learning systems?\n",
      "⁄\n",
      " The main steps in a typical M achine Learning project.\n",
      "⁄\n",
      " Learning by fitting a model to da ta.\n",
      "⁄\n",
      "Optimizing a cost function.\n",
      "⁄\n",
      " H andling, cleaning, and preparing da ta.\n",
      "⁄\n",
      " Selecting and engineering fea tures.\n",
      "⁄\n",
      " Selecting a model and tuning h yperparameters using cross-valida tion.\n",
      "⁄\n",
      " The main challenges of M achine Learning, in particular underfitting and overfitƒ\n",
      " ting (the bias/variance tradeoff ).\n",
      "⁄\n",
      " Reducing the dimensionality of the training da ta to figh t the curse of dimensionƒ\n",
      " ality .\n",
      "⁄\n",
      " Other unsuper vised learning techniques, including clustering, density estima tion\n",
      "and anomaly detection.\n",
      " Preface  |  xiii\n",
      "\n",
      "⁄\n",
      " The most common learning algorithms: Linear and P olynomial Regression,\n",
      " Logistic Regression, k-N earest N eighbors, Support V ector M achines, Decision\n",
      " T rees, R andom F orests, and Ensemble methods.\n",
      " xiv  |  Preface\n",
      "\n",
      " P art II, \n",
      " N eu r a l N e tw o r ks a n d D e e p L e a r n i n g\n",
      ", covers the following topics:\n",
      "⁄\n",
      " Wha t are neural nets? Wha t are they good for?\n",
      "⁄\n",
      " Building and training neural nets using T ensorFlow and K eras.\n",
      "⁄\n",
      " The most im portan t neural net architectures: feedfor ward neural nets, con voluƒ\n",
      " tional nets, recurren t nets, long short-term memor y (LSTM) nets, a utoencoders\n",
      " and genera tive adversarial networks (GAN s).\n",
      "⁄\n",
      " T echniques for training deep neural nets.\n",
      "⁄\n",
      " Scaling neural networks for large da tasets.\n",
      "⁄\n",
      " Learning stra tegies with Reinforcemen t Learning.\n",
      "⁄\n",
      " H andling uncertain ty with Ba yesian Deep Learning.\n",
      " The first part is based mostly on Scikit-Learn while the second part uses T ensorFlow\n",
      " and K eras.\n",
      " Don ‡ t \n",
      " jum p in to deep wa ters too hastily : while Deep Learning is no\n",
      " doubt one of the most exciting areas in M achine Learning, you\n",
      " should master the fundamen tals first. M oreover , most problems\n",
      " can be solved quite well using sim pler techniques such as R andom\n",
      " F orests and Ensemble methods (discussed in \n",
      " P art I\n",
      "). Deep Learnƒ\n",
      " ing is best suited for com plex problems such as image recognition,\n",
      " speech recognition, or na tural language processing, provided you\n",
      " ha ve enough da ta, com puting power , and pa tience.\n",
      "Other Resources\n",
      " M an y resources are a vailable to learn about M achine Learning. Andrew N g ‡ s \n",
      "ML\n",
      "course on Coursera\n",
      "  and Geoffrey Hin ton ‡ s \n",
      "course on neural networks and Deep\n",
      "Learning\n",
      "  are amazing, although they both require a significan t time in vestmen t\n",
      " (think mon ths).\n",
      " There are also man y in teresting websites about M achine Learning, including of\n",
      " course Scikit-Learn ‡ s exceptional \n",
      " U ser Guide\n",
      " . Y ou ma y also en joy \n",
      " Da taquest\n",
      ", \n",
      "which\n",
      " provides ver y nice in teractive tutorials, and ML blogs such as those listed on \n",
      "Quora\n",
      ".\n",
      " Finally , the \n",
      "Deep Learning website\n",
      " has a good list of resources to learn more.\n",
      " Of course there are also man y other in troductor y books about M achine Learning, in\n",
      " particular :\n",
      "⁄\n",
      " J oel Grus, \n",
      " D a t a S ci en c e f r o m S cr a t c h\n",
      "  (O ‡Reilly). This book presen ts the fundaƒ\n",
      " men tals of M achine Learning, and im plemen ts some of the main algorithms in\n",
      " pure Python (from scra tch, as the name suggests).\n",
      " Preface  |  xv\n",
      "\n",
      "⁄\n",
      " Stephen M arsland, \n",
      " M a c h i n e L e a r n i n g: An Al go r i t h m i c P er s p e c t i v e\n",
      " \n",
      " (Cha pman and\n",
      " H all). This book is a grea t in troduction to M achine Learning, covering a wide\n",
      " range of topics in depth, with code exam ples in Python (also from scra tch, but\n",
      " using N umPy).\n",
      "⁄\n",
      " Sebastian R aschka, \n",
      " P y t h o n M a c h i n e L e a r n i n g\n",
      "  (P ackt Publishing). Also a grea t\n",
      " in troduction to M achine Learning, this book leverages Python open source libraƒ\n",
      "ries (Pylearn 2 and Theano).\n",
      "⁄\n",
      "Fran‹ois Chollet, \n",
      " D e e p L e a r n i n g w i t h P y t h o n\n",
      "  (M anning). A ver y practical book\n",
      " tha t covers a large range of topics in a clear and concise wa y , as you migh t expect\n",
      " from the a uthor of the excellen t K eras librar y . I t fa vors code exam ples over ma thƒ\n",
      " ema tical theor y .\n",
      "⁄\n",
      " Y aser S. A bu-M ostafa, M alik M agdon-I smail, and H suan-T ien Lin, \n",
      " L e a r n i n g f r o m\n",
      " D a t a\n",
      "  (AMLB ook). A ra ther theoretical a pproach to ML, this book provides deep\n",
      " insigh ts, in particular on the bias/variance tradeoff (see \n",
      " Cha pter 4\n",
      ").\n",
      "⁄\n",
      " Stuart R ussell and P eter N or vig, \n",
      "Arti†cial\n",
      "  I n t e l l i gen c e: A M o d er n Ap p r o a c h, 3r d\n",
      " Ed i t i o n\n",
      " \n",
      " (P earson). This is a grea t (and h uge) book covering an incredible amoun t\n",
      " of topics, including M achine Learning. I t helps put ML in to perspective.\n",
      " Finally , a grea t wa y to learn is to join ML com petition websites such as \n",
      "Kaggle.com\n",
      "this will allow you to practice your skills on real-world problems, with help and\n",
      " insigh ts from some of the best ML professionals out there.\n",
      "Conventions Used in This Book\n",
      " The following typogra phical con ven tions are used in this book:\n",
      " I t a l i c\n",
      " Indica tes new terms, URLs, email addresses, filenames, and file extensions.\n",
      "Constant width\n",
      " U sed for program listings, as well as within paragra phs to refer to program eleƒ\n",
      " men ts such as variable or function names, da tabases, da ta types, en vironmen t\n",
      " variables, sta temen ts and keywords.\n",
      "Constant width bold\n",
      " Shows commands or other text tha t should be typed literally by the user .\n",
      "Constant width italic\n",
      " Shows text tha t should be replaced with user -supplied values or by values deterƒ\n",
      " mined by con text.\n",
      " xvi  |  Preface\n",
      "\n",
      " This elemen t signifies a tip or suggestion.\n",
      " This elemen t signifies a general note.\n",
      " This elemen t indica tes a warning or ca ution.\n",
      "Code Examples\n",
      " Supplemen tal ma terial (code exam ples, exercises, etc.) is a vailable for download a t\n",
      " h ttp s://g i t h u b .c o m/a ger o n/h a n d s o n-m l2\n",
      " . I t is mostly com posed of J upyter notebooks.\n",
      " Some of the code exam ples in the book lea ve out some repetitive sections, or details\n",
      " tha t are obvious or unrela ted to M achine Learning. This keeps the focus on the\n",
      " im portan t parts of the code, and it sa ves space to cover more topics. H owever , if you\n",
      " wan t the full code exam ples, they are all a vailable in the J upyter notebooks.\n",
      " N ote tha t when the code exam ples displa y some outputs, then these code exam ples\n",
      " are shown with Python prom pts (\n",
      ">>>\n",
      "   and \n",
      "...\n",
      "), as in a Python shell, to clearly distinƒ\n",
      " guish the code from the outputs. F or exam ple, this code defines the \n",
      "square()\n",
      " \n",
      "funcƒ\n",
      " tion then it com putes and displa ys the square of 3:\n",
      ">>> \n",
      "def\n",
      " \n",
      "square\n",
      "(\n",
      "x\n",
      "):\n",
      "... \n",
      "    \n",
      "return\n",
      " \n",
      "x\n",
      " \n",
      "**\n",
      " \n",
      "2\n",
      "...\n",
      ">>> \n",
      "result\n",
      " \n",
      "=\n",
      " \n",
      "square\n",
      "(\n",
      "3\n",
      ")\n",
      ">>> \n",
      "result\n",
      "9\n",
      " When code does not displa y an ything, prom pts are not used. H owever , the result ma y\n",
      " sometimes be shown as a commen t like this:\n",
      "def\n",
      " \n",
      "square\n",
      "(\n",
      "x\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "x\n",
      " \n",
      "**\n",
      " \n",
      "2\n",
      "result\n",
      " \n",
      "=\n",
      " \n",
      "square\n",
      "(\n",
      "3\n",
      ")\n",
      "  \n",
      "# result is 9\n",
      " Preface  |  xvii\n",
      "\n",
      "Using Code Examples\n",
      " This book is here to help you get your job done. In general, if exam ple code is offered\n",
      " with this book, you ma y use it in your programs and documen ta tion. Y ou do not\n",
      " need to con tact us for permission unless you ‡ re reproducing a significan t portion of\n",
      " the code. F or exam ple, writing a program tha t uses several ch unks of code from this\n",
      " book does not require permission. Selling or distributing a CD-ROM of exam ples\n",
      " from O ‡Reilly books does require permission. Answering a question by citing this\n",
      " book and quoting exam ple code does not require permission. Incorpora ting a signifiƒ\n",
      " can t amoun t of exam ple code from this book in to your product ‡ s documen ta tion does\n",
      "require permission.\n",
      " W e a pprecia te, but do not require, a ttribution. An a ttribution usually includes the\n",
      " title, a uthor , publisher , and ISBN. F or exam ple: —\n",
      " H a n d s-O n M a c h i n e L e a r n i n g w i t h\n",
      " S ci k i t-L e a r n, K er as a n d T ens o rF l o w\n",
      "  by A ur•lien G•ron (O ‡Reilly). Copyrigh t 2019\n",
      " A ur•lien G•ron, 978-1-492-03264-9. – If you feel your use of code exam ples falls outƒ\n",
      " side fair use or the permission given above, feel free to con tact us a t \n",
      " p er m i s‡\n",
      " s i o ns@o r ei l l y .c o m\n",
      ".\n",
      "O†Reilly Safari\n",
      " S a f a r i\n",
      "  (formerly Safari B ooks Online) is a membership-based\n",
      " training and reference pla tform for en terprise, governmen t,\n",
      " educa tors, and individuals.\n",
      " M embers ha ve access to thousands of books, training videos, Learning P a ths, in teracƒ\n",
      " tive tutorials, and cura ted pla ylists from over 250 publishers, including O ‡Reilly\n",
      " M edia, H ar vard Business Review , Pren tice H all Professional, A ddison-W esley Profesƒ\n",
      " sional, Microsoft Press, Sams, Que, P each pit Press, A dobe, F ocal Press, Cisco Press,\n",
      " J ohn W iley & Sons, Syngress, M organ Ka ufmann, IBM Redbooks, P ackt, A dobe\n",
      " Press, FT Press, A press, M anning, N ew Riders, M cGra w-Hill, J ones & Bartlett, and\n",
      " Course T echnolog y , among others.\n",
      " F or more informa tion, please visit \n",
      " h ttp://o r ei l l y .c o m/s a f a r i\n",
      ".\n",
      "How to Contact Us\n",
      " Please address commen ts and questions concerning this book to the publisher :\n",
      " O ‡Reilly M edia, Inc.\n",
      " 1005 Gra venstein High wa y N orth\n",
      "Sebastopol, CA 95472\n",
      " 800-998-9938 (in the U nited Sta tes or Canada)\n",
      " xviii  |  Preface\n",
      "\n",
      " 707-829-0515 (in terna tional or local)\n",
      "707-829-0104 (fax)\n",
      " W e ha ve a web page for this book, where we list erra ta, exam ples, and an y additional\n",
      " informa tion. Y ou can access this page a t \n",
      "http://bit.ly/hands-on-machine-learning-\n",
      "with-scikit-learn-and-tensor…ow\n",
      " or \n",
      " h ttp s://h o m l.i n f o/o r ei l l y\n",
      ".\n",
      " T o commen t or ask technical questions about this book, send email to \n",
      " b o o k q u e s‡\n",
      " t i o ns@o r ei l l y .c o m\n",
      ".\n",
      " F or more informa tion about our books, courses, conferences, and news, see our webƒ\n",
      " site a t \n",
      " h ttp://w w w .o r ei l l y .c o m\n",
      ".\n",
      " Find us on F acebook: \n",
      " h ttp://f a c e b o o k.c o m/o r ei l l y\n",
      " F ollow us on T witter : \n",
      " h ttp://tw i tt er .c o m/o r ei l l y m e d i a\n",
      " W a tch us on Y ouT ube: \n",
      " h ttp://w w w .y o u t u b e.c o m/o r ei l l y m e d i a\n",
      "Changes in the Second Edition\n",
      "This second edition has five main objectives:\n",
      "1.\n",
      " Cover additional topics: additional unsuper vised learning techniques (including\n",
      " clustering, anomaly detection, density estima tion and mixture models), addiƒ\n",
      "tional techniques for training deep nets (including self-normalized networks),\n",
      " additional com puter vision techniques (including the Xception, SEN et, object\n",
      " detection with Y OLO , and seman tic segmen ta tion using R -CNN), handling\n",
      " sequences using CNN s (including W a veN et), na tural language processing using\n",
      " RNN s, CNN s and T ransformers, genera tive adversarial networks, deploying T enƒ\n",
      "sorFlow models, and more.\n",
      "2.\n",
      " U pda te the book to men tion some of the la test results from Deep Learning\n",
      "research.\n",
      "3.\n",
      " Migra te all T ensorFlow cha pters to T ensorFlow 2, and use T ensorFlow‡ s im pleƒ\n",
      " men ta tion of the K eras API (called tf.keras) whenever possible, to sim plif y the\n",
      " code exam ples.\n",
      "4.\n",
      " U pda te the code exam ples to use the la test version of Scikit-Learn, N umPy , P anƒ\n",
      " das, M a tplotlib and other libraries.\n",
      "5.\n",
      " Clarif y some sections and fix some errors, thanks to plen ty of grea t feedback\n",
      "from readers.\n",
      " Some cha pters were added, others were rewritten and a few were reordered. \n",
      " T able P -1\n",
      " shows the ma pping between the 1\n",
      "st\n",
      "  edition cha pters and the 2\n",
      "nd\n",
      "  edition cha pters:\n",
      " Preface  |  xix\n",
      "\n",
      " T a b l e P -1. Ch a p t er m a p p i n g b e tw e en 1\n",
      " s t\n",
      "  a n d 2\n",
      " n d\n",
      "  e d i t i o n\n",
      "1\n",
      "st\n",
      "   Ed.   chapter\n",
      "2\n",
      "nd\n",
      "   Ed.   Chapter\n",
      " %   Changes\n",
      "2\n",
      "nd\n",
      "   Ed.   T itle\n",
      "1\n",
      "1\n",
      "<10%\n",
      " The   Machine   Learning   Landscape\n",
      "2\n",
      "2\n",
      "<10%\n",
      " End-to-End   Machine   Learning   Project\n",
      "3\n",
      "3\n",
      "<10%\n",
      "Classi•cation\n",
      "4\n",
      "4\n",
      "<10%\n",
      " Training   Models\n",
      "5\n",
      "5\n",
      "<10%\n",
      " Support   Vector   Machines\n",
      "6\n",
      "6\n",
      "<10%\n",
      " Decision   Trees\n",
      "7\n",
      "7\n",
      "<10%\n",
      " Ensemble   Learning   and   Random   Forests\n",
      "8\n",
      "8\n",
      "<10%\n",
      " Dimensionality   Reduction\n",
      "N/A\n",
      "9\n",
      " 100%   new\n",
      " Unsupervised   Learning   Techniques\n",
      "10\n",
      "10\n",
      "~75%\n",
      " Introduction   to  \n",
      "Arti•cial\n",
      "   Neural   Networks   with   Keras\n",
      "11\n",
      "11\n",
      "~50%\n",
      " Training   Deep   Neural   Networks\n",
      "9\n",
      "12\n",
      " 100%   rewritten\n",
      " Custom   Models   and   Training   with   TensorFlow\n",
      " Part   of   12\n",
      "13\n",
      " 100%   rewritten\n",
      " Loading   and   Preprocessing   Data   with   TensorFlow\n",
      "13\n",
      "14\n",
      "~50%\n",
      " Deep   Computer   Vision   Using   Convolutional   Neural   Networks\n",
      " Part   of   14\n",
      "15\n",
      "~75%\n",
      " Processing   Sequences   Using   RNNs   and   CNNs\n",
      " Part   of   14\n",
      "16\n",
      "~90%\n",
      " Natural   Language   Processing   with   RNNs   and   Attention\n",
      "15\n",
      "17\n",
      "~75%\n",
      " Autoencoders   and   GANs\n",
      "16\n",
      "18\n",
      "~75%\n",
      " Reinforcement   Learning\n",
      " Part   of   12\n",
      "19\n",
      " 100%   rewritten\n",
      " Deploying   your   TensorFlow   Models\n",
      " M ore specifically , here are the main changes for each 2\n",
      "nd\n",
      "  edition cha pter (other than\n",
      " clarifica tions, corrections and code upda tes):\n",
      "⁄\n",
      " Cha pter 1\n",
      "›\n",
      " A dded a section on handling misma tch between the training set and the valiƒ\n",
      " da tion & test sets.\n",
      "⁄\n",
      " Cha pter 2\n",
      "›\n",
      " A dded how to com pute a confidence in ter val.\n",
      "›\n",
      " Im proved the installa tion instructions (e.g., for W indows).\n",
      "›\n",
      " In troduced the upgraded \n",
      "OneHotEncoder\n",
      " and the new \n",
      "ColumnTransformer\n",
      ".\n",
      "⁄\n",
      " Cha pter 4\n",
      "›\n",
      " Explained the need for training instances to be Independen t and I den tically\n",
      "Distributed (IID).\n",
      "⁄\n",
      " Cha pter 7\n",
      "›\n",
      " A dded a short section about X GB oost.\n",
      " xx  |  Preface\n",
      "\n",
      "⁄\n",
      " Cha pter 9 − new cha pter including:\n",
      "›\n",
      " Clustering with K-M eans, how to choose the n umber of clusters, how to use it\n",
      " for dimensionality reduction, semi-super vised learning, image segmen ta tion,\n",
      "and more.\n",
      "›\n",
      " The DBSCAN clustering algorithm and an over view of other clustering algoƒ\n",
      " rithms a vailable in Scikit-Learn.\n",
      "›\n",
      " Ga ussian mixture models, the Expecta tion-M aximiza tion (EM) algorithm,\n",
      " Ba yesian varia tional inference, and how mixture models can be used for clusƒ\n",
      " tering, density estima tion, anomaly detection and novelty detection.\n",
      "›\n",
      " O ver view of other anomaly detection and novelty detection algorithms.\n",
      "⁄\n",
      " Cha pter 10 (mostly new)\n",
      "›\n",
      " A dded an in troduction to the K eras API, including all its API s (Sequen tial,\n",
      " F unctional and Subclassing), persistence and callbacks (including the \n",
      "Tensor\n",
      "Board\n",
      " callback).\n",
      "⁄\n",
      " Cha pter 11 (man y changes)\n",
      "›\n",
      " In troduced self-normalizing nets, the SEL U activa tion function and Alpha\n",
      "Dropout.\n",
      "›\n",
      " In troduced self-super vised learning.\n",
      "›\n",
      " A dded N adam optimiza tion.\n",
      "›\n",
      " A dded M on te-Carlo Dropout.\n",
      "›\n",
      " A dded a note about the risks of ada ptive optimiza tion methods.\n",
      "›\n",
      " U pda ted the practical guidelines.\n",
      "⁄\n",
      " Cha pter 12 − com pletely rewritten cha pter , including:\n",
      "›\n",
      " A tour of T ensorFlow 2\n",
      "›\n",
      " T ensorFlow‡ s lower -level Python API\n",
      "›\n",
      " W riting custom loss functions, metrics, la yers, models\n",
      "›\n",
      " U sing a uto-differen tia tion and crea ting custom training algorithms.\n",
      "›\n",
      " T ensorFlow F unctions and gra phs (including tracing and a utogra ph).\n",
      "⁄\n",
      " Cha pter 13 − new cha pter , including:\n",
      "›\n",
      " The Da ta API\n",
      "›\n",
      " Loading/Storing da ta efficien tly using TFRecords\n",
      "›\n",
      " The F ea tures API (including an in troduction to embeddings).\n",
      "›\n",
      " An over view of TF T ransform and TF Da tasets\n",
      "›\n",
      " M oved the low-level im plemen ta tion of the neural network to the exercises.\n",
      " Preface  |  xxi\n",
      "\n",
      "›\n",
      " Removed details about queues and readers tha t are now superseded by the\n",
      " Da ta API.\n",
      "⁄\n",
      " Cha pter 14\n",
      "›\n",
      " A dded Xception and SEN et architectures.\n",
      "›\n",
      " A dded a K eras im plemen ta tion of ResN et-34.\n",
      "›\n",
      " Showed how to use pretrained models using K eras.\n",
      "›\n",
      " A dded an end-to-end transfer learning exam ple.\n",
      "›\n",
      " A dded classifica tion and localiza tion.\n",
      "›\n",
      " In troduced F ully Con volutional N etworks (FCN s).\n",
      "›\n",
      " In troduced object detection using the Y OLO architecture.\n",
      "›\n",
      " In troduced seman tic segmen ta tion using R -CNN.\n",
      "⁄\n",
      " Cha pter 15\n",
      "›\n",
      " A dded an in troduction to W a venet.\n",
      "›\n",
      " M oved the Encoder −Decoder architecture and Bidirectional RNN s to Cha pter\n",
      "16.\n",
      "⁄\n",
      " Cha pter 16\n",
      "›\n",
      " Explained how to use the Da ta API to handle sequen tial da ta.\n",
      "›\n",
      " Showed an end-to-end exam ple of text genera tion using a Character RNN,\n",
      " using both a sta teless and a sta teful RNN.\n",
      "›\n",
      " Showed an end-to-end exam ple of sen timen t analysis using an LSTM.\n",
      "›\n",
      " Explained masking in K eras.\n",
      "›\n",
      " Showed how to reuse pretrained embeddings using TF H ub .\n",
      "›\n",
      " Showed how to build an Encoder −Decoder for N eural M achine T ransla tion\n",
      " using T ensorFlow A ddons/seq2seq.\n",
      "›\n",
      " In troduced beam search.\n",
      "›\n",
      " Explained a tten tion mechanisms.\n",
      "›\n",
      " A dded a short over view of visual a tten tion and a note on explainability .\n",
      "›\n",
      " In troduced the fully a tten tion-based T ransformer architecture, including posiƒ\n",
      " tional embeddings and m ulti-head a tten tion.\n",
      "›\n",
      " A dded an over view of recen t language models (2018).\n",
      "⁄\n",
      " Cha pters 17, 18 and 19: coming soon.\n",
      " xxii  |  Preface\n",
      "\n",
      "3\n",
      " —Deep Learning with Python, – Fran‹ois Chollet (2017).\n",
      "Acknowledgments\n",
      " N ever in m y wildest dreams did I imagine tha t the first edition of this book would get\n",
      " such a large a udience. I received so man y messages from readers, man y asking quesƒ\n",
      " tions, some kindly poin ting out erra ta, and most sending me encouraging words. I\n",
      " cannot express how gra teful I am to all these readers for their tremendous support.\n",
      " Thank you all so ver y m uch! Please do not hesita te to \n",
      " file issues on gith ub\n",
      " \n",
      "if you find\n",
      " errors in the code exam ples (or just to ask questions), or to submit \n",
      " erra ta\n",
      " \n",
      "if you find\n",
      "errors in the text. Some readers also shared how this book helped them get their first\n",
      " job , or how it helped them solve a concrete problem they were working on: I find\n",
      " such feedback incredibly motiva ting. If you find this book helpful, I would love it if\n",
      " you could share your stor y with me, either priva tely (e.g., via \n",
      "LinkedIn\n",
      ") or publicly\n",
      "(e.g., in an \n",
      "Amazon review\n",
      ").\n",
      "I am also incredibly thankful to all the amazing people who took time out of their\n",
      " busy lives to review m y book with such care. In particular , I would like to thank Franƒ\n",
      " ‹ois Chollet for reviewing all the cha pters based on K eras & T ensorFlow , and giving\n",
      " me some grea t, in-depth feedback. Since K eras is one of the main additions to this 2\n",
      "nd\n",
      " edition, ha ving its a uthor review the book was in valuable. I highly recommend Franƒ\n",
      " ‹ois ‡ s excellen t book \n",
      "Deep Learning with Python\n",
      "3\n",
      ": it has the conciseness, clarity and\n",
      " depth of the K eras librar y itself. Big thanks as well to Ankur P a tel, who reviewed\n",
      " ever y cha pter of this 2\n",
      "nd\n",
      "  edition and ga ve me excellen t feedback.\n",
      " This book also benefited from plen ty of help from members of the T ensorFlow team,\n",
      " in particular M artin W icke, who tirelessly answered dozens of m y questions and disƒ\n",
      " pa tched the rest to the righ t people, including Alexandre P assos, Allen La voie, Andr•\n",
      " Susano Pin to , Anna Revinska ya, An thon y Pla tanios, Clemens M ewald, Dan M oldoƒ\n",
      " van, Daniel Dobson, Dustin T ran, Edd W ilder -J ames, Goldie Gadde, J iri Simsa, Karƒ\n",
      " mel Allison, N ick F elt, P aige Bailey , P ete W arden (who also reviewed the 1\n",
      "st\n",
      " \n",
      "edition),\n",
      " R yan Sepassi, Sandeep Gupta, Sean M organ, T odd W ang, T om O ‡M alley , W illiam\n",
      " Chargin, and Y uefeng Zhou, all of whom were tremendously helpful. A h uge thank\n",
      " you to all of you, and to all other members of the T ensorFlow team. N ot just for your\n",
      " help , but also for making such a grea t librar y .\n",
      " Big thanks to H aesun P ark, who ga ve me plen ty of excellen t feedback and ca ugh t sevƒ\n",
      " eral errors while he was writing the K orean transla tion of the 1\n",
      "st\n",
      " \n",
      "edition of this book.\n",
      " H e also transla ted the J upyter notebooks to K orean, not to men tion T ensorFlow‡ s\n",
      " documen ta tion. I do not speak K orean, but judging by the quality of his feedback, all\n",
      " his transla tions m ust be truly excellen t! M oreover , he kindly con tributed some of the\n",
      "solutions to the exercises in this book.\n",
      " Preface  |  xxiii\n",
      "\n",
      " M an y thanks as well to O ‡Reilly‡ s fan tastic staff, in particular N icole T ache, who ga ve\n",
      " me insigh tful feedback, alwa ys cheerful, encouraging, and helpful: I could not dream\n",
      " of a better editor . Big thanks to Michele Cronin as well, who was ver y helpful (and\n",
      " pa tien t) a t the start of this 2\n",
      "nd\n",
      " \n",
      " edition. Thanks to M arie B ea ugurea u, B en Lorica, Mike\n",
      " Loukides, and La urel R uma for believing in this project and helping me define its\n",
      " scope. Thanks to M a tt H acker and all of the A tlas team for answering all m y technical\n",
      " questions regarding forma tting, asciidoc, and LaT eX, and thanks to R achel M onaƒ\n",
      " ghan, N ick A dams, and all of the production team for their final review and their\n",
      " h undreds of corrections.\n",
      " I would also like to thank m y former Google colleagues, in particular the Y ouT ube\n",
      " video classifica tion team, for teaching me so m uch about M achine Learning. I could\n",
      " never ha ve started the first edition without them. Special thanks to m y personal ML\n",
      " gurus: Cl•men t Courbet, J ulien Dubois, M a thias K ende, Daniel Kitachewsky , J ames\n",
      " P ack, Alexander P ak, Anosh R a j, V itor Sessak, W iktor T omczak, Ingrid von Glehn,\n",
      " Rich W ashington, and ever yone I worked with a t Y ouT ube and in the amazing Gooƒ\n",
      " gle research teams in M oun tain V iew . All these people are just as nice and helpful as\n",
      " they are brigh t, and tha t ‡ s sa ying a lot.\n",
      "I will never forget the kind people who reviewed the 1\n",
      "st\n",
      "   edition of this book, including\n",
      " Da vid Andrzejewski, Eddy H ung, Gr•goire M esnil, I ain Smears, Ingrid von Glehn,\n",
      " J ustin Francis, Karim M a trah, L ukas Biewald, Michel T essier , Salim S•maoune, V inƒ\n",
      " cen t Guilbea u and of course m y dear brother Sylvain.\n",
      " Last but not least, I am infinitely gra teful to m y beloved wife, Emman uelle, and to our\n",
      "three wonderful children, Alexandre, R•mi, and Gabrielle, for encouraging me to\n",
      " work hard on this book, as well as for their insa tiable curiosity : explaining some of\n",
      " the most difficult concepts in this book to m y wife and children helped me clarif y m y\n",
      " though ts and directly im proved man y parts of this book. Plus, they keep bringing me\n",
      " cookies and coffee! Wha t more can one dream of ?\n",
      " xxiv  |  Preface\n",
      "\n",
      "PART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "\n",
      "\n",
      "CHAPTER 1\n",
      "The Machine Learning Landscape\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 1 in the final\n",
      "release of the book.\n",
      " When most people hear — M achine Learning, – they picture a robot: a dependable butƒ\n",
      " ler or a deadly T ermina tor depending on who you ask. But M achine Learning is not\n",
      " just a futuristic fan tasy , it ‡ s already here. In fact, it has been around for decades in\n",
      " some specialized a pplica tions, such as \n",
      " O p t i c a l Ch a r a c t er R e c og n i t i o n\n",
      "  (O CR). \n",
      "But the\n",
      " first ML a pplica tion tha t really became mainstream, im proving the lives of h undreds\n",
      "of millions of people, took over the world back in the 1990s: it was the \n",
      " s p a m \n",
      "†lter\n",
      ".\n",
      " N ot exactly a self-a ware Skynet, but it does technically qualif y as M achine Learning\n",
      " (it has actually learned so well tha t you seldom need to flag an email as spam an yƒ\n",
      " more). I t was followed by h undreds of ML a pplica tions tha t now quietly power h unƒ\n",
      " dreds of products and fea tures tha t you use regularly , from better recommenda tions\n",
      "to voice search.\n",
      " Where does M achine Learning start and where does it end? Wha t exactly does it\n",
      "mean for a machine to \n",
      " l e a r n\n",
      "  something? If I download a copy of W ikipedia, has m y\n",
      " com puter really —learned – something? I s it suddenly smarter? In this cha pter we will\n",
      " start by clarif ying wha t M achine Learning is and wh y you ma y wan t to use it.\n",
      " Then, before we set out to explore the M achine Learning con tinen t, we will take a\n",
      " look a t the ma p and learn about the main regions and the most notable landmarks:\n",
      " super vised versus unsuper vised learning, online versus ba tch learning, instance-\n",
      " based versus model-based learning. Then we will look a t the workflow of a typical ML\n",
      " project, discuss the main challenges you ma y face, and cover how to evalua te and\n",
      " fine-tune a M achine Learning system.\n",
      "3\n",
      "\n",
      " This cha pter in troduces a lot of fundamen tal concepts (and jargon) tha t ever y da ta\n",
      " scien tist should know by heart. I t will be a high-level over view (the only cha pter\n",
      " without m uch code), all ra ther sim ple, but you should make sure ever ything is\n",
      " cr ystal-clear to you before con tin uing to the rest of the book. So grab a coffee and let ‡ s\n",
      "get started!\n",
      " If you already know all the M achine Learning basics, you ma y wan t\n",
      "to skip directly to \n",
      " Cha pter 2\n",
      " . If you are not sure, tr y to answer all\n",
      " the questions listed a t the end of the cha pter before moving on.\n",
      "What Is Machine Learning?\n",
      " M achine Learning is the science (and art) of programming com puters so they can\n",
      " l e a r n f r o m d a t a\n",
      ".\n",
      " H ere is a sligh tly more general definition:\n",
      " [M achine Learning is the] field of study tha t gives com puters the ability to learn\n",
      "without being explicitly programmed.\n",
      " ›Arth ur Sam uel, \n",
      "1959\n",
      " And a more engineering-orien ted one:\n",
      " A com puter program is said to learn from experience E with respect to some task T\n",
      " and some performance measure P , if its performance on T , as measured by P , im proves\n",
      "with experience E.\n",
      " ›T om Mitchell, \n",
      "1997\n",
      " F or exam ple, your spam filter is a M achine Learning program tha t can learn to flag\n",
      " spam given exam ples of spam emails (e.g., flagged by users) and exam ples of regular\n",
      " (nonspam, also called —ham –) emails. The exam ples tha t the system uses to learn are\n",
      "called the \n",
      " t r a i n i n g s e t\n",
      " . Each training exam ple is called a \n",
      " t r a i n i n g i ns t a n c e\n",
      " \n",
      "(or \n",
      " s a m p l e\n",
      ").\n",
      "In this case, the task T is to flag spam for new emails, the experience E is the \n",
      " t r a i n i n g\n",
      " d a t a\n",
      " , and the performance measure P needs to be defined; for exam ple, you can use\n",
      " the ra tio of correctly classified emails. This particular performance measure is called\n",
      " a c cu r a c y\n",
      "  and it is often used in classifica tion tasks.\n",
      " If you just download a copy of W ikipedia, your com puter has a lot more da ta, but it is\n",
      " not suddenly better a t an y task. Th us, it is not M achine Learning.\n",
      "Why Use Machine Learning?\n",
      "Consider how you would write a spam filter using traditional programming techniƒ\n",
      "ques (\n",
      "Figure 1-1\n",
      "):\n",
      " 4  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "1.\n",
      " First you would look a t wha t spam typically looks like. Y ou migh t notice tha t\n",
      " some words or phrases (such as —4U , – — credit card, – — free, – and — amazing –) tend to\n",
      " come up a lot in the subject. P erha ps you would also notice a few other pa tterns\n",
      " in the sender‡ s name, the email ‡ s body , and so on.\n",
      "2.\n",
      " Y ou would write a detection algorithm for each of the pa tterns tha t you noticed,\n",
      " and your program would flag emails as spam if a n umber of these pa tterns are\n",
      "detected.\n",
      "3.\n",
      " Y ou would test your program, and repea t steps 1 and 2 un til it is good enough.\n",
      " F i g u r e 1-1. \n",
      "•e\n",
      "  t r a d i t i o n a l a p p r o a c h\n",
      "Since the problem is not trivial, your program will likely become a long list of comƒ\n",
      " plex rules›pretty hard to main tain.\n",
      " In con trast, a spam filter based on M achine Learning techniques a utoma tically learns\n",
      " which words and phrases are good predictors of spam by detecting un usually freƒ\n",
      " quen t pa tterns of words in the spam exam ples com pared to the ham exam ples\n",
      "(\n",
      "Figure 1-2\n",
      " ). The program is m uch shorter , easier to main tain, and most likely more\n",
      " accura te.\n",
      " Why Use Machine Learning?  |  5\n",
      "\n",
      " F i g u r e 1-2. M a c h i n e L e a r n i n g a p p r o a c h\n",
      " M oreover , if spammers notice tha t all their emails con taining —4U– are blocked, they\n",
      " migh t start writing —F or U– instead. A spam filter using traditional programming\n",
      " techniques would need to be upda ted to flag —F or U– emails. If spammers keep workƒ\n",
      " ing around your spam filter , you will need to keep writing new rules forever .\n",
      " In con trast, a spam filter based on M achine Learning techniques a utoma tically notiƒ\n",
      " ces tha t —F or U– has become un usually frequen t in spam flagged by users, and it starts\n",
      " flagging them without your in ter ven tion (\n",
      "Figure 1-3\n",
      ").\n",
      " F i g u r e 1-3. Au t o m a t i c a l l y a d a p t i n g t o c h a n ge\n",
      "Another \n",
      " area where M achine Learning shines is for problems tha t either are too comƒ\n",
      " plex for traditional a pproaches or ha ve no known algorithm. F or exam ple, consider \n",
      " speech recognition: sa y you wan t to start sim ple and write a program ca pable of disƒ\n",
      " tinguishing the words — one – and — two . – Y ou migh t notice tha t the word — two – starts\n",
      " with a high-pitch sound (— T –), so you could hardcode an algorithm tha t measures\n",
      " high-pitch sound in tensity and use tha t to distinguish ones and twos. Obviously this\n",
      " technique will not scale to thousands of words spoken by millions of ver y differen t\n",
      " 6  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " people in noisy en vironmen ts and in dozens of languages. The best solution (a t least\n",
      " toda y) is to write an algorithm tha t learns by itself, given man y exam ple recordings\n",
      "for each word.\n",
      " Finally , M achine Learning can help h umans learn (\n",
      "Figure 1-4\n",
      "): ML algorithms can be\n",
      " inspected to see wha t they ha ve learned (although for some algorithms this can be\n",
      " tricky). F or instance, once the spam filter has been trained on enough spam, it can\n",
      " easily be inspected to reveal the list of words and combina tions of words tha t it\n",
      "believes are the best predictors of spam. Sometimes this will reveal unsuspected corƒ\n",
      " rela tions or new trends, and thereby lead to a better understanding of the problem.\n",
      " A pplying ML techniques to dig in to large amoun ts of da ta can help discover pa tterns\n",
      " tha t were not immedia tely a pparen t. This is called \n",
      " d a t a m i n i n g\n",
      ".\n",
      " F i g u r e 1-4. M a c h i n e L e a r n i n g c a n h e l p h u m a ns l e a r n\n",
      " T o summarize, M achine Learning is grea t for :\n",
      "⁄\n",
      "Problems for which existing solutions require a lot of hand-tuning or long lists of\n",
      " rules: one M achine Learning algorithm can often sim plif y code and perform betƒ\n",
      " ter .\n",
      "⁄\n",
      " Com plex problems for which there is no good solution a t all using a traditional\n",
      " a pproach: the best M achine Learning techniques can find a solution.\n",
      "⁄\n",
      " Fluctua ting en vironmen ts: a M achine Learning system can ada pt to new da ta.\n",
      "⁄\n",
      " Getting insigh ts about com plex problems and large amoun ts of da ta.\n",
      " Why Use Machine Learning?  |  7\n",
      "\n",
      "Types of Machine Learning Systems\n",
      " There are so man y differen t types of M achine Learning systems tha t it is useful to\n",
      " classif y them in broad ca tegories based on:\n",
      "⁄\n",
      " Whether or not they are trained with h uman super vision (super vised, unsuperƒ\n",
      " vised, semisuper vised, and Reinforcemen t Learning)\n",
      "⁄\n",
      " Whether or not they can learn incremen tally on the fly (online versus ba tch\n",
      "learning)\n",
      "⁄\n",
      " Whether they work by sim ply com paring new da ta poin ts to known da ta poin ts,\n",
      " or instead detect pa tterns in the training da ta and build a predictive model, m uch\n",
      " like scien tists do (instance-based versus model-based learning)\n",
      " These criteria are not exclusive; you can combine them in an y wa y you like. F or\n",
      " exam ple, a sta te-of-the-art spam filter ma y learn on the fly using a deep neural netƒ\n",
      " work model trained using exam ples of spam and ham; this makes it an online, model-\n",
      " based, super vised learning system.\n",
      " Let ‡ s look a t each of these criteria a bit more closely .\n",
      "Supervised/Unsupervised Learning\n",
      " M achine Learning systems can be classified according to the amoun t and type of\n",
      " super vision they get during training. There are four ma jor ca tegories: super vised\n",
      " learning, unsuper vised learning, semisuper vised learning, and Reinforcemen t Learnƒ\n",
      "ing.\n",
      "Supervised learning\n",
      "In \n",
      " s u p er v i s e d l e a r n i n g\n",
      " , the training da ta you feed to the algorithm includes the desired\n",
      "solutions, called \n",
      " l a b e l s\n",
      " (\n",
      "Figure 1-5\n",
      ").\n",
      " F i g u r e 1-5. A l a b e l e d t r a i n i n g s e t f o r s u p er v i s e d l e a r n i n g (e.g ., s p a m \n",
      "classi†cation)\n",
      " 8  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "1\n",
      " F un fact: this odd-sounding name is a sta tistics term in troduced by Francis Galton while he was studying the\n",
      " fact tha t the children of tall people tend to be shorter than their paren ts. Since children were shorter , he called\n",
      "this \n",
      " r e g r e s s i o n t o t h e m e a n\n",
      " . This name was then a pplied to the methods he used to analyze correla tions\n",
      "between variables.\n",
      " A typical super vised learning task is \n",
      "classi†cation\n",
      " . The spam filter is a good exam ple\n",
      " of this: it is trained with man y exam ple emails along with their \n",
      " c l as s\n",
      " (spam or ham),\n",
      " and it m ust learn how to classif y new emails.\n",
      "Another typical task is to predict a \n",
      " t a r ge t\n",
      "  n umeric value, such as the price of a car ,\n",
      "given a set of \n",
      " f e a t u r e s\n",
      " (mileage, age, brand, etc.) called \n",
      " p r e d i c t o r s\n",
      ". This sort of task is \n",
      "called \n",
      " r e g r e s s i o n\n",
      "   (\n",
      "Figure 1-6\n",
      ").\n",
      "1\n",
      " \n",
      " T o train the system, you need to give it man y exam ples\n",
      "of cars, including both their predictors and their labels (i.e., their prices).\n",
      " In M achine Learning an \n",
      " a tt r i b u t e\n",
      "  is a da ta type (e.g., — Mileage –),\n",
      "while a \n",
      " f e a t u r e\n",
      "  has several meanings depending on the con text, but\n",
      " generally means an a ttribute plus its value (e.g., — Mileage =\n",
      " 15,000–). M an y people use the words \n",
      " a tt r i b u t e\n",
      " and \n",
      " f e a t u r e\n",
      " \n",
      " in terƒ\n",
      " changeably , though.\n",
      " F i g u r e 1-6. R e g r e s s i o n\n",
      " N ote tha t some regression algorithms can be used for classifica tion as well, and vice\n",
      " versa. F or exam ple, \n",
      " L og i s t i c R e g r e s s i o n\n",
      "  is commonly used for classifica tion, as it can\n",
      " output a value tha t corresponds to the probability of belonging to a given class (e.g.,\n",
      "20% chance of being spam).\n",
      " Types of Machine Learning Systems  |  9\n",
      "\n",
      "2\n",
      " Some neural network architectures can be unsuper vised, such as a utoencoders and restricted B oltzmann\n",
      " machines. They can also be semisuper vised, such as in deep belief networks and unsuper vised pretraining.\n",
      " H ere are some of the most im portan t super vised learning algorithms (covered in this\n",
      "book):\n",
      "⁄\n",
      " k-N earest N eighbors\n",
      "⁄\n",
      "Linear Regression\n",
      "⁄\n",
      "Logistic Regression\n",
      "⁄\n",
      " Support V ector M achines (SVMs)\n",
      "⁄\n",
      " Decision T rees and R andom F orests\n",
      "⁄\n",
      " N eural networks\n",
      "2\n",
      "Unsupervised learning\n",
      "In \n",
      " u ns u p er v i s e d l e a r n i n g\n",
      " , as you migh t guess, the training da ta is unlabeled\n",
      "(\n",
      "Figure 1-7\n",
      " ). The system tries to learn without a teacher .\n",
      " F i g u r e 1-7. An u n l a b e l e d t r a i n i n g s e t f o r u ns u p er v i s e d l e a r n i n g\n",
      " H ere are some of the most im portan t unsuper vised learning algorithms (most of\n",
      "these are covered in \n",
      " Cha pter 8\n",
      " and \n",
      " Cha pter 9\n",
      "):\n",
      "⁄\n",
      "Clustering\n",
      "›\n",
      " K-M eans\n",
      "›\n",
      "DBSCAN\n",
      "›\n",
      "Hierarchical Cluster Analysis (HCA)\n",
      "⁄\n",
      "Anomaly detection and novelty detection\n",
      "›\n",
      "One-class SVM\n",
      "›\n",
      " I sola tion F orest\n",
      " 10  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "⁄\n",
      " V isualiza tion and dimensionality reduction\n",
      "›\n",
      " Principal Com ponen t Analysis (PCA)\n",
      "›\n",
      " K ernel PCA\n",
      "›\n",
      "Locally-Linear Embedding (LLE)\n",
      "›\n",
      " t-distributed Stochastic N eighbor Embedding (t-SNE)\n",
      "⁄\n",
      " Associa tion rule learning\n",
      "›\n",
      " A priori\n",
      "›\n",
      " Ecla t\n",
      " F or exam ple, sa y you ha ve a lot of da ta about your blog ‡ s visitors. Y ou ma y wan t to\n",
      "run a \n",
      " c l u s t er i n g\n",
      "  algorithm to tr y to detect groups of similar visitors (\n",
      "Figure 1-8\n",
      " ). A t\n",
      " no poin t do you tell the algorithm which group a visitor belongs to: it finds those\n",
      " connections without your help . F or exam ple, it migh t notice tha t 40% of your visitors\n",
      "are males who love comic books and generally read your blog in the evening, while\n",
      "20% are young sci-fi lovers who visit during the weekends, and so on. If you use \n",
      "a\n",
      " h i er a r c h i c a l c l u s t er i n g\n",
      "  algorithm, it ma y also subdivide each group in to smaller\n",
      " groups. This ma y help you target your posts for each group .\n",
      " F i g u r e 1-8. Cl u s t er i n g\n",
      " V i s u a l iz a t i o n\n",
      "   algorithms \n",
      " are also good exam ples of unsuper vised learning algorithms:\n",
      " you feed them a lot of com plex and unlabeled da ta, and they output a 2D or 3D repƒ\n",
      " resen ta tion of your da ta tha t can easily be plotted (\n",
      "Figure 1-9\n",
      " ). These algorithms tr y\n",
      " to preser ve as m uch structure as they can (e.g., tr ying to keep separa te clusters in the\n",
      " in put space from overla pping in the visualiza tion), so you can understand how the\n",
      " da ta is organized and perha ps iden tif y unsuspected pa tterns.\n",
      " Types of Machine Learning Systems  |  11\n",
      "\n",
      "3\n",
      " N otice how animals are ra ther well separa ted from vehicles, how horses are close to deer but far from birds,\n",
      " and so on. Figure reproduced with permission from Socher , Gan joo , M anning, and N g (2013), — T -SNE visualƒ\n",
      " iza tion of the seman tic word space. –\n",
      " F i g u r e 1-9. E x a m p l e o f a t-S NE v i s u a l iz a t i o n h i gh l i gh t i n g s em a n t i c c l u s t er s\n",
      "3\n",
      " A rela ted task is \n",
      " d i m ens i o n a l i ty r e d u c t i o n\n",
      " , in which the goal is to sim plif y the da ta\n",
      " without losing too m uch informa tion. One wa y to do this is to merge several correlaƒ\n",
      " ted fea tures in to one. F or exam ple, a car‡ s mileage ma y be ver y correla ted with its age,\n",
      " so the dimensionality reduction algorithm will merge them in to one fea ture tha t repƒ\n",
      " resen ts the car‡ s wear and tear . This is called \n",
      " f e a t u r e ext r a c t i o n\n",
      ".\n",
      " I t is often a good idea to tr y to reduce the dimension of your trainƒ\n",
      " ing da ta using a dimensionality reduction algorithm before you\n",
      " feed it to another M achine Learning algorithm (such as a superƒ\n",
      " vised learning algorithm). I t will run m uch faster , the da ta will take\n",
      " up less disk and memor y space, and in some cases it ma y also perƒ\n",
      " form better .\n",
      " Y et another im portan t unsuper vised task is \n",
      " a n o m a l y d e t e c t i o n\n",
      " ›for exam ple, detectƒ\n",
      " ing un usual credit card transactions to preven t fra ud, ca tching man ufacturing defects,\n",
      " or a utoma tically removing outliers from a da taset before feeding it to another learnƒ\n",
      "ing algorithm. The system is shown mostly normal instances during training, so it\n",
      "learns to recognize them and when it sees a new instance it can tell whether it looks\n",
      " 12  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "4\n",
      " Tha t ‡ s when the system works perfectly . In practice it often crea tes a few clusters per person, and sometimes\n",
      " mixes up two people who look alike, so you need to provide a few labels per person and man ually clean up\n",
      "some clusters.\n",
      "like a normal one or whether it is likely an anomaly (see \n",
      "Figure 1-10\n",
      " ). A ver y similar\n",
      "task is \n",
      " n o v e l ty d e t e c t i o n\n",
      " : the difference is tha t novelty detection algorithms expect to\n",
      " see only normal da ta during training, while anomaly detection algorithms are usually\n",
      " more toleran t, they can often perform well even with a small percen tage of outliers in\n",
      "the training set.\n",
      " F i g u r e 1-10. An o m a l y d e t e c t i o n\n",
      " Finally , another common unsuper vised task is \n",
      " as s o ci a t i o n r u l e l e a r n i n g\n",
      ", in which the\n",
      " goal is to dig in to large amoun ts of da ta and discover in teresting rela tions between\n",
      " a ttributes. F or exam ple, suppose you own a supermarket. R unning an associa tion rule\n",
      " on your sales logs ma y reveal tha t people who purchase barbecue sa uce and pota to\n",
      " chips also tend to buy steak. Th us, you ma y wan t to place these items close to each \n",
      " other .\n",
      "Semisupervised learning\n",
      " Some algorithms can deal with partially labeled training da ta, usually a lot of unlaƒ\n",
      " beled da ta and a little bit of labeled da ta. This is called \n",
      " s em i s u p er v i s e d l e a r n i n g\n",
      "(\n",
      "Figure 1-11\n",
      ").\n",
      " Some  photo-hosting ser vices, such as \n",
      " Google Photos, are good exam ples of this. Once\n",
      " you upload all your family photos to the ser vice, it a utoma tically recognizes tha t the\n",
      "same person A shows up in photos 1, 5, and 11, while another person B shows up in\n",
      " photos 2, 5, and 7. This is the unsuper vised part of the algorithm (clustering). N ow all\n",
      " the system needs is for you to tell it who these people are. J ust one label per person,\n",
      "4\n",
      " and it is able to name ever yone in ever y photo , which is useful for searching photos.\n",
      " Types of Machine Learning Systems  |  13\n",
      "\n",
      " F i g u r e 1-11. S em i s u p er v i s e d l e a r n i n g\n",
      " M ost semisuper vised learning algorithms are combina tions of unsuper vised and\n",
      " super vised algorithms. F or exam ple, \n",
      " d e e p b e l i ef n e tw o r ks\n",
      "  (DBN s) are based on unsuƒ\n",
      " per vised com ponen ts called \n",
      " r e s t r i c t e d B o l tzm a n n m a c h i n e s\n",
      " (RBMs) stacked on top of\n",
      " one another . RBMs are trained sequen tially in an unsuper vised manner , and then the\n",
      " whole system is fine-tuned using super vised learning techniques.\n",
      "Reinforcement Learning\n",
      " R ei n f o r c em en t L e a r n i n g\n",
      "  is a ver y differen t beast. The learning system, called an \n",
      " a gen t\n",
      " in this con text, can obser ve the en vironmen t, select and perform actions, and get\n",
      " r e w a r d s\n",
      " in return (or \n",
      " p en a l t i e s\n",
      "  in the form of nega tive rewards, as in \n",
      "Figure 1-12\n",
      " ). I t\n",
      " m ust then learn by itself wha t is the best stra teg y , called a \n",
      " p o l i c y\n",
      ", to get the most\n",
      " reward over time. A policy defines wha t action the agen t should choose when it is in a\n",
      " given situa tion.\n",
      " 14  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " F i g u r e 1-12. R ei n f o r c em en t L e a r n i n g\n",
      " F or exam ple, man y robots im plemen t Reinforcemen t Learning algorithms to learn\n",
      " how to walk. DeepMind ‡ s AlphaGo program is also a good exam ple of Reinforcemen t\n",
      " Learning: it made the headlines in M a y 2017 when it bea t the world cham pion K e J ie\n",
      " a t the game of \n",
      " G o\n",
      " . I t learned its winning policy by analyzing millions of games, and\n",
      " then pla ying man y games against itself. N ote tha t learning was turned off during the\n",
      " games against the cham pion; AlphaGo was just a pplying the policy it had learned.\n",
      "Batch and Online Learning\n",
      " Another criterion used to classif y M achine Learning systems is whether or not the\n",
      " system can learn incremen tally from a stream of incoming da ta.\n",
      "Batch learning\n",
      "In \n",
      " b a t c h l e a r n i n g\n",
      " , the system is inca pable of learning incremen tally : it m ust be trained\n",
      " using all the a vailable da ta. This will generally take a lot of time and com puting\n",
      "resources, so it is typically done offline. First the system is trained, and then it is\n",
      " la unched in to production and runs without learning an ymore; it just a pplies wha t it\n",
      "has learned. This is called \n",
      "o—ine\n",
      "  l e a r n i n g\n",
      ".\n",
      " If you wan t a ba tch learning system to know about new da ta (such as a new type of\n",
      " spam), you need to train a new version of the system from scra tch on the full da taset\n",
      " (not just the new da ta, but also the old da ta), then stop the old system and replace it\n",
      "with the new one.\n",
      " F ortuna tely , the whole process of training, evalua ting, and la unching a M achine\n",
      " Learning system can be a utoma ted fairly easily (as shown in \n",
      "Figure 1-3\n",
      "), so even a\n",
      " Types of Machine Learning Systems  |  15\n",
      "\n",
      " ba tch learning system can ada pt to change. Sim ply upda te the da ta and train a new\n",
      " version of the system from scra tch as often as needed.\n",
      " This solution is sim ple and often works fine, but training using the full set of da ta can\n",
      " take man y hours, so you would typically train a new system only ever y 24 hours or\n",
      " even just weekly . If your system needs to ada pt to ra pidly changing da ta (e.g., to preƒ\n",
      "dict stock prices), then you need a more reactive solution.\n",
      " Also , training on the full set of da ta requires a lot of com puting resources (CPU ,\n",
      " memor y space, disk space, disk I/O , network I/O , etc.). If you ha ve a lot of da ta and\n",
      " you a utoma te your system to train from scra tch ever y da y , it will end up costing you a\n",
      " lot of money . If the amoun t of da ta is h uge, it ma y even be im possible to use a ba tch\n",
      "learning algorithm.\n",
      " Finally , if your system needs to be able to learn a utonomously and it has limited\n",
      " resources (e.g., a smartphone a pplica tion or a rover on M ars), then carr ying around\n",
      " large amoun ts of training da ta and taking up a lot of resources to train for hours\n",
      " ever y da y is a showstopper .\n",
      " F ortuna tely , a better option in all these cases is to use algorithms tha t are ca pable of\n",
      " learning incremen tally .\n",
      "Online learning\n",
      "In \n",
      " o n l i n e l e a r n i n g\n",
      " , you train the system incremen tally by feeding it da ta instances\n",
      " sequen tially , either individually or by small groups called \n",
      " m i n i-b a t c h e s\n",
      ". Each learning\n",
      " step is fast and chea p , so the system can learn about new da ta on the fly , as it arrives\n",
      "(see \n",
      "Figure 1-13\n",
      ").\n",
      " F i g u r e 1-13. O n l i n e l e a r n i n g\n",
      " Online learning is grea t for systems tha t receive da ta as a con tin uous flow (e.g., stock\n",
      " prices) and need to ada pt to change ra pidly or a utonomously . I t is also a good option\n",
      " 16  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " if you ha ve limited com puting resources: once an online learning system has learned\n",
      " about new da ta instances, it does not need them an ymore, so you can discard them\n",
      " (unless you wan t to be able to roll back to a previous sta te and — repla y– the da ta). This\n",
      " can sa ve a h uge amoun t of space.\n",
      " Online learning algorithms can also be used to train systems on h uge da tasets tha t\n",
      " cannot fit in one machine ‡ s main memor y (this is called \n",
      " o u t-o f-c o r e\n",
      " learning). The\n",
      " algorithm loads part of the da ta, runs a training step on tha t da ta, and repea ts the\n",
      " process un til it has run on all of the da ta (see \n",
      "Figure 1-14\n",
      ").\n",
      "Out-of-core learning is usually done offline (i.e., not on the live\n",
      "system), so \n",
      " o n l i n e l e a r n i n g\n",
      " can be a confusing name. Think of it \n",
      "as\n",
      " i n cr em en t a l l e a r n i n g\n",
      ".\n",
      " F i g u r e 1-14. U s i n g o n l i n e l e a r n i n g t o h a n d l e h u ge d a t as e ts\n",
      " One im portan t parameter of online learning systems is how fast they should ada pt to\n",
      " changing da ta: this is called \n",
      "the \n",
      " l e a r n i n g r a t e\n",
      " . If you set a high learning ra te, then your\n",
      " system will ra pidly ada pt to new da ta, but it will also tend to quickly forget the old\n",
      " da ta (you don ‡ t wan t a spam filter to flag only the la test kinds of spam it was shown).\n",
      " Con versely , if you set a low learning ra te, the system will ha ve more inertia; tha t is, it\n",
      " will learn more slowly , but it will also be less sensitive to noise in the new da ta or to\n",
      " sequences of nonrepresen ta tive da ta poin ts (outliers).\n",
      " A big challenge with online learning is tha t if bad da ta is fed to the system, the sysƒ\n",
      " tem ‡ s performance will gradually decline. If we are talking about a live system, your\n",
      " clien ts will notice. F or exam ple, bad da ta could come from a malfunctioning sensor\n",
      " on a robot, or from someone spamming a search engine to tr y to rank high in search\n",
      " Types of Machine Learning Systems  |  17\n",
      "\n",
      " results. T o reduce this risk, you need to monitor your system closely and prom ptly\n",
      " switch learning off (and possibly revert to a previously working sta te) if you detect a\n",
      " drop in performance. Y ou ma y also wan t to monitor the in put da ta and react to\n",
      " abnormal da ta (e.g., using an anomaly detection algorithm).\n",
      "Instance-Based Versus Model-Based Learning\n",
      " One more wa y to ca tegorize M achine Learning systems is by how they \n",
      " gen er a l iz e\n",
      ".\n",
      " M ost M achine Learning tasks are about making predictions. This means tha t given a\n",
      " n umber of training exam ples, the system needs to be able to generalize to exam ples it\n",
      " has never seen before. H a ving a good performance measure on the training da ta is\n",
      " good, but insufficien t; the true goal is to perform well on new instances.\n",
      " There are two main a pproaches to generaliza tion: instance-based learning and\n",
      "model-based learning.\n",
      "Instance-based learning\n",
      " P ossibly the most trivial form of learning is sim ply to learn by heart. If you were to\n",
      " crea te a spam filter this wa y , it would just flag all emails tha t are iden tical to emails\n",
      " tha t ha ve already been flagged by users›not the worst solution, but certainly not the\n",
      "best.\n",
      " Instead of just flagging emails tha t are iden tical to known spam emails, your spam\n",
      " filter could be programmed to also flag emails tha t are ver y similar to known spam\n",
      "emails. This requires a \n",
      " m e as u r e o f s i m i l a r i ty\n",
      "  between two emails. A (ver y basic) simiƒ\n",
      " larity measure between two emails could be to coun t the n umber of words they ha ve\n",
      " in common. The system would flag an email as spam if it has man y words in comƒ\n",
      "mon with a known spam email.\n",
      "This is called \n",
      " i ns t a n c e-b as e d l e a r n i n g\n",
      " : the system learns the exam ples by heart, then\n",
      " generalizes to new cases by com paring them to the learned exam ples (or a subset of\n",
      " them), using a similarity measure. F or exam ple, in \n",
      "Figure 1-15\n",
      " the new instance\n",
      " would be classified as a triangle beca use the ma jority of the most similar instances\n",
      " belong to tha t class.\n",
      " 18  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " F i g u r e 1-15. I ns t a n c e-b as e d l e a r n i n g\n",
      "Model-based learning\n",
      " Another wa y to generalize from a set of exam ples is to build a model of these examƒ\n",
      " ples, then use tha t model to make \n",
      " p r e d i c t i o ns\n",
      ". This is called \n",
      " m o d e l-b as e d l e a r n i n g\n",
      "(\n",
      "Figure 1-16\n",
      ").\n",
      " F i g u r e 1-16. M o d e l-b as e d l e a r n i n g\n",
      " F or exam ple, suppose you wan t to know if money makes people ha ppy , so you downƒ\n",
      "load the \n",
      " B e tt er L i f e I n d ex\n",
      "  da ta from the \n",
      " OECD ‡ s website\n",
      "  as well as sta ts about GDP\n",
      " per ca pita from the \n",
      " IMF‡ s website\n",
      " . Then you join the tables and sort by GDP per ca pƒ\n",
      "ita. \n",
      " T able 1-1\n",
      "  shows an excerpt of wha t you get.\n",
      " Types of Machine Learning Systems  |  19\n",
      "\n",
      "5\n",
      " By con ven tion, the Greek letter ‰ (theta) is frequen tly used to represen t model parameters.\n",
      " T a b l e 1-1. D o e s m o n e y m a k e p e o p l e h a p p i er?\n",
      "Country\n",
      " GDP   per   capita   (USD)\n",
      " Life   satisfaction\n",
      "Hungary\n",
      "12,240\n",
      "4.9\n",
      "Korea\n",
      "27,195\n",
      "5.8\n",
      "France\n",
      "37,675\n",
      "6.5\n",
      "Australia\n",
      "50,962\n",
      "7.3\n",
      " United   States\n",
      "55,805\n",
      "7.2\n",
      " Let ‡ s plot the da ta for a few random coun tries (\n",
      "Figure 1-17\n",
      ").\n",
      " F i g u r e 1-17. D o y o u s e e a t r en d h er e?\n",
      " There does seem to be a trend here! Although the da ta is \n",
      " n o i s y\n",
      "   (i.e., partly random), it\n",
      " looks like life sa tisfaction goes up more or less linearly as the coun tr y‡ s GDP per ca pƒ\n",
      " ita increases. So you decide to model life sa tisfaction as a linear function of GDP per\n",
      " ca pita. This step is called \n",
      " m o d e l s e l e c t i o n\n",
      ": you selected a \n",
      " l i n e a r m o d e l\n",
      "  of life sa tisfacƒ\n",
      " tion with just one a ttribute, GDP per ca pita (\n",
      " Equa tion 1-1\n",
      ").\n",
      " Eq u a t i o n 1-1. A s i m p l e l i n e a r m o d e l\n",
      " life_satisfaction =\n",
      "–\n",
      "0\n",
      "+\n",
      "–\n",
      "1\n",
      " „ GDP_per_capita\n",
      "This model has two \n",
      " m o d e l p a r a m e t er s\n",
      ", \n",
      "–\n",
      "0\n",
      " and \n",
      "–\n",
      "1\n",
      ".\n",
      "5\n",
      " \n",
      "By tweaking these parameters, you\n",
      " can make your model represen t an y linear function, as shown in \n",
      "Figure 1-18\n",
      ".\n",
      " 20  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " F i g u r e 1-18. A f e w p o s s i b l e l i n e a r m o d e l s\n",
      " B efore you can use your model, you need to define the parameter values \n",
      "–\n",
      "0\n",
      " and \n",
      "–\n",
      "1\n",
      ".\n",
      " H ow can you know which values will make your model perform best? T o answer this\n",
      " question, you need to specif y a performance measure. Y ou can either define a \n",
      " u t i l i ty\n",
      " f u n c t i o n\n",
      "   (or \n",
      "†tness\n",
      "  f u n c t i o n\n",
      " )  tha t  measures how \n",
      " go o d\n",
      " \n",
      "your model is, or you can define\n",
      "a \n",
      " c o s t f u n c t i o n\n",
      "  tha t measures how \n",
      " b a d\n",
      "  it is. F or linear regression problems, people\n",
      " typically use a cost function tha t measures the distance between the linear model ‡ s\n",
      " predictions and the training exam ples; the objective is to minimize this distance.\n",
      "This is where the Linear Regression algorithm comes in: you feed it your training\n",
      " exam ples and it finds the parameters tha t make the linear model fit best to your da ta.\n",
      "This is called \n",
      " t r a i n i n g\n",
      "  the model. In our case the algorithm finds tha t the optimal\n",
      "parameter values are \n",
      "–\n",
      "0\n",
      " = 4.85 and \n",
      "–\n",
      "1\n",
      " = 4.91 „ 10\n",
      "−5\n",
      ".\n",
      " N ow the model fits the training da ta as closely as possible (for a linear model), as you\n",
      "can see in \n",
      "Figure 1-19\n",
      ".\n",
      " F i g u r e 1-19. \n",
      "•e\n",
      "  l i n e a r m o d e l t h a t \n",
      "†ts\n",
      "  t h e t r a i n i n g d a t a b e s t\n",
      " Types of Machine Learning Systems  |  21\n",
      "\n",
      "6\n",
      "The \n",
      "prepare_country_stats()\n",
      "  function ‡ s definition is not shown here (see this cha pter‡ s J upyter notebook if\n",
      " you wan t all the gor y details). I t ‡ s just boring P andas code tha t joins the life sa tisfaction da ta from the OECD\n",
      " with the GDP per ca pita da ta from the IMF .\n",
      "7\n",
      " I t ‡ s oka y if you don ‡ t understand all the code yet; we will presen t Scikit-Learn in the following cha pters.\n",
      " Y ou are finally ready to run the model to make predictions. F or exam ple, sa y you\n",
      " wan t to know how ha ppy Cypriots are, and the OECD da ta does not ha ve the answer .\n",
      " F ortuna tely , you can use your model to make a good prediction: you look up Cyprus ‡ s\n",
      " GDP per ca pita, find $22,587, and then a pply your model and find tha t life sa tisfacƒ\n",
      "tion is likely to be somewhere around 4.85 + 22,587 „ 4.91 „ 10\n",
      "-5\n",
      " = 5.96.\n",
      " T o whet your a ppetite, \n",
      " Exam ple 1-1\n",
      "  shows the Python code tha t loads the da ta, preƒ\n",
      "pares it,\n",
      "6\n",
      "  crea tes a sca tterplot for visualiza tion, and then trains a linear model and\n",
      "makes a prediction.\n",
      "7\n",
      " E x a m p l e 1-1. T r a i n i n g a n d r u n n i n g a l i n e a r m o d e l u s i n g S ci k i t-L e a r n\n",
      "import\n",
      " \n",
      "matplotlib.pyplot\n",
      " \n",
      "as\n",
      " \n",
      "plt\n",
      "import\n",
      " \n",
      "numpy\n",
      " \n",
      "as\n",
      " \n",
      "np\n",
      "import\n",
      " \n",
      "pandas\n",
      " \n",
      "as\n",
      " \n",
      "pd\n",
      "import\n",
      " \n",
      "sklearn.linear_model\n",
      "# Load the data\n",
      "oecd_bli\n",
      " \n",
      "=\n",
      " \n",
      "pd\n",
      ".\n",
      "read_csv\n",
      "(\n",
      "\"oecd_bli_2015.csv\"\n",
      ",\n",
      " \n",
      "thousands\n",
      "=\n",
      "•,•\n",
      ")\n",
      "gdp_per_capita\n",
      " \n",
      "=\n",
      " \n",
      "pd\n",
      ".\n",
      "read_csv\n",
      "(\n",
      "\"gdp_per_capita.csv\"\n",
      ",\n",
      "thousands\n",
      "=\n",
      "•,•\n",
      ",\n",
      "delimiter\n",
      "=\n",
      "•\n",
      "\\t\n",
      "•\n",
      ",\n",
      "                             \n",
      "encoding\n",
      "=\n",
      "•latin1•\n",
      ",\n",
      " \n",
      "na_values\n",
      "=\n",
      "\"n/a\"\n",
      ")\n",
      "# Prepare the data\n",
      "country_stats\n",
      " \n",
      "=\n",
      " \n",
      "prepare_country_stats\n",
      "(\n",
      "oecd_bli\n",
      ",\n",
      " \n",
      "gdp_per_capita\n",
      ")\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "country_stats\n",
      "[\n",
      "\"GDP per capita\"\n",
      "]]\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "country_stats\n",
      "[\n",
      "\"Life satisfaction\"\n",
      "]]\n",
      "# Visualize the data\n",
      "country_stats\n",
      ".\n",
      "plot\n",
      "(\n",
      "kind\n",
      "=\n",
      "•scatter•\n",
      ",\n",
      " \n",
      "x\n",
      "=\n",
      "\"GDP per capita\"\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "•Life satisfaction•\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      "# Select a linear model\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "sklearn\n",
      ".\n",
      "linear_model\n",
      ".\n",
      "LinearRegression\n",
      "()\n",
      "# Train the model\n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      "# Make a prediction for Cyprus\n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "[[\n",
      "22587\n",
      "]]\n",
      "  \n",
      "# Cyprus• GDP per capita\n",
      "print\n",
      "(\n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      "))\n",
      " \n",
      "# outputs [[ 5.96242338]]\n",
      " 22  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "If you had used an instance-based learning algorithm instead, you\n",
      " would ha ve found tha t Slovenia has the closest GDP per ca pita to\n",
      " tha t of Cyprus ($20,732), and since the OECD da ta tells us tha t\n",
      " Slovenians ‡ life sa tisfaction is 5.7, you would ha ve predicted a life\n",
      " sa tisfaction of 5.7 for Cyprus. If you zoom out a bit and look a t the\n",
      " two next closest coun tries, you will find P ortugal and Spain with\n",
      " life sa tisfactions of 5.1 and 6.5, respectively . A veraging these three\n",
      "values, you get 5.77, which is pretty close to your model-based preƒ\n",
      " diction. This sim ple algorithm is called \n",
      " k-N e a r e s t N ei gh b o r s\n",
      " \n",
      "regresƒ\n",
      " sion (in this exam ple, \n",
      "k\n",
      " = 3).\n",
      " Replacing the Linear Regression model with k-N earest N eighbors\n",
      " regression in the previous code is as sim ple as replacing these two\n",
      "lines:\n",
      "import\n",
      " \n",
      "sklearn.linear_model\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "sklearn\n",
      ".\n",
      "linear_model\n",
      ".\n",
      "LinearRegression\n",
      "()\n",
      "with these two:\n",
      "import\n",
      " \n",
      "sklearn.neighbors\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "sklearn\n",
      ".\n",
      "neighbors\n",
      ".\n",
      "KNeighborsRegressor\n",
      "(\n",
      "n_neighbors\n",
      "=\n",
      "3\n",
      ")\n",
      " If all wen t well, your model will make good predictions. If not, you ma y need to use\n",
      " more a ttributes (em ploymen t ra te, health, air pollution, etc.), get more or better qualƒ\n",
      " ity training da ta, or perha ps select a more powerful model (e.g., a P olynomial Regresƒ\n",
      "sion model).\n",
      " In summar y :\n",
      "⁄\n",
      " Y ou studied the da ta.\n",
      "⁄\n",
      " Y ou selected a model.\n",
      "⁄\n",
      " Y ou trained it on the training da ta (i.e., the learning algorithm searched for the\n",
      " model parameter values tha t minimize a cost function).\n",
      "⁄\n",
      " Finally , you a pplied the model to make predictions on new cases (this is called\n",
      " i n f er en c e\n",
      " ), hoping tha t this model will generalize well.\n",
      " This is wha t a typical M achine Learning project looks like. In \n",
      " Cha pter 2\n",
      " \n",
      "you will\n",
      "experience this first-hand by going through an end-to-end project.\n",
      " W e ha ve covered a lot of ground so far : you now know wha t M achine Learning is\n",
      " really about, wh y it is useful, wha t some of the most common ca tegories of ML sysƒ\n",
      " tems are, and wha t a typical project workflow looks like. N ow let ‡ s look a t wha t can go\n",
      " wrong in learning and preven t you from making accura te predictions.\n",
      " Types of Machine Learning Systems  |  23\n",
      "\n",
      "Main Challenges of Machine Learning\n",
      "In short, since your main task is to select a learning algorithm and train it on some\n",
      " da ta, the two things tha t can go wrong are —bad algorithm – and —bad da ta. – Let ‡ s start\n",
      " with exam ples of bad da ta.\n",
      "Insu‡cient\n",
      " Quantity of Training Data\n",
      " F or a toddler to learn wha t an a pple is, all it takes is for you to poin t to an a pple and\n",
      " sa y — a pple – (possibly repea ting this procedure a few times). N ow the child is able to\n",
      " recognize a pples in all sorts of colors and sha pes. Genius.\n",
      " M achine Learning is not quite there yet; it takes a lot of da ta for most M achine Learnƒ\n",
      " ing algorithms to work properly . E ven for ver y sim ple problems you typically need\n",
      " thousands of exam ples, and for com plex problems such as image or speech recogniƒ\n",
      " tion you ma y need millions of exam ples (unless you can reuse parts of an existing\n",
      "model).\n",
      " 24  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "8\n",
      " F or exam ple, knowing whether to write — to , – — two , – or — too – depending on the con text.\n",
      "9\n",
      " Figure reproduced with permission from Banko and Brill (2001), —Learning Cur ves for Confusion Set Disamƒ\n",
      " bigua tion. –\n",
      "10\n",
      " — The U nreasonable Effectiveness of Da ta, – P eter N or vig et al. (2009).\n",
      "The Unreasonable \n",
      "E…ectiveness\n",
      " of Data\n",
      "In a \n",
      " famous pa per\n",
      " published in 2001, Microsoft researchers Michele Banko and Eric\n",
      " Brill showed tha t ver y differen t M achine Learning algorithms, including fairly sim ple\n",
      " ones, performed almost iden tically well on a com plex problem of na tural language\n",
      " disambigua tion\n",
      "8\n",
      "  once they were given enough da ta (as you can see in \n",
      "Figure 1-20\n",
      ").\n",
      " F i g u r e 1-20. \n",
      "•e\n",
      "  i m p o r t a n c e o f d a t a v er s u s a l go r i t h ms\n",
      "9\n",
      " As the a uthors put it: — these results suggest tha t we ma y wan t to reconsider the trade-\n",
      " off between spending time and money on algorithm developmen t versus spending it\n",
      " on corpus developmen t. –\n",
      " The idea tha t da ta ma tters more than algorithms for com plex problems was further\n",
      " popularized by P eter N or vig et al. in a pa per titled \n",
      " — The U nreasonable Effectiveness\n",
      " of Da ta –\n",
      " published in 2009.\n",
      "10\n",
      "  I t should be noted, however , tha t small- and medium-\n",
      " sized da tasets are still ver y common, and it is not alwa ys easy or chea p to get extra\n",
      " training da ta, so don ‡ t abandon algorithms just yet.\n",
      " Main Challenges of Machine Learning  |  25\n",
      "\n",
      "Nonrepresentative Training Data\n",
      "In \n",
      " order to generalize well, it is crucial tha t your training da ta be represen ta tive of the\n",
      " new cases you wan t to generalize to . This is true whether you use instance-based\n",
      "learning or model-based learning.\n",
      " F or exam ple, the set of coun tries we used earlier for training the linear model was not\n",
      " perfectly represen ta tive; a few coun tries were missing. \n",
      "Figure 1-21\n",
      "  shows wha t the\n",
      " da ta looks like when you add the missing coun tries.\n",
      " F i g u r e 1-21. A m o r e r e p r e s en t a t i v e t r a i n i n g s a m p l e\n",
      " If you train a linear model on this da ta, you get the solid line, while the old model is\n",
      " represen ted by the dotted line. As you can see, not only does adding a few missing\n",
      " coun tries significan tly alter the model, but it makes it clear tha t such a sim ple linear\n",
      " model is probably never going to work well. I t seems tha t ver y rich coun tries are not\n",
      " ha ppier than modera tely rich coun tries (in fact they seem unha ppier), and con versely\n",
      " some poor coun tries seem ha ppier than man y rich coun tries.\n",
      " By using a nonrepresen ta tive training set, we trained a model tha t is unlikely to make\n",
      " accura te predictions, especially for ver y poor and ver y rich coun tries.\n",
      " I t is crucial to use a training set tha t is represen ta tive of the cases you wan t to generalƒ\n",
      " ize to . This is often harder than it sounds: if the sam ple is too small, you will \n",
      " ha ve\n",
      " s a m p l i n g n o i s e\n",
      "  (i.e., nonrepresen ta tive da ta as a result of chance), but even ver y large\n",
      " sam ples can be nonrepresen ta tive if the sam pling method is fla wed. This is called\n",
      " s a m p l i n g b i as\n",
      ".\n",
      "A Famous Example of Sampling Bias\n",
      " P erha ps the most famous exam ple of sam pling bias ha ppened during the US presiƒ\n",
      " den tial election in 1936, which pitted Landon against Roosevelt: the \n",
      " L i t er a r y D i ge s t\n",
      " conducted a ver y large poll, sending mail to about 10 million people. I t got 2.4 million\n",
      " answers, and predicted with high confidence tha t Landon would get 57% of the votes.\n",
      " 26  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " Instead, Roosevelt won with 62% of the votes. The fla w was in the \n",
      " L i t er a r y D i ge s t\n",
      " ‡ s\n",
      " sam pling method:\n",
      "⁄\n",
      " First, to obtain the addresses to send the polls to , the \n",
      " L i t er a r y D i ge s t\n",
      " used teleƒ\n",
      "phone directories, lists of magazine subscribers, club membership lists, and the\n",
      " like. All of these lists tend to fa vor wealthier people, who are more likely to vote\n",
      "Republican (hence Landon).\n",
      "⁄\n",
      "Second, less than 25% of the people who received the poll answered. Again, this\n",
      " in troduces a sam pling bias, by ruling out people who don ‡ t care m uch about poliƒ\n",
      " tics, people who don ‡ t like the \n",
      " L i t er a r y D i ge s t\n",
      ", and other key groups. This is a speƒ\n",
      " cial type of sam pling bias called \n",
      " n o n r e s p o ns e b i as\n",
      ".\n",
      " H ere is another exam ple: sa y you wan t to build a system to recognize funk m usic vidƒ\n",
      " eos. One wa y to build your training set is to search — funk m usic – on Y ouT ube and use\n",
      " the resulting videos. But this assumes tha t Y ouT ube ‡ s search engine returns a set of\n",
      " videos tha t are represen ta tive of all the funk m usic videos on Y ouT ube. In reality , the\n",
      "search results are likely to be biased toward popular artists (and if you live in Brazil\n",
      " you will get a lot of — funk carioca – videos, which sound nothing like J ames Brown).\n",
      "On the other hand, how else can you get a large training set?\n",
      "Poor-Quality Data\n",
      " Obviously , if your training da ta is full of errors, outliers, and noise (e.g., due to poor -\n",
      " quality measuremen ts), it will make it harder for the system to detect the underlying\n",
      " pa tterns, so your system is less likely to perform well. I t is often well worth the effort\n",
      " to spend time cleaning up your training da ta. The truth is, most da ta scien tists spend\n",
      " a significan t part of their time doing just tha t. F or exam ple:\n",
      "⁄\n",
      " If some instances are clearly outliers, it ma y help to sim ply discard them or tr y to\n",
      " fix the errors man ually .\n",
      "⁄\n",
      " If some instances are missing a few fea tures (e.g., 5% of your customers did not\n",
      " specif y their age), you m ust decide whether you wan t to ignore this a ttribute altoƒ\n",
      " gether , ignore these instances, fill in the missing values (e.g., with the median\n",
      " age), or train one model with the fea ture and one model without it, and so on.\n",
      "Irrelevant Features\n",
      "As \n",
      " the sa ying goes: garbage in, garbage out. Y our system will only be ca pable of learnƒ\n",
      " ing if the training da ta con tains enough relevan t fea tures and not too man y irrelevan t\n",
      " ones. A critical part of the success of a M achine Learning project is coming up with a\n",
      " good set of fea tures to train on. This process, called \n",
      " f e a t u r e en g i n e er i n g\n",
      " , in volves:\n",
      " Main Challenges of Machine Learning  |  27\n",
      "\n",
      "⁄\n",
      " F e a t u r e s e l e c t i o n\n",
      " : selecting the most useful fea tures to train on among existing\n",
      " fea tures.\n",
      "⁄\n",
      " F e a t u r e ext r a c t i o n\n",
      " : combining existing fea tures to produce a more useful one (as\n",
      " we sa w earlier , dimensionality reduction algorithms can help).\n",
      "⁄\n",
      " Crea ting new fea tures by ga thering new da ta.\n",
      " N ow tha t we ha ve looked a t man y exam ples of bad da ta, let ‡ s look a t a couple of examƒ\n",
      "ples of bad algorithms.\n",
      "Over•tting\n",
      " the Training Data\n",
      " Sa y you are visiting a foreign coun tr y and the taxi driver rips you off. Y ou migh t be\n",
      " tem pted to sa y tha t \n",
      " a l l\n",
      "  taxi drivers in tha t coun tr y are thieves. O vergeneralizing is\n",
      " something tha t we h umans do all too often, and unfortuna tely machines can fall in to\n",
      " the same tra p if we are not careful. In M achine Learning this is called \n",
      "over†tting\n",
      ": it\n",
      " means tha t the model performs well on the training da ta, but it does not generalize\n",
      "well.\n",
      "Figure 1-22\n",
      "  shows an exam ple of a high-degree polynomial life sa tisfaction model\n",
      " tha t strongly overfits the training da ta. E ven though it performs m uch better on the\n",
      " training da ta than the sim ple linear model, would you really trust its predictions?\n",
      " F i g u r e 1-22. \n",
      "Over†tting\n",
      "  t h e t r a i n i n g d a t a\n",
      " Com plex models such as deep neural networks can detect subtle pa tterns in the da ta,\n",
      " but if the training set is noisy , or if it is too small (which in troduces sam pling noise),\n",
      " then the model is likely to detect pa tterns in the noise itself. Obviously these pa tterns\n",
      " will not generalize to new instances. F or exam ple, sa y you feed your life sa tisfaction\n",
      " model man y more a ttributes, including uninforma tive ones such as the coun tr y‡ s\n",
      " name. In tha t case, a com plex model ma y detect pa tterns like the fact tha t all counƒ\n",
      " tries in the training da ta with a \n",
      "w\n",
      " \n",
      " in their name ha ve a life sa tisfaction grea ter than 7:\n",
      " N ew Zealand (7.3), N or wa y (7.4), Sweden (7.2), and Switzerland (7.5). H ow confiden t\n",
      " 28  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      " are you tha t the W -sa tisfaction rule generalizes to R wanda or Zimbabwe? Obviously\n",
      " this pa ttern occurred in the training da ta by pure chance, but the model has no wa y\n",
      " to tell whether a pa ttern is real or sim ply the result of noise in the da ta.\n",
      " O verfitting ha ppens when the model is too com plex rela tive to the\n",
      " amoun t and noisiness of the training da ta. The possible solutions\n",
      "are:\n",
      "⁄\n",
      " T o sim plif y the model by selecting one with fewer parameters\n",
      " (e.g., a linear model ra ther than a high-degree polynomial\n",
      " model), by reducing the n umber of a ttributes in the training\n",
      " da ta or by constraining the model\n",
      "⁄\n",
      " T o ga ther more training da ta\n",
      "⁄\n",
      " T o reduce the noise in the training da ta (e.g., fix da ta errors\n",
      "and remove outliers)\n",
      " Constraining a model to make it sim pler and reduce the risk of overfitting is \n",
      "called\n",
      " r e g u l a r iz a t i o n\n",
      " . F or exam ple, the linear model we defined earlier has two parameters,\n",
      "–\n",
      "0\n",
      "   and \n",
      "–\n",
      "1\n",
      ". This gives the learning algorithm two \n",
      " d e g r e e s o f f r e e d o m\n",
      "   to \n",
      " ada pt the model\n",
      " to the training da ta: it can tweak both the heigh t (\n",
      "–\n",
      "0\n",
      ") and the slope (\n",
      "–\n",
      "1\n",
      ") of the line. If\n",
      "we forced \n",
      "–\n",
      "1\n",
      "  = 0, the algorithm would ha ve only one degree of freedom and would\n",
      " ha ve a m uch harder time fitting the da ta properly : all it could do is move the line up\n",
      "or down to get as close as possible to the training instances, so it would end up\n",
      " around the mean. A ver y sim ple model indeed! If we allow the algorithm to modif y \n",
      "–\n",
      "1\n",
      " but we force it to keep it small, then the learning algorithm will effectively ha ve someƒ\n",
      " where in between one and two degrees of freedom. I t will produce a sim pler model\n",
      " than with two degrees of freedom, but more com plex than with just one. Y ou wan t to\n",
      " find the righ t balance between fitting the training da ta perfectly and keeping the\n",
      " model sim ple enough to ensure tha t it will generalize well.\n",
      "Figure 1-23\n",
      "  shows three models: the dotted line represen ts the original model tha t\n",
      " was trained with a few coun tries missing, the dashed line is our second model trained\n",
      " with all coun tries, and the solid line is a linear model trained with the same da ta as\n",
      " the first model but with a regulariza tion constrain t. Y ou can see tha t regulariza tion\n",
      " forced the model to ha ve a smaller slope, which fits a bit less the training da ta tha t the\n",
      " model was trained on, but actually allows it to generalize better to new exam ples.\n",
      " Main Challenges of Machine Learning  |  29\n",
      "\n",
      " F i g u r e 1-23. R e g u l a r iz a t i o n r e d u c e s t h e r i s k o f \n",
      "over†tting\n",
      " The amoun t of regulariza tion to a pply during learning can be con trolled by a \n",
      " h y p er‡\n",
      " p a r a m e t er\n",
      " . A h yperparameter is a parameter of a learning algorithm (not of the\n",
      " model). As such, it is not affected by the learning algorithm itself; it m ust be set prior\n",
      " to training and remains constan t during training. If you set the regulariza tion h yperƒ\n",
      " parameter to a ver y large value, you will get an almost fla t model (a slope close to\n",
      " zero); the learning algorithm will almost certainly not overfit the training da ta, but it\n",
      " will be less likely to find a good solution. T uning h yperparameters is an im portan t\n",
      " part of building a M achine Learning system (you will see a detailed exam ple in the\n",
      " next cha pter).\n",
      "Under•tting\n",
      " the Training Data\n",
      " As you migh t guess, \n",
      "under†tting\n",
      " is the opposite of overfitting: it occurs when your\n",
      " model is too sim ple to learn the underlying structure of the da ta. F or exam ple, a linƒ\n",
      " ear model of life sa tisfaction is prone to underfit; reality is just more com plex than\n",
      " the model, so its predictions are bound to be inaccura te, even on the training examƒ\n",
      "ples.\n",
      "The main options to fix this problem are:\n",
      "⁄\n",
      "Selecting a more powerful model, with more parameters\n",
      "⁄\n",
      " F eeding better fea tures to the learning algorithm (fea ture engineering)\n",
      "⁄\n",
      " Reducing the constrain ts on the model (e.g., reducing the regulariza tion h yperƒ\n",
      "parameter)\n",
      "Stepping Back\n",
      " By now you already know a lot about M achine Learning. H owever , we wen t through\n",
      " so man y concepts tha t you ma y be feeling a little lost, so let ‡ s step back and look a t the\n",
      "big picture:\n",
      " 30  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "⁄\n",
      " M achine Learning is about making machines get better a t some task by learning\n",
      " from da ta, instead of ha ving to explicitly code rules.\n",
      "⁄\n",
      " There are man y differen t types of ML systems: super vised or not, ba tch or online,\n",
      "instance-based or model-based, and so on.\n",
      "⁄\n",
      " In a ML project you ga ther da ta in a training set, and you feed the training set to\n",
      "a learning algorithm. If the algorithm is model-based it tunes some parameters to\n",
      "fit the model to the training set (i.e., to make good predictions on the training set\n",
      " itself ), and then hopefully it will be able to make good predictions on new cases\n",
      " as well. If the algorithm is instance-based, it just learns the exam ples by heart and\n",
      " generalizes to new instances by com paring them to the learned instances using a\n",
      "similarity measure.\n",
      "⁄\n",
      " The system will not perform well if your training set is too small, or if the da ta is\n",
      " not represen ta tive, noisy , or polluted with irrelevan t fea tures (garbage in, garbage\n",
      " out). Lastly , your model needs to be neither too sim ple (in which case it will\n",
      " underfit) nor too com plex (in which case it will overfit).\n",
      " There ‡ s just one last im portan t topic to cover : once you ha ve trained a model, you\n",
      " don ‡ t wan t to just —hope – it generalizes to new cases. Y ou wan t to evalua te it, and fine-\n",
      " tune it if necessar y . Let ‡ s see how .\n",
      "Testing and Validating\n",
      "The \n",
      " only wa y to know how well a model will generalize to new cases is to actually tr y\n",
      " it out on new cases. One wa y to do tha t is to put your model in production and moniƒ\n",
      "tor how well it performs. This works well, but if your model is horribly bad, your\n",
      " users will com plain›not the best idea.\n",
      " A better option is to split your da ta in to two sets: the \n",
      " t r a i n i n g s e t\n",
      " and the \n",
      " t e s t s e t\n",
      ". As\n",
      " these names im ply , you train your model using the training set, and you test it using\n",
      " the test set. The error ra te on new cases is called the \n",
      " gen er a l iz a t i o n er r o r\n",
      " \n",
      "(or \n",
      " o u t-o f-\n",
      " s a m p l e er r o r\n",
      "), \n",
      " and by evalua ting your model on the test set, you get an estima te of this\n",
      " error . This value tells you how well your model will perform on instances it has never\n",
      "seen before.\n",
      "If the training error is low (i.e., your model makes few mistakes on the training set)\n",
      " but the generaliza tion error is high, it means tha t your model is overfitting the trainƒ\n",
      " ing da ta.\n",
      " I t is common to use 80% of the da ta for training and \n",
      " h o l d o u t\n",
      " \n",
      "20%\n",
      " for testing. H owever , this depends on the size of the da taset: if it\n",
      " con tains 10 million instances, then holding out 1% means your test\n",
      " set will con tain 100,000 instances: tha t ‡ s probably more than\n",
      " enough to get a good estima te of the generaliza tion error .\n",
      " Testing and Validating  |  31\n",
      "\n",
      "Hyperparameter Tuning and Model Selection\n",
      " So evalua ting a model is sim ple enough: just use a test set. N ow suppose you are hesiƒ\n",
      " ta ting between two models (sa y a linear model and a polynomial model): how can\n",
      " you decide? One option is to train both and com pare how well they generalize using\n",
      "the test set.\n",
      " N ow suppose tha t the linear model generalizes better , but you wan t to a pply some \n",
      " regulariza tion to a void overfitting. The question is: how do you choose the value of\n",
      " the regulariza tion h yperparameter? One option is to train 100 differen t models using\n",
      " 100 differen t values for this h yperparameter . Suppose you find the best h yperparameƒ\n",
      " ter value tha t produces a model with the lowest generaliza tion error , sa y just 5% error .\n",
      " So you la unch this model in to production, but unfortuna tely it does not perform as\n",
      " well as expected and produces 15% errors. Wha t just ha ppened?\n",
      " The problem is tha t you measured the generaliza tion error m ultiple times on the test\n",
      " set, and you ada pted the model and h yperparameters to produce the best model \n",
      " f o r\n",
      " t h a t p a r t i cu l a r s e t\n",
      " . This means tha t the model is unlikely to perform as well on new\n",
      " da ta.\n",
      "A common solution to this problem is called \n",
      " h o l d o u t v a l i d a t i o n\n",
      " : you sim ply hold out\n",
      " part of the training set to evalua te several candida te models and select the best one.\n",
      "The new heldout set is called the \n",
      " v a l i d a t i o n s e t\n",
      " (or sometimes the \n",
      " d e v e l o p m en t s e t\n",
      ", or\n",
      " d e v s e t\n",
      " ). M ore specifically , you train m ultiple models with various h yperparameters\n",
      " on the reduced training set (i.e., the full training set min us the valida tion set), and\n",
      " you select the model tha t performs best on the valida tion set. After this holdout valiƒ\n",
      " da tion process, you train the best model on the full training set (including the validaƒ\n",
      " tion set), and this gives you the final model. Lastly , you evalua te this final model on\n",
      " the test set to get an estima te of the generaliza tion error .\n",
      " This solution usually works quite well. H owever , if the valida tion set is too small, then\n",
      " model evalua tions will be im precise: you ma y end up selecting a suboptimal model by\n",
      " mistake. Con versely , if the valida tion set is too large, then the remaining training set\n",
      " will be m uch smaller than the full training set. Wh y is this bad? W ell, since the final\n",
      " model will be trained on the full training set, it is not ideal to com pare candida te\n",
      " models trained on a m uch smaller training set. I t would be like selecting the fastest\n",
      " sprin ter to participa te in a mara thon. One wa y to solve this problem is to perform\n",
      " repea ted \n",
      " cr o s s-v a l i d a t i o n\n",
      " , using man y small valida tion sets. Each model is evalua ted\n",
      " once per valida tion set, after it is trained on the rest of the da ta. By a veraging out all\n",
      " the evalua tions of a model, we get a m uch more accura te measure of its performance.\n",
      " H owever , there is a dra wback: the training time is m ultiplied by the n umber of validaƒ\n",
      "tion sets.\n",
      " 32  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "11\n",
      " — The Lack of A Priori Distinctions B etween Learning Algorithms, – D . W olpert (1996).\n",
      "Data Mismatch\n",
      " In some cases, it is easy to get a large amoun t of da ta for training, but it is not perƒ\n",
      " fectly represen ta tive of the da ta tha t will be used in production. F or exam ple, suppose\n",
      " you wan t to crea te a mobile a pp to take pictures of flowers and a utoma tically deterƒ\n",
      " mine their species. Y ou can easily download millions of pictures of flowers on the\n",
      " web , but they won ‡ t be perfectly represen ta tive of the pictures tha t will actually be\n",
      " taken using the a pp on a mobile device. P erha ps you only ha ve 10,000 represen ta tive\n",
      " pictures (i.e., actually taken with the a pp). In this case, the most im portan t rule to\n",
      " remember is tha t the valida tion set and the test m ust be as represen ta tive as possible\n",
      " of the da ta you expect to use in production, so they should be com posed exclusively\n",
      " of represen ta tive pictures: you can sh uffle them and put half in the valida tion set, and\n",
      " half in the test set (making sure tha t no duplica tes or near -duplica tes end up in both\n",
      " sets). After training your model on the web pictures, if you obser ve tha t the perforƒ\n",
      " mance of your model on the valida tion set is disa ppoin ting, you will not know\n",
      " whether this is beca use your model has overfit the training set, or whether this is just\n",
      " due to the misma tch between the web pictures and the mobile a pp pictures. One solƒ\n",
      " ution is to hold out part of the training pictures (from the web) in yet another set tha t\n",
      " Andrew N g calls the \n",
      " t r a i n-d e v s e t\n",
      ". After the model is trained (on the training set, \n",
      " n o t\n",
      " on the train-dev set), you can evalua te it on the train-dev set: if it performs well, then\n",
      " the model is not overfitting the training set, so if performs poorly on the valida tion\n",
      " set, the problem m ust come from the da ta misma tch. Y ou can tr y to tackle this probƒ\n",
      " lem by preprocessing the web images to make them look more like the pictures tha t\n",
      " will be taken by the mobile a pp , and then retraining the model. Con versely , if the\n",
      " model performs poorly on the train-dev set, then the model m ust ha ve overfit the\n",
      " training set, so you should tr y to sim plif y or regularize the model, get more training\n",
      " da ta and clean up the training da ta, as discussed earlier .\n",
      "No Free Lunch Theorem\n",
      " A model is a sim plified version of the obser va tions. The sim plifica tions are mean t to\n",
      " discard the superfluous details tha t are unlikely to generalize to new instances. H owƒ\n",
      " ever , to decide wha t da ta to discard and wha t da ta to keep , you m ust make \n",
      " as s u m p‡\n",
      " t i o ns\n",
      " . F or exam ple, a linear model makes the assum ption tha t the da ta is\n",
      " fundamen tally linear and tha t the distance between the instances and the straigh t line\n",
      "is just noise, which can safely be ignored.\n",
      "In a \n",
      " famous 1996 pa per\n",
      ",\n",
      "11\n",
      "  Da vid W olpert demonstra ted tha t if you make absolutely\n",
      " no assum ption about the da ta, then there is no reason to prefer one model over an y\n",
      " other . This is called the \n",
      " N o F r e e L u n c h\n",
      "  (NFL) theorem. F or some da tasets the best\n",
      " Testing and Validating  |  33\n",
      "\n",
      " model is a linear model, while for other da tasets it is a neural network. There is no\n",
      " model tha t is \n",
      " a p r i o r i\n",
      "   guaran teed to work better (hence the name of the theorem). The\n",
      " only wa y to know for sure which model is best is to evalua te them all. Since this is not\n",
      " possible, in practice you make some reasonable assum ptions about the da ta and you\n",
      " evalua te only a few reasonable models. F or exam ple, for sim ple tasks you ma y evaluƒ\n",
      " a te linear models with various levels of regulariza tion, and for a com plex problem you\n",
      " ma y evalua te various neural networks.\n",
      "Exercises\n",
      " In this cha pter we ha ve covered some of the most im portan t concepts in M achine\n",
      " Learning. In the next cha pters we will dive deeper and write more code, but before we\n",
      " do , make sure you know how to answer the following questions:\n",
      "1.\n",
      " H ow would you define M achine Learning?\n",
      "2.\n",
      "Can you name four types of problems where it shines?\n",
      "3.\n",
      " Wha t is a labeled training set?\n",
      "4.\n",
      " Wha t are the two most common super vised tasks?\n",
      "5.\n",
      " Can you name four common unsuper vised tasks?\n",
      "6.\n",
      " Wha t type of M achine Learning algorithm would you use to allow a robot to\n",
      "walk in various unknown terrains?\n",
      "7.\n",
      " Wha t type of algorithm would you use to segmen t your customers in to m ultiple\n",
      "groups?\n",
      "8.\n",
      " W ould you frame the problem of spam detection as a super vised learning probƒ\n",
      " lem or an unsuper vised learning problem?\n",
      "9.\n",
      " Wha t is an online learning system?\n",
      "10.\n",
      " Wha t is out-of-core learning?\n",
      "11.\n",
      " Wha t type of learning algorithm relies on a similarity measure to make predicƒ\n",
      "tions?\n",
      "12.\n",
      " Wha t is the difference between a model parameter and a learning algorithm ‡ s\n",
      " h yperparameter?\n",
      "13.\n",
      " Wha t do model-based learning algorithms search for? Wha t is the most common\n",
      " stra teg y they use to succeed? H ow do they make predictions?\n",
      "14.\n",
      " Can you name four of the main challenges in M achine Learning?\n",
      "15.\n",
      " If your model performs grea t on the training da ta but generalizes poorly to new\n",
      " instances, wha t is ha ppening? Can you name three possible solutions?\n",
      "16.\n",
      " Wha t is a test set and wh y would you wan t to use it?\n",
      " 34  |  Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "17.\n",
      " Wha t is the purpose of a valida tion set?\n",
      "18.\n",
      " Wha t can go wrong if you tune h yperparameters using the test set?\n",
      "19.\n",
      " Wha t is repea ted cross-valida tion and wh y would you prefer it to using a single\n",
      " valida tion set?\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " Exercises  |  35\n",
      "\n",
      "\n",
      "1\n",
      " The exam ple project is com pletely fictitious; the goal is just to illustra te the main steps of a M achine Learning\n",
      " project, not to learn an ything about the real esta te business.\n",
      "CHAPTER 2\n",
      "End-to-End Machine Learning Project\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 2 in the final\n",
      "release of the book.\n",
      "In \n",
      " this cha pter , you will go through an exam ple project end to end, pretending to be a\n",
      " recen tly hired da ta scien tist in a real esta te com pan y .\n",
      "1\n",
      "   H ere are the main steps you will\n",
      "go through:\n",
      "1.\n",
      " Look a t the big picture.\n",
      "2.\n",
      " Get the da ta.\n",
      "3.\n",
      " Discover and visualize the da ta to gain insigh ts.\n",
      "4.\n",
      " Prepare the da ta for M achine Learning algorithms.\n",
      "5.\n",
      "Select a model and train it.\n",
      "6.\n",
      "Fine-tune your model.\n",
      "7.\n",
      " Presen t your solution.\n",
      "8.\n",
      " La unch, monitor , and main tain your system.\n",
      "37\n",
      "\n",
      "2\n",
      " The original da taset a ppeared in R. K elley P ace and Ronald Barr y , — Sparse Spa tial A utoregressions, – \n",
      " S t a t i s t i cs\n",
      " & P r o b a b i l i ty L e tt er s\n",
      "  33, no . 3 (1997): 291−297.\n",
      "Working with Real Data\n",
      "When \n",
      " you are learning about M achine Learning it is best to actually experimen t with\n",
      " real-world da ta, not just artificial da tasets. F ortuna tely , there are thousands of open\n",
      " da tasets to choose from, ranging across all sorts of domains. H ere are a few places\n",
      " you can look to get da ta:\n",
      "⁄\n",
      " P opular open da ta repositories:\n",
      "›\n",
      " UC Ir vine M achine Learning Repositor y\n",
      "›\n",
      " Kaggle da tasets\n",
      "›\n",
      " Amazon ‡ s A W S da tasets\n",
      "⁄\n",
      " M eta portals (they list open da ta repositories):\n",
      "›\n",
      " h ttp://d a t a p o r t a l s.o r g/\n",
      "›\n",
      " h ttp://o p en d a t a m o n i t o r .eu/\n",
      "›\n",
      " h ttp://q u a n d l.c o m/\n",
      "⁄\n",
      " Other pages listing man y popular open da ta repositories:\n",
      "›\n",
      " W ikipedia ‡ s list of M achine Learning da tasets\n",
      "›\n",
      "Quora.com question\n",
      "›\n",
      " Da tasets subreddit\n",
      " In this cha pter we chose the California H ousing Prices da taset from the Sta tLib reposƒ\n",
      " itor y\n",
      "2\n",
      " (see \n",
      "Figure 2-1\n",
      " ). This da taset was based on da ta from the 1990 California cenƒ\n",
      " sus. I t is not exactly recen t (you could still afford a nice house in the Ba y Area a t the\n",
      " time), but it has man y qualities for learning, so we will pretend it is recen t da ta. W e\n",
      " also added a ca tegorical a ttribute and removed a few fea tures for teaching purposes.\n",
      " 38  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " F i g u r e 2-1. C a l i f o r n i a h o u s i n g p r i c e s\n",
      "Look at the Big Picture\n",
      " W elcome to M achine Learning H ousing Corpora tion! The first task you are asked to\n",
      "perform is to build a model of housing prices in California using the California cenƒ\n",
      " sus da ta. This da ta has metrics such as the popula tion, median income, median housƒ\n",
      "ing price, and so on for each block group in California. Block groups are the smallest\n",
      " geogra phical unit for which the US Census Burea u publishes sam ple da ta (a block\n",
      " group typically has a popula tion of 600 to 3,000 people). W e will just call them — disƒ\n",
      " tricts – for short.\n",
      " Y our model should learn from this da ta and be able to predict the median housing\n",
      " price in an y district, given all the other metrics.\n",
      " Since you are a well-organized da ta scien tist, the first thing you do\n",
      " is to pull out your M achine Learning project checklist. Y ou can\n",
      "start with the one in \n",
      "???\n",
      "; it should work reasonably well for most\n",
      " M achine Learning projects but make sure to ada pt it to your needs.\n",
      " In this cha pter we will go through man y checklist items, but we will\n",
      " also skip a few , either beca use they are self-explana tor y or beca use\n",
      " they will be discussed in la ter cha pters.\n",
      "Frame the Problem\n",
      "The \n",
      " first question to ask your boss is wha t exactly is the business objective; building a\n",
      " model is probably not the end goal. H ow does the com pan y expect to use and benefit\n",
      " Look at the Big Picture  |  39\n",
      "\n",
      "3\n",
      " A piece of informa tion fed to a M achine Learning system is often called a \n",
      " s i g n a l\n",
      "  in reference to Shannon ‡ s\n",
      " informa tion theor y : you wan t a high signal/noise ra tio .\n",
      " from this model? This is im portan t beca use it will determine how you frame the\n",
      " problem, wha t algorithms you will select, wha t performance measure you will use to\n",
      " evalua te your model, and how m uch effort you should spend tweaking it.\n",
      " Y our boss answers tha t your model ‡ s output (a prediction of a district ‡ s median housƒ\n",
      " ing price) will be fed to another M achine Learning system (see \n",
      "Figure 2-2\n",
      "), along\n",
      " with man y other \n",
      " s i g n a l s\n",
      ".\n",
      "3\n",
      " \n",
      "This downstream system will determine whether it is worth\n",
      " in vesting in a given area or not. Getting this righ t is critical, as it directly affects reveƒ\n",
      " n ue.\n",
      " F i g u r e 2-2. A M a c h i n e L e a r n i n g p i p e l i n e f o r r e a l e s t a t e i n v e s t m en ts\n",
      "Pipelines\n",
      " A sequence of da ta processing \n",
      " c o m p o n en ts\n",
      "  is called a da ta \n",
      " p i p e l i n e\n",
      " . Pipelines are ver y\n",
      " common in M achine Learning systems, since there is a lot of da ta to manipula te and\n",
      " man y da ta transforma tions to a pply .\n",
      " Com ponen ts typically run asynchronously . Each com ponen t pulls in a large amoun t\n",
      " of da ta, processes it, and spits out the result in another da ta store, and then some time\n",
      " la ter the next com ponen t in the pipeline pulls this da ta and spits out its own output,\n",
      " and so on. Each com ponen t is fairly self-con tained: the in terface between com ponen ts\n",
      " is sim ply the da ta store. This makes the system quite sim ple to grasp (with the help of\n",
      " a da ta flow gra ph), and differen t teams can focus on differen t com ponen ts. M oreover ,\n",
      " if a com ponen t breaks down, the downstream com ponen ts can often con tin ue to run\n",
      " normally (a t least for a while) by just using the last output from the broken com poƒ\n",
      " nen t. This makes the architecture quite robust.\n",
      " 40  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " On the other hand, a broken com ponen t can go unnoticed for some time if proper\n",
      " monitoring is not im plemen ted. The da ta gets stale and the overall system ‡ s perforƒ\n",
      "mance drops.\n",
      " The next question to ask is wha t the curren t solution looks like (if an y). I t will often\n",
      " give you a reference performance, as well as insigh ts on how to solve the problem.\n",
      " Y our boss answers tha t the district housing prices are curren tly estima ted man ually\n",
      " by experts: a team ga thers up-to-da te informa tion about a district, and when they\n",
      " cannot get the median housing price, they estima te it using com plex rules.\n",
      " This is costly and time-consuming, and their estima tes are not grea t; in cases where\n",
      " they manage to find out the actual median housing price, they often realize tha t their\n",
      " estima tes were off by more than 20%. This is wh y the com pan y thinks tha t it would\n",
      " be useful to train a model to predict a district ‡ s median housing price given other da ta\n",
      " about tha t district. The census da ta looks like a grea t da taset to exploit for this purƒ\n",
      "pose, since it includes the median housing prices of thousands of districts, as well as\n",
      " other da ta.\n",
      " Oka y , with all this informa tion you are now ready to start designing your system.\n",
      " First, you need to frame the problem: is it super vised, unsuper vised, or Reinforceƒ\n",
      " men t Learning? I s it a classifica tion task, a regression task, or something else? Should\n",
      " you use ba tch learning or online learning techniques? B efore you read on, pa use and\n",
      " tr y to answer these questions for yourself.\n",
      " H a ve you found the answers? Let ‡ s see: it is clearly a typical super vised learning task\n",
      "since you are \n",
      "given \n",
      " l a b e l e d\n",
      " \n",
      " training exam ples (each instance comes with the expected\n",
      " output, i.e., the district ‡ s median housing price). M oreover , it is also a typical regresƒ\n",
      " sion task, since you are asked to predict a value. M ore specifically , this is a \n",
      " m u l t i p l e\n",
      " r e g r e s s i o n\n",
      " \n",
      " problem since the system will use m ultiple fea tures to make a prediction (it\n",
      " will use the district ‡ s popula tion, the median income, etc.). I t is also a \n",
      " u n i v a r i a t e\n",
      " r e g r e s s i o n\n",
      "  problem since we are only tr ying to predict a single value for each district.\n",
      " If we were tr ying to predict m ultiple values per district, it would be a \n",
      " m u l t i v a r i a t e\n",
      " r e g r e s s i o n\n",
      " \n",
      " problem. Finally , there is no con tin uous flow of da ta coming in the system,\n",
      " there is no particular need to adjust to changing da ta ra pidly , and the da ta is small\n",
      " enough to fit in memor y , so plain ba tch learning should do just fine.\n",
      " If the da ta was h uge, you could either split your ba tch learning\n",
      " work across m ultiple ser vers (using the \n",
      " M a pR e d u c e\n",
      " technique), or\n",
      "you could use an online learning technique instead.\n",
      " Look at the Big Picture  |  41\n",
      "\n",
      "Select a Performance Measure\n",
      " Y our \n",
      "next step is to select a performance measure. A typical performance measure for\n",
      " regression problems is the Root M ean Square Error (RMSE). I t gives an idea of how\n",
      " m uch error the system typically makes in its predictions, with a higher weigh t for\n",
      "large errors. \n",
      " Equa tion 2-1\n",
      "  shows the ma thema tical form ula to com pute the RMSE.\n",
      " Eq u a t i o n 2-1. R o o t M e a n S q u a r e E r r o r (RMS E)\n",
      "RMSE\n",
      "X\n",
      ",\n",
      "h\n",
      "=\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "h\n",
      "x\n",
      "i\n",
      "”\n",
      "y\n",
      "i\n",
      "2\n",
      " 42  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "4\n",
      " Recall tha t the transpose opera tor flips a column vector in to a row vector (and vice versa).\n",
      "Notations\n",
      " This equa tion in troduces several ver y common M achine Learning nota tions tha t we\n",
      "will use throughout this book:\n",
      "⁄\n",
      "m\n",
      "  is the n umber of instances in the da taset you are measuring the RMSE on.\n",
      "›\n",
      " F or exam ple, if you are evalua ting the RMSE on a valida tion set of 2,000 disƒ\n",
      "tricts, then \n",
      "m\n",
      " = 2,000.\n",
      "⁄\n",
      "x\n",
      "(i)\n",
      "  is a vector of all the fea ture values (excluding the label) of the \n",
      "i\n",
      " t h\n",
      " \n",
      "instance in\n",
      " the da taset, and \n",
      "y\n",
      "(i)\n",
      "  is its label (the desired output value for tha t instance).\n",
      "›\n",
      " F or exam ple, if the first district in the da taset is loca ted a t longitude −118.29‘,\n",
      " la titude 33.91‘, and it has 1,416 inhabitan ts with a median income of $38,372,\n",
      " and the median house value is $156,400 (ignoring the other fea tures for now),\n",
      "then:\n",
      "x\n",
      "1\n",
      "=\n",
      " ” 118 . 29\n",
      " 33 . 91\n",
      " 1, 416\n",
      " 38, 372\n",
      "and:\n",
      "y\n",
      "1\n",
      " = 156, 400\n",
      "⁄\n",
      "X\n",
      " \n",
      " is a ma trix con taining all the fea ture values (excluding labels) of all instances in\n",
      " the da taset. There is one row per instance and the \n",
      "i\n",
      " t h\n",
      "   row is equal to the transpose\n",
      "of \n",
      "x\n",
      "(i)\n",
      ", noted (\n",
      "x\n",
      "(i)\n",
      ")\n",
      "T\n",
      ".\n",
      "4\n",
      "›\n",
      " F or exam ple, if the first district is as just described, then the ma trix \n",
      "X\n",
      " \n",
      "looks\n",
      "like this:\n",
      "X\n",
      "=\n",
      "x\n",
      "1\n",
      "T\n",
      "x\n",
      "2\n",
      "T\n",
      "x\n",
      "1999\n",
      "T\n",
      "x\n",
      "2000\n",
      "T\n",
      "=\n",
      " ” 118 . 29 33 . 91 1, 416 38, 372\n",
      "\n",
      " Look at the Big Picture  |  43\n",
      "\n",
      "⁄\n",
      "h\n",
      " \n",
      " is your system ‡ s prediction function, also called \n",
      "a \n",
      " h y p o t h e s i s\n",
      ". When your system\n",
      " is given an instance ‡ s fea ture vector \n",
      "x\n",
      "(i)\n",
      ", it outputs a predicted value \n",
      "ƒ\n",
      "(i)\n",
      " \n",
      "= \n",
      "h\n",
      "(\n",
      "x\n",
      "(i)\n",
      ")\n",
      " for tha t instance (\n",
      "ƒ\n",
      "  is pronounced —y-ha t –).\n",
      "›\n",
      " F or exam ple, if your system predicts tha t the median housing price in the first\n",
      "district is $158,400, then \n",
      "ƒ\n",
      "(1)\n",
      " = \n",
      "h\n",
      "(\n",
      "x\n",
      "(1)\n",
      ") = 158,400. The prediction error for this\n",
      "district is \n",
      "ƒ\n",
      "(1)\n",
      " − \n",
      "y\n",
      "(1)\n",
      " = 2,000.\n",
      "⁄\n",
      "RMSE(\n",
      "X\n",
      ",\n",
      "h\n",
      " ) is the cost function measured on the set of exam ples using your\n",
      " h ypothesis \n",
      "h\n",
      ".\n",
      " W e use lowercase italic fon t for scalar values (such as \n",
      "m\n",
      " or \n",
      "y\n",
      "(i)\n",
      ") and function names\n",
      "(such as \n",
      "h\n",
      " ), lowercase bold fon t for vectors (such as \n",
      "x\n",
      "(i)\n",
      " ), and uppercase bold fon t for\n",
      " ma trices (such as \n",
      "X\n",
      ").\n",
      " E ven though the RMSE is generally the preferred performance measure for regression\n",
      " tasks, in some con texts you ma y prefer to use another function. F or exam ple, suppose\n",
      " tha t there are man y outlier districts. In tha t case, you ma y consider using \n",
      "the \n",
      " M e a n\n",
      " Ab s o l u t e E r r o r\n",
      "  (also called the A verage A bsolute Devia tion; see \n",
      " Equa tion 2-2\n",
      "):\n",
      " Eq u a t i o n 2-2. M e a n Ab s o l u t e E r r o r\n",
      "MAE\n",
      "X\n",
      ",\n",
      "h\n",
      "=\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "h\n",
      "x\n",
      "i\n",
      "”\n",
      "y\n",
      "i\n",
      " B oth the RMSE and the MAE are wa ys to measure the distance between two vectors:\n",
      " the vector of predictions and the vector of target values. V arious distance measures,\n",
      "or \n",
      " n o r ms\n",
      ", are possible:\n",
      "⁄\n",
      " Com puting the root of a sum of squares (RMSE) corresponds to the \n",
      " E u c l i d e a n\n",
      " n o r m\n",
      " : it is the notion of distance you are familiar with. I t is also called the …\n",
      "2\n",
      " n o r m\n",
      ", noted \n",
      " ’ \n",
      "2\n",
      " (or just \n",
      " ’ \n",
      ").\n",
      "⁄\n",
      " Com puting the sum of absolutes (MAE) corresponds to the …\n",
      "1\n",
      " \n",
      " n o r m\n",
      " ,  noted \n",
      " ’ \n",
      "1\n",
      ".\n",
      " I t is sometimes called the \n",
      " M a n h a tt a n n o r m\n",
      "  beca use it measures the distance\n",
      " between two poin ts in a city if you can only tra vel along orthogonal city blocks.\n",
      "⁄\n",
      " M ore generally , the …\n",
      "k\n",
      " \n",
      " n o r m\n",
      " of a vector \n",
      "v\n",
      "  con taining \n",
      "n\n",
      " \n",
      " elemen ts is defined as\n",
      "\n",
      "k\n",
      "=\n",
      "v\n",
      "0\n",
      "k\n",
      "+\n",
      "v\n",
      "1\n",
      "k\n",
      "+\n",
      "+\n",
      "v\n",
      "n\n",
      "k\n",
      "1\n",
      "k\n",
      ". …\n",
      "0\n",
      "  just gives the n umber of non-zero eleƒ\n",
      " men ts in the vector , and …\n",
      "‚\n",
      "  gives the maxim um absolute value in the vector .\n",
      "⁄\n",
      "The higher the norm index, the more it focuses on large values and neglects small\n",
      " ones. This is wh y the RMSE is more sensitive to outliers than the MAE. But when\n",
      " 44  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "5\n",
      " The la test version of Python 3 is recommended. Python 2.7+ ma y work too , but it is now depreca ted, all ma jor\n",
      " scien tific libraries are dropping support for it, so you should migra te to Python 3 as soon as possible.\n",
      " outliers are exponen tially rare (like in a bell-sha ped cur ve), the RMSE performs\n",
      " ver y well and is generally preferred.\n",
      "Check the Assumptions\n",
      " Lastly , it is good practice to list and verif y the assum ptions tha t were made so far (by\n",
      " you or others); this can ca tch serious issues early on. F or exam ple, the district prices\n",
      " tha t your system outputs are going to be fed in to a downstream M achine Learning\n",
      " system, and we assume tha t these prices are going to be used as such. But wha t if the\n",
      " downstream system actually con verts the prices in to ca tegories (e.g., — chea p , –\n",
      " — medium, – or — expensive –) and then uses those ca tegories instead of the prices themƒ\n",
      " selves? In this case, getting the price perfectly righ t is not im portan t a t all; your sysƒ\n",
      " tem just needs to get the ca tegor y righ t. If tha t ‡ s so , then the problem should ha ve\n",
      " been framed as a classifica tion task, not a regression task. Y ou don ‡ t wan t to find this\n",
      " out after working on a regression system for mon ths.\n",
      " F ortuna tely , after talking with the team in charge of the downstream system, you are\n",
      " confiden t tha t they do indeed need the actual prices, not just ca tegories. Grea t! Y ou ‡ re\n",
      " all set, the ligh ts are green, and you can start coding now!\n",
      "Get the Data\n",
      " I t ‡ s time to get your hands dirty . Don ‡ t hesita te to pick up your la ptop and walk\n",
      " through the following code exam ples in a J upyter notebook. The full J upyter noteƒ\n",
      " book is a vailable a t \n",
      " h ttp s://g i t h u b .c o m/a ger o n/h a n d s o n-m l2\n",
      ".\n",
      "Create the Workspace\n",
      " First you will need to ha ve Python installed. I t is probably already installed on your\n",
      " system. If not, you can get it a t \n",
      " h ttp s://w w w .p y t h o n.o r g/\n",
      ".\n",
      "5\n",
      " N ext you need to crea te a workspace director y for your M achine Learning code and\n",
      " da tasets. Open a terminal and type the following commands (after the \n",
      "$\n",
      "  prom pts):\n",
      "$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\n",
      "$ mkdir -p $ML_PATH\n",
      " Y ou will need a n umber of Python modules: J upyter , N umPy , P andas, M a tplotlib , and\n",
      " Scikit-Learn. If you already ha ve J upyter running with all these modules installed,\n",
      "you can safely skip to \n",
      " —Download the Da ta – on page \n",
      "49\n",
      " . If you don ‡ t ha ve them yet,\n",
      " there are man y wa ys to install them (and their dependencies). Y ou can use your sysƒ\n",
      " Get the Data  |  45\n",
      "\n",
      "6\n",
      " W e will show the installa tion steps using pip in a bash shell on a Lin ux or M acOS system. Y ou ma y need to\n",
      " ada pt these commands to your own system. On W indows, we recommend installing Anaconda instead.\n",
      "7\n",
      " If you wan t to upgrade pip for all users on your machine ra ther than just your own user , you should remove\n",
      "the \n",
      "--user\n",
      "  option and make sure you ha ve administra tor righ ts (e.g., by adding \n",
      "sudo\n",
      " before the whole comƒ\n",
      " mand on Lin ux or M acOSX).\n",
      "8\n",
      " Alterna tive tools include ven v (ver y similar to virtualen v and included in the standard librar y), virtualen vƒ\n",
      " wra pper (provides extra functionalities on top of virtualen v), pyen v (allows easy switching between Python\n",
      " versions), and pipen v (a grea t packaging tool by the same a uthor as the popular \n",
      "requests\n",
      "  librar y , built on top\n",
      " of pip , virtualen v and more).\n",
      " tem ‡ s packaging system (e.g., a pt-get on Ubun tu, or M acP orts or H omeBrew on\n",
      " M acOS), install a Scien tific Python distribution such as Anaconda \n",
      "and use its packagƒ\n",
      " ing system, or just use Python ‡ s own packaging system, pip , which is included by\n",
      " defa ult with the Python binar y installers (since Python 2.7.9).\n",
      "6\n",
      " \n",
      " Y ou can check to see if\n",
      "pip is installed by typing the following command:\n",
      "$ python3 -m pip --version\n",
      "pip 19.0.2 from [...]/lib/python3.6/site-packages (python 3.6)\n",
      " Y ou should make sure you ha ve a recen t version of pip installed. T o upgrade the pip\n",
      "module, type:\n",
      "7\n",
      "$ python3 -m pip install --user -U pip\n",
      "Collecting pip\n",
      "[...]\n",
      "Successfully installed pip-19.0.2\n",
      "Creating an Isolated Environment\n",
      " If you would like to work in an isola ted en vironmen t (which is strongly recomƒ\n",
      " mended so you can work on differen t projects without ha ving conflicting librar y verƒ\n",
      " sions), install virtualen v\n",
      "8\n",
      "  by running the following pip command (again, if you wan t\n",
      " virtualen v to be installed for all users on your machine, remove \n",
      "--user\n",
      " \n",
      "and run this\n",
      " command with administra tor righ ts):\n",
      "$ python3 -m pip install --user -U virtualenv\n",
      "Collecting virtualenv\n",
      "[...]\n",
      "Successfully installed virtualenv\n",
      " N ow you can crea te an isola ted Python en vironmen t by typing:\n",
      "$ cd $ML_PATH\n",
      "$ virtualenv env\n",
      "Using base prefix •[...]•\n",
      "New python executable in [...]/ml/env/bin/python3.6\n",
      "Also creating executable in [...]/ml/env/bin/python\n",
      "Installing setuptools, pip, wheel...done.\n",
      " 46  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "9\n",
      " N ote tha t J upyter can handle m ultiple versions of Python, and even man y other languages such as R or\n",
      " Octa ve.\n",
      " N ow ever y time you wan t to activa te this en vironmen t, just open a terminal and type:\n",
      "$ cd $ML_PATH\n",
      "$ source env/bin/activate # on Linux or MacOSX\n",
      "$ .\\env\\Scripts\\activate  # on Windows\n",
      " T o deactiva te this en vironmen t, just type \n",
      "deactivate\n",
      " . While the en vironmen t is\n",
      " active, an y package you install using pip will be installed in this isola ted en vironmen t,\n",
      " and Python will only ha ve access to these packages (if you also wan t access to the sysƒ\n",
      " tem ‡ s packages, you should crea te the en vironmen t using virtualen v‡ s \n",
      "--system-site-\n",
      "packages\n",
      "  option). Check out virtualen v‡ s documen ta tion for more informa tion.\n",
      " N ow you can install all the required modules and their dependencies using this simƒ\n",
      " ple pip command (if you are not using a virtualen v , you will need the \n",
      "--user\n",
      " \n",
      "option\n",
      " or administra tor righ ts):\n",
      "$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl\n",
      "Collecting matplotlib\n",
      "  [...]\n",
      " T o check your installa tion, tr y to im port ever y module like this:\n",
      "$ python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"\n",
      " There should be no output and no error . N ow you can fire up J upyter by typing:\n",
      "$ jupyter notebook\n",
      "[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml\n",
      "[I 15:24 NotebookApp] 0 active kernels\n",
      "[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n",
      "[I 15:24 NotebookApp] Use Control-C to stop this server and shut down all\n",
      "kernels (twice to skip confirmation).\n",
      " A J upyter ser ver is now running in your terminal, listening to port 8888. Y ou can visit\n",
      " this ser ver by opening your web browser to \n",
      " h ttp://l o c a l h o s t:8888/\n",
      "  (this usually ha pƒ\n",
      " pens a utoma tically when the ser ver starts). Y ou should see your em pty workspace\n",
      " director y (con taining only the \n",
      " en v\n",
      "  director y if you followed the preceding virtualen v\n",
      "instructions).\n",
      " N ow crea te a new Python notebook by clicking on the N ew button and selecting the\n",
      " a ppropria te Python version\n",
      "9\n",
      " (see \n",
      "Figure 2-3\n",
      ").\n",
      " This does three things: first, it crea tes a new notebook file called \n",
      " U n t i t l e d.i p y n b\n",
      " \n",
      "in\n",
      " your workspace; second, it starts a J upyter Python kernel to run this notebook; and\n",
      " Get the Data  |  47\n",
      "\n",
      " third, it opens this notebook in a new tab . Y ou should start by renaming this noteƒ\n",
      " book to —H ousing – (this will a utoma tically rename the file to \n",
      " H o u s i n g .i p y n b\n",
      ") by clickƒ\n",
      " ing U n titled and typing the new name.\n",
      " F i g u r e 2-3. Y o u r w o r ks p a c e i n J u p y t er\n",
      " A notebook con tains a list of cells. Each cell can con tain executable code or forma tted\n",
      " text. Righ t now the notebook con tains only one em pty code cell, labeled —In [1]:– . T r y\n",
      "typing \n",
      "print(\"Hello world!\")\n",
      "  in the cell, and click on the pla y button (see\n",
      "Figure 2-4\n",
      " ) or press Shift-En ter . This sends the curren t cell to this notebook ‡ s Python\n",
      " kernel, which runs it and returns the output. The result is displa yed below the cell,\n",
      " and since we reached the end of the notebook, a new cell is a utoma tically crea ted. Go\n",
      " through the U ser In terface T our from J upyter‡ s H elp men u to learn the basics.\n",
      " F i g u r e 2-4. H e l l o w o r l d P y t h o n n o t e b o o k\n",
      " 48  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "10\n",
      " Y ou migh t also need to check legal constrain ts, such as priva te fields tha t should never be copied to unsafe\n",
      " da tastores.\n",
      "11\n",
      " In a real project you would sa ve this code in a Python file, but for now you can just write it in your J upyter\n",
      "notebook.\n",
      "Download the Data\n",
      " In typical en vironmen ts your da ta would be a vailable in a rela tional da tabase (or\n",
      " some other common da tastore) and spread across m ultiple tables/documen ts/files. T o\n",
      " access it, you would first need to get your creden tials and access a uthoriza tions,\n",
      "10\n",
      "   and\n",
      " familiarize yourself with the da ta schema. In this project, however , things are m uch\n",
      " sim pler : you will just download a single com pressed file, \n",
      " h o u s i n g .t gz\n",
      " , which con tains a\n",
      " comma-separa ted value (CSV) file called \n",
      " h o u s i n g .cs v\n",
      "  with all the da ta.\n",
      " Y ou could use your web browser to download it, and run \n",
      "tar xzf housing.tgz\n",
      " \n",
      "to\n",
      " decom press the file and extract the CSV file, but it is preferable to crea te a small funcƒ\n",
      " tion to do tha t. I t is useful in particular if da ta changes regularly , as it allows you to\n",
      " write a small script tha t you can run whenever you need to fetch the la test da ta (or\n",
      " you can set up a scheduled job to do tha t a utoma tically a t regular in ter vals). A utoƒ\n",
      " ma ting the process of fetching the da ta is also useful if you need to install the da taset\n",
      " on m ultiple machines.\n",
      " H ere is the function to fetch the da ta:\n",
      "11\n",
      "import\n",
      " \n",
      "os\n",
      "import\n",
      " \n",
      "tarfile\n",
      "from\n",
      " \n",
      "six.moves\n",
      " \n",
      "import\n",
      " \n",
      "urllib\n",
      "DOWNLOAD_ROOT\n",
      " \n",
      "=\n",
      " \n",
      "\"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH\n",
      " \n",
      "=\n",
      " \n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "join\n",
      "(\n",
      "\"datasets\"\n",
      ",\n",
      " \n",
      "\"housing\"\n",
      ")\n",
      "HOUSING_URL\n",
      " \n",
      "=\n",
      " \n",
      "DOWNLOAD_ROOT\n",
      " \n",
      "+\n",
      " \n",
      "\"datasets/housing/housing.tgz\"\n",
      "def\n",
      " \n",
      "fetch_housing_data\n",
      "(\n",
      "housing_url\n",
      "=\n",
      "HOUSING_URL\n",
      ",\n",
      " \n",
      "housing_path\n",
      "=\n",
      "HOUSING_PATH\n",
      "):\n",
      "    \n",
      "if\n",
      " \n",
      "not\n",
      " \n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "isdir\n",
      "(\n",
      "housing_path\n",
      "):\n",
      "        \n",
      "os\n",
      ".\n",
      "makedirs\n",
      "(\n",
      "housing_path\n",
      ")\n",
      "    \n",
      "tgz_path\n",
      " \n",
      "=\n",
      " \n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "join\n",
      "(\n",
      "housing_path\n",
      ",\n",
      " \n",
      "\"housing.tgz\"\n",
      ")\n",
      "    \n",
      "urllib\n",
      ".\n",
      "request\n",
      ".\n",
      "urlretrieve\n",
      "(\n",
      "housing_url\n",
      ",\n",
      " \n",
      "tgz_path\n",
      ")\n",
      "    \n",
      "housing_tgz\n",
      " \n",
      "=\n",
      " \n",
      "tarfile\n",
      ".\n",
      "open\n",
      "(\n",
      "tgz_path\n",
      ")\n",
      "    \n",
      "housing_tgz\n",
      ".\n",
      "extractall\n",
      "(\n",
      "path\n",
      "=\n",
      "housing_path\n",
      ")\n",
      "    \n",
      "housing_tgz\n",
      ".\n",
      "close\n",
      "()\n",
      " N ow when you call \n",
      "fetch_housing_data()\n",
      " , it crea tes a \n",
      " d a t as e ts/h o u s i n g\n",
      " \n",
      " director y in\n",
      "your workspace, downloads the \n",
      " h o u s i n g .t gz\n",
      "   file, and extracts the \n",
      " h o u s i n g .cs v\n",
      "   from it in\n",
      " this director y .\n",
      " N ow let ‡ s load the da ta using P andas. Once again you should write a small function to\n",
      " load the da ta:\n",
      " Get the Data  |  49\n",
      "\n",
      "import\n",
      " \n",
      "pandas\n",
      " \n",
      "as\n",
      " \n",
      "pd\n",
      "def\n",
      " \n",
      "load_housing_data\n",
      "(\n",
      "housing_path\n",
      "=\n",
      "HOUSING_PATH\n",
      "):\n",
      "    \n",
      "csv_path\n",
      " \n",
      "=\n",
      " \n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "join\n",
      "(\n",
      "housing_path\n",
      ",\n",
      " \n",
      "\"housing.csv\"\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "pd\n",
      ".\n",
      "read_csv\n",
      "(\n",
      "csv_path\n",
      ")\n",
      " This function returns a P andas Da taFrame object con taining all the da ta.\n",
      "Take a Quick Look at the Data Structure\n",
      " Let ‡ s take a look a t the top five rows using the Da taFrame ‡ s \n",
      "head()\n",
      " \n",
      "method (see\n",
      "Figure 2-5\n",
      ").\n",
      " F i g u r e 2-5. T o p \n",
      "†ve\n",
      "  r o ws i n t h e d a t as e t\n",
      " Each row represen ts one district. There are 10 a ttributes (you can see the first 6 in the\n",
      "screenshot): \n",
      "longitude\n",
      ", \n",
      "latitude\n",
      ", \n",
      "housing_median_age\n",
      ", \n",
      "total_rooms\n",
      ", \n",
      "total_bed\n",
      "rooms\n",
      ", \n",
      "population\n",
      ", \n",
      "households\n",
      ", \n",
      "median_income\n",
      ", \n",
      "median_house_value\n",
      ", and\n",
      "ocean_proximity\n",
      ".\n",
      "The \n",
      "info()\n",
      "  method is useful to get a quick description of the da ta, in particular the\n",
      " total n umber of rows, and each a ttribute ‡ s type and n umber of non-n ull values (see\n",
      "Figure 2-6\n",
      ").\n",
      " 50  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " F i g u r e 2-6. H o u s i n g i n f o\n",
      " There are 20,640 instances in the da taset, which means tha t it is fairly small by\n",
      " M achine Learning standards, but it ‡ s perfect to get started. N otice tha t the \n",
      "total_bed\n",
      "rooms\n",
      "  a ttribute has only 20,433 non-n ull values, meaning tha t 207 districts are missƒ\n",
      " ing this fea ture. W e will need to take care of this la ter .\n",
      " All a ttributes are n umerical, except the \n",
      "ocean_proximity\n",
      "  field. I ts type is \n",
      "object\n",
      ", so it\n",
      " could hold an y kind of Python object, but since you loaded this da ta from a CSV file\n",
      " you know tha t it m ust be a text a ttribute. When you looked a t the top five rows, you\n",
      " probably noticed tha t the values in the \n",
      "ocean_proximity\n",
      " \n",
      "column were repetitive,\n",
      " which means tha t it is probably a ca tegorical a ttribute. Y ou can find out wha t ca teƒ\n",
      " gories exist and how man y districts belong to each ca tegor y by using the\n",
      "value_counts()\n",
      " method:\n",
      ">>> \n",
      "housing\n",
      "[\n",
      "\"ocean_proximity\"\n",
      "]\n",
      ".\n",
      "value_counts\n",
      "()\n",
      "<1H OCEAN     9136\n",
      "INLAND        6551\n",
      "NEAR OCEAN    2658\n",
      "NEAR BAY      2290\n",
      "ISLAND           5\n",
      "Name: ocean_proximity, dtype: int64\n",
      " Let ‡ s look a t the other fields. The \n",
      "describe()\n",
      " \n",
      " method shows a summar y of the\n",
      " n umerical a ttributes (\n",
      "Figure 2-7\n",
      ").\n",
      " Get the Data  |  51\n",
      "\n",
      "12\n",
      " The standard devia tion is generally denoted ™ (the Greek letter sigma), and it is the square root of the \n",
      " v a r‡\n",
      " i a n c e\n",
      " , which is the a verage of the squared devia tion from the mean. When a fea ture has a bell-sha ped \n",
      " n o r m a l\n",
      " d i s t r i b u t i o n\n",
      " (also called a \n",
      " G a u s s i a n d i s t r i b u t i o n\n",
      " ), which is ver y common, the —68-95-99.7– rule a pplies: about\n",
      "68% of the values fall within 1™ of the mean, 95% within 2™, and 99.7% within 3™.\n",
      " F i g u r e 2-7. S u m m a r y o f e a c h n u m er i c a l a tt r i b u t e\n",
      "The \n",
      "count\n",
      ", \n",
      "mean\n",
      ", \n",
      "min\n",
      ", and \n",
      "max\n",
      " \n",
      " rows are self-explana tor y . N ote tha t the n ull values are\n",
      " ignored (so , for exam ple, \n",
      "count\n",
      " of \n",
      "total_bedrooms\n",
      " is 20,433, not 20,640). The \n",
      "std\n",
      "row shows the \n",
      " s t a n d a r d d e v i a t i o n\n",
      ", which measures how dispersed the values are.\n",
      "12\n",
      "The 25%, 50%, and 75% rows show the corresponding \n",
      " p er c en t i l e s\n",
      " : a percen tile indiƒ\n",
      " ca tes the value below which a given percen tage of obser va tions in a group of obser vaƒ\n",
      " tions falls. F or exam ple, 25% of the districts ha ve a \n",
      "housing_median_age\n",
      " \n",
      "lower than\n",
      "18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\n",
      "25\n",
      "th\n",
      "  percen tile (or 1\n",
      "st\n",
      " \n",
      " q u a r t i l e\n",
      "), the median, and the 75\n",
      "th\n",
      "  percen tile (or 3\n",
      "rd\n",
      " quartile).\n",
      " Another quick wa y to get a feel of the type of da ta you are dealing with is to plot a \n",
      " histogram for each n umerical a ttribute. A histogram shows the n umber of instances\n",
      " (on the vertical axis) tha t ha ve a given value range (on the horizon tal axis). Y ou can\n",
      " either plot this one a ttribute a t a time, or you can call the \n",
      "hist()\n",
      " \n",
      "method on the\n",
      " whole da taset, and it will plot a histogram for each n umerical a ttribute (see\n",
      "Figure 2-8\n",
      " ). F or exam ple, you can see tha t sligh tly over 800 districts ha ve a\n",
      "median_house_value\n",
      " equal to about $100,000.\n",
      "%\n",
      "matplotlib\n",
      " \n",
      "inline\n",
      "   \n",
      "# only in a Jupyter notebook\n",
      "import\n",
      " \n",
      "matplotlib.pyplot\n",
      " \n",
      "as\n",
      " \n",
      "plt\n",
      "housing\n",
      ".\n",
      "hist\n",
      "(\n",
      "bins\n",
      "=\n",
      "50\n",
      ",\n",
      " \n",
      "figsize\n",
      "=\n",
      "(\n",
      "20\n",
      ",\n",
      "15\n",
      "))\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " 52  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "The \n",
      "hist()\n",
      "  method relies on M a tplotlib , which in turn relies on a\n",
      " user -specified gra phical backend to dra w on your screen. So before\n",
      " you can plot an ything, you need to specif y which backend M a tplotƒ\n",
      " lib should use. The sim plest option is to use J upyter‡ s magic comƒ\n",
      "mand \n",
      "%matplotlib inline\n",
      " . This tells J upyter to set up M a tplotlib\n",
      " so it uses J upyter‡ s own backend. Plots are then rendered within the\n",
      " notebook itself. N ote tha t calling \n",
      "show()\n",
      "  is optional in a J upyter\n",
      " notebook, as J upyter will a utoma tically displa y plots when a cell is\n",
      "executed.\n",
      " F i g u r e 2-8. A h i s t og r a m f o r e a c h n u m er i c a l a tt r i b u t e\n",
      " N otice a few things in these histograms:\n",
      "1.\n",
      " First, the median income a ttribute does not look like it is expressed in US dollars\n",
      " (USD). After checking with the team tha t collected the da ta, you are told tha t the\n",
      " da ta has been scaled and ca pped a t 15 (actually 15.0001) for higher median\n",
      " incomes, and a t 0.5 (actually 0.4999) for lower median incomes. The n umbers\n",
      " represen t roughly tens of thousands of dollars (e.g., 3 actually means about\n",
      " $30,000). W orking with  preprocessed a ttributes is common in M achine Learning,\n",
      " Get the Data  |  53\n",
      "\n",
      " and it is not necessarily a problem, but you should tr y to understand how the\n",
      " da ta was com puted.\n",
      "2.\n",
      " The housing median age and the median house value were also ca pped. The la tƒ\n",
      " ter ma y be a serious problem since it is your target a ttribute (your labels). Y our\n",
      " M achine Learning algorithms ma y learn tha t prices never go beyond tha t limit.\n",
      " Y ou need to check with your clien t team (the team tha t will use your system ‡ s outƒ\n",
      " put) to see if this is a problem or not. If they tell you tha t they need precise preƒ\n",
      " dictions even beyond $500,000, then you ha ve mainly two options:\n",
      "a.\n",
      " Collect proper labels for the districts whose labels were ca pped.\n",
      " b .\n",
      "Remove those districts from the training set (and also from the test set, since\n",
      " your system should not be evalua ted poorly if it predicts values beyond\n",
      "$500,000).\n",
      "3.\n",
      " These a ttributes ha ve ver y differen t scales. W e will discuss this la ter in this cha pƒ\n",
      " ter when we explore fea ture scaling.\n",
      "4.\n",
      " Finally , man y histograms are \n",
      " t a i l h e a v y\n",
      " : they extend m uch farther to the righ t of\n",
      " the median than to the left. This ma y make it a bit harder for some M achine\n",
      " Learning algorithms to detect pa tterns. W e will tr y transforming these a ttributes\n",
      " la ter on to ha ve more bell-sha ped distributions.\n",
      " H opefully you now ha ve a better understanding of the kind of da ta you \n",
      "are dealing\n",
      "with.\n",
      " W ait! B efore you look a t the da ta an y further , you need to crea te a\n",
      " test set, put it aside, and never look a t it.\n",
      "Create a Test Set\n",
      " I t ma y sound strange to volun tarily set aside part of the da ta a t this stage. After all,\n",
      " you ha ve only taken a quick glance a t the da ta, and surely you should learn a whole\n",
      " lot more about it before you decide wha t algorithms to use, righ t? This is true, but\n",
      " your brain is an amazing pa ttern detection system, which means tha t it is highly\n",
      " prone to  overfitting: if you look a t the test set, you ma y stumble upon some seemingly\n",
      " in teresting pa ttern in the test da ta tha t leads you to select a particular kind of\n",
      " M achine Learning model. When you estima te the generaliza tion error using the test\n",
      " set, your estima te will be too optimistic and you will la unch a system tha t will not\n",
      "perform as well as expected. This is called \n",
      " d a t a s n o o p i n g\n",
      " bias.\n",
      " Crea ting a test set is theoretically quite sim ple: just pick some instances randomly ,\n",
      " typically 20% of the da taset (or less if your da taset is ver y large), and set them aside:\n",
      " 54  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "13\n",
      " In this book, when a code exam ple con tains a mix of code and outputs, as is the case here, it is forma tted like\n",
      " in the Python in terpreter , for better readability : the code lines are prefixed with \n",
      ">>>\n",
      " (or \n",
      "...\n",
      "  for inden ted\n",
      " blocks), and the outputs ha ve no prefix.\n",
      "14\n",
      " Y ou will often see people set the random seed to 42. This n umber has no special property , other than to be\n",
      " The Answer to the Ultima te Question of Life, the U niverse, and E ver ything.\n",
      "import\n",
      " \n",
      "numpy\n",
      " \n",
      "as\n",
      " \n",
      "np\n",
      "def\n",
      " \n",
      "split_train_test\n",
      "(\n",
      "data\n",
      ",\n",
      " \n",
      "test_ratio\n",
      "):\n",
      "    \n",
      "shuffled_indices\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "permutation\n",
      "(\n",
      "len\n",
      "(\n",
      "data\n",
      "))\n",
      "    \n",
      "test_set_size\n",
      " \n",
      "=\n",
      " \n",
      "int\n",
      "(\n",
      "len\n",
      "(\n",
      "data\n",
      ")\n",
      " \n",
      "*\n",
      " \n",
      "test_ratio\n",
      ")\n",
      "    \n",
      "test_indices\n",
      " \n",
      "=\n",
      " \n",
      "shuffled_indices\n",
      "[:\n",
      "test_set_size\n",
      "]\n",
      "    \n",
      "train_indices\n",
      " \n",
      "=\n",
      " \n",
      "shuffled_indices\n",
      "[\n",
      "test_set_size\n",
      ":]\n",
      "    \n",
      "return\n",
      " \n",
      "data\n",
      ".\n",
      "iloc\n",
      "[\n",
      "train_indices\n",
      "],\n",
      " \n",
      "data\n",
      ".\n",
      "iloc\n",
      "[\n",
      "test_indices\n",
      "]\n",
      " Y ou can then use this function like this:\n",
      "13\n",
      ">>> \n",
      "train_set\n",
      ",\n",
      " \n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "split_train_test\n",
      "(\n",
      "housing\n",
      ",\n",
      " \n",
      "0.2\n",
      ")\n",
      ">>> \n",
      "len\n",
      "(\n",
      "train_set\n",
      ")\n",
      "16512\n",
      ">>> \n",
      "len\n",
      "(\n",
      "test_set\n",
      ")\n",
      "4128\n",
      " W ell, this works, but it is not perfect: if you run the program again, it will genera te a\n",
      " differen t test set! O ver time, you (or your M achine Learning algorithms) will get to\n",
      " see the whole da taset, which is wha t you wan t to a void.\n",
      " One solution is to sa ve the test set on the first run and then load it in subsequen t\n",
      " runs. Another option is to set the random n umber genera tor‡ s seed (e.g., \n",
      "np.ran\n",
      "dom.seed(42)\n",
      ")\n",
      "14\n",
      " \n",
      "before calling \n",
      "np.random.permutation()\n",
      " , so tha t it alwa ys genera tes\n",
      " the same sh uffled indices.\n",
      " But both these solutions will break next time you fetch an upda ted da taset. A comƒ\n",
      " mon solution is to use each instance ‡ s iden tifier to decide whether or not it should go\n",
      " in the test set (assuming instances ha ve a unique and imm utable iden tifier). F or\n",
      " exam ple, you could com pute a hash of each instance ‡ s iden tifier and put tha t instance\n",
      " in the test set if the hash is lower or equal to 20% of the maxim um hash value. This\n",
      " ensures tha t the test set will remain consisten t across m ultiple runs, even if you\n",
      " refresh the da taset. The new test set will con tain 20% of the new instances, but it will\n",
      " not con tain an y instance tha t was previously in the training set. H ere is a possible\n",
      " im plemen ta tion:\n",
      "from\n",
      " \n",
      "zlib\n",
      " \n",
      "import\n",
      " \n",
      "crc32\n",
      "def\n",
      " \n",
      "test_set_check\n",
      "(\n",
      "identifier\n",
      ",\n",
      " \n",
      "test_ratio\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "crc32\n",
      "(\n",
      "np\n",
      ".\n",
      "int64\n",
      "(\n",
      "identifier\n",
      "))\n",
      " \n",
      "&\n",
      " \n",
      "0xffffffff\n",
      " \n",
      "<\n",
      " \n",
      "test_ratio\n",
      " \n",
      "*\n",
      " \n",
      "2\n",
      "**\n",
      "32\n",
      "def\n",
      " \n",
      "split_train_test_by_id\n",
      "(\n",
      "data\n",
      ",\n",
      " \n",
      "test_ratio\n",
      ",\n",
      " \n",
      "id_column\n",
      "):\n",
      "    \n",
      "ids\n",
      " \n",
      "=\n",
      " \n",
      "data\n",
      "[\n",
      "id_column\n",
      "]\n",
      " Get the Data  |   55\n",
      "\n",
      "15\n",
      " The loca tion informa tion is actually quite coarse, and as a result man y districts will ha ve the exact same ID , so\n",
      " they will end up in the same set (test or train). This in troduces some unfortuna te sam pling bias.\n",
      "    \n",
      "in_test_set\n",
      " \n",
      "=\n",
      " \n",
      "ids\n",
      ".\n",
      "apply\n",
      "(\n",
      "lambda\n",
      " \n",
      "id_\n",
      ":\n",
      " \n",
      "test_set_check\n",
      "(\n",
      "id_\n",
      ",\n",
      " \n",
      "test_ratio\n",
      "))\n",
      "    \n",
      "return\n",
      " \n",
      "data\n",
      ".\n",
      "loc\n",
      "[\n",
      "~\n",
      "in_test_set\n",
      "],\n",
      " \n",
      "data\n",
      ".\n",
      "loc\n",
      "[\n",
      "in_test_set\n",
      "]\n",
      " U nfortuna tely , the housing da taset does not ha ve an iden tifier column. The sim plest\n",
      "solution is to use the row index as the ID:\n",
      "housing_with_id\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "reset_index\n",
      "()\n",
      "   \n",
      "# adds an †index† column\n",
      "train_set\n",
      ",\n",
      " \n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "split_train_test_by_id\n",
      "(\n",
      "housing_with_id\n",
      ",\n",
      " \n",
      "0.2\n",
      ",\n",
      " \n",
      "\"index\"\n",
      ")\n",
      " If you use the row index as a unique iden tifier , you need to make sure tha t new da ta\n",
      " gets a ppended to the end of the da taset, and no row ever gets deleted. If this is not\n",
      " possible, then you can tr y to use the most stable fea tures to build a unique iden tifier .\n",
      " F or exam ple, a district ‡ s la titude and longitude are guaran teed to be stable for a few\n",
      " million years, so you could combine them in to an ID like so:\n",
      "15\n",
      "housing_with_id\n",
      "[\n",
      "\"id\"\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      "[\n",
      "\"longitude\"\n",
      "]\n",
      " \n",
      "*\n",
      " \n",
      "1000\n",
      " \n",
      "+\n",
      " \n",
      "housing\n",
      "[\n",
      "\"latitude\"\n",
      "]\n",
      "train_set\n",
      ",\n",
      " \n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "split_train_test_by_id\n",
      "(\n",
      "housing_with_id\n",
      ",\n",
      " \n",
      "0.2\n",
      ",\n",
      " \n",
      "\"id\"\n",
      ")\n",
      " Scikit-Learn provides a few functions to split da tasets in to m ultiple subsets in various\n",
      " wa ys. The sim plest function is \n",
      "train_test_split\n",
      " , which does pretty m uch the same\n",
      "thing as the function \n",
      "split_train_test\n",
      "  defined earlier , with a couple of additional\n",
      " fea tures. First there is a \n",
      "random_state\n",
      "  parameter tha t allows you to set the random\n",
      " genera tor seed as explained previously , and second you can pass it m ultiple da tasets\n",
      " with an iden tical n umber of rows, and it will split them on the same indices (this is\n",
      " ver y useful, for exam ple, if you ha ve a separa te Da taFrame for labels):\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "train_test_split\n",
      "train_set\n",
      ",\n",
      " \n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "train_test_split\n",
      "(\n",
      "housing\n",
      ",\n",
      " \n",
      "test_size\n",
      "=\n",
      "0.2\n",
      ",\n",
      " \n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      " So far we ha ve considered purely random sam pling methods. This is generally fine if\n",
      " your da taset is large enough (especially rela tive to the n umber of a ttributes), but if it\n",
      " is not, you run the risk of in troducing a significan t sam pling bias. When a sur vey\n",
      " com pan y decides to call 1,000 people to ask them a few questions, they don ‡ t just pick\n",
      " 1,000 people randomly in a phone book. They tr y to ensure tha t these 1,000 people\n",
      " are represen ta tive of the whole popula tion. F or exam ple, the US popula tion is comƒ\n",
      " posed of 51.3% female and 48.7% male, so a well-conducted sur vey in the US would\n",
      " tr y to main tain this ra tio in the sam ple: 513 female and 487 male. This is called \n",
      " s t r a t i‡\n",
      "†ed\n",
      "  s a m p l i n g\n",
      " : the popula tion is divided in to homogeneous subgroups called \n",
      " s t r a t a\n",
      ",\n",
      " and the righ t n umber of instances is sam pled from each stra tum to guaran tee tha t the\n",
      " test set is represen ta tive of the overall popula tion. If they used purely random samƒ\n",
      " pling, there would be about 12% chance of sam pling a skewed test set with either less\n",
      " than 49% female or more than 54% female. Either wa y , the sur vey results would be\n",
      " significan tly biased.\n",
      " 56  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " Suppose you cha tted with experts who told you tha t the median income is a ver y\n",
      " im portan t a ttribute to predict median housing prices. Y ou ma y wan t to ensure tha t\n",
      " the test set is represen ta tive of the various ca tegories of incomes in the whole da taset.\n",
      " Since the median income is a con tin uous n umerical a ttribute, you first need to crea te\n",
      " an income ca tegor y a ttribute. Let ‡ s look a t the median income histogram more closely\n",
      "(back in \n",
      "Figure 2-8\n",
      "): most median income values are clustered around 1.5 to 6 (i.e.,\n",
      " $15,000−$60,000), but some median incomes go far beyond 6. I t is im portan t to ha ve\n",
      " a sufficien t n umber of instances in your da taset for each stra tum, or else the estima te\n",
      " of the stra tum ‡ s im portance ma y be biased. This means tha t you should not ha ve too\n",
      " man y stra ta, and each stra tum should be large enough. The following code uses the\n",
      "pd.cut()\n",
      "  function to crea te an income ca tegor y a ttribute with 5 ca tegories (labeled\n",
      " from 1 to 5): ca tegor y 1 ranges from 0 to 1.5 (i.e., less than $15,000), ca tegor y 2 from\n",
      "1.5 to 3, and so on:\n",
      "housing\n",
      "[\n",
      "\"income_cat\"\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "pd\n",
      ".\n",
      "cut\n",
      "(\n",
      "housing\n",
      "[\n",
      "\"median_income\"\n",
      "],\n",
      "                               \n",
      "bins\n",
      "=\n",
      "[\n",
      "0.\n",
      ",\n",
      " \n",
      "1.5\n",
      ",\n",
      " \n",
      "3.0\n",
      ",\n",
      " \n",
      "4.5\n",
      ",\n",
      " \n",
      "6.\n",
      ",\n",
      " \n",
      "np\n",
      ".\n",
      "inf\n",
      "],\n",
      "                               \n",
      "labels\n",
      "=\n",
      "[\n",
      "1\n",
      ",\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "4\n",
      ",\n",
      " \n",
      "5\n",
      "])\n",
      " These income ca tegories are represen ted in \n",
      "Figure 2-9\n",
      ":\n",
      "housing\n",
      "[\n",
      "\"income_cat\"\n",
      "]\n",
      ".\n",
      "hist\n",
      "()\n",
      " F i g u r e 2-9. H i s t og r a m o f i n c o m e c a t e go r i e s\n",
      " N ow you are ready to do stra tified sam pling based on the income ca tegor y . F or this\n",
      " you can use Scikit-Learn ‡ s \n",
      "StratifiedShuffleSplit\n",
      " class:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "StratifiedShuffleSplit\n",
      "split\n",
      " \n",
      "=\n",
      " \n",
      "StratifiedShuffleSplit\n",
      "(\n",
      "n_splits\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "test_size\n",
      "=\n",
      "0.2\n",
      ",\n",
      " \n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      "for\n",
      " \n",
      "train_index\n",
      ",\n",
      " \n",
      "test_index\n",
      " \n",
      "in\n",
      " \n",
      "split\n",
      ".\n",
      "split\n",
      "(\n",
      "housing\n",
      ",\n",
      " \n",
      "housing\n",
      "[\n",
      "\"income_cat\"\n",
      "]):\n",
      "    \n",
      "strat_train_set\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "loc\n",
      "[\n",
      "train_index\n",
      "]\n",
      "    \n",
      "strat_test_set\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "loc\n",
      "[\n",
      "test_index\n",
      "]\n",
      " Get the Data  |  57\n",
      "\n",
      " Let ‡ s see if this worked as expected. Y ou can start by looking a t the income ca tegor y\n",
      "proportions in the test set:\n",
      ">>> \n",
      "strat_test_set\n",
      "[\n",
      "\"income_cat\"\n",
      "]\n",
      ".\n",
      "value_counts\n",
      "()\n",
      " \n",
      "/\n",
      " \n",
      "len\n",
      "(\n",
      "strat_test_set\n",
      ")\n",
      "3    0.350533\n",
      "2    0.318798\n",
      "4    0.176357\n",
      "5    0.114583\n",
      "1    0.039729\n",
      "Name: income_cat, dtype: float64\n",
      " W ith similar code you can measure the income ca tegor y proportions in the full da taƒ\n",
      "set. \n",
      "Figure 2-10\n",
      "  com pares the income ca tegor y proportions in the overall da taset, in\n",
      " the test set genera ted with stra tified sam pling, and in a test set genera ted using purely\n",
      " random sam pling. As you can see, the test set genera ted using stra tified sam pling has\n",
      " income ca tegor y proportions almost iden tical to those in the full da taset, whereas the\n",
      " test set genera ted using purely random sam pling is quite skewed.\n",
      " F i g u r e 2-10. S a m p l i n g b i as c o m p a r i s o n o f \n",
      "strati†ed\n",
      "  v er s u s p u r e l y r a n d o m s a m p l i n g\n",
      " N ow you should remove the \n",
      "income_cat\n",
      "  a ttribute so the da ta is back to its original\n",
      " sta te:\n",
      "for\n",
      " \n",
      "set_\n",
      " \n",
      "in\n",
      " \n",
      "(\n",
      "strat_train_set\n",
      ",\n",
      " \n",
      "strat_test_set\n",
      "):\n",
      "    \n",
      "set_\n",
      ".\n",
      "drop\n",
      "(\n",
      "\"income_cat\"\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "inplace\n",
      "=\n",
      "True\n",
      ")\n",
      " W e spen t quite a bit of time on test set genera tion for a good reason: this is an often\n",
      " neglected but critical part of a M achine Learning project. M oreover , man y of these\n",
      " ideas will be useful la ter when we discuss cross-valida tion. N ow it ‡ s time to move on\n",
      " to the next stage: exploring the da ta.\n",
      "Discover and Visualize the Data to Gain Insights\n",
      " So far you ha ve only taken a quick glance a t the da ta to get a general understanding of\n",
      " the kind of da ta you are manipula ting. N ow the goal is to go a little bit more in depth.\n",
      " First, make sure you ha ve put the test set aside and you are only exploring the trainƒ\n",
      " ing set. Also , if the training set is ver y large, you ma y wan t to sam ple an explora tion\n",
      " 58  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " set, to make manipula tions easy and fast. In our case, the set is quite small so you can\n",
      " just work directly on the full set. Let ‡ s crea te a copy so you can pla y with it without\n",
      "harming the training set:\n",
      "housing\n",
      " \n",
      "=\n",
      " \n",
      "strat_train_set\n",
      ".\n",
      "copy\n",
      "()\n",
      "Visualizing Geographical Data\n",
      " Since there is geogra phical informa tion (la titude and longitude), it is a good idea to\n",
      " crea te a sca tterplot of all districts to visualize the da ta (\n",
      "Figure 2-11\n",
      "):\n",
      "housing\n",
      ".\n",
      "plot\n",
      "(\n",
      "kind\n",
      "=\n",
      "\"scatter\"\n",
      ",\n",
      " \n",
      "x\n",
      "=\n",
      "\"longitude\"\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "\"latitude\"\n",
      ")\n",
      " F i g u r e 2-11. A ge og r a p h i c a l s c a tt er p l o t o f t h e d a t a\n",
      " This looks like California all righ t, but other than tha t it is hard to see an y particular\n",
      " pa ttern. Setting the \n",
      "alpha\n",
      " option to \n",
      "0.1\n",
      "  makes it m uch easier to visualize the places\n",
      " where there is a high density of da ta poin ts (\n",
      "Figure 2-12\n",
      "):\n",
      "housing\n",
      ".\n",
      "plot\n",
      "(\n",
      "kind\n",
      "=\n",
      "\"scatter\"\n",
      ",\n",
      " \n",
      "x\n",
      "=\n",
      "\"longitude\"\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "\"latitude\"\n",
      ",\n",
      " \n",
      "alpha\n",
      "=\n",
      "0.1\n",
      ")\n",
      " Discover and Visualize the Data to Gain Insights  |  59\n",
      "\n",
      "16\n",
      " If you are reading this in gra yscale, grab a red pen and scribble over most of the coastline from the Ba y Area\n",
      " down to San Diego (as you migh t expect). Y ou can add a pa tch of yellow around Sacramen to as well.\n",
      " F i g u r e 2-12. A b e tt er v i s u a l iz a t i o n h i gh l i gh t i n g h i gh-d ens i ty a r e as\n",
      " N ow tha t ‡ s m uch better : you can clearly see the high-density areas, namely the Ba y\n",
      " Area and around Los Angeles and San Diego , plus a long line of fairly high density in\n",
      " the Cen tral V alley , in particular around Sacramen to and Fresno .\n",
      " M ore generally , our brains are ver y good a t spotting pa tterns on pictures, but you\n",
      " ma y need to pla y around with visualiza tion parameters to make the pa tterns stand\n",
      "out.\n",
      " N ow let ‡ s look a t the housing prices (\n",
      "Figure 2-13\n",
      " ). The radius of each circle represen ts\n",
      " the district ‡ s popula tion (option \n",
      "s\n",
      " ), and the color represen ts the price (option \n",
      "c\n",
      " ). W e\n",
      " will use a predefined color ma p (option \n",
      "cmap\n",
      ") called \n",
      "jet\n",
      ", which ranges from blue\n",
      "(low values) to red (high prices):\n",
      "16\n",
      "housing\n",
      ".\n",
      "plot\n",
      "(\n",
      "kind\n",
      "=\n",
      "\"scatter\"\n",
      ",\n",
      " \n",
      "x\n",
      "=\n",
      "\"longitude\"\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "\"latitude\"\n",
      ",\n",
      " \n",
      "alpha\n",
      "=\n",
      "0.4\n",
      ",\n",
      "    \n",
      "s\n",
      "=\n",
      "housing\n",
      "[\n",
      "\"population\"\n",
      "]\n",
      "/\n",
      "100\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"population\"\n",
      ",\n",
      " \n",
      "figsize\n",
      "=\n",
      "(\n",
      "10\n",
      ",\n",
      "7\n",
      "),\n",
      "    \n",
      "c\n",
      "=\n",
      "\"median_house_value\"\n",
      ",\n",
      " \n",
      "cmap\n",
      "=\n",
      "plt\n",
      ".\n",
      "get_cmap\n",
      "(\n",
      "\"jet\"\n",
      "),\n",
      " \n",
      "colorbar\n",
      "=\n",
      "True\n",
      ",\n",
      ")\n",
      "plt\n",
      ".\n",
      "legend\n",
      "()\n",
      " 60  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " F i g u r e 2-13. C a l i f o r n i a h o u s i n g p r i c e s\n",
      " Discover and Visualize the Data to Gain Insights  |  61\n",
      "\n",
      " This image tells you tha t the housing prices are ver y m uch rela ted to the loca tion\n",
      " (e.g., close to the ocean) and to the popula tion density , as you probably knew already .\n",
      " I t will probably be useful to use a clustering algorithm to detect the main clusters, and\n",
      " add new fea tures tha t measure the proximity to the cluster cen ters. The ocean proxƒ\n",
      " imity a ttribute ma y be useful as well, although in N orthern California the housing\n",
      " prices in coastal districts are not too high, so it is not a sim ple rule.\n",
      "Looking for Correlations\n",
      " Since the da taset is not too large, you can easily com pute the \n",
      " s t a n d a r d c o r r e l a t i o n\n",
      "coe⁄cient\n",
      " (also called \n",
      " P e a r s o n ‹ s r\n",
      " ) between ever y pair of a ttributes using the \n",
      "corr()\n",
      "method:\n",
      "corr_matrix\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "corr\n",
      "()\n",
      " N ow let ‡ s look a t how m uch each a ttribute correla tes with the median house value:\n",
      ">>> \n",
      "corr_matrix\n",
      "[\n",
      "\"median_house_value\"\n",
      "]\n",
      ".\n",
      "sort_values\n",
      "(\n",
      "ascending\n",
      "=\n",
      "False\n",
      ")\n",
      "median_house_value    1.000000\n",
      "median_income         0.687170\n",
      "total_rooms           0.135231\n",
      "housing_median_age    0.114220\n",
      "households            0.064702\n",
      "total_bedrooms        0.047865\n",
      "population           -0.026699\n",
      "longitude            -0.047279\n",
      "latitude             -0.142826\n",
      "Name: median_house_value, dtype: float64\n",
      " The correla tion coefficien t ranges from −1 to 1. When it is close to 1, it means tha t\n",
      " there is a strong positive correla tion; for exam ple, the median house value tends to go\n",
      " up when the median income goes up . When the coefficien t is close to −1, it means\n",
      " tha t there is a strong nega tive correla tion; you can see a small nega tive correla tion\n",
      " between the la titude and the median house value (i.e., prices ha ve a sligh t tendency to\n",
      " go down when you go north). Finally , coefficien ts close to zero mean tha t there is no\n",
      " linear correla tion. \n",
      "Figure 2-14\n",
      "  shows various plots along with the correla tion coeffiƒ\n",
      " cien t between their horizon tal and vertical axes.\n",
      " 62  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " F i g u r e 2-14. S t a n d a r d c o r r e l a t i o n \n",
      "coe⁄cient\n",
      "  o f v a r i o u s d a t as e ts (s o u r c e: W i k i p e d i a;\n",
      " p u b l i c d o m a i n i m a ge)\n",
      " The correla tion coefficien t only measures linear correla tions (— if \n",
      "x\n",
      " goes up , then \n",
      "y\n",
      "  generally goes up/down –). I t ma y com pletely miss\n",
      " out on nonlinear rela tionships (e.g., — if \n",
      "x\n",
      "   is close to zero then \n",
      "y\n",
      "   genƒ\n",
      " erally goes up –). N ote how all the plots of the bottom row ha ve a\n",
      " correla tion coefficien t equal to zero despite the fact tha t their axes\n",
      " are clearly not independen t: these are exam ples of nonlinear relaƒ\n",
      " tionships. Also , the second row shows exam ples where the correlaƒ\n",
      " tion coefficien t is equal to 1 or −1; notice tha t this has nothing to\n",
      " do with the slope. F or exam ple, your heigh t in inches has a correlaƒ\n",
      " tion coefficien t of 1 with your heigh t in feet or in nanometers.\n",
      " Another wa y to check for correla tion between a ttributes is to use \n",
      " P andas ‡\n",
      "scatter_matrix\n",
      "  function, which plots ever y n umerical a ttribute against ever y other\n",
      " n umerical a ttribute. Since there are now 11 n umerical a ttributes, you would get 11\n",
      "2\n",
      "   =\n",
      " 121 plots, which would not fit on a page, so let ‡ s just focus on a few promising\n",
      " a ttributes tha t seem most correla ted with the median housing value (\n",
      "Figure 2-15\n",
      "):\n",
      "from\n",
      " \n",
      "pandas.plotting\n",
      " \n",
      "import\n",
      " \n",
      "scatter_matrix\n",
      "attributes\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "\"median_house_value\"\n",
      ",\n",
      " \n",
      "\"median_income\"\n",
      ",\n",
      " \n",
      "\"total_rooms\"\n",
      ",\n",
      "              \n",
      "\"housing_median_age\"\n",
      "]\n",
      "scatter_matrix\n",
      "(\n",
      "housing\n",
      "[\n",
      "attributes\n",
      "],\n",
      " \n",
      "figsize\n",
      "=\n",
      "(\n",
      "12\n",
      ",\n",
      " \n",
      "8\n",
      "))\n",
      " Discover and Visualize the Data to Gain Insights  |  63\n",
      "\n",
      " F i g u r e 2-15. S c a tt er m a t r ix\n",
      " The main diagonal (top left to bottom righ t) would be full of straigh t lines if P andas\n",
      " plotted each variable against itself, which would not be ver y useful. So instead P andas\n",
      " displa ys a histogram of each a ttribute (other options are a vailable; see \n",
      " P andas ‡ docuƒ\n",
      " men ta tion for more details).\n",
      " The most promising a ttribute to predict the median house value is the median\n",
      " income, so let ‡ s zoom in on their correla tion sca tterplot (\n",
      "Figure 2-16\n",
      "):\n",
      "housing\n",
      ".\n",
      "plot\n",
      "(\n",
      "kind\n",
      "=\n",
      "\"scatter\"\n",
      ",\n",
      " \n",
      "x\n",
      "=\n",
      "\"median_income\"\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "\"median_house_value\"\n",
      ",\n",
      "             \n",
      "alpha\n",
      "=\n",
      "0.1\n",
      ")\n",
      " This plot reveals a few things. First, the correla tion is indeed ver y strong; you can\n",
      " clearly see the upward trend and the poin ts are not too dispersed. Second, the price\n",
      " ca p tha t we noticed earlier is clearly visible as a horizon tal line a t $500,000. But this\n",
      " plot reveals other less obvious straigh t lines: a horizon tal line around $450,000,\n",
      " another around $350,000, perha ps one around $280,000, and a few more below tha t.\n",
      " Y ou ma y wan t to tr y removing the corresponding districts to preven t your algorithms\n",
      " from learning to reproduce these da ta quirks.\n",
      " 64  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " F i g u r e 2-16. M e d i a n i n c o m e v er s u s m e d i a n h o u s e v a l u e\n",
      "Experimenting with Attribute Combinations\n",
      " H opefully the previous sections ga ve you an idea of a few wa ys you can explore the\n",
      " da ta and gain insigh ts. Y ou iden tified a few da ta quirks tha t you ma y wan t to clean up\n",
      " before feeding the da ta to a M achine Learning algorithm, and you found in teresting\n",
      " correla tions between a ttributes, in particular with the target a ttribute. Y ou also\n",
      " noticed tha t some a ttributes ha ve a tail-hea vy distribution, so you ma y wan t to transƒ\n",
      " form them (e.g., by com puting their logarithm). Of course, your mileage will var y\n",
      " considerably with each project, but the general ideas are similar .\n",
      " One last thing you ma y wan t to do before actually preparing the da ta for M achine\n",
      " Learning algorithms is to tr y out various a ttribute combina tions. F or exam ple, the\n",
      " total n umber of rooms in a district is not ver y useful if you don ‡ t know how man y\n",
      " households there are. Wha t you really wan t is the n umber of rooms per household.\n",
      " Similarly , the total n umber of bedrooms by itself is not ver y useful: you probably\n",
      " wan t to com pare it to the n umber of rooms. And the popula tion per household also\n",
      " seems like an in teresting a ttribute combina tion to look a t. Let ‡ s crea te these new\n",
      " a ttributes:\n",
      "housing\n",
      "[\n",
      "\"rooms_per_household\"\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      "[\n",
      "\"total_rooms\"\n",
      "]\n",
      "/\n",
      "housing\n",
      "[\n",
      "\"households\"\n",
      "]\n",
      "housing\n",
      "[\n",
      "\"bedrooms_per_room\"\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      "[\n",
      "\"total_bedrooms\"\n",
      "]\n",
      "/\n",
      "housing\n",
      "[\n",
      "\"total_rooms\"\n",
      "]\n",
      "housing\n",
      "[\n",
      "\"population_per_household\"\n",
      "]\n",
      "=\n",
      "housing\n",
      "[\n",
      "\"population\"\n",
      "]\n",
      "/\n",
      "housing\n",
      "[\n",
      "\"households\"\n",
      "]\n",
      " And now let ‡ s look a t the correla tion ma trix again:\n",
      ">>> \n",
      "corr_matrix\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "corr\n",
      "()\n",
      ">>> \n",
      "corr_matrix\n",
      "[\n",
      "\"median_house_value\"\n",
      "]\n",
      ".\n",
      "sort_values\n",
      "(\n",
      "ascending\n",
      "=\n",
      "False\n",
      ")\n",
      "median_house_value          1.000000\n",
      " Discover and Visualize the Data to Gain Insights  |  65\n",
      "\n",
      "median_income               0.687160\n",
      "rooms_per_household         0.146285\n",
      "total_rooms                 0.135097\n",
      "housing_median_age          0.114110\n",
      "households                  0.064506\n",
      "total_bedrooms              0.047689\n",
      "population_per_household   -0.021985\n",
      "population                 -0.026920\n",
      "longitude                  -0.047432\n",
      "latitude                   -0.142724\n",
      "bedrooms_per_room          -0.259984\n",
      "Name: median_house_value, dtype: float64\n",
      " H ey , not bad! The new \n",
      "bedrooms_per_room\n",
      "  a ttribute is m uch more correla ted with\n",
      " the median house value than the total n umber of rooms or bedrooms. A pparen tly\n",
      " houses with a lower bedroom/room ra tio tend to be more expensive. The n umber of\n",
      " rooms per household is also more informa tive than the total n umber of rooms in a\n",
      "district›obviously the larger the houses, the more expensive they are.\n",
      " This round of explora tion does not ha ve to be absolutely thorough; the poin t is to\n",
      " start off on the righ t foot and quickly gain insigh ts tha t will help you get a first reaƒ\n",
      " sonably good prototype. But this is an itera tive process: once you get a prototype up\n",
      " and running, you can analyze its output to gain more insigh ts and come back to this\n",
      " explora tion step .\n",
      "Prepare the Data for Machine Learning Algorithms\n",
      " I t ‡ s time to prepare the da ta for your M achine Learning algorithms. Instead of just\n",
      " doing this man ually , you should write functions to do tha t, for several good reasons:\n",
      "⁄\n",
      " This will allow you to reproduce these transforma tions easily on an y da taset (e.g.,\n",
      " the next time you get a fresh da taset).\n",
      "⁄\n",
      " Y ou will gradually build a librar y of transforma tion functions tha t you can reuse\n",
      "in future projects.\n",
      "⁄\n",
      " Y ou can use these functions in your live system to transform the new da ta before\n",
      "feeding it to your algorithms.\n",
      "⁄\n",
      " This will make it possible for you to easily tr y various transforma tions and see\n",
      " which combina tion of transforma tions works best.\n",
      " But first let ‡ s revert to a clean training set (by copying \n",
      "strat_train_set\n",
      " \n",
      "once again),\n",
      " and let ‡ s separa te the predictors and the labels since we don ‡ t necessarily wan t to a pply\n",
      " the same transforma tions to the predictors and the target values (note tha t \n",
      "drop()\n",
      " \n",
      " crea tes a copy of the da ta and does not affect \n",
      "strat_train_set\n",
      "):\n",
      "housing\n",
      " \n",
      "=\n",
      " \n",
      "strat_train_set\n",
      ".\n",
      "drop\n",
      "(\n",
      "\"median_house_value\"\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "1\n",
      ")\n",
      "housing_labels\n",
      " \n",
      "=\n",
      " \n",
      "strat_train_set\n",
      "[\n",
      "\"median_house_value\"\n",
      "]\n",
      ".\n",
      "copy\n",
      "()\n",
      " 66  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "Data Cleaning\n",
      " M ost M achine Learning algorithms cannot work with missing fea tures, so let ‡ s crea te\n",
      " a few functions to take care of them. Y ou noticed earlier tha t the \n",
      "total_bedrooms\n",
      " a ttribute has some missing values, so let ‡ s fix this. Y ou ha ve three options:\n",
      "⁄\n",
      "Get rid of the corresponding districts.\n",
      "⁄\n",
      " Get rid of the whole a ttribute.\n",
      "⁄\n",
      " Set the values to some value (zero , the mean, the median, etc.).\n",
      " Y ou can accom plish these easily using Da taFrame ‡ s \n",
      "dropna()\n",
      ", \n",
      "drop()\n",
      ", and \n",
      "fillna()\n",
      "methods:\n",
      "housing\n",
      ".\n",
      "dropna\n",
      "(\n",
      "subset\n",
      "=\n",
      "[\n",
      "\"total_bedrooms\"\n",
      "])\n",
      "    \n",
      "# option 1\n",
      "housing\n",
      ".\n",
      "drop\n",
      "(\n",
      "\"total_bedrooms\"\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "1\n",
      ")\n",
      "       \n",
      "# option 2\n",
      "median\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      "[\n",
      "\"total_bedrooms\"\n",
      "]\n",
      ".\n",
      "median\n",
      "()\n",
      "  \n",
      "# option 3\n",
      "housing\n",
      "[\n",
      "\"total_bedrooms\"\n",
      "]\n",
      ".\n",
      "fillna\n",
      "(\n",
      "median\n",
      ",\n",
      " \n",
      "inplace\n",
      "=\n",
      "True\n",
      ")\n",
      " If you choose option 3, you should com pute the median value on the training set, and\n",
      " use it to fill the missing values in the training set, but also don ‡ t forget to sa ve the\n",
      " median value tha t you ha ve com puted. Y ou will need it la ter to replace missing values\n",
      " in the test set when you wan t to evalua te your system, and also once the system goes\n",
      " live to replace missing values in new da ta.\n",
      "Scikit-Learn provides a handy class to take care of missing values: \n",
      "SimpleImputer\n",
      ".\n",
      " H ere is how to use it. First, you need to crea te a \n",
      "SimpleImputer\n",
      " \n",
      " instance, specif ying\n",
      " tha t you wan t to replace each a ttribute ‡ s missing values with the median of tha t\n",
      " a ttribute:\n",
      "from\n",
      " \n",
      "sklearn.impute\n",
      " \n",
      "import\n",
      " \n",
      "SimpleImputer\n",
      "imputer\n",
      " \n",
      "=\n",
      " \n",
      "SimpleImputer\n",
      "(\n",
      "strategy\n",
      "=\n",
      "\"median\"\n",
      ")\n",
      " Since the median can only be com puted on n umerical a ttributes, we need to crea te a\n",
      " copy of the da ta without the text a ttribute \n",
      "ocean_proximity\n",
      ":\n",
      "housing_num\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "drop\n",
      "(\n",
      "\"ocean_proximity\"\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "1\n",
      ")\n",
      " N ow you can fit the \n",
      "imputer\n",
      "  instance to the training da ta using the \n",
      "fit()\n",
      " method:\n",
      "imputer\n",
      ".\n",
      "fit\n",
      "(\n",
      "housing_num\n",
      ")\n",
      "The \n",
      "imputer\n",
      "  has sim ply com puted the median of each a ttribute and stored the result\n",
      "in its \n",
      "statistics_\n",
      " instance variable. Only the \n",
      "total_bedrooms\n",
      "  a ttribute had missing\n",
      " values, but we cannot be sure tha t there won ‡ t be an y missing values in new da ta after\n",
      " the system goes live, so it is safer to a pply the \n",
      "imputer\n",
      "  to all the n umerical a ttributes:\n",
      ">>> \n",
      "imputer\n",
      ".\n",
      "statistics_\n",
      "array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n",
      " Prepare the Data for Machine Learning Algorithms  |  67\n",
      "\n",
      "17\n",
      " F or more details on the design principles, see — API design for machine learning software: experiences from\n",
      " the scikit-learn project, – L. Buitinck, G. Louppe, M. Blondel, F . P edregosa, A. M ﬁller , et al. (2013).\n",
      ">>> \n",
      "housing_num\n",
      ".\n",
      "median\n",
      "()\n",
      ".\n",
      "values\n",
      "array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n",
      " N ow you can use this — trained – \n",
      "imputer\n",
      " to transform the training set by replacing\n",
      "missing values by the learned medians:\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "imputer\n",
      ".\n",
      "transform\n",
      "(\n",
      "housing_num\n",
      ")\n",
      " The result is a plain N umPy arra y con taining the transformed fea tures. If you wan t to\n",
      " put it back in to a P andas Da taFrame, it ‡ s sim ple:\n",
      "housing_tr\n",
      " \n",
      "=\n",
      " \n",
      "pd\n",
      ".\n",
      "DataFrame\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "columns\n",
      "=\n",
      "housing_num\n",
      ".\n",
      "columns\n",
      ")\n",
      "Scikit-Learn Design\n",
      " Scikit-Learn ‡ s API is remarkably well designed. The \n",
      "main design principles\n",
      " are:\n",
      "17\n",
      "⁄\n",
      " C o ns is t e nc y\n",
      " . All objects share a consisten t and sim ple in terface:\n",
      "›\n",
      " E s t i m a t o r s\n",
      " . An y object tha t can estima te some parameters based on a da taset\n",
      "is called an \n",
      " e s t i m a t o r\n",
      "   (e.g., an \n",
      "imputer\n",
      " \n",
      " is an estima tor). The estima tion itself is\n",
      "performed by the \n",
      "fit()\n",
      " \n",
      " method, and it takes only a da taset as a parameter (or\n",
      " two for super vised learning algorithms; the second da taset con tains the\n",
      " labels). An y other parameter needed to guide the estima tion process is conƒ\n",
      " sidered a h yperparameter (such as an \n",
      "imputer\n",
      " ‡ s \n",
      "strategy\n",
      " ), and it m ust be set\n",
      "as an instance variable (generally via a constructor parameter).\n",
      "›\n",
      " T r a ns f o r m er s\n",
      " . Some estima tors (such as an \n",
      "imputer\n",
      ") can also transform a\n",
      " da taset; these are called \n",
      " t r a ns f o r m er s\n",
      " . Once again, the API is quite sim ple: the\n",
      " transforma tion is performed by the \n",
      "transform()\n",
      "  method with the da taset to\n",
      " transform as a parameter . I t returns the transformed da taset. This transformaƒ\n",
      "tion generally relies on the learned parameters, as is the case for an \n",
      "imputer\n",
      ".\n",
      " All transformers also ha ve a con venience method called \n",
      "fit_transform()\n",
      " \n",
      " tha t is equivalen t to calling \n",
      "fit()\n",
      " and then \n",
      "transform()\n",
      " \n",
      "(but sometimes\n",
      "fit_transform()\n",
      "  is optimized and runs m uch faster).\n",
      "›\n",
      " P r e d i c t o r s\n",
      " . Finally , some estima tors are ca pable of making predictions given a\n",
      " da taset; they are called \n",
      " p r e d i c t o r s\n",
      " . F or exam ple, the \n",
      "LinearRegression\n",
      "   model \n",
      " in the previous cha pter was a predictor : it predicted life sa tisfaction given a\n",
      " coun tr y‡ s GDP per ca pita. A predictor has a \n",
      "predict()\n",
      "  method tha t takes a\n",
      " da taset of new instances and returns a da taset of corresponding predictions. I t\n",
      "also has a \n",
      "score()\n",
      "  method tha t measures the quality of the predictions given\n",
      " 68  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "18\n",
      "Some predictors also provide methods to measure the confidence of their predictions.\n",
      "19\n",
      " This class is a vailable since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\n",
      " P andas ‡ \n",
      "Series.factorize()\n",
      " method.\n",
      " a test set (and the corresponding labels in the case of super vised learning\n",
      "algorithms).\n",
      "18\n",
      "⁄\n",
      " I ns p e c t i o n\n",
      " . All the estima tor‡ s h yperparameters are accessible directly via public\n",
      "instance variables (e.g., \n",
      "imputer.strategy\n",
      " ), and all the estima tor‡ s learned\n",
      "parameters are also accessible via public instance variables with an underscore\n",
      "suffix (e.g., \n",
      "imputer.statistics_\n",
      ").\n",
      "⁄\n",
      " N o n p r o lif e r at i o n o f cl ass es\n",
      " . Da tasets are represen ted as N umPy arra ys or SciPy\n",
      " sparse ma trices, instead of homemade classes. H yperparameters are just regular\n",
      " Python strings or n umbers.\n",
      "⁄\n",
      " C o m p os i t i o n\n",
      " . Existing building blocks are reused as m uch as possible. F or\n",
      " exam ple, it is easy to crea te a \n",
      "Pipeline\n",
      "  estima tor from an arbitrar y sequence of\n",
      " transformers followed by a final estima tor , as we will see.\n",
      "⁄\n",
      " S e ns i b l e d e f au l ts\n",
      " . Scikit-Learn provides reasonable defa ult values for most\n",
      " parameters, making it easy to crea te a baseline working system quickly .\n",
      "Handling Text and Categorical Attributes\n",
      " Earlier we left out the ca tegorical a ttribute \n",
      "ocean_proximity\n",
      " \n",
      " beca use it is a text\n",
      " a ttribute so we cannot com pute its median:\n",
      ">>> \n",
      "housing_cat\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      "[[\n",
      "\"ocean_proximity\"\n",
      "]]\n",
      ">>> \n",
      "housing_cat\n",
      ".\n",
      "head\n",
      "(\n",
      "10\n",
      ")\n",
      "      ocean_proximity\n",
      "17606       <1H OCEAN\n",
      "18632       <1H OCEAN\n",
      "14650      NEAR OCEAN\n",
      "3230           INLAND\n",
      "3555        <1H OCEAN\n",
      "19480          INLAND\n",
      "8879        <1H OCEAN\n",
      "13685          INLAND\n",
      "4937        <1H OCEAN\n",
      "4861        <1H OCEAN\n",
      " M ost M achine Learning algorithms prefer to work with n umbers an ywa y , so let ‡ s conƒ\n",
      " vert these ca tegories from text to n umbers. F or this, we can use Scikit-Learn ‡ s \n",
      "Ordina\n",
      "lEncoder\n",
      " class\n",
      "19\n",
      ":\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "OrdinalEncoder\n",
      ">>> \n",
      "ordinal_encoder\n",
      " \n",
      "=\n",
      " \n",
      "OrdinalEncoder\n",
      "()\n",
      " Prepare the Data for Machine Learning Algorithms  |  69\n",
      "\n",
      "20\n",
      " B efore Scikit-Learn 0.20, it could only encode in teger ca tegorical values, but since 0.20 it can also handle\n",
      " other types of in puts, including text ca tegorical in puts.\n",
      ">>> \n",
      "housing_cat_encoded\n",
      " \n",
      "=\n",
      " \n",
      "ordinal_encoder\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "housing_cat\n",
      ")\n",
      ">>> \n",
      "housing_cat_encoded\n",
      "[:\n",
      "10\n",
      "]\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [4.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.]])\n",
      " Y ou can get the list of ca tegories using the \n",
      "categories_\n",
      "  instance variable. I t is a list\n",
      " con taining a 1D arra y of ca tegories for each ca tegorical a ttribute (in this case, a list\n",
      " con taining a single arra y since there is just one ca tegorical a ttribute):\n",
      ">>> \n",
      "ordinal_encoder\n",
      ".\n",
      "categories_\n",
      "[array([•<1H OCEAN•, •INLAND•, •ISLAND•, •NEAR BAY•, •NEAR OCEAN•],\n",
      "       dtype=object)]\n",
      " One issue with this represen ta tion is tha t ML algorithms will assume tha t two nearby\n",
      " values are more similar than two distan t values. This ma y be fine in some cases (e.g.,\n",
      " for ordered ca tegories such as —bad – , — a verage – , — good – , — excellen t –), but it is obviously\n",
      "not the case for the \n",
      "ocean_proximity\n",
      "  column (for exam ple, ca tegories 0 and 4 are\n",
      " clearly more similar than ca tegories 0 and 1). T o fix this issue, a common solution is\n",
      " to crea te one binar y a ttribute per ca tegor y : one a ttribute equal to 1 when the ca tegor y\n",
      " is —<1H O CEAN– (and 0 other wise), another a ttribute equal to 1 when the ca tegor y is\n",
      " —INLAND – (and 0 other wise), and so on. This is called \n",
      " o n e-h o t en c o d i n g\n",
      ", \n",
      " beca use\n",
      " only one a ttribute will be equal to 1 (hot), while the others will be 0 (cold). The new\n",
      " a ttributes are sometimes called \n",
      " d u m m y\n",
      "  a ttributes. Scikit-Learn provides a \n",
      "OneHotEn\n",
      "coder\n",
      "  class to con vert ca tegorical values in to one-hot vectors\n",
      "20\n",
      ":\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "OneHotEncoder\n",
      ">>> \n",
      "cat_encoder\n",
      " \n",
      "=\n",
      " \n",
      "OneHotEncoder\n",
      "()\n",
      ">>> \n",
      "housing_cat_1hot\n",
      " \n",
      "=\n",
      " \n",
      "cat_encoder\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "housing_cat\n",
      ")\n",
      ">>> \n",
      "housing_cat_1hot\n",
      "<16512x5 sparse matrix of type •<class •numpy.float64•>•\n",
      "  with 16512 stored elements in Compressed Sparse Row format>\n",
      " N otice tha t the output is a SciPy \n",
      " s p a r s e m a t r ix\n",
      ", \n",
      " instead of a N umPy arra y . This is ver y\n",
      " useful when you ha ve ca tegorical a ttributes with thousands of ca tegories. After one-\n",
      " hot encoding we get a ma trix with thousands of columns, and the ma trix is full of\n",
      " zeros except for a single 1 per row . U sing up tons of memor y mostly to store zeros\n",
      " would be ver y wasteful, so instead a sparse ma trix only stores the loca tion of the nonƒ\n",
      " 70  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "21\n",
      " See SciPy‡ s documen ta tion for more details.\n",
      " zero elemen ts. Y ou can use it mostly like a normal 2D arra y ,\n",
      "21\n",
      " \n",
      " but if you really wan t to\n",
      " con vert it to a (dense) N umPy arra y , just call the \n",
      "toarray()\n",
      " method:\n",
      ">>> \n",
      "housing_cat_1hot\n",
      ".\n",
      "toarray\n",
      "()\n",
      "array([[1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.]])\n",
      " Once again, you can get the list of ca tegories using the encoder‡ s \n",
      "categories_\n",
      "instance variable:\n",
      ">>> \n",
      "cat_encoder\n",
      ".\n",
      "categories_\n",
      "[array([•<1H OCEAN•, •INLAND•, •ISLAND•, •NEAR BAY•, •NEAR OCEAN•],\n",
      "       dtype=object)]\n",
      " If a ca tegorical a ttribute has a large n umber of possible ca tegories\n",
      " (e.g., coun tr y code, profession, species, etc.), then one-hot encodƒ\n",
      " ing will result in a large n umber of in put fea tures. This ma y slow\n",
      " down training and degrade performance. If this ha ppens, you ma y\n",
      " wan t to replace the ca tegorical in put with useful n umerical fea tures\n",
      " rela ted to the ca tegories: for exam ple, you could replace the\n",
      "ocean_proximity\n",
      "  fea ture with the distance to the ocean (similarly ,\n",
      " a coun tr y code could be replaced with the coun tr y‡ s popula tion and\n",
      " GDP per ca pita). Alterna tively , you could replace each ca tegor y\n",
      "with a learnable low dimensional vector called an \n",
      " em b e d d i n g\n",
      ". Each\n",
      " ca tegor y‡ s represen ta tion would be learned during training: this is\n",
      " an exam ple of \n",
      " r e p r e s en t a t i o n l e a r n i n g\n",
      " (see \n",
      " Cha pter 13\n",
      " and \n",
      "???\n",
      " \n",
      "for\n",
      "more details).\n",
      "Custom Transformers\n",
      " Although Scikit-Learn provides man y useful transformers, you will need to write\n",
      " your own for tasks such as custom clean up opera tions or combining specific\n",
      " a ttributes. Y ou will wan t your transformer to work seamlessly with Scikit-Learn funcƒ\n",
      "tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inherƒ\n",
      " itance), all you need is to crea te a class and im plemen t three methods: \n",
      "fit()\n",
      "(returning \n",
      "self\n",
      "), \n",
      "transform()\n",
      ", and \n",
      "fit_transform()\n",
      " . Y ou can get the last one for\n",
      " free by sim ply adding \n",
      "TransformerMixin\n",
      " \n",
      " as a base class. Also , if you add \n",
      "BaseEstima\n",
      "tor\n",
      "  as a base class (and a void \n",
      "*args\n",
      " and \n",
      "**kargs\n",
      " in your constructor) you will get\n",
      "two extra methods (\n",
      "get_params()\n",
      " \n",
      "and \n",
      "set_params()\n",
      " ) tha t will be useful for a utoƒ\n",
      " Prepare the Data for Machine Learning Algorithms  |  71\n",
      "\n",
      " ma tic h yperparameter tuning. F or exam ple, here is a small transformer class tha t adds\n",
      " the combined a ttributes we discussed earlier :\n",
      "from\n",
      " \n",
      "sklearn.base\n",
      " \n",
      "import\n",
      " \n",
      "BaseEstimator\n",
      ",\n",
      " \n",
      "TransformerMixin\n",
      "rooms_ix\n",
      ",\n",
      " \n",
      "bedrooms_ix\n",
      ",\n",
      " \n",
      "population_ix\n",
      ",\n",
      " \n",
      "households_ix\n",
      " \n",
      "=\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "4\n",
      ",\n",
      " \n",
      "5\n",
      ",\n",
      " \n",
      "6\n",
      "class\n",
      " \n",
      "CombinedAttributesAdder\n",
      "(\n",
      "BaseEstimator\n",
      ",\n",
      " \n",
      "TransformerMixin\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "add_bedrooms_per_room\n",
      " \n",
      "=\n",
      " \n",
      "True\n",
      "):\n",
      " \n",
      "# no *args or **kargs\n",
      "        \n",
      "self\n",
      ".\n",
      "add_bedrooms_per_room\n",
      " \n",
      "=\n",
      " \n",
      "add_bedrooms_per_room\n",
      "    \n",
      "def\n",
      " \n",
      "fit\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "None\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "self\n",
      "  \n",
      "# nothing else to do\n",
      "    \n",
      "def\n",
      " \n",
      "transform\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "None\n",
      "):\n",
      "        \n",
      "rooms_per_household\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "[:,\n",
      " \n",
      "rooms_ix\n",
      "]\n",
      " \n",
      "/\n",
      " \n",
      "X\n",
      "[:,\n",
      " \n",
      "households_ix\n",
      "]\n",
      "        \n",
      "population_per_household\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "[:,\n",
      " \n",
      "population_ix\n",
      "]\n",
      " \n",
      "/\n",
      " \n",
      "X\n",
      "[:,\n",
      " \n",
      "households_ix\n",
      "]\n",
      "        \n",
      "if\n",
      " \n",
      "self\n",
      ".\n",
      "add_bedrooms_per_room\n",
      ":\n",
      "            \n",
      "bedrooms_per_room\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "[:,\n",
      " \n",
      "bedrooms_ix\n",
      "]\n",
      " \n",
      "/\n",
      " \n",
      "X\n",
      "[:,\n",
      " \n",
      "rooms_ix\n",
      "]\n",
      "            \n",
      "return\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "X\n",
      ",\n",
      " \n",
      "rooms_per_household\n",
      ",\n",
      " \n",
      "population_per_household\n",
      ",\n",
      "                         \n",
      "bedrooms_per_room\n",
      "]\n",
      "        \n",
      "else\n",
      ":\n",
      "            \n",
      "return\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "X\n",
      ",\n",
      " \n",
      "rooms_per_household\n",
      ",\n",
      " \n",
      "population_per_household\n",
      "]\n",
      "attr_adder\n",
      " \n",
      "=\n",
      " \n",
      "CombinedAttributesAdder\n",
      "(\n",
      "add_bedrooms_per_room\n",
      "=\n",
      "False\n",
      ")\n",
      "housing_extra_attribs\n",
      " \n",
      "=\n",
      " \n",
      "attr_adder\n",
      ".\n",
      "transform\n",
      "(\n",
      "housing\n",
      ".\n",
      "values\n",
      ")\n",
      " In this exam ple the transformer has one h yperparameter , \n",
      "add_bedrooms_per_room\n",
      ",\n",
      "set to \n",
      "True\n",
      " \n",
      " by defa ult (it is often helpful to provide sensible defa ults). This \n",
      " h yperparaƒ\n",
      " meter will allow you to easily find out whether adding this a ttribute helps the\n",
      " M achine Learning algorithms or not. M ore generally , you can add a h yperparameter\n",
      " to ga te an y da ta prepara tion step tha t you are not 100% sure about. The more you\n",
      " a utoma te these da ta prepara tion steps, the more combina tions you can a utoma tically\n",
      " tr y out, making it m uch more likely tha t you will find a grea t combina tion (and sa vƒ\n",
      "ing you a lot of time).\n",
      "Feature Scaling\n",
      " One of the most im portan t transforma tions you need to a pply to your da ta is \n",
      " f e a t u r e\n",
      " s c a l i n g\n",
      " . W ith few exceptions, M achine Learning algorithms don ‡ t perform well when\n",
      " the in put n umerical a ttributes ha ve ver y differen t scales. This is the case for the housƒ\n",
      " ing da ta: the total n umber of rooms ranges from about 6 to 39,320, while the median\n",
      " incomes only range from 0 to 15. N ote tha t scaling the target values is generally not\n",
      "required.\n",
      " There are two common wa ys to get all a ttributes to ha ve the same scale: \n",
      " m i n-m ax\n",
      " s c a l i n g\n",
      " and \n",
      " s t a n d a r d iz a t i o n\n",
      ".\n",
      " Min-max scaling (man y people call this \n",
      " n o r m a l iz a t i o n\n",
      " ) is quite sim ple: values are\n",
      " shifted and rescaled so tha t they end up ranging from 0 to 1. W e do this by subtractƒ\n",
      " ing the min value and dividing by the max min us the min. Scikit-Learn provides a\n",
      " 72  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "transformer called \n",
      "MinMaxScaler\n",
      "  for this. I t has a \n",
      "feature_range\n",
      " \n",
      " h yperparameter\n",
      " tha t lets you change the range if you don ‡ t wan t 0−1 for some reason.\n",
      " Standardiza tion is quite differen t: first it subtracts the mean value (so standardized\n",
      " values alwa ys ha ve a zero mean), and then it divides by the standard devia tion so tha t\n",
      " the resulting distribution has unit variance. U nlike min-max scaling, standardiza tion\n",
      " does not bound values to a specific range, which ma y be a problem for some algoƒ\n",
      " rithms (e.g., neural networks often expect an in put value ranging from 0 to 1). H owƒ\n",
      " ever , standardiza tion is m uch less affected by outliers. F or exam ple, suppose a district\n",
      "had a median income equal to 100 (by mistake). Min-max scaling would then crush\n",
      " all the other values from 0−15 down to 0−0.15, whereas standardiza tion would not be\n",
      " m uch affected. Scikit-Learn provides a transformer called \n",
      "StandardScaler\n",
      "   for standƒ\n",
      " ardiza tion.\n",
      " As with all the transforma tions, it is im portan t to fit the scalers to\n",
      " the training da ta only , not to the full da taset (including the test set).\n",
      "Only then can you use them to transform the training set and the\n",
      " test set (and new da ta).\n",
      "Transformation Pipelines\n",
      " As you can see, there are man y da ta transforma tion steps tha t need to be executed in\n",
      " the righ t order . F ortuna tely , Scikit-Learn provides the \n",
      "Pipeline\n",
      " class to help with\n",
      " such sequences of transforma tions. H ere is a small pipeline for the \n",
      " n umerical\n",
      " a ttributes:\n",
      "from\n",
      " \n",
      "sklearn.pipeline\n",
      " \n",
      "import\n",
      " \n",
      "Pipeline\n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "StandardScaler\n",
      "num_pipeline\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "•imputer•\n",
      ",\n",
      " \n",
      "SimpleImputer\n",
      "(\n",
      "strategy\n",
      "=\n",
      "\"median\"\n",
      ")),\n",
      "        \n",
      "(\n",
      "•attribs_adder•\n",
      ",\n",
      " \n",
      "CombinedAttributesAdder\n",
      "()),\n",
      "        \n",
      "(\n",
      "•std_scaler•\n",
      ",\n",
      " \n",
      "StandardScaler\n",
      "()),\n",
      "    \n",
      "])\n",
      "housing_num_tr\n",
      " \n",
      "=\n",
      " \n",
      "num_pipeline\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "housing_num\n",
      ")\n",
      "The \n",
      "Pipeline\n",
      "  constructor takes a list of name/estima tor pairs defining a sequence of\n",
      " steps. All but the last estima tor m ust be transformers (i.e., they m ust ha ve a\n",
      "fit_transform()\n",
      "  method). The names can be an ything you like (as long as they are\n",
      " unique and don ‡ t con tain double underscores —\n",
      "__\n",
      " –): they will come in handy la ter for\n",
      " h yperparameter tuning.\n",
      " When you call the pipeline ‡ s \n",
      "fit()\n",
      " method, it calls \n",
      "fit_transform()\n",
      " \n",
      " sequen tially on\n",
      " all transformers, passing the output of each call as the parameter to the next call, un til\n",
      " it reaches the final estima tor , for which it just calls the \n",
      "fit()\n",
      " method.\n",
      " Prepare the Data for Machine Learning Algorithms  |  73\n",
      "\n",
      "22\n",
      " J ust like for pipelines, the name can be an ything as long as it does not con tain double underscores.\n",
      " The pipeline exposes the same methods as the final estima tor . In this exam ple, the last\n",
      " estima tor is a \n",
      "StandardScaler\n",
      " , which is a transformer , so the pipeline has \n",
      "a \n",
      "trans\n",
      "form()\n",
      "  method tha t a pplies all the transforms to the da ta in sequence (and of course\n",
      "also a \n",
      "fit_transform()\n",
      " method, which is the one we used).\n",
      " So far , we ha ve handled the ca tegorical columns and the n umerical columns sepaƒ\n",
      " ra tely . I t would be more con venien t to ha ve a single transformer able to handle all colƒ\n",
      " umns, a pplying the a ppropria te transforma tions to each column. In version 0.20,\n",
      " Scikit-Learn in troduced the \n",
      "ColumnTransformer\n",
      " for this purpose, and the good news\n",
      " is tha t it works grea t with P andas Da taFrames. Let ‡ s use it to a pply all the transformaƒ\n",
      " tions to the housing da ta:\n",
      "from\n",
      " \n",
      "sklearn.compose\n",
      " \n",
      "import\n",
      " \n",
      "ColumnTransformer\n",
      "num_attribs\n",
      " \n",
      "=\n",
      " \n",
      "list\n",
      "(\n",
      "housing_num\n",
      ")\n",
      "cat_attribs\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "\"ocean_proximity\"\n",
      "]\n",
      "full_pipeline\n",
      " \n",
      "=\n",
      " \n",
      "ColumnTransformer\n",
      "([\n",
      "        \n",
      "(\n",
      "\"num\"\n",
      ",\n",
      " \n",
      "num_pipeline\n",
      ",\n",
      " \n",
      "num_attribs\n",
      "),\n",
      "        \n",
      "(\n",
      "\"cat\"\n",
      ",\n",
      " \n",
      "OneHotEncoder\n",
      "(),\n",
      " \n",
      "cat_attribs\n",
      "),\n",
      "    \n",
      "])\n",
      "housing_prepared\n",
      " \n",
      "=\n",
      " \n",
      "full_pipeline\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "housing\n",
      ")\n",
      " H ere is how this works: first we im port the \n",
      "ColumnTransformer\n",
      " \n",
      "class, next we get the\n",
      " list of n umerical column names and the list of ca tegorical column names, and we\n",
      "construct a \n",
      "ColumnTransformer\n",
      ". The constructor requires a list of tuples, where each\n",
      " tuple con tains a name\n",
      "22\n",
      ", a transformer and a list of names (or indices) of columns\n",
      " tha t the transformer should be a pplied to . In this exam ple, we specif y tha t the n umerƒ\n",
      "ical columns should be transformed using the \n",
      "num_pipeline\n",
      "  tha t we defined earlier ,\n",
      " and the ca tegorical columns should be transformed using a \n",
      "OneHotEncoder\n",
      " . Finally ,\n",
      " we a pply this \n",
      "ColumnTransformer\n",
      "  to the housing da ta: it a pplies each transformer to\n",
      " the a ppropria te columns and conca tena tes the outputs along the second axis (the\n",
      " transformers m ust return the same n umber of rows).\n",
      " N ote tha t the \n",
      "OneHotEncoder\n",
      "   returns a sparse ma trix, while the \n",
      "num_pipeline\n",
      "   returns\n",
      " a dense ma trix. When there is such a mix of sparse and dense ma trices, the \n",
      "Colum\n",
      "nTransformer\n",
      "  estima tes the density of the final ma trix (i.e., the ra tio of non-zero\n",
      " cells), and it returns a sparse ma trix if the density is lower than a given threshold (by\n",
      " defa ult, \n",
      "sparse_threshold=0.3\n",
      " ). In this exam ple, it returns a dense ma trix. And\n",
      " tha t ‡ s it! W e ha ve a preprocessing pipeline tha t takes the full housing da ta and a pplies\n",
      " the a ppropria te transforma tions to each column.\n",
      " 74  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " Instead of a transformer , you can specif y the string \n",
      "\"drop\"\n",
      " if you\n",
      " wan t the columns to be dropped. Or you can specif y \n",
      "\"pass\n",
      "through\"\n",
      " \n",
      " if you wan t the columns to be left un touched. By defa ult,\n",
      " the remaining columns (i.e., the ones tha t were not listed) will be\n",
      "dropped, but you can set the \n",
      "remainder\n",
      "  h yperparameter to an y\n",
      "transformer (or to \n",
      "\"passthrough\"\n",
      " ) if you wan t these columns to be\n",
      " handled differen tly .\n",
      " If you are using Scikit-Learn 0.19 or earlier , you can use a third-party librar y such as\n",
      "sklearn-pandas\n",
      ", or roll out your own custom transformer to get the same functionƒ\n",
      "ality as the \n",
      "ColumnTransformer\n",
      " . Alterna tively , you can use the \n",
      "FeatureUnion\n",
      " \n",
      "class\n",
      " which can also a pply differen t transformers and conca tena te their outputs, but you\n",
      " cannot specif y differen t columns for each transformer , they all a pply to the whole\n",
      " da ta. I t is possible to work around this limita tion using a custom transformer for colƒ\n",
      " umn selection (see the J upyter notebook for an exam ple).\n",
      "Select and Train a Model\n",
      " A t last! Y ou framed the problem, you got the da ta and explored it, you sam pled a\n",
      " training set and a test set, and you wrote transforma tion pipelines to clean up and\n",
      " prepare your da ta for M achine Learning algorithms a utoma tically . Y ou are now ready\n",
      " to select and train a M achine Learning model.\n",
      "Training and Evaluating on the Training Set\n",
      " The good news is tha t thanks to all these previous steps, things are now going to be\n",
      " m uch sim pler than you migh t think. Let ‡ s first train a Linear Regression model, like\n",
      " we did in the previous cha pter :\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "LinearRegression\n",
      "lin_reg\n",
      " \n",
      "=\n",
      " \n",
      "LinearRegression\n",
      "()\n",
      "lin_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "housing_prepared\n",
      ",\n",
      " \n",
      "housing_labels\n",
      ")\n",
      " Done! Y ou now ha ve a working Linear Regression model. Let ‡ s tr y it out on a few\n",
      "instances from the training set:\n",
      ">>> \n",
      "some_data\n",
      " \n",
      "=\n",
      " \n",
      "housing\n",
      ".\n",
      "iloc\n",
      "[:\n",
      "5\n",
      "]\n",
      ">>> \n",
      "some_labels\n",
      " \n",
      "=\n",
      " \n",
      "housing_labels\n",
      ".\n",
      "iloc\n",
      "[:\n",
      "5\n",
      "]\n",
      ">>> \n",
      "some_data_prepared\n",
      " \n",
      "=\n",
      " \n",
      "full_pipeline\n",
      ".\n",
      "transform\n",
      "(\n",
      "some_data\n",
      ")\n",
      ">>> \n",
      "print\n",
      "(\n",
      "\"Predictions:\"\n",
      ",\n",
      " \n",
      "lin_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "some_data_prepared\n",
      "))\n",
      "Predictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\n",
      ">>> \n",
      "print\n",
      "(\n",
      "\"Labels:\"\n",
      ",\n",
      " \n",
      "list\n",
      "(\n",
      "some_labels\n",
      "))\n",
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n",
      " Select and Train a Model  |  75\n",
      "\n",
      " I t works, although the predictions are not exactly accura te (e.g., the first prediction is\n",
      " off by close to 40%!). Let ‡ s measure this regression model ‡ s RMSE on the whole trainƒ\n",
      " ing set using Scikit-Learn ‡ s \n",
      "mean_squared_error\n",
      " function:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "mean_squared_error\n",
      ">>> \n",
      "housing_predictions\n",
      " \n",
      "=\n",
      " \n",
      "lin_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "housing_prepared\n",
      ")\n",
      ">>> \n",
      "lin_mse\n",
      " \n",
      "=\n",
      " \n",
      "mean_squared_error\n",
      "(\n",
      "housing_labels\n",
      ",\n",
      " \n",
      "housing_predictions\n",
      ")\n",
      ">>> \n",
      "lin_rmse\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "lin_mse\n",
      ")\n",
      ">>> \n",
      "lin_rmse\n",
      "68628.19819848922\n",
      " Oka y , this is better than nothing but clearly not a grea t score: most districts ‡\n",
      "median_housing_values\n",
      " range between $120,000 and $265,000, so a typical predicƒ\n",
      " tion error of $68,628 is not ver y sa tisf ying. This is an exam ple of a model \n",
      "underfitting\n",
      " the training da ta. When this ha ppens it can mean tha t the fea tures do not provide\n",
      " enough informa tion to make good predictions, or tha t the model is not powerful\n",
      " enough. As we sa w in the previous cha pter , the main wa ys to fix underfitting are to\n",
      " select a more powerful model, to feed the training algorithm with better fea tures, or\n",
      " to reduce the constrain ts on the model. This model is not regularized, so this rules\n",
      " out the last option. Y ou could tr y to add more fea tures (e.g., the log of the populaƒ\n",
      " tion), but first let ‡ s tr y a more com plex model to see how it does.\n",
      " Let ‡ s train a \n",
      "DecisionTreeRegressor\n",
      " . This is a powerful model, ca pable of finding\n",
      " com plex nonlinear rela tionships in the da ta (Decision T rees are presen ted in more\n",
      "detail in \n",
      " Cha pter 6\n",
      " ). The code should look familiar by now :\n",
      "from\n",
      " \n",
      "sklearn.tree\n",
      " \n",
      "import\n",
      " \n",
      "DecisionTreeRegressor\n",
      "tree_reg\n",
      " \n",
      "=\n",
      " \n",
      "DecisionTreeRegressor\n",
      "()\n",
      "tree_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "housing_prepared\n",
      ",\n",
      " \n",
      "housing_labels\n",
      ")\n",
      " N ow tha t the model is trained, let ‡ s evalua te it on the training set:\n",
      ">>> \n",
      "housing_predictions\n",
      " \n",
      "=\n",
      " \n",
      "tree_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "housing_prepared\n",
      ")\n",
      ">>> \n",
      "tree_mse\n",
      " \n",
      "=\n",
      " \n",
      "mean_squared_error\n",
      "(\n",
      "housing_labels\n",
      ",\n",
      " \n",
      "housing_predictions\n",
      ")\n",
      ">>> \n",
      "tree_rmse\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "tree_mse\n",
      ")\n",
      ">>> \n",
      "tree_rmse\n",
      "0.0\n",
      " W ait, wha t!? N o error a t all? Could this model really be absolutely perfect? Of course,\n",
      " it is m uch more likely tha t the model has badly overfit the da ta. H ow can you be sure?\n",
      " As we sa w earlier , you don ‡ t wan t to touch the test set un til you are ready to la unch a\n",
      " model you are confiden t about, so you need to use part of the training set for trainƒ\n",
      " ing, and part for model valida tion.\n",
      "Better Evaluation Using Cross-Validation\n",
      " One wa y to evalua te the Decision T ree model would be to use the \n",
      "train_test_split\n",
      " function to split the training set in to a smaller training set and a valida tion set, then\n",
      " 76  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " train your models against the smaller training set and evalua te them against the valiƒ\n",
      " da tion set. I t ‡ s a bit of work, but nothing too difficult and it would work fairly well.\n",
      " A grea t alterna tive is to use Scikit-Learn ‡ s \n",
      " K-f o l d cr o s s-v a l i d a t i o n\n",
      "  fea ture. The followƒ\n",
      " ing code randomly splits the training set in to 10 distinct subsets called \n",
      " f o l d s\n",
      ", then it\n",
      " trains and evalua tes the Decision T ree model 10 times, picking a differen t fold for\n",
      " evalua tion ever y time and training on the other 9 folds. The result is an arra y conƒ\n",
      " taining the 10 evalua tion scores:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "cross_val_score\n",
      "scores\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_score\n",
      "(\n",
      "tree_reg\n",
      ",\n",
      " \n",
      "housing_prepared\n",
      ",\n",
      " \n",
      "housing_labels\n",
      ",\n",
      "                         \n",
      "scoring\n",
      "=\n",
      "\"neg_mean_squared_error\"\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "10\n",
      ")\n",
      "tree_rmse_scores\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "-\n",
      "scores\n",
      ")\n",
      " Scikit-Learn ‡ s cross-valida tion fea tures expect a utility function\n",
      " (grea ter is better) ra ther than a cost function (lower is better), so\n",
      "the scoring function is actually the opposite of the MSE (i.e., a negƒ\n",
      " a tive value), which is wh y the preceding code com putes \n",
      "-scores\n",
      " before calcula ting the square root.\n",
      " Let ‡ s look a t the results:\n",
      ">>> \n",
      "def\n",
      " \n",
      "display_scores\n",
      "(\n",
      "scores\n",
      "):\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "\"Scores:\"\n",
      ",\n",
      " \n",
      "scores\n",
      ")\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "\"Mean:\"\n",
      ",\n",
      " \n",
      "scores\n",
      ".\n",
      "mean\n",
      "())\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "\"Standard deviation:\"\n",
      ",\n",
      " \n",
      "scores\n",
      ".\n",
      "std\n",
      "())\n",
      "...\n",
      ">>> \n",
      "display_scores\n",
      "(\n",
      "tree_rmse_scores\n",
      ")\n",
      "Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\n",
      " 71115.88230639 75585.14172901 70262.86139133 70273.6325285\n",
      " 75366.87952553 71231.65726027]\n",
      "Mean: 71407.68766037929\n",
      "Standard deviation: 2439.4345041191004\n",
      " N ow the Decision T ree doesn ‡ t look as good as it did earlier . In fact, it seems to perƒ\n",
      " form worse than the Linear Regression model! N otice tha t cross-valida tion allows\n",
      " you to get not only an estima te of the performance of your model, but also a measure\n",
      " of how precise this estima te is (i.e., its standard devia tion). The Decision T ree has a\n",
      " score of a pproxima tely 71,407, generally ﬂ2,439. Y ou would not ha ve this informa tion\n",
      " if you just used one valida tion set. But cross-valida tion comes a t the cost of training\n",
      " the model several times, so it is not alwa ys possible.\n",
      " Let ‡ s com pute the same scores for the Linear Regression model just to be sure:\n",
      ">>> \n",
      "lin_scores\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_score\n",
      "(\n",
      "lin_reg\n",
      ",\n",
      " \n",
      "housing_prepared\n",
      ",\n",
      " \n",
      "housing_labels\n",
      ",\n",
      "... \n",
      "                             \n",
      "scoring\n",
      "=\n",
      "\"neg_mean_squared_error\"\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "10\n",
      ")\n",
      "...\n",
      ">>> \n",
      "lin_rmse_scores\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "-\n",
      "lin_scores\n",
      ")\n",
      ">>> \n",
      "display_scores\n",
      "(\n",
      "lin_rmse_scores\n",
      ")\n",
      " Select and Train a Model  |  77\n",
      "\n",
      "Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n",
      " 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n",
      " 71552.91566558 67665.10082067]\n",
      "Mean: 69052.46136345083\n",
      "Standard deviation: 2731.674001798348\n",
      " Tha t ‡ s righ t: the Decision T ree model is overfitting so badly tha t it performs worse\n",
      "than the Linear Regression model.\n",
      " Let ‡ s tr y one last model now : the \n",
      "RandomForestRegressor\n",
      ". As we will see in \n",
      " Cha pƒ\n",
      "ter 7\n",
      " , R andom F orests work by training man y Decision T rees on random subsets of\n",
      " the fea tures, then a veraging out their predictions. Building a model on top of man y\n",
      "other models is \n",
      "called \n",
      " E ns em b l e L e a r n i n g\n",
      " , and it is often a grea t wa y to push ML algoƒ\n",
      " rithms even further . W e will skip most of the code since it is essen tially the same as\n",
      "for the other models:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "RandomForestRegressor\n",
      ">>> \n",
      "forest_reg\n",
      " \n",
      "=\n",
      " \n",
      "RandomForestRegressor\n",
      "()\n",
      ">>> \n",
      "forest_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "housing_prepared\n",
      ",\n",
      " \n",
      "housing_labels\n",
      ")\n",
      ">>> \n",
      "[\n",
      "...\n",
      "]\n",
      ">>> \n",
      "forest_rmse\n",
      "18603.515021376355\n",
      ">>> \n",
      "display_scores\n",
      "(\n",
      "forest_rmse_scores\n",
      ")\n",
      "Scores: [49519.80364233 47461.9115823  50029.02762854 52325.28068953\n",
      " 49308.39426421 53446.37892622 48634.8036574  47585.73832311\n",
      " 53490.10699751 50021.5852922 ]\n",
      "Mean: 50182.303100336096\n",
      "Standard deviation: 2097.0810550985693\n",
      " W ow , this is m uch better : R andom F orests look ver y promising. H owever , note tha t\n",
      " the score on the training set is still m uch lower than on the valida tion sets, meaning\n",
      " tha t the model is still overfitting the training set. P ossible solutions for overfitting are\n",
      " to sim plif y the model, constrain it (i.e., regularize it), or get a lot more training da ta.\n",
      " H owever , before you dive m uch deeper in R andom F orests, you should tr y out man y\n",
      " other models from various ca tegories of M achine Learning algorithms (several Supƒ\n",
      " port V ector M achines with differen t kernels, possibly a neural network, etc.), without\n",
      " spending too m uch time tweaking the h yperparameters. The goal is to shortlist a few\n",
      "(two to five) promising models.\n",
      " 78  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " Y ou should sa ve ever y model you experimen t with, so you can\n",
      " come back easily to an y model you wan t. M ake sure you sa ve both\n",
      " the h yperparameters and the trained parameters, as well as the\n",
      " cross-valida tion scores and perha ps the actual predictions as well.\n",
      " This will allow you to easily com pare scores across model types,\n",
      " and com pare the types of errors they make. Y ou can easily sa ve\n",
      " Scikit-Learn models by using Python ‡ s \n",
      "pickle\n",
      " module, or \n",
      "using\n",
      "sklearn.externals.joblib\n",
      " , which is more efficien t a t serializing \n",
      " large N umPy arra ys:\n",
      "from\n",
      " \n",
      "sklearn.externals\n",
      " \n",
      "import\n",
      " \n",
      "joblib\n",
      "joblib\n",
      ".\n",
      "dump\n",
      "(\n",
      "my_model\n",
      ",\n",
      " \n",
      "\"my_model.pkl\"\n",
      ")\n",
      "# and later...\n",
      "my_model_loaded\n",
      " \n",
      "=\n",
      " \n",
      "joblib\n",
      ".\n",
      "load\n",
      "(\n",
      "\"my_model.pkl\"\n",
      ")\n",
      "Fine-Tune Your Model\n",
      " Let ‡ s assume tha t you now ha ve a shortlist of promising models. Y ou now need to\n",
      " fine-tune them. Let ‡ s look a t a few wa ys you can do tha t.\n",
      "Grid Search\n",
      " One wa y to do tha t would be to fiddle with the h yperparameters man ually , un til you\n",
      " find a grea t combina tion of h yperparameter values. This would be ver y tedious work,\n",
      " and you ma y not ha ve time to explore man y combina tions.\n",
      "Instead you should get \n",
      " Scikit-Learn ‡ s \n",
      "GridSearchCV\n",
      " \n",
      "to search for you. All you need to\n",
      " do is tell it which h yperparameters you wan t it to experimen t with, and wha t values to\n",
      " tr y out, and it will evalua te all the possible combina tions of h yperparameter values,\n",
      " using cross-valida tion. F or exam ple, the following code searches for the best combiƒ\n",
      " na tion of h yperparameter values for the \n",
      "RandomForestRegressor\n",
      ":\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "GridSearchCV\n",
      "param_grid\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "    \n",
      "{\n",
      "•n_estimators•\n",
      ":\n",
      " \n",
      "[\n",
      "3\n",
      ",\n",
      " \n",
      "10\n",
      ",\n",
      " \n",
      "30\n",
      "],\n",
      " \n",
      "•max_features•\n",
      ":\n",
      " \n",
      "[\n",
      "2\n",
      ",\n",
      " \n",
      "4\n",
      ",\n",
      " \n",
      "6\n",
      ",\n",
      " \n",
      "8\n",
      "]},\n",
      "    \n",
      "{\n",
      "•bootstrap•\n",
      ":\n",
      " \n",
      "[\n",
      "False\n",
      "],\n",
      " \n",
      "•n_estimators•\n",
      ":\n",
      " \n",
      "[\n",
      "3\n",
      ",\n",
      " \n",
      "10\n",
      "],\n",
      " \n",
      "•max_features•\n",
      ":\n",
      " \n",
      "[\n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "4\n",
      "]},\n",
      "  \n",
      "]\n",
      "forest_reg\n",
      " \n",
      "=\n",
      " \n",
      "RandomForestRegressor\n",
      "()\n",
      "grid_search\n",
      " \n",
      "=\n",
      " \n",
      "GridSearchCV\n",
      "(\n",
      "forest_reg\n",
      ",\n",
      " \n",
      "param_grid\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "5\n",
      ",\n",
      "                           \n",
      "scoring\n",
      "=\n",
      "•neg_mean_squared_error•\n",
      ",\n",
      "                           \n",
      "return_train_score\n",
      "=\n",
      "True\n",
      ")\n",
      "grid_search\n",
      ".\n",
      "fit\n",
      "(\n",
      "housing_prepared\n",
      ",\n",
      " \n",
      "housing_labels\n",
      ")\n",
      " Fine-Tune Your Model  |  79\n",
      "\n",
      " When you ha ve no idea wha t value a h yperparameter should ha ve,\n",
      " a sim ple a pproach is to tr y out consecutive powers of 10 (or a\n",
      " smaller n umber if you wan t a more fine-grained search, as shown\n",
      " in this exam ple with the \n",
      "n_estimators\n",
      "  h yperparameter).\n",
      "This \n",
      "param_grid\n",
      "  tells Scikit-Learn to first evalua te all 3 „ 4 = 12 combina tions of\n",
      "n_estimators\n",
      " and \n",
      "max_features\n",
      "  h yperparameter values specified in the first \n",
      "dict\n",
      " (don ‡ t worr y about wha t these h yperparameters mean for now ; they will be explained\n",
      "in \n",
      " Cha pter 7\n",
      " ), then tr y all 2 „ 3 = 6 combina tions of h yperparameter values in the\n",
      "second \n",
      "dict\n",
      ", but this time with the \n",
      "bootstrap\n",
      "   h yperparameter set to \n",
      "False\n",
      "   instead of\n",
      "True\n",
      "  (which is the defa ult value for this h yperparameter).\n",
      " All in all, the grid search will explore 12 + 6 = 18 combina tions of \n",
      "RandomForestRe\n",
      "gressor\n",
      "  h yperparameter values, and it will train each model five times (since we are\n",
      " using five-fold cross valida tion). In other words, all in all, there will be 18 „ 5 = 90\n",
      " rounds of training! I t ma y take quite a long time, but when it is done you can get the\n",
      " best combina tion of parameters like this:\n",
      ">>> \n",
      "grid_search\n",
      ".\n",
      "best_params_\n",
      "{•max_features•: 8, •n_estimators•: 30}\n",
      " Since 8 and 30 are the maxim um values tha t were evalua ted, you\n",
      " should probably tr y searching again with higher values, since the\n",
      " score ma y con tin ue to im prove.\n",
      " Y ou can also get the best estima tor directly :\n",
      ">>> \n",
      "grid_search\n",
      ".\n",
      "best_estimator_\n",
      "RandomForestRegressor(bootstrap=True, criterion=•mse•, max_depth=None,\n",
      "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=30, n_jobs=None, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "If \n",
      "GridSearchCV\n",
      " is initialized with \n",
      "refit=True\n",
      " (which is the\n",
      " defa ult), then once it finds the best estima tor using cross-\n",
      " valida tion, it retrains it on the whole training set. This is usually a\n",
      " good idea since feeding it more da ta will likely im prove its perforƒ\n",
      "mance.\n",
      " And of course the evalua tion scores are also a vailable:\n",
      ">>> \n",
      "cvres\n",
      " \n",
      "=\n",
      " \n",
      "grid_search\n",
      ".\n",
      "cv_results_\n",
      ">>> \n",
      "for\n",
      " \n",
      "mean_score\n",
      ",\n",
      " \n",
      "params\n",
      " \n",
      "in\n",
      " \n",
      "zip\n",
      "(\n",
      "cvres\n",
      "[\n",
      "\"mean_test_score\"\n",
      "],\n",
      " \n",
      "cvres\n",
      "[\n",
      "\"params\"\n",
      "]):\n",
      " 80  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "-\n",
      "mean_score\n",
      "),\n",
      " \n",
      "params\n",
      ")\n",
      "...\n",
      "63669.05791727153 {•max_features•: 2, •n_estimators•: 3}\n",
      "55627.16171305252 {•max_features•: 2, •n_estimators•: 10}\n",
      "53384.57867637289 {•max_features•: 2, •n_estimators•: 30}\n",
      "60965.99185930139 {•max_features•: 4, •n_estimators•: 3}\n",
      "52740.98248528835 {•max_features•: 4, •n_estimators•: 10}\n",
      "50377.344409590376 {•max_features•: 4, •n_estimators•: 30}\n",
      "58663.84733372485 {•max_features•: 6, •n_estimators•: 3}\n",
      "52006.15355973719 {•max_features•: 6, •n_estimators•: 10}\n",
      "50146.465964159885 {•max_features•: 6, •n_estimators•: 30}\n",
      "57869.25504027614 {•max_features•: 8, •n_estimators•: 3}\n",
      "51711.09443660957 {•max_features•: 8, •n_estimators•: 10}\n",
      "49682.25345942335 {•max_features•: 8, •n_estimators•: 30}\n",
      "62895.088889905004 {•bootstrap•: False, •max_features•: 2, •n_estimators•: 3}\n",
      "54658.14484390074 {•bootstrap•: False, •max_features•: 2, •n_estimators•: 10}\n",
      "59470.399594730654 {•bootstrap•: False, •max_features•: 3, •n_estimators•: 3}\n",
      "52725.01091081235 {•bootstrap•: False, •max_features•: 3, •n_estimators•: 10}\n",
      "57490.612956065226 {•bootstrap•: False, •max_features•: 4, •n_estimators•: 3}\n",
      "51009.51445842374 {•bootstrap•: False, •max_features•: 4, •n_estimators•: 10}\n",
      " In this exam ple, we obtain the best solution by setting the \n",
      "max_features\n",
      " \n",
      " h yperparaƒ\n",
      "meter to \n",
      "8\n",
      ", and the \n",
      "n_estimators\n",
      "  h yperparameter to \n",
      "30\n",
      ". The RMSE score for this\n",
      " combina tion is 49,682, which is sligh tly better than the score you got earlier using the\n",
      " defa ult h yperparameter values (which was 50,182). Congra tula tions, you ha ve sucƒ\n",
      "cessfully fine-tuned your best model!\n",
      " Don ‡ t forget tha t you can trea t some of the da ta prepara tion steps as\n",
      " h yperparameters. F or exam ple, the grid search will a utoma tically\n",
      " find out whether or not to add a fea ture you were not sure about\n",
      "(e.g., using the \n",
      "add_bedrooms_per_room\n",
      "  h yperparameter of your\n",
      "CombinedAttributesAdder\n",
      "  transformer). I t ma y similarly be used\n",
      " to a utoma tically find the best wa y to handle outliers, missing feaƒ\n",
      " tures, fea ture selection, and more.\n",
      "Randomized Search\n",
      "The \n",
      " grid search a pproach is fine when you are exploring rela tively few combina tions,\n",
      " like in the previous exam ple, but when the h yperparameter \n",
      " s e a r c h s p a c e\n",
      " is large, it is\n",
      "often preferable to use \n",
      "RandomizedSearchCV\n",
      "  instead. This class can be used in m uch\n",
      " the same wa y as the \n",
      "GridSearchCV\n",
      "  class, but instead of tr ying out all possible combiƒ\n",
      " na tions, it evalua tes a given n umber of random combina tions by selecting a random\n",
      " value for each h yperparameter a t ever y itera tion. This a pproach has two main beneƒ\n",
      "fits:\n",
      " Fine-Tune Your Model  |  81\n",
      "\n",
      "⁄\n",
      " If you let the randomized search run for , sa y , 1,000 itera tions, this a pproach will\n",
      " explore 1,000 differen t values for each h yperparameter (instead of just a few valƒ\n",
      " ues per h yperparameter with the grid search a pproach).\n",
      "⁄\n",
      " Y ou ha ve more con trol over the com puting budget you wan t to alloca te to h yperƒ\n",
      " parameter search, sim ply by setting the n umber of itera tions.\n",
      "Ensemble Methods\n",
      " Another wa y to fine-tune your system is to tr y to combine the models tha t perform\n",
      " best. The group (or — ensemble –) will often perform better than the best individual\n",
      " model (just like R andom F orests perform better than the individual Decision T rees\n",
      " they rely on), especially if the individual models make ver y differen t types of errors.\n",
      " W e will cover this topic in more detail in \n",
      " Cha pter 7\n",
      ".\n",
      "Analyze the Best Models and Their Errors\n",
      " Y ou will often gain good insigh ts on the problem by inspecting the best models. F or\n",
      " exam ple, the \n",
      "RandomForestRegressor\n",
      "  can indica te the rela tive im portance of each\n",
      " a ttribute for making accura te predictions:\n",
      ">>> \n",
      "feature_importances\n",
      " \n",
      "=\n",
      " \n",
      "grid_search\n",
      ".\n",
      "best_estimator_\n",
      ".\n",
      "feature_importances_\n",
      ">>> \n",
      "feature_importances\n",
      "array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n",
      "       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n",
      "       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n",
      "       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\n",
      " Let ‡ s displa y these im portance scores next to their corresponding a ttribute names:\n",
      ">>> \n",
      "extra_attribs\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "\"rooms_per_hhold\"\n",
      ",\n",
      " \n",
      "\"pop_per_hhold\"\n",
      ",\n",
      " \n",
      "\"bedrooms_per_room\"\n",
      "]\n",
      ">>> \n",
      "cat_encoder\n",
      " \n",
      "=\n",
      " \n",
      "full_pipeline\n",
      ".\n",
      "named_transformers_\n",
      "[\n",
      "\"cat\"\n",
      "]\n",
      ">>> \n",
      "cat_one_hot_attribs\n",
      " \n",
      "=\n",
      " \n",
      "list\n",
      "(\n",
      "cat_encoder\n",
      ".\n",
      "categories_\n",
      "[\n",
      "0\n",
      "])\n",
      ">>> \n",
      "attributes\n",
      " \n",
      "=\n",
      " \n",
      "num_attribs\n",
      " \n",
      "+\n",
      " \n",
      "extra_attribs\n",
      " \n",
      "+\n",
      " \n",
      "cat_one_hot_attribs\n",
      ">>> \n",
      "sorted\n",
      "(\n",
      "zip\n",
      "(\n",
      "feature_importances\n",
      ",\n",
      " \n",
      "attributes\n",
      "),\n",
      " \n",
      "reverse\n",
      "=\n",
      "True\n",
      ")\n",
      "[(0.3661589806181342, •median_income•),\n",
      " (0.1647809935615905, •INLAND•),\n",
      " (0.10879295677551573, •pop_per_hhold•),\n",
      " (0.07334423551601242, •longitude•),\n",
      " (0.0629090704826203, •latitude•),\n",
      " (0.05641917918195401, •rooms_per_hhold•),\n",
      " (0.05335107734767581, •bedrooms_per_room•),\n",
      " (0.041143798478729635, •housing_median_age•),\n",
      " (0.014874280890402767, •population•),\n",
      " (0.014672685420543237, •total_rooms•),\n",
      " (0.014257599323407807, •households•),\n",
      " (0.014106483453584102, •total_bedrooms•),\n",
      " (0.010311488326303787, •<1H OCEAN•),\n",
      " (0.002856474637320158, •NEAR OCEAN•),\n",
      " 82  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      " (0.00196041559947807, •NEAR BAY•),\n",
      " (6.028038672736599e-05, •ISLAND•)]\n",
      " W ith this informa tion, you ma y wan t to tr y dropping some of the less useful fea tures\n",
      " (e.g., a pparen tly only one \n",
      "ocean_proximity\n",
      " \n",
      " ca tegor y is really useful, so you could tr y\n",
      "dropping the others).\n",
      " Y ou should also look a t the specific errors tha t your system makes, then tr y to underƒ\n",
      " stand wh y it makes them and wha t could fix the problem (adding extra fea tures or , on\n",
      " the con trar y , getting rid of uninforma tive ones, cleaning up outliers, etc.).\n",
      "Evaluate Your System on the Test Set\n",
      " After tweaking your models for a while, you even tually ha ve a system tha t performs\n",
      " sufficien tly well. N ow is the time to evalua te the final model on the test set. There is\n",
      "nothing special about this process; just get the predictors and the labels from your\n",
      "test set, run your \n",
      "full_pipeline\n",
      " \n",
      " to transform the da ta (call \n",
      "transform()\n",
      ", \n",
      " n o t\n",
      "fit_transform()\n",
      " , you do not wan t to fit the test set!), and evalua te the final model\n",
      "on the test set:\n",
      "final_model\n",
      " \n",
      "=\n",
      " \n",
      "grid_search\n",
      ".\n",
      "best_estimator_\n",
      "X_test\n",
      " \n",
      "=\n",
      " \n",
      "strat_test_set\n",
      ".\n",
      "drop\n",
      "(\n",
      "\"median_house_value\"\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "1\n",
      ")\n",
      "y_test\n",
      " \n",
      "=\n",
      " \n",
      "strat_test_set\n",
      "[\n",
      "\"median_house_value\"\n",
      "]\n",
      ".\n",
      "copy\n",
      "()\n",
      "X_test_prepared\n",
      " \n",
      "=\n",
      " \n",
      "full_pipeline\n",
      ".\n",
      "transform\n",
      "(\n",
      "X_test\n",
      ")\n",
      "final_predictions\n",
      " \n",
      "=\n",
      " \n",
      "final_model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test_prepared\n",
      ")\n",
      "final_mse\n",
      " \n",
      "=\n",
      " \n",
      "mean_squared_error\n",
      "(\n",
      "y_test\n",
      ",\n",
      " \n",
      "final_predictions\n",
      ")\n",
      "final_rmse\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "final_mse\n",
      ")\n",
      "   \n",
      "# => evaluates to 47,730.2\n",
      " In some cases, such a poin t estima te of the generaliza tion error will not be quite\n",
      " enough to con vince you to la unch: wha t if it is just 0.1% better than the model curƒ\n",
      " ren tly in production? Y ou migh t wan t to ha ve an idea of how precise this estima te is.\n",
      " F or this, you can com pute a 95% \n",
      "con†dence\n",
      "  i n t er v a l\n",
      " \n",
      " for the generaliza tion error using\n",
      "scipy.stats.t.interval()\n",
      ":\n",
      ">>> \n",
      "from\n",
      " \n",
      "scipy\n",
      " \n",
      "import\n",
      " \n",
      "stats\n",
      ">>> \n",
      "confidence\n",
      " \n",
      "=\n",
      " \n",
      "0.95\n",
      ">>> \n",
      "squared_errors\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "final_predictions\n",
      " \n",
      "-\n",
      " \n",
      "y_test\n",
      ")\n",
      " \n",
      "**\n",
      " \n",
      "2\n",
      ">>> \n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "stats\n",
      ".\n",
      "t\n",
      ".\n",
      "interval\n",
      "(\n",
      "confidence\n",
      ",\n",
      " \n",
      "len\n",
      "(\n",
      "squared_errors\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "1\n",
      ",\n",
      "... \n",
      "                         \n",
      "loc\n",
      "=\n",
      "squared_errors\n",
      ".\n",
      "mean\n",
      "(),\n",
      "... \n",
      "                         \n",
      "scale\n",
      "=\n",
      "stats\n",
      ".\n",
      "sem\n",
      "(\n",
      "squared_errors\n",
      ")))\n",
      "...\n",
      "array([45685.10470776, 49691.25001878])\n",
      " The performance will usually be sligh tly worse than wha t you measured using cross-\n",
      " valida tion if you did a lot of h yperparameter tuning (beca use your system ends up\n",
      " fine-tuned to perform well on the valida tion da ta, and will likely not perform as well\n",
      " Fine-Tune Your Model  |  83\n",
      "\n",
      " on unknown da tasets). I t is not the case in this exam ple, but when this ha ppens you\n",
      " m ust resist the tem pta tion to tweak the h yperparameters to make the n umbers look\n",
      " good on the test set; the im provemen ts would be unlikely to generalize to new da ta.\n",
      " N ow comes the project prela unch phase: you need to presen t your solution (highƒ\n",
      " ligh ting wha t you ha ve learned, wha t worked and wha t did not, wha t assum ptions\n",
      " were made, and wha t your system ‡ s limita tions are), documen t ever ything, and crea te\n",
      " nice presen ta tions with clear visualiza tions and easy-to-remember sta temen ts (e.g.,\n",
      " — the median income is the n umber one predictor of housing prices –). In this Califorƒ\n",
      " nia housing exam ple, the final performance of the system is not better than the\n",
      " experts ‡ , but it ma y still be a good idea to la unch it, especially if this frees up some\n",
      " time for the experts so they can work on more in teresting and productive tasks.\n",
      "Launch, Monitor, and Maintain Your System\n",
      " P erfect, you got a pproval to la unch! Y ou need to get your solution ready for producƒ\n",
      " tion, in particular by plugging the production in put da ta sources in to your system\n",
      "and writing tests.\n",
      " Y ou also need to write monitoring code to check your system ‡ s live performance a t\n",
      " regular in ter vals and trigger alerts when it drops. This is im portan t to ca tch not only\n",
      " sudden breakage, but also performance degrada tion. This is quite common beca use\n",
      " models tend to — rot – as da ta evolves over time, unless the models are regularly trained\n",
      " on fresh da ta.\n",
      " E valua ting your system ‡ s performance will require sam pling the system ‡ s predictions\n",
      " and evalua ting them. This will generally require a h uman analysis. These analysts\n",
      " ma y be field experts, or workers on a crowdsourcing pla tform (such as Amazon\n",
      " M echanical T urk or CrowdFlower). Either wa y , you need to plug the h uman evaluaƒ\n",
      " tion pipeline in to your system.\n",
      " Y ou should also make sure you evalua te the system ‡ s in put da ta quality . Sometimes\n",
      " performance will degrade sligh tly beca use of a poor quality signal (e.g., a malfuncƒ\n",
      " tioning sensor sending random values, or another team ‡ s output becoming stale), but\n",
      " it ma y take a while before your system ‡ s performance degrades enough to trigger an\n",
      " alert. If you monitor your system ‡ s in puts, you ma y ca tch this earlier . M onitoring the\n",
      " in puts is particularly im portan t for online learning systems.\n",
      " Finally , you will generally wan t to train your models on a regular basis using fresh\n",
      " da ta. Y ou should a utoma te this process as m uch as possible. If you don ‡ t, you are ver y\n",
      " likely to refresh your model only ever y six mon ths (a t best), and your system ‡ s perforƒ\n",
      " mance ma y fluctua te severely over time. If your system is an online learning system,\n",
      " you should make sure you sa ve sna pshots of its sta te a t regular in ter vals so you can\n",
      " easily roll back to a previously working sta te.\n",
      " 84  |  Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "Try It Out!\n",
      " H opefully this cha pter ga ve you a good idea of wha t a M achine Learning project\n",
      " looks like, and showed you some of the tools you can use to train a grea t system. As\n",
      " you can see, m uch of the work is in the da ta prepara tion step , building monitoring\n",
      " tools, setting up h uman evalua tion pipelines, and a utoma ting regular model training.\n",
      " The M achine Learning algorithms are also im portan t, of course, but it is probably\n",
      "preferable to be comfortable with the overall process and know three or four algoƒ\n",
      " rithms well ra ther than to spend all your time exploring advanced algorithms and not\n",
      "enough time on the overall process.\n",
      " So , if you ha ve not already done so , now is a good time to pick up a la ptop , select a\n",
      " da taset tha t you are in terested in, and tr y to go through the whole process from A to\n",
      " Z. A good place to start is on a com petition website such as \n",
      " h ttp://k a g gl e.c o m/\n",
      ": you\n",
      " will ha ve a da taset to pla y with, a clear goal, and people to share the experience with.\n",
      "Exercises\n",
      " U sing this cha pter‡ s housing da taset:\n",
      "1.\n",
      " T r y a Support V ector M achine regressor (\n",
      "sklearn.svm.SVR\n",
      "), \n",
      " with various h yperƒ\n",
      "parameters such as \n",
      "kernel=\"linear\"\n",
      " (with various values for the \n",
      "C\n",
      " \n",
      " h yperparaƒ\n",
      "meter) or \n",
      "kernel=\"rbf\"\n",
      " (with various values for the \n",
      "C\n",
      " \n",
      "and \n",
      "gamma\n",
      " h yperparameters). Don ‡ t worr y about wha t these h yperparameters mean for now .\n",
      " H ow does the best \n",
      "SVR\n",
      " predictor perform?\n",
      "2.\n",
      " T r y replacing \n",
      "GridSearchCV\n",
      " with \n",
      "RandomizedSearchCV\n",
      ".\n",
      "3.\n",
      " T r y adding a transformer in the prepara tion pipeline to select only the most\n",
      " im portan t a ttributes.\n",
      "4.\n",
      " T r y crea ting a single pipeline tha t does the full da ta prepara tion plus the final\n",
      "prediction.\n",
      "5.\n",
      " A utoma tically explore some prepara tion options using \n",
      "GridSearchCV\n",
      ".\n",
      " Solutions to these exercises are a vailable in the online J upyter notebooks a t \n",
      " h ttp s://\n",
      " g i t h u b .c o m/a ger o n/h a n d s o n-m l2\n",
      ".\n",
      " Try It Out!  |  85\n",
      "\n",
      "\n",
      "1\n",
      " By defa ult Scikit-Learn caches downloaded da tasets in a director y called \n",
      " $H O ME/s ci k i t_l e a r n_d a t a\n",
      ".\n",
      "CHAPTER 3\n",
      "Classi•cation\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 3 in the final\n",
      "release of the book.\n",
      "In \n",
      " Cha pter 1\n",
      "  we men tioned tha t the most common super vised learning tasks are\n",
      " regression (predicting values) and classifica tion (predicting classes). In \n",
      " Cha pter 2\n",
      " \n",
      "we\n",
      "explored a regression task, predicting housing values, using various algorithms such\n",
      " as Linear Regression, Decision T rees, and R andom F orests (which will be explained\n",
      " in further detail in la ter cha pters). N ow we will turn our a tten tion to classifica tion\n",
      "systems.\n",
      "MNIST\n",
      " In this cha pter , we will be using the MNIST da taset, which is a set of 70,000 small\n",
      " images of digits handwritten by high school studen ts and em ployees of the US Cenƒ\n",
      " sus Burea u. Each image is labeled with the digit it represen ts. This set has been studƒ\n",
      " ied so m uch tha t it is often called the —H ello W orld – of M achine Learning: whenever\n",
      " people come up with a new classifica tion algorithm, they are curious to see how it\n",
      " will perform on MNIST . Whenever someone learns M achine Learning, sooner or\n",
      " la ter they tackle MNIST .\n",
      " Scikit-Learn provides man y helper functions to download popular da tasets. MNIST is\n",
      " one of them. The following code fetches the MNIST da taset:\n",
      "1\n",
      "87\n",
      "\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "fetch_openml\n",
      ">>> \n",
      "mnist\n",
      " \n",
      "=\n",
      " \n",
      "fetch_openml\n",
      "(\n",
      "•mnist_784•\n",
      ",\n",
      " \n",
      "version\n",
      "=\n",
      "1\n",
      ")\n",
      ">>> \n",
      "mnist\n",
      ".\n",
      "keys\n",
      "()\n",
      "dict_keys([•data•, •target•, •feature_names•, •DESCR•, •details•,\n",
      "           •categories•, •url•])\n",
      " Da tasets loaded by Scikit-Learn generally ha ve a similar dictionar y structure includƒ\n",
      "ing:\n",
      "⁄\n",
      "A \n",
      "DESCR\n",
      "  key describing the da taset\n",
      "⁄\n",
      "A \n",
      "data\n",
      "  key con taining an arra y with one row per instance and one column per\n",
      " fea ture\n",
      "⁄\n",
      "A \n",
      "target\n",
      "  key con taining an arra y with the labels\n",
      " Let ‡ s look a t these arra ys:\n",
      ">>> \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "mnist\n",
      "[\n",
      "\"data\"\n",
      "],\n",
      " \n",
      "mnist\n",
      "[\n",
      "\"target\"\n",
      "]\n",
      ">>> \n",
      "X\n",
      ".\n",
      "shape\n",
      "(70000, 784)\n",
      ">>> \n",
      "y\n",
      ".\n",
      "shape\n",
      "(70000,)\n",
      " There are 70,000 images, and each image has 784 fea tures. This is beca use each image\n",
      " is 28„28 pixels, and each fea ture sim ply represen ts one pixel ‡ s in tensity , from 0\n",
      " (white) to 255 (black). Let ‡ s take a peek a t one digit from the da taset. All you need to\n",
      " do is grab an instance ‡ s fea ture vector , resha pe it to a 28„28 arra y , and displa y it using\n",
      " M a tplotlib ‡ s \n",
      "imshow()\n",
      " function:\n",
      "import\n",
      " \n",
      "matplotlib\n",
      " \n",
      "as\n",
      " \n",
      "mpl\n",
      "import\n",
      " \n",
      "matplotlib.pyplot\n",
      " \n",
      "as\n",
      " \n",
      "plt\n",
      "some_digit\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "[\n",
      "0\n",
      "]\n",
      "some_digit_image\n",
      " \n",
      "=\n",
      " \n",
      "some_digit\n",
      ".\n",
      "reshape\n",
      "(\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      ")\n",
      "plt\n",
      ".\n",
      "imshow\n",
      "(\n",
      "some_digit_image\n",
      ",\n",
      " \n",
      "cmap\n",
      " \n",
      "=\n",
      " \n",
      "mpl\n",
      ".\n",
      "cm\n",
      ".\n",
      "binary\n",
      ",\n",
      " \n",
      "interpolation\n",
      "=\n",
      "\"nearest\"\n",
      ")\n",
      "plt\n",
      ".\n",
      "axis\n",
      "(\n",
      "\"off\"\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " This looks like a 5, and indeed tha t ‡ s wha t the label tells us:\n",
      " 88  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      ">>> \n",
      "y\n",
      "[\n",
      "0\n",
      "]\n",
      "•5•\n",
      " N ote tha t the label is a string. W e prefer n umbers, so let ‡ s cast \n",
      "y\n",
      "  to in tegers:\n",
      ">>> \n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "y\n",
      ".\n",
      "astype\n",
      "(\n",
      "np\n",
      ".\n",
      "uint8\n",
      ")\n",
      "Figure 3-1\n",
      "  shows a few more images from the MNIST da taset to give you a feel for\n",
      " the com plexity of the classifica tion task.\n",
      " F i g u r e 3-1. A f e w d i g i ts f r o m t h e MNIS T d a t as e t\n",
      " But wait! Y ou should alwa ys crea te a test set and set it aside before inspecting the da ta\n",
      " closely . The MNIST da taset is actually already split in to a training set (the first 60,000\n",
      "images) and a test set (the last 10,000 images):\n",
      "X_train\n",
      ",\n",
      " \n",
      "X_test\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "y_test\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "[:\n",
      "60000\n",
      "],\n",
      " \n",
      "X\n",
      "[\n",
      "60000\n",
      ":],\n",
      " \n",
      "y\n",
      "[:\n",
      "60000\n",
      "],\n",
      " \n",
      "y\n",
      "[\n",
      "60000\n",
      ":]\n",
      " The training set is already sh uffled for us, which is good as this guaran tees tha t all\n",
      " cross-valida tion \n",
      " folds will be similar (you don ‡ t wan t one fold to be missing some digƒ\n",
      " its). M oreover , some learning algorithms are sensitive to the order of the training\n",
      " MNIST  |  89\n",
      "\n",
      "2\n",
      " Sh uffling ma y be a bad idea in some con texts›for exam ple, if you are working on time series da ta (such as\n",
      " stock market prices or wea ther conditions). W e will explore this in the next cha pters.\n",
      " instances, and they perform poorly if they get man y similar instances in a row . Sh ufƒ\n",
      " fling the da taset ensures tha t this won ‡ t ha ppen.\n",
      "2\n",
      "Training a Binary \n",
      "Classi•er\n",
      " Let ‡ s sim plif y the problem for now and only tr y to iden tif y one digit›for exam ple,\n",
      " the n umber 5. This —5-detector– will be an exam ple of a \n",
      " b i n a r y \n",
      "classi†er\n",
      " , ca pable of\n",
      " distinguishing between just two classes, 5 and not-5. Let ‡ s crea te the target vectors for\n",
      " this classifica tion task:\n",
      "y_train_5\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_train\n",
      " \n",
      "==\n",
      " \n",
      "5\n",
      ")\n",
      "  \n",
      "# True for all 5s, False for all other digits.\n",
      "y_test_5\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_test\n",
      " \n",
      "==\n",
      " \n",
      "5\n",
      ")\n",
      " Oka y , now let ‡ s pick a classifier and train it. A good place to start is with a \n",
      " S t o c h as t i c\n",
      " G r a d i en t D e s c en t\n",
      "   (SGD) classifier , using  Scikit-Learn ‡ s \n",
      "SGDClassifier\n",
      "   class. This clasƒ\n",
      " sifier has the advan tage of being ca pable of handling ver y large da tasets efficien tly .\n",
      " This is in part beca use SGD deals with training instances independen tly , one a t a time\n",
      "(which also makes SGD well suited for \n",
      " o n l i n e l e a r n i n g\n",
      " ), as we will see la ter . Let ‡ s crea te\n",
      "an \n",
      "SGDClassifier\n",
      " and train it on the whole training set:\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "SGDClassifier\n",
      "sgd_clf\n",
      " \n",
      "=\n",
      " \n",
      "SGDClassifier\n",
      "(\n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      "sgd_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      ")\n",
      "The \n",
      "SGDClassifier\n",
      " relies on randomness during training (hence\n",
      " the name — stochastic –). If you wan t reproducible results, you\n",
      "should set the \n",
      "random_state\n",
      "  parameter .\n",
      " N ow you can use it to detect images of the n umber 5:\n",
      ">>> \n",
      "sgd_clf\n",
      ".\n",
      "predict\n",
      "([\n",
      "some_digit\n",
      "])\n",
      "array([ True])\n",
      " The classifier guesses tha t this image represen ts a 5 (\n",
      "True\n",
      " ). Looks like it guessed righ t\n",
      " in this particular case! N ow , let ‡ s evalua te this model ‡ s performance.\n",
      "Performance Measures\n",
      " E valua ting a classifier is often significan tly trickier than evalua ting a regressor , so we\n",
      " will spend a large part of this cha pter on this topic. There are man y performance\n",
      " 90  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " measures a vailable, so grab another coffee and get ready to learn man y new concepts\n",
      " and acron yms!\n",
      "Measuring Accuracy Using Cross-Validation\n",
      " A good wa y to evalua te a model is to use cross-valida tion, just as you did in \n",
      " Cha pƒ\n",
      "ter 2\n",
      ".\n",
      "Implementing Cross-Validation\n",
      " Occasionally you will need more con trol over the cross-valida tion process than wha t\n",
      " Scikit-Learn provides off-the-shelf. In these cases, you can im plemen t cross-\n",
      " valida tion yourself; it is actually fairly straigh tfor ward. The following code does\n",
      " roughly the same thing as Scikit-Learn ‡ s \n",
      "cross_val_score()\n",
      " \n",
      " function, and prin ts the \n",
      "same result:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "StratifiedKFold\n",
      "from\n",
      " \n",
      "sklearn.base\n",
      " \n",
      "import\n",
      " \n",
      "clone\n",
      "skfolds\n",
      " \n",
      "=\n",
      " \n",
      "StratifiedKFold\n",
      "(\n",
      "n_splits\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      "for\n",
      " \n",
      "train_index\n",
      ",\n",
      " \n",
      "test_index\n",
      " \n",
      "in\n",
      " \n",
      "skfolds\n",
      ".\n",
      "split\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      "):\n",
      "    \n",
      "clone_clf\n",
      " \n",
      "=\n",
      " \n",
      "clone\n",
      "(\n",
      "sgd_clf\n",
      ")\n",
      "    \n",
      "X_train_folds\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[\n",
      "train_index\n",
      "]\n",
      "    \n",
      "y_train_folds\n",
      " \n",
      "=\n",
      " \n",
      "y_train_5\n",
      "[\n",
      "train_index\n",
      "]\n",
      "    \n",
      "X_test_fold\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[\n",
      "test_index\n",
      "]\n",
      "    \n",
      "y_test_fold\n",
      " \n",
      "=\n",
      " \n",
      "y_train_5\n",
      "[\n",
      "test_index\n",
      "]\n",
      "    \n",
      "clone_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_folds\n",
      ",\n",
      " \n",
      "y_train_folds\n",
      ")\n",
      "    \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "clone_clf\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test_fold\n",
      ")\n",
      "    \n",
      "n_correct\n",
      " \n",
      "=\n",
      " \n",
      "sum\n",
      "(\n",
      "y_pred\n",
      " \n",
      "==\n",
      " \n",
      "y_test_fold\n",
      ")\n",
      "    \n",
      "print\n",
      "(\n",
      "n_correct\n",
      " \n",
      "/\n",
      " \n",
      "len\n",
      "(\n",
      "y_pred\n",
      "))\n",
      "  \n",
      "# prints 0.9502, 0.96565 and 0.96495\n",
      "The \n",
      "StratifiedKFold\n",
      "  class performs stra tified sam pling (as explained in \n",
      " Cha pter 2\n",
      ")\n",
      " to produce folds tha t con tain a represen ta tive ra tio of each class. A t each itera tion the\n",
      " code crea tes a clone of the classifier , trains tha t clone on the training folds, and makes\n",
      " predictions on the test fold. Then it coun ts the n umber of correct predictions and\n",
      " outputs the ra tio of correct predictions.\n",
      " Let ‡ s use the \n",
      "cross_val_score()\n",
      "  function to evalua te your \n",
      "SGDClassifier\n",
      " \n",
      "model\n",
      " using K-fold cross-valida tion, with three folds. Remember tha t K-fold cross-\n",
      " valida tion means splitting the training set in to K-folds (in this case, three), then makƒ\n",
      " ing predictions and evalua ting them on each fold using a model trained on the\n",
      "remaining folds (see \n",
      " Cha pter 2\n",
      "):\n",
      " Performance Measures  |  91\n",
      "\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "cross_val_score\n",
      ">>> \n",
      "cross_val_score\n",
      "(\n",
      "sgd_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "scoring\n",
      "=\n",
      "\"accuracy\"\n",
      ")\n",
      "array([0.96355, 0.93795, 0.95615])\n",
      " W ow! A bove 93% \n",
      " a c cu r a c y\n",
      "   (ra tio of correct predictions) on all cross-valida tion folds? \n",
      " This looks amazing, doesn ‡ t it? W ell, before you get too excited, let ‡ s look a t a ver y\n",
      " dumb classifier tha t just classifies ever y single image in the — not-5– class:\n",
      "from\n",
      " \n",
      "sklearn.base\n",
      " \n",
      "import\n",
      " \n",
      "BaseEstimator\n",
      "class\n",
      " \n",
      "Never5Classifier\n",
      "(\n",
      "BaseEstimator\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "fit\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      "=\n",
      "None\n",
      "):\n",
      "        \n",
      "pass\n",
      "    \n",
      "def\n",
      " \n",
      "predict\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "np\n",
      ".\n",
      "zeros\n",
      "((\n",
      "len\n",
      "(\n",
      "X\n",
      "),\n",
      " \n",
      "1\n",
      "),\n",
      " \n",
      "dtype\n",
      "=\n",
      "bool\n",
      ")\n",
      " Can you guess this model ‡ s accuracy? Let ‡ s find out:\n",
      ">>> \n",
      "never_5_clf\n",
      " \n",
      "=\n",
      " \n",
      "Never5Classifier\n",
      "()\n",
      ">>> \n",
      "cross_val_score\n",
      "(\n",
      "never_5_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "scoring\n",
      "=\n",
      "\"accuracy\"\n",
      ")\n",
      "array([0.91125, 0.90855, 0.90915])\n",
      " Tha t ‡ s righ t, it has over 90% accuracy! This is sim ply beca use only about 10% of the\n",
      " images are 5s, so if you alwa ys guess tha t an image is \n",
      " n o t\n",
      "  a 5, you will be righ t about\n",
      " 90% of the time. B ea ts N ostradam us.\n",
      " This demonstra tes wh y accuracy is generally not the preferred performance measure\n",
      "for classifiers, especially when you are dealing with \n",
      " s k e w e d d a t as e ts\n",
      " \n",
      "(i.e., when some\n",
      " classes are m uch more frequen t than others).\n",
      "Confusion Matrix\n",
      " A m uch better wa y to evalua te the performance of a classifier is to look a t the \n",
      " c o n f u‡\n",
      " s i o n m a t r ix\n",
      " . The general idea is to coun t the n umber of times instances of class A are\n",
      " classified as class B . F or exam ple, to know the n umber of times the classifier confused\n",
      "images of 5s with 3s, you would look in the 5\n",
      "th\n",
      " row and 3\n",
      "rd\n",
      " column of the confusion\n",
      " ma trix.\n",
      " T o com pute the confusion ma trix, you first need to ha ve a set of predictions, so they\n",
      " can be com pared to the actual targets. Y ou could make predictions on the test set, but\n",
      " let ‡ s keep it un touched for now (remember tha t you wan t to use the test set only a t the\n",
      " ver y end of your project, once you ha ve a classifier tha t you are ready to la unch).\n",
      "Instead, you can use the \n",
      "cross_val_predict()\n",
      " function:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "cross_val_predict\n",
      "y_train_pred\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_predict\n",
      "(\n",
      "sgd_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ")\n",
      " J ust like the \n",
      "cross_val_score()\n",
      " \n",
      "function, \n",
      "cross_val_predict()\n",
      " performs K-fold\n",
      " cross-valida tion, but instead of returning the evalua tion scores, it returns the predicƒ\n",
      " 92  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " tions made on each test fold. This means tha t you get a clean prediction for each\n",
      " instance in the training set (— clean – meaning tha t the prediction is made by a model\n",
      " tha t never sa w the da ta during training).\n",
      " N ow you are ready to get the confusion ma trix using the \n",
      "confusion_matrix()\n",
      " \n",
      "funcƒ\n",
      " tion. J ust pass it the target classes (\n",
      "y_train_5\n",
      ") and the predicted classes\n",
      "(\n",
      "y_train_pred\n",
      "):\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "confusion_matrix\n",
      ">>> \n",
      "confusion_matrix\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_pred\n",
      ")\n",
      "array([[53057,  1522],\n",
      "       [ 1325,  4096]])\n",
      " Each row in a confusion ma trix represen ts an \n",
      " a c t u a l c l as s\n",
      ", while each column repreƒ\n",
      " sen ts a \n",
      " p r e d i c t e d c l as s\n",
      " . The first row of this ma trix considers non-5 images (the \n",
      " n e ga‡\n",
      " t i v e c l as s\n",
      "): 53,057 of them were correctly classified as non-5s (they are called \n",
      " t r u e\n",
      " n e ga t i v e s\n",
      "), while the remaining 1,522 were wrongly classified as 5s (\n",
      " f a l s e p o s i t i v e s\n",
      ").\n",
      "The second row considers the images of 5s (the \n",
      " p o s i t i v e c l as s\n",
      "): 1,325 were wrongly\n",
      "classified as non-5s (\n",
      " f a l s e n e ga t i v e s\n",
      "), while the remaining 4,096 were correctly classiƒ\n",
      "fied as 5s (\n",
      " t r u e p o s i t i v e s\n",
      " ). A perfect classifier would ha ve only true positives and true\n",
      " nega tives, so its confusion ma trix would ha ve nonzero values only on its main diagoƒ\n",
      " nal (top left to bottom righ t):\n",
      ">>> \n",
      "y_train_perfect_predictions\n",
      " \n",
      "=\n",
      " \n",
      "y_train_5\n",
      "  \n",
      "# pretend we reached perfection\n",
      ">>> \n",
      "confusion_matrix\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_perfect_predictions\n",
      ")\n",
      "array([[54579,     0],\n",
      "       [    0,  5421]])\n",
      " The confusion ma trix gives you a lot of informa tion, but sometimes you ma y prefer a\n",
      " more concise metric. An in teresting one to look a t is the accuracy of the positive preƒ\n",
      "dictions; this is called the \n",
      " p r e ci s i o n\n",
      " of the classifier (\n",
      " Equa tion 3-1\n",
      ").\n",
      " Eq u a t i o n 3-1. P r e ci s i o n\n",
      " precision =\n",
      " T P\n",
      " T P\n",
      "+\n",
      " F P\n",
      " TP is the n umber of true positives, and FP is the n umber of false positives.\n",
      " A trivial wa y to ha ve perfect precision is to make one single positive prediction and\n",
      " ensure it is correct (precision = 1/1 = 100%). This would not be ver y useful since the\n",
      "classifier would ignore all but one positive instance. So precision is typically used\n",
      "along with another metric named \n",
      " r e c a l l\n",
      ", also called \n",
      " s ens i t i v i ty\n",
      " \n",
      "or \n",
      " t r u e p o s i t i v e r a t e\n",
      " Performance Measures  |  93\n",
      "\n",
      "(\n",
      " TP R\n",
      " ): this is the ra tio of positive instances tha t are correctly detected by the classifier\n",
      "(\n",
      " Equa tion 3-2\n",
      ").\n",
      " Eq u a t i o n 3-2. R e c a l l\n",
      " recall =\n",
      " T P\n",
      " T P\n",
      "+\n",
      " F N\n",
      " FN is of course the n umber of false nega tives.\n",
      " If you are confused about the confusion ma trix, \n",
      "Figure 3-2\n",
      "  ma y help .\n",
      " F i g u r e 3-2. An i l l u s t r a t e d c o n f u s i o n m a t r ix\n",
      "Precision and Recall\n",
      "Scikit-Learn \n",
      " provides several functions to com pute classifier metrics, including \n",
      "preciƒ\n",
      "sion and recall:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "precision_score\n",
      ",\n",
      " \n",
      "recall_score\n",
      ">>> \n",
      "precision_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_pred\n",
      ")\n",
      " \n",
      "# == 4096 / (4096 + 1522)\n",
      "0.7290850836596654\n",
      ">>> \n",
      "recall_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_pred\n",
      ")\n",
      " \n",
      "# == 4096 / (4096 + 1325)\n",
      "0.7555801512636044\n",
      " N ow your 5-detector does not look as shin y as it did when you looked a t its accuracy .\n",
      " When it claims an image represen ts a 5, it is correct only 72.9% of the time. M oreƒ\n",
      " over , it only detects 75.6% of the 5s.\n",
      " I t is often con venien t to combine precision and recall in to a single metric called the \n",
      "F\n",
      "1\n",
      " s c o r e\n",
      " , in particular if you need a sim ple wa y to com pare two classifiers. The F\n",
      "1\n",
      " score is \n",
      "the \n",
      " h a r m o n i c m e a n\n",
      " \n",
      "of precision and recall (\n",
      " Equa tion 3-3\n",
      "). Whereas the regular mean\n",
      " 94  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " trea ts all values equally , the harmonic mean gives m uch more weigh t to low values.\n",
      "As a result, the classifier will only get a high F\n",
      "1\n",
      " score if both recall and precision are\n",
      "high.\n",
      " Eq u a t i o n 3-3. F\n",
      "1\n",
      "F\n",
      "1\n",
      "=\n",
      "2\n",
      "1\n",
      "precision\n",
      "+\n",
      "1\n",
      "recall\n",
      " = 2 „\n",
      " precision „ recall\n",
      " precision + recall\n",
      "=\n",
      " T P\n",
      " T P\n",
      "+\n",
      " F N\n",
      "+\n",
      " F P\n",
      "2\n",
      " T o com pute the F\n",
      "1\n",
      "  score, sim ply call the \n",
      "f1_score()\n",
      " function:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "f1_score\n",
      ">>> \n",
      "f1_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_pred\n",
      ")\n",
      "0.7420962043663375\n",
      "The F\n",
      "1\n",
      " \n",
      " score fa vors classifiers tha t ha ve similar precision and recall. This is not alwa ys\n",
      " wha t you wan t: in some con texts you mostly care about precision, and in other conƒ\n",
      " texts you really care about recall. F or exam ple, if you trained a classifier to detect vidƒ\n",
      " eos tha t are safe for kids, you would probably prefer a classifier tha t rejects man y\n",
      " good videos (low recall) but keeps only safe ones (high precision), ra ther than a clasƒ\n",
      " sifier tha t has a m uch higher recall but lets a few really bad videos show up in your\n",
      " product (in such cases, you ma y even wan t to add a h uman pipeline to check the clasƒ\n",
      " sifier‡ s video selection). On the other hand, suppose you train a classifier to detect\n",
      " shoplifters on sur veillance images: it is probably fine if your classifier has only 30%\n",
      "precision as long as it has 99% recall (sure, the security guards will get a few false\n",
      " alerts, but almost all shoplifters will get ca ugh t).\n",
      " U nfortuna tely , you can ‡ t ha ve it both wa ys: increasing precision reduces recall, and\n",
      "vice versa. This is called the \n",
      " p r e ci s i o n/r e c a l l \n",
      "tradeo›\n",
      ".\n",
      "Precision/Recall \n",
      "Tradeo…\n",
      " T o understand this tradeoff, let ‡ s look a t how the \n",
      "SGDClassifier\n",
      " \n",
      "makes its classificaƒ\n",
      " tion decisions. F or each instance, it com putes a score based on a \n",
      " d e ci s i o n f u n c t i o n\n",
      ", \n",
      " and if tha t score is grea ter than a threshold, it assigns the instance to the positive\n",
      " class, or else it assigns it to the nega tive class. \n",
      "Figure 3-3\n",
      "   shows a few digits positioned\n",
      " from the lowest score on the left to the highest score on the righ t. Suppose the \n",
      " d e ci‡\n",
      " s i o n t h r e s h o l d\n",
      "  is positioned a t the cen tral arrow (between the two 5s): you will find 4\n",
      " true positives (actual 5s) on the righ t of tha t threshold, and one false positive (actually\n",
      " a 6). Therefore, with tha t threshold, the precision is 80% (4 out of 5). But out of 6\n",
      " actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). N ow if you\n",
      " raise the threshold (move it to the arrow on the righ t), the false positive (the 6)\n",
      " becomes a true nega tive, thereby increasing precision (up to 100% in this case), but\n",
      " one true positive becomes a false nega tive, decreasing recall down to 50%. Con versely ,\n",
      "lowering the threshold increases recall and reduces precision.\n",
      " Performance Measures  |  95\n",
      "\n",
      " F i g u r e 3-3. D e ci s i o n t h r e s h o l d a n d p r e ci s i o n/r e c a l l \n",
      "tradeo›\n",
      " Scikit-Learn does not let you set the threshold directly , but it does give you access to\n",
      " the decision scores tha t it uses to make predictions. Instead of calling the classifier‡ s\n",
      "predict()\n",
      " method, you can call its \n",
      "decision_function()\n",
      " method, which returns a\n",
      " score for each instance, and then make predictions based on those scores using an y\n",
      " threshold you wan t:\n",
      ">>> \n",
      "y_scores\n",
      " \n",
      "=\n",
      " \n",
      "sgd_clf\n",
      ".\n",
      "decision_function\n",
      "([\n",
      "some_digit\n",
      "])\n",
      ">>> \n",
      "y_scores\n",
      "array([2412.53175101])\n",
      ">>> \n",
      "threshold\n",
      " \n",
      "=\n",
      " \n",
      "0\n",
      ">>> \n",
      "y_some_digit_pred\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_scores\n",
      " \n",
      ">\n",
      " \n",
      "threshold\n",
      ")\n",
      "array([ True])\n",
      "The \n",
      "SGDClassifier\n",
      " uses a threshold equal to 0, so the previous code returns the same\n",
      "result as the \n",
      "predict()\n",
      " method (i.e., \n",
      "True\n",
      " ). Let ‡ s raise the threshold:\n",
      ">>> \n",
      "threshold\n",
      " \n",
      "=\n",
      " \n",
      "8000\n",
      ">>> \n",
      "y_some_digit_pred\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_scores\n",
      " \n",
      ">\n",
      " \n",
      "threshold\n",
      ")\n",
      ">>> \n",
      "y_some_digit_pred\n",
      "array([False])\n",
      " This confirms tha t raising the threshold decreases recall. The image actually repreƒ\n",
      " sen ts a 5, and the classifier detects it when the threshold is 0, but it misses it when the\n",
      "threshold is increased to 8,000.\n",
      " N ow how do you decide which threshold to use? F or this you will first need to get the\n",
      "scores of all instances in the training set using the \n",
      "cross_val_predict()\n",
      " \n",
      "function\n",
      " again, but this time specif ying tha t you wan t it to return decision scores instead of\n",
      "predictions:\n",
      "y_scores\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_predict\n",
      "(\n",
      "sgd_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      "                             \n",
      "method\n",
      "=\n",
      "\"decision_function\"\n",
      ")\n",
      " N ow with these scores you can com pute precision and recall for all possible threshƒ\n",
      "olds using the \n",
      "precision_recall_curve()\n",
      " function:\n",
      " 96  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "precision_recall_curve\n",
      "precisions\n",
      ",\n",
      " \n",
      "recalls\n",
      ",\n",
      " \n",
      "thresholds\n",
      " \n",
      "=\n",
      " \n",
      "precision_recall_curve\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_scores\n",
      ")\n",
      " Finally , you can plot precision and recall as functions of the threshold value using\n",
      " M a tplotlib (\n",
      "Figure 3-4\n",
      "):\n",
      "def\n",
      " \n",
      "plot_precision_recall_vs_threshold\n",
      "(\n",
      "precisions\n",
      ",\n",
      " \n",
      "recalls\n",
      ",\n",
      " \n",
      "thresholds\n",
      "):\n",
      "    \n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "thresholds\n",
      ",\n",
      " \n",
      "precisions\n",
      "[:\n",
      "-\n",
      "1\n",
      "],\n",
      " \n",
      "\"b--\"\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"Precision\"\n",
      ")\n",
      "    \n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "thresholds\n",
      ",\n",
      " \n",
      "recalls\n",
      "[:\n",
      "-\n",
      "1\n",
      "],\n",
      " \n",
      "\"g-\"\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"Recall\"\n",
      ")\n",
      "    \n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# highlight the threshold, add the legend, axis label and grid\n",
      "plot_precision_recall_vs_threshold\n",
      "(\n",
      "precisions\n",
      ",\n",
      " \n",
      "recalls\n",
      ",\n",
      " \n",
      "thresholds\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " F i g u r e 3-4. P r e ci s i o n a n d r e c a l l v er s u s t h e d e ci s i o n t h r e s h o l d\n",
      " Y ou ma y wonder wh y the precision cur ve is bum pier than the recall\n",
      " cur ve in \n",
      "Figure 3-4\n",
      " . The reason is tha t precision ma y sometimes go\n",
      "down when you raise the threshold (although in general it will go\n",
      " up). T o understand wh y , look back a t \n",
      "Figure 3-3\n",
      "  and notice wha t\n",
      " ha ppens when you start from the cen tral threshold and move it just\n",
      " one digit to the righ t: precision goes from 4/5 (80%) down to 3/4\n",
      "(75%). On the other hand, recall can only go down when the thresƒ\n",
      " hold is increased, which explains wh y its cur ve looks smooth.\n",
      " Another wa y to select a good precision/recall tradeoff is to plot precision directly\n",
      "against recall, as shown in \n",
      "Figure 3-5\n",
      " (the same threshold as earlier is highlighed).\n",
      " Performance Measures  |  97\n",
      "\n",
      " F i g u r e 3-5. P r e ci s i o n v er s u s r e c a l l\n",
      " Y ou can see tha t precision really starts to fall sharply around 80% recall. Y ou will\n",
      " probably wan t to select a precision/recall tradeoff just before tha t drop›for exam ple,\n",
      " a t around 60% recall. But of course the choice depends on your project.\n",
      " So let ‡ s suppose you decide to aim for 90% precision. Y ou look up the first plot and\n",
      " find tha t you need to use a threshold of about 8,000. T o be more precise you can\n",
      " search for the lowest threshold tha t gives you a t least 90% precision (\n",
      "np.argmax()\n",
      " will give us the first index of the maxim um value, which in this case means the first\n",
      "True\n",
      " value):\n",
      "threshold_90_precision\n",
      " \n",
      "=\n",
      " \n",
      "thresholds\n",
      "[\n",
      "np\n",
      ".\n",
      "argmax\n",
      "(\n",
      "precisions\n",
      " \n",
      ">=\n",
      " \n",
      "0.90\n",
      ")]\n",
      " \n",
      "# ~7816\n",
      " T o make predictions (on the training set for now), instead of calling the classifier‡ s\n",
      "predict()\n",
      " method, you can just run this code:\n",
      "y_train_pred_90\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_scores\n",
      " \n",
      ">=\n",
      " \n",
      "threshold_90_precision\n",
      ")\n",
      " Let ‡ s check these predictions ‡ precision and recall:\n",
      ">>> \n",
      "precision_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_pred_90\n",
      ")\n",
      "0.9000380083618396\n",
      ">>> \n",
      "recall_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_train_pred_90\n",
      ")\n",
      "0.4368197749492714\n",
      " Grea t, you ha ve a 90% precision classifier ! As you can see, it is fairly easy to crea te a\n",
      " classifier with virtually an y precision you wan t: just set a high enough threshold, and\n",
      " you ‡ re done. Hmm, not so fast. A high-precision classifier is not ver y useful if its \n",
      "recall is too low!\n",
      " 98  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " If someone sa ys —let ‡ s reach 99% precision, – you should ask, — a t\n",
      " wha t recall? –\n",
      "The ROC Curve\n",
      "The\n",
      " \n",
      " r e c ei v er o p er a t i n g c h a r a c t er i s t i c\n",
      "  (RO C) cur ve is another common tool used with\n",
      " binar y classifiers. I t is ver y similar to the precision/recall cur ve, but instead of plotƒ\n",
      " ting precision versus recall, the RO C cur ve plots the \n",
      " t r u e p o s i t i v e r a t e\n",
      " (another name\n",
      "for recall) against the \n",
      " f a l s e p o s i t i v e r a t e\n",
      ". \n",
      " The FPR is the ra tio of nega tive instances tha t\n",
      " are incorrectly classified as positive. I t is equal to one min us the \n",
      " t r u e n e ga t i v e r a t e\n",
      ", \n",
      " which is the ra tio of nega tive instances tha t are correctly classified as nega tive. The\n",
      "TNR is also called \n",
      "speci†city\n",
      " . H ence the RO C cur ve plots \n",
      " s ens i t i v i ty\n",
      " \n",
      "(recall) versus\n",
      "1 − \n",
      "speci†city\n",
      ".\n",
      " T o plot the RO C cur ve, you first need to com pute the TPR and FPR for various thresƒ\n",
      "hold values, using the \n",
      "roc_curve()\n",
      " function:\n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "roc_curve\n",
      "fpr\n",
      ",\n",
      " \n",
      "tpr\n",
      ",\n",
      " \n",
      "thresholds\n",
      " \n",
      "=\n",
      " \n",
      "roc_curve\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_scores\n",
      ")\n",
      " Then you can plot the FPR against the TPR using M a tplotlib . This code produces the\n",
      "plot in \n",
      "Figure 3-6\n",
      ":\n",
      "def\n",
      " \n",
      "plot_roc_curve\n",
      "(\n",
      "fpr\n",
      ",\n",
      " \n",
      "tpr\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "None\n",
      "):\n",
      "    \n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "fpr\n",
      ",\n",
      " \n",
      "tpr\n",
      ",\n",
      " \n",
      "linewidth\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "label\n",
      ")\n",
      "    \n",
      "plt\n",
      ".\n",
      "plot\n",
      "([\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "[\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "•k--•\n",
      ")\n",
      " \n",
      "# dashed diagonal\n",
      "    \n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# Add axis labels and grid\n",
      "plot_roc_curve\n",
      "(\n",
      "fpr\n",
      ",\n",
      " \n",
      "tpr\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " Performance Measures  |  99\n",
      "\n",
      " F i g u r e 3-6. R O C cu r v e\n",
      "Once again there is a tradeoff: the higher the recall (TPR), the more false positives\n",
      " (FPR) the classifier produces. The dotted line represen ts the RO C cur ve of a purely\n",
      " random classifier ; a good classifier sta ys as far a wa y from tha t line as possible (toward\n",
      "the top-left corner).\n",
      " One wa y to com pare classifiers is to measure the \n",
      " a r e a u n d er t h e cu r v e\n",
      "  (A UC). \n",
      "A perƒ\n",
      " fect classifier will ha ve a \n",
      " R O C AUC\n",
      " \n",
      "equal to 1, whereas a purely random classifier will\n",
      " ha ve a RO C A UC equal to 0.5. Scikit-Learn provides a function to com pute the RO C\n",
      " A UC:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "roc_auc_score\n",
      ">>> \n",
      "roc_auc_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_scores\n",
      ")\n",
      "0.9611778893101814\n",
      " Since the RO C cur ve is so similar to the precision/recall (or PR)\n",
      " cur ve, you ma y wonder how to decide which one to use. As a rule\n",
      " of th umb , you should prefer the PR cur ve whenever the positive\n",
      "class is rare or when you care more about the false positives than\n",
      " the false nega tives, and the RO C cur ve other wise. F or exam ple,\n",
      " looking a t the previous RO C cur ve (and the RO C A UC score), you\n",
      " ma y think tha t the classifier is really good. But this is mostly\n",
      " beca use there are few positives (5s) com pared to the nega tives\n",
      " (non-5s). In con trast, the PR cur ve makes it clear tha t the classifier\n",
      " has room for im provemen t (the cur ve could be closer to the top-\n",
      " righ t corner).\n",
      " 100  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " Let ‡ s train a \n",
      "RandomForestClassifier\n",
      "  and com pare its RO C cur ve and RO C A UC\n",
      "score to the \n",
      "SGDClassifier\n",
      ". First, you need to get scores for each instance in the\n",
      " training set. But due to the wa y it works (see \n",
      " Cha pter 7\n",
      "), the \n",
      "RandomForestClassi\n",
      "fier\n",
      "  class does not ha ve a \n",
      "decision_function()\n",
      " method. Instead it has a \n",
      "pre\n",
      "dict_proba()\n",
      "  method. Scikit-Learn classifiers generally ha ve one or the other . The\n",
      "predict_proba()\n",
      "  method returns an arra y con taining a row per instance and a colƒ\n",
      " umn per class, each con taining the probability tha t the given instance belongs to the\n",
      " given class (e.g., 70% chance tha t the image represen ts a 5):\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "RandomForestClassifier\n",
      "forest_clf\n",
      " \n",
      "=\n",
      " \n",
      "RandomForestClassifier\n",
      "(\n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      "y_probas_forest\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_predict\n",
      "(\n",
      "forest_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_5\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      "                                    \n",
      "method\n",
      "=\n",
      "\"predict_proba\"\n",
      ")\n",
      " But to plot a RO C cur ve, you need scores, not probabilities. A sim ple solution is to\n",
      " use the positive class ‡ s probability as the score:\n",
      "y_scores_forest\n",
      " \n",
      "=\n",
      " \n",
      "y_probas_forest\n",
      "[:,\n",
      " \n",
      "1\n",
      "]\n",
      "   \n",
      "# score = proba of positive class\n",
      "fpr_forest\n",
      ",\n",
      " \n",
      "tpr_forest\n",
      ",\n",
      " \n",
      "thresholds_forest\n",
      " \n",
      "=\n",
      " \n",
      "roc_curve\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      "y_scores_forest\n",
      ")\n",
      " N ow you are ready to plot the RO C cur ve. I t is useful to plot the first RO C cur ve as\n",
      " well to see how they com pare (\n",
      "Figure 3-7\n",
      "):\n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "fpr\n",
      ",\n",
      " \n",
      "tpr\n",
      ",\n",
      " \n",
      "\"b:\"\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"SGD\"\n",
      ")\n",
      "plot_roc_curve\n",
      "(\n",
      "fpr_forest\n",
      ",\n",
      " \n",
      "tpr_forest\n",
      ",\n",
      " \n",
      "\"Random Forest\"\n",
      ")\n",
      "plt\n",
      ".\n",
      "legend\n",
      "(\n",
      "loc\n",
      "=\n",
      "\"lower right\"\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " F i g u r e 3-7. C o m p a r i n g R O C cu r v e s\n",
      " Performance Measures  |  101\n",
      "\n",
      "As you can see in \n",
      "Figure 3-7\n",
      ", the \n",
      "RandomForestClassifier\n",
      " ‡ s RO C cur ve looks m uch\n",
      "better than the \n",
      "SGDClassifier\n",
      " ‡ s: it comes m uch closer to the top-left corner . As a\n",
      " result, its RO C A UC score is also significan tly better :\n",
      ">>> \n",
      "roc_auc_score\n",
      "(\n",
      "y_train_5\n",
      ",\n",
      " \n",
      "y_scores_forest\n",
      ")\n",
      "0.9983436731328145\n",
      " T r y measuring the precision and recall scores: you should find 99.0% precision and\n",
      " 86.6% recall. N ot too bad!\n",
      " H opefully you now know how to train binar y classifiers, choose the a ppropria te metƒ\n",
      " ric for your task, evalua te your classifiers using cross-valida tion, select the precision/\n",
      " recall tradeoff tha t fits your needs, and com pare various models using RO C cur ves\n",
      " and RO C A UC scores. N ow let ‡ s tr y to detect more than just the 5s.\n",
      "Multiclass \n",
      "Classi•cation\n",
      " Whereas binar y classifiers distinguish between two classes, \n",
      " m u l t i c l as s \n",
      "classi†ers\n",
      " \n",
      "(also\n",
      "called \n",
      " m u l t i n o m i a l \n",
      "classi†ers\n",
      ") can distinguish between more than two classes.\n",
      " Some algorithms (such as R andom F orest classifiers or naive Ba yes classifiers) are\n",
      " ca pable of handling m ultiple classes directly . Others (such as Support V ector M achine\n",
      " classifiers or Linear classifiers) are strictly binar y classifiers. H owever , there are variƒ\n",
      " ous stra tegies tha t you can use to perform m ulticlass classifica tion using m ultiple\n",
      " binar y classifiers.\n",
      " F or exam ple, one wa y to crea te a system tha t can classif y the digit images in to 10\n",
      " classes (from 0 to 9) is to train 10 binar y classifiers, one for each digit (a 0-detector , a\n",
      " 1-detector , a 2-detector , and so on). Then when you wan t to classif y an image, you get\n",
      " the decision score from each classifier for tha t image and you select the class whose\n",
      "classifier outputs the highest score. This is called the \n",
      " o n e-v er s u s-a l l\n",
      " \n",
      " (O vA) stra teg y \n",
      "(also called \n",
      " o n e-v er s u s-t h e-r e s t\n",
      ").\n",
      " Another stra teg y is to train a binar y classifier for ever y pair of digits: one to distinƒ\n",
      "guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\n",
      "This is called the \n",
      " o n e-v er s u s-o n e\n",
      "  (O vO) stra teg y . If there are \n",
      "N\n",
      " classes, you need to\n",
      "train \n",
      "N\n",
      " „ (\n",
      "N\n",
      "  − 1) / 2 classifiers. F or the MNIST problem, this means training 45\n",
      " binar y classifiers! When you wan t to classif y an image, you ha ve to run the image\n",
      "through all 45 classifiers and see which class wins the most duels. The main advanƒ\n",
      " tage of O vO is tha t each classifier only needs to be trained on the part of the training\n",
      " set for the two classes tha t it m ust distinguish.\n",
      " Some algorithms (such as Support V ector M achine classifiers) scale poorly with the\n",
      " size of the training set, so for these algorithms O vO is preferred since it is faster to\n",
      " train man y classifiers on small training sets than training few classifiers on large\n",
      " training sets. F or most binar y classifica tion algorithms, however , O vA is preferred.\n",
      " 102  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " Scikit-Learn detects when you tr y to use a binar y classifica tion algorithm for a m ultiƒ\n",
      " class classifica tion task, and it a utoma tically runs O vA (except for SVM classifiers for\n",
      " which it uses O vO). Let ‡ s tr y this with the \n",
      "SGDClassifier\n",
      ":\n",
      ">>> \n",
      "sgd_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "  \n",
      "# y_train, not y_train_5\n",
      ">>> \n",
      "sgd_clf\n",
      ".\n",
      "predict\n",
      "([\n",
      "some_digit\n",
      "])\n",
      "array([5], dtype=uint8)\n",
      " Tha t was easy! This code trains the \n",
      "SGDClassifier\n",
      "   on the training set using the origiƒ\n",
      "nal target classes from 0 to 9 (\n",
      "y_train\n",
      "), instead of the 5-versus-all target classes\n",
      "(\n",
      "y_train_5\n",
      " ). Then it makes a prediction (a correct one in this case). U nder the hood,\n",
      " Scikit-Learn actually trained 10 binar y classifiers, got their decision scores for the\n",
      "image, and selected the class with the highest score.\n",
      " T o see tha t this is indeed the case, you can call the \n",
      "decision_function()\n",
      " \n",
      "method.\n",
      "Instead of returning just one score per instance, it now returns 10 scores, one per\n",
      "class:\n",
      ">>> \n",
      "some_digit_scores\n",
      " \n",
      "=\n",
      " \n",
      "sgd_clf\n",
      ".\n",
      "decision_function\n",
      "([\n",
      "some_digit\n",
      "])\n",
      ">>> \n",
      "some_digit_scores\n",
      "array([[-15955.22627845, -38080.96296175, -13326.66694897,\n",
      "           573.52692379, -17680.6846644 ,   2412.53175101,\n",
      "        -25526.86498156, -12290.15704709,  -7946.05205023,\n",
      "        -10631.35888549]])\n",
      "The highest score is indeed the one corresponding to class 5:\n",
      ">>> \n",
      "np\n",
      ".\n",
      "argmax\n",
      "(\n",
      "some_digit_scores\n",
      ")\n",
      "5\n",
      ">>> \n",
      "sgd_clf\n",
      ".\n",
      "classes_\n",
      "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n",
      ">>> \n",
      "sgd_clf\n",
      ".\n",
      "classes_\n",
      "[\n",
      "5\n",
      "]\n",
      "5\n",
      "When a classifier is trained, it stores the list of target classes in its\n",
      "classes_\n",
      " \n",
      " a ttribute, ordered by value. In this case, the index of each\n",
      "class in the \n",
      "classes_\n",
      "  arra y con venien tly ma tches the class itself\n",
      " (e.g., the class a t index 5 ha ppens to be class 5), but in general you\n",
      " won ‡ t be so lucky .\n",
      " If you wan t to force ScikitLearn to use one-versus-one or one-versus-all, you can use\n",
      "the \n",
      "OneVsOneClassifier\n",
      " or \n",
      "OneVsRestClassifier\n",
      " \n",
      " classes. Sim ply crea te an instance\n",
      " and pass a binar y classifier to its constructor . F or exam ple, this code crea tes a m ultiƒ\n",
      " class classifier using the O vO stra teg y , based on a \n",
      "SGDClassifier\n",
      ":\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.multiclass\n",
      " \n",
      "import\n",
      " \n",
      "OneVsOneClassifier\n",
      ">>> \n",
      "ovo_clf\n",
      " \n",
      "=\n",
      " \n",
      "OneVsOneClassifier\n",
      "(\n",
      "SGDClassifier\n",
      "(\n",
      "random_state\n",
      "=\n",
      "42\n",
      "))\n",
      ">>> \n",
      "ovo_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      ">>> \n",
      "ovo_clf\n",
      ".\n",
      "predict\n",
      "([\n",
      "some_digit\n",
      "])\n",
      "Multiclass \n",
      "Classi•cation\n",
      "   |  103\n",
      "\n",
      "array([5], dtype=uint8)\n",
      ">>> \n",
      "len\n",
      "(\n",
      "ovo_clf\n",
      ".\n",
      "estimators_\n",
      ")\n",
      "45\n",
      " T raining a \n",
      "RandomForestClassifier\n",
      "  is just as easy :\n",
      ">>> \n",
      "forest_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      ">>> \n",
      "forest_clf\n",
      ".\n",
      "predict\n",
      "([\n",
      "some_digit\n",
      "])\n",
      "array([5], dtype=uint8)\n",
      " This time Scikit-Learn did not ha ve to run O vA or O vO beca use R andom F orest\n",
      "classifiers\n",
      "  can directly classif y instances in to m ultiple classes. Y ou can call\n",
      "predict_proba()\n",
      "  to get the list of probabilities tha t the classifier assigned to each\n",
      "instance for each class:\n",
      ">>> \n",
      "forest_clf\n",
      ".\n",
      "predict_proba\n",
      "([\n",
      "some_digit\n",
      "])\n",
      "array([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\n",
      " Y ou can see tha t the classifier is fairly confiden t about its prediction: the 0.9 a t the 5\n",
      "th\n",
      " index in the arra y means tha t the model estima tes a 90% probability tha t the image\n",
      " represen ts a 5. I t also thinks tha t the image could instead be a 2, a 3 or a 9, respecƒ\n",
      " tively with 1%, 8% and 1% probability .\n",
      " N ow of course you wan t to evalua te these classifiers. As usual, you wan t to use cross-\n",
      " valida tion. Let ‡ s evalua te the \n",
      "SGDClassifier\n",
      " ‡ s accuracy using the \n",
      "cross_val_score()\n",
      "function:\n",
      ">>> \n",
      "cross_val_score\n",
      "(\n",
      "sgd_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "scoring\n",
      "=\n",
      "\"accuracy\"\n",
      ")\n",
      "array([0.8489802 , 0.87129356, 0.86988048])\n",
      " I t gets over 84% on all test folds. If you used a random classifier , you would get 10%\n",
      " accuracy , so this is not such a bad score, but you can still do m uch better . F or examƒ\n",
      " ple, sim ply scaling the in puts (as discussed in \n",
      " Cha pter 2\n",
      ") increases \n",
      "accuracy above\n",
      "89%:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "StandardScaler\n",
      ">>> \n",
      "scaler\n",
      " \n",
      "=\n",
      " \n",
      "StandardScaler\n",
      "()\n",
      ">>> \n",
      "X_train_scaled\n",
      " \n",
      "=\n",
      " \n",
      "scaler\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ".\n",
      "astype\n",
      "(\n",
      "np\n",
      ".\n",
      "float64\n",
      "))\n",
      ">>> \n",
      "cross_val_score\n",
      "(\n",
      "sgd_clf\n",
      ",\n",
      " \n",
      "X_train_scaled\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "scoring\n",
      "=\n",
      "\"accuracy\"\n",
      ")\n",
      "array([0.89707059, 0.8960948 , 0.90693604])\n",
      "Error Analysis\n",
      " Of course, if this were a real project, you would follow the steps in your M achine\n",
      "Learning project checklist (see \n",
      "???\n",
      " ): exploring da ta prepara tion options, tr ying out\n",
      " m ultiple models, shortlisting the best ones and fine-tuning their h yperparameters\n",
      "using \n",
      "GridSearchCV\n",
      " , and a utoma ting as m uch as possible, as you did in the previous\n",
      " cha pter . H ere, we will assume tha t you ha ve found a promising model and you wan t\n",
      " to find wa ys to im prove it. One wa y to do this is to analyze the types of errors it\n",
      "makes.\n",
      " 104  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " First, you can look a t the confusion ma trix. Y ou need to make predictions using the\n",
      "cross_val_predict()\n",
      "   function, \n",
      "then call the \n",
      "confusion_matrix()\n",
      " \n",
      "function, just like\n",
      " you did earlier :\n",
      ">>> \n",
      "y_train_pred\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_predict\n",
      "(\n",
      "sgd_clf\n",
      ",\n",
      " \n",
      "X_train_scaled\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ")\n",
      ">>> \n",
      "conf_mx\n",
      " \n",
      "=\n",
      " \n",
      "confusion_matrix\n",
      "(\n",
      "y_train\n",
      ",\n",
      " \n",
      "y_train_pred\n",
      ")\n",
      ">>> \n",
      "conf_mx\n",
      "array([[5578,    0,   22,    7,    8,   45,   35,    5,  222,    1],\n",
      "       [   0, 6410,   35,   26,    4,   44,    4,    8,  198,   13],\n",
      "       [  28,   27, 5232,  100,   74,   27,   68,   37,  354,   11],\n",
      "       [  23,   18,  115, 5254,    2,  209,   26,   38,  373,   73],\n",
      "       [  11,   14,   45,   12, 5219,   11,   33,   26,  299,  172],\n",
      "       [  26,   16,   31,  173,   54, 4484,   76,   14,  482,   65],\n",
      "       [  31,   17,   45,    2,   42,   98, 5556,    3,  123,    1],\n",
      "       [  20,   10,   53,   27,   50,   13,    3, 5696,  173,  220],\n",
      "       [  17,   64,   47,   91,    3,  125,   24,   11, 5421,   48],\n",
      "       [  24,   18,   29,   67,  116,   39,    1,  174,  329, 5152]])\n",
      " Tha t ‡ s a lot of n umbers. I t ‡ s often more con venien t to look a t an image represen ta tion\n",
      " of the confusion ma trix, using M a tplotlib ‡ s \n",
      "matshow()\n",
      " function:\n",
      "plt\n",
      ".\n",
      "matshow\n",
      "(\n",
      "conf_mx\n",
      ",\n",
      " \n",
      "cmap\n",
      "=\n",
      "plt\n",
      ".\n",
      "cm\n",
      ".\n",
      "gray\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " This confusion ma trix looks fairly good, since most images are on the main diagonal,\n",
      " which means tha t they were classified correctly . The 5s look sligh tly darker than the\n",
      " other digits, which could mean tha t there are fewer images of 5s in the da taset or tha t\n",
      " the classifier does not perform as well on 5s as on other digits. In fact, you can verif y\n",
      " tha t both are the case.\n",
      " Let ‡ s focus the plot on the errors. First, you need to divide each value in the confusion\n",
      " ma trix by the n umber of images in the corresponding class, so you can com pare error\n",
      " Error Analysis  |  105\n",
      "\n",
      " ra tes instead of absolute n umber of errors (which would make abundan t classes look\n",
      "unfairly bad):\n",
      "row_sums\n",
      " \n",
      "=\n",
      " \n",
      "conf_mx\n",
      ".\n",
      "sum\n",
      "(\n",
      "axis\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "keepdims\n",
      "=\n",
      "True\n",
      ")\n",
      "norm_conf_mx\n",
      " \n",
      "=\n",
      " \n",
      "conf_mx\n",
      " \n",
      "/\n",
      " \n",
      "row_sums\n",
      " N ow let ‡ s fill the diagonal with zeros to keep only the errors, and let ‡ s plot the result:\n",
      "np\n",
      ".\n",
      "fill_diagonal\n",
      "(\n",
      "norm_conf_mx\n",
      ",\n",
      " \n",
      "0\n",
      ")\n",
      "plt\n",
      ".\n",
      "matshow\n",
      "(\n",
      "norm_conf_mx\n",
      ",\n",
      " \n",
      "cmap\n",
      "=\n",
      "plt\n",
      ".\n",
      "cm\n",
      ".\n",
      "gray\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " N ow you can clearly see the kinds of errors the classifier makes. Remember tha t rows\n",
      " represen t actual classes, while columns represen t predicted classes. The column for\n",
      " class 8 is quite brigh t, which tells you tha t man y images get misclassified as 8s. H owƒ\n",
      " ever , the row for class 8 is not tha t bad, telling you tha t actual 8s in general get propƒ\n",
      " erly classified as 8s. As you can see, the confusion ma trix is not necessarily\n",
      " symmetrical. Y ou can also see tha t 3s and 5s often get confused (in both directions).\n",
      " Analyzing the confusion ma trix can often give you insigh ts on wa ys to im prove your\n",
      " classifier . Looking a t this plot, it seems tha t your efforts should be spen t on reducing\n",
      " the false 8s. F or exam ple, you could tr y to ga ther more training da ta for digits tha t\n",
      "look like 8s (but are not) so the classifier can learn to distinguish them from real 8s.\n",
      " Or you could engineer new fea tures tha t would help the classifier ›for exam ple, writƒ\n",
      " ing an algorithm to coun t the n umber of closed loops (e.g., 8 has two , 6 has one, 5 has\n",
      " none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow , or\n",
      " OpenCV) to make some pa tterns stand out more, such as closed loops.\n",
      " Analyzing individual errors can also be a good wa y to gain insigh ts on wha t your\n",
      " classifier is doing and wh y it is failing, but it is more difficult and time-consuming.\n",
      " 106  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      "3\n",
      " But remember tha t our brain is a fan tastic pa ttern recognition system, and our visual system does a lot of\n",
      " com plex preprocessing before an y informa tion reaches our consciousness, so the fact tha t it feels sim ple does\n",
      " not mean tha t it is.\n",
      " F or exam ple, let ‡ s plot exam ples of 3s and 5s (the \n",
      "plot_digits()\n",
      " function just uses\n",
      " M a tplotlib ‡ s \n",
      "imshow()\n",
      "  function; see this cha pter‡ s J upyter notebook for details):\n",
      "cl_a\n",
      ",\n",
      " \n",
      "cl_b\n",
      " \n",
      "=\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "5\n",
      "X_aa\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[(\n",
      "y_train\n",
      " \n",
      "==\n",
      " \n",
      "cl_a\n",
      ")\n",
      " \n",
      "&\n",
      " \n",
      "(\n",
      "y_train_pred\n",
      " \n",
      "==\n",
      " \n",
      "cl_a\n",
      ")]\n",
      "X_ab\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[(\n",
      "y_train\n",
      " \n",
      "==\n",
      " \n",
      "cl_a\n",
      ")\n",
      " \n",
      "&\n",
      " \n",
      "(\n",
      "y_train_pred\n",
      " \n",
      "==\n",
      " \n",
      "cl_b\n",
      ")]\n",
      "X_ba\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[(\n",
      "y_train\n",
      " \n",
      "==\n",
      " \n",
      "cl_b\n",
      ")\n",
      " \n",
      "&\n",
      " \n",
      "(\n",
      "y_train_pred\n",
      " \n",
      "==\n",
      " \n",
      "cl_a\n",
      ")]\n",
      "X_bb\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[(\n",
      "y_train\n",
      " \n",
      "==\n",
      " \n",
      "cl_b\n",
      ")\n",
      " \n",
      "&\n",
      " \n",
      "(\n",
      "y_train_pred\n",
      " \n",
      "==\n",
      " \n",
      "cl_b\n",
      ")]\n",
      "plt\n",
      ".\n",
      "figure\n",
      "(\n",
      "figsize\n",
      "=\n",
      "(\n",
      "8\n",
      ",\n",
      "8\n",
      "))\n",
      "plt\n",
      ".\n",
      "subplot\n",
      "(\n",
      "221\n",
      ");\n",
      " \n",
      "plot_digits\n",
      "(\n",
      "X_aa\n",
      "[:\n",
      "25\n",
      "],\n",
      " \n",
      "images_per_row\n",
      "=\n",
      "5\n",
      ")\n",
      "plt\n",
      ".\n",
      "subplot\n",
      "(\n",
      "222\n",
      ");\n",
      " \n",
      "plot_digits\n",
      "(\n",
      "X_ab\n",
      "[:\n",
      "25\n",
      "],\n",
      " \n",
      "images_per_row\n",
      "=\n",
      "5\n",
      ")\n",
      "plt\n",
      ".\n",
      "subplot\n",
      "(\n",
      "223\n",
      ");\n",
      " \n",
      "plot_digits\n",
      "(\n",
      "X_ba\n",
      "[:\n",
      "25\n",
      "],\n",
      " \n",
      "images_per_row\n",
      "=\n",
      "5\n",
      ")\n",
      "plt\n",
      ".\n",
      "subplot\n",
      "(\n",
      "224\n",
      ");\n",
      " \n",
      "plot_digits\n",
      "(\n",
      "X_bb\n",
      "[:\n",
      "25\n",
      "],\n",
      " \n",
      "images_per_row\n",
      "=\n",
      "5\n",
      ")\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      "The two 5„5 blocks on the left show digits classified as 3s, and the two 5„5 blocks on\n",
      " the righ t show images classified as 5s. Some of the digits tha t the classifier gets wrong\n",
      " (i.e., in the bottom-left and top-righ t blocks) are so badly written tha t even a h uman\n",
      " would ha ve trouble classif ying them (e.g., the 5 on the 1\n",
      "st\n",
      " row and 2\n",
      "nd\n",
      " column truly\n",
      " looks like a badly written 3). H owever , most misclassified images seem like obvious\n",
      " errors to us, and it ‡ s hard to understand wh y the classifier made the mistakes it did.\n",
      "3\n",
      " The reason is tha t we used a sim ple \n",
      "SGDClassifier\n",
      ", which is a linear model. All it\n",
      " does is assign a weigh t per class to each pixel, and when it sees a new image it just\n",
      " sums up the weigh ted pixel in tensities to get a score for each class. So since 3s and 5s\n",
      "differ only by a few pixels, this model will easily confuse them.\n",
      " Error Analysis  |  107\n",
      "\n",
      " The main difference between 3s and 5s is the position of the small line tha t joins the\n",
      " top line to the bottom arc. If you dra w a 3 with the junction sligh tly shifted to the left,\n",
      " the classifier migh t classif y it as a 5, and vice versa. In other words, this classifier is\n",
      " quite sensitive to image shifting and rota tion. So one wa y to reduce the 3/5 confusion\n",
      " would be to preprocess the images to ensure tha t they are well cen tered and not too\n",
      " rota ted. This will probably help reduce other errors as well.\n",
      "Multilabel \n",
      "Classi•cation\n",
      " U n til \n",
      " now each instance has alwa ys been assigned to just one class. In some cases you\n",
      " ma y wan t your classifier to output m ultiple classes for each instance. F or exam ple,\n",
      " consider a face-recognition classifier : wha t should it do if it recognizes several people\n",
      " on the same picture? Of course it should a ttach one tag per person it recognizes. Sa y\n",
      " the classifier has been trained to recognize three faces, Alice, B ob , and Charlie; then\n",
      "when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\n",
      " — Alice yes, B ob no , Charlie yes –). Such a classifica tion system tha t outputs m ultiple\n",
      " binar y tags is called a \n",
      " m u l t i l a b e l \n",
      "classi†cation\n",
      " system.\n",
      " W e won ‡ t go in to face recognition just yet, but let ‡ s look a t a sim pler exam ple, just for\n",
      " illustra tion purposes:\n",
      "from\n",
      " \n",
      "sklearn.neighbors\n",
      " \n",
      "import\n",
      " \n",
      "KNeighborsClassifier\n",
      "y_train_large\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_train\n",
      " \n",
      ">=\n",
      " \n",
      "7\n",
      ")\n",
      "y_train_odd\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "y_train\n",
      " \n",
      "%\n",
      " \n",
      "2\n",
      " \n",
      "==\n",
      " \n",
      "1\n",
      ")\n",
      "y_multilabel\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "y_train_large\n",
      ",\n",
      " \n",
      "y_train_odd\n",
      "]\n",
      "knn_clf\n",
      " \n",
      "=\n",
      " \n",
      "KNeighborsClassifier\n",
      "()\n",
      "knn_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_multilabel\n",
      ")\n",
      " This code crea tes a \n",
      "y_multilabel\n",
      "  arra y con taining two target labels for each digit\n",
      " image: the first indica tes whether or not the digit is large (7, 8, or 9) and the second\n",
      " indica tes whether or not it is odd. The next lines crea te a \n",
      "KNeighborsClassifier\n",
      " \n",
      " instance (which supports m ultilabel classifica tion, but not all classifiers do) and we\n",
      " train it using the m ultiple targets arra y . N ow you can make a prediction, and notice\n",
      " tha t it outputs two labels:\n",
      ">>> \n",
      "knn_clf\n",
      ".\n",
      "predict\n",
      "([\n",
      "some_digit\n",
      "])\n",
      "array([[False,  True]])\n",
      " And it gets it righ t! The digit 5 is indeed not large (\n",
      "False\n",
      ") and odd (\n",
      "True\n",
      ").\n",
      " There are man y wa ys to evalua te a m ultilabel classifier , and selecting the righ t metric\n",
      " really depends on your project. F or exam ple, one a pproach is to measure the F\n",
      "1\n",
      "   score\n",
      " for each individual label (or an y other binar y classifier metric discussed earlier), then\n",
      " sim ply com pute the a verage score. This code com putes the a verage F\n",
      "1\n",
      " \n",
      "score across \n",
      "all\n",
      "labels:\n",
      " 108  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      "4\n",
      " Scikit-Learn offers a few other a veraging options and m ultilabel classifier metrics; see the documen ta tion for\n",
      "more details.\n",
      ">>> \n",
      "y_train_knn_pred\n",
      " \n",
      "=\n",
      " \n",
      "cross_val_predict\n",
      "(\n",
      "knn_clf\n",
      ",\n",
      " \n",
      "X_train\n",
      ",\n",
      " \n",
      "y_multilabel\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ")\n",
      ">>> \n",
      "f1_score\n",
      "(\n",
      "y_multilabel\n",
      ",\n",
      " \n",
      "y_train_knn_pred\n",
      ",\n",
      " \n",
      "average\n",
      "=\n",
      "\"macro\"\n",
      ")\n",
      "0.976410265560605\n",
      " This assumes tha t all labels are equally im portan t, which ma y not be the case. In parƒ\n",
      " ticular , if you ha ve man y more pictures of Alice than of B ob or Charlie, you ma y wan t\n",
      " to give more weigh t to the classifier‡ s score on pictures of Alice. One sim ple option is\n",
      " to give each label a weigh t equal to its \n",
      " s u p p o r t\n",
      " \n",
      " (i.e., the n umber of instances with tha t\n",
      " target label). T o do this, sim ply set \n",
      "average=\"weighted\"\n",
      " in the preceding code.\n",
      "4\n",
      "Multioutput \n",
      "Classi•cation\n",
      " The last type of classifica tion task we are going to discuss here is called \n",
      " m u l t i o u tp u t-\n",
      " m u l t i c l as s \n",
      "classi†cation\n",
      "   (or sim ply \n",
      " m u l t i o u tp u t \n",
      "classi†cation\n",
      " ). I t is sim ply a generalizaƒ\n",
      " tion of m ultilabel classifica tion where each label can be m ulticlass (i.e., it can ha ve\n",
      "more than two possible values).\n",
      " T o illustra te this, let ‡ s build a system tha t removes noise from images. I t will take as\n",
      " in put a noisy digit image, and it will (hopefully) output a clean digit image, repreƒ\n",
      " sen ted as an arra y of pixel in tensities, just like the MNIST images. N otice tha t the\n",
      " classifier‡ s output is m ultilabel (one label per pixel) and each label can ha ve m ultiple\n",
      " values (pixel in tensity ranges from 0 to 255). I t is th us an exam ple of a m ultioutput\n",
      " classifica tion system.\n",
      " The line between classifica tion and regression is sometimes blurr y ,\n",
      " such as in this exam ple. Arguably , predicting pixel in tensity is more\n",
      " akin to regression than to classifica tion. M oreover , m ultioutput\n",
      " systems are not limited to classifica tion tasks; you could even ha ve\n",
      " a system tha t outputs m ultiple labels per instance, including both\n",
      "class labels and value labels.\n",
      " Let ‡ s start by crea ting the training and test sets by taking the MNIST images and\n",
      " adding noise to their pixel in tensities using N umPy‡ s \n",
      "randint()\n",
      " function. The target\n",
      "images will be the original images:\n",
      "noise\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randint\n",
      "(\n",
      "0\n",
      ",\n",
      " \n",
      "100\n",
      ",\n",
      " \n",
      "(\n",
      "len\n",
      "(\n",
      "X_train\n",
      "),\n",
      " \n",
      "784\n",
      "))\n",
      "X_train_mod\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      " \n",
      "+\n",
      " \n",
      "noise\n",
      "noise\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randint\n",
      "(\n",
      "0\n",
      ",\n",
      " \n",
      "100\n",
      ",\n",
      " \n",
      "(\n",
      "len\n",
      "(\n",
      "X_test\n",
      "),\n",
      " \n",
      "784\n",
      "))\n",
      "X_test_mod\n",
      " \n",
      "=\n",
      " \n",
      "X_test\n",
      " \n",
      "+\n",
      " \n",
      "noise\n",
      "y_train_mod\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "y_test_mod\n",
      " \n",
      "=\n",
      " \n",
      "X_test\n",
      "Multioutput \n",
      "Classi•cation\n",
      "   |  109\n",
      "\n",
      "5\n",
      " Y ou can use the \n",
      "shift()\n",
      " function from the \n",
      "scipy.ndimage.interpolation\n",
      "  module. F or exam ple,\n",
      "shift(image, [2, 1], cval=0)\n",
      "  shifts the image 2 pixels down and 1 pixel to the righ t.\n",
      " Let ‡ s take a peek a t an image from the test set (yes, we ‡ re snooping on the test da ta, so\n",
      " you should be frowning righ t now):\n",
      " On the left is the noisy in put image, and on the righ t is the clean target image. N ow\n",
      " let ‡ s train the classifier and make it clean this image:\n",
      "knn_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_mod\n",
      ",\n",
      " \n",
      "y_train_mod\n",
      ")\n",
      "clean_digit\n",
      " \n",
      "=\n",
      " \n",
      "knn_clf\n",
      ".\n",
      "predict\n",
      "([\n",
      "X_test_mod\n",
      "[\n",
      "some_index\n",
      "]])\n",
      "plot_digit\n",
      "(\n",
      "clean_digit\n",
      ")\n",
      " Looks close enough to the target! This concludes our tour of classifica tion. H opefully\n",
      " you should now know how to select good metrics for classifica tion tasks, pick the\n",
      " a ppropria te precision/recall tradeoff, com pare classifiers, and more generally build\n",
      " good classifica tion systems for a variety of tasks.\n",
      "Exercises\n",
      "1.\n",
      " T r y to build a classifier for the MNIST da taset tha t achieves over 97% accuracy\n",
      " on the test set. Hin t: the \n",
      "KNeighborsClassifier\n",
      " works quite well for this task;\n",
      " you just need to find good h yperparameter values (tr y a grid search on the\n",
      "weights\n",
      " and \n",
      "n_neighbors\n",
      "  h yperparameters).\n",
      "2.\n",
      " W rite a function tha t can shift an MNIST image in an y direction (left, righ t, up ,\n",
      "or down) by one pixel.\n",
      "5\n",
      " \n",
      " Then, for each image in the training set, crea te four shifƒ\n",
      " 110  |  Chapter 3: \n",
      "Classi•cation\n",
      "\n",
      " ted copies (one per direction) and add them to the training set. Finally , train your\n",
      "best model on this expanded training set and measure its accuracy on the test set.\n",
      " Y ou should obser ve tha t your model performs even better now! This technique of\n",
      "artificially growing the training set is called \n",
      " d a t a a u g m en t a t i o n\n",
      " \n",
      "or \n",
      " t r a i n i n g s e t\n",
      " exp a ns i o n\n",
      ".\n",
      "3.\n",
      " T ackle the \n",
      " T i t a n i c\n",
      "  da taset. A grea t place to start is on \n",
      "Kaggle\n",
      ".\n",
      "4.\n",
      "Build a spam classifier (a more challenging exercise):\n",
      "⁄\n",
      " Download exam ples of spam and ham from \n",
      " A pache SpamAssassin ‡ s public\n",
      " da tasets\n",
      ".\n",
      "⁄\n",
      " U nzip the da tasets and familiarize yourself with the da ta forma t.\n",
      "⁄\n",
      " Split the da tasets in to a training set and a test set.\n",
      "⁄\n",
      " W rite a da ta prepara tion pipeline to con vert each email in to a fea ture vector .\n",
      " Y our prepara tion pipeline should transform an email in to a (sparse) vector\n",
      " indica ting the presence or absence of each possible word. F or exam ple, if all\n",
      " emails only ever con tain four words, —H ello , – —how , – — are, – —you, – then the email\n",
      " —H ello you H ello H ello you – would be con verted in to a vector [1, 0, 0, 1]\n",
      " (meaning [—H ello – is presen t, —how– is absen t, — are – is absen t, —you – is\n",
      " presen t]), or [3, 0, 0, 2] if you prefer to coun t the n umber of occurrences of\n",
      "each word.\n",
      "⁄\n",
      " Y ou ma y wan t to add h yperparameters to your prepara tion pipeline to con trol\n",
      " whether or not to strip off email headers, con vert each email to lowercase,\n",
      " remove punctua tion, replace all URLs with —URL, – replace all n umbers with\n",
      " —NUMBER, – or even perform \n",
      " s t em m i n g\n",
      " (i.e., trim off word endings; there are\n",
      " Python libraries a vailable to do this).\n",
      "⁄\n",
      " Then tr y out several classifiers and see if you can build a grea t spam classifier ,\n",
      "with both high recall and high precision.\n",
      " Solutions to these exercises are a vailable in the online J upyter notebooks a t \n",
      " h ttp s://\n",
      " g i t h u b .c o m/a ger o n/h a n d s o n-m l2\n",
      ".\n",
      " Exercises  |  111\n",
      "\n",
      "\n",
      "CHAPTER 4\n",
      "Training Models\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 4 in the final\n",
      "release of the book.\n",
      "So \n",
      " far we ha ve trea ted M achine Learning models and their training algorithms mostly\n",
      " like black boxes. If you wen t through some of the exercises in the previous cha pters,\n",
      " you ma y ha ve been surprised by how m uch you can get done without knowing an yƒ\n",
      " thing about wha t ‡ s under the hood: you optimized a regression system, you im proved\n",
      " a digit image classifier , and you even built a spam classifier from scra tch›all this\n",
      " without knowing how they actually work. Indeed, in man y situa tions you don ‡ t really\n",
      " need to know the im plemen ta tion details.\n",
      " H owever , ha ving a good understanding of how things work can help you quickly\n",
      " home in on the a ppropria te model, the righ t training algorithm to use, and a good set\n",
      " of h yperparameters for your task. U nderstanding wha t ‡ s under the hood will also help\n",
      " you debug issues and perform error analysis more efficien tly . Lastly , most of the topƒ\n",
      " ics discussed in this cha pter will be essen tial in understanding, building, and training\n",
      "neural networks (discussed in \n",
      " P art II\n",
      " of this book).\n",
      " In this cha pter , we will start by looking a t the Linear Regression model, one of the\n",
      " sim plest models there is. W e will discuss two ver y differen t wa ys to train it:\n",
      "⁄\n",
      " U sing a direct — closed-form – equa tion tha t directly com putes the model parameƒ\n",
      " ters tha t best fit the model to the training set (i.e., the model parameters tha t\n",
      "minimize the cost function over the training set).\n",
      "113\n",
      "\n",
      "⁄\n",
      " U sing an itera tive optimiza tion a pproach, called Gradien t Descen t (GD), tha t\n",
      "gradually tweaks the model parameters to minimize the cost function over the\n",
      " training set, even tually con verging to the same set of parameters as the first\n",
      " method. W e will look a t a few varian ts of Gradien t Descen t tha t we will use again\n",
      "and again when we study neural networks in \n",
      " P art II\n",
      " : Ba tch GD , Mini-ba tch GD ,\n",
      " and Stochastic GD .\n",
      " N ext we will look a t P olynomial Regression, a more com plex model tha t can fit nonƒ\n",
      " linear da tasets. Since this model has more parameters than Linear Regression, it is\n",
      " more prone to overfitting the training da ta, so we will look a t how to detect whether\n",
      " or not this is the case, using learning cur ves, and then we will look a t several regulariƒ\n",
      " za tion techniques tha t can reduce the risk of overfitting the training set.\n",
      " Finally , we will look a t two more models tha t are commonly used for classifica tion\n",
      "tasks: Logistic Regression and Softmax Regression.\n",
      " There will be quite a few ma th equa tions in this cha pter , using basic\n",
      " notions of linear algebra and calculus. T o understand these equaƒ\n",
      " tions, you will need to know wha t vectors and ma trices are, how to\n",
      " transpose them, m ultiply them, and in verse them, and wha t partial\n",
      " deriva tives are. If you are unfamiliar with these concepts, please go\n",
      " through the linear algebra and calculus in troductor y tutorials a vailƒ\n",
      " able as J upyter notebooks in the online supplemen tal ma terial. F or\n",
      " those who are truly allergic to ma thema tics, you should still go\n",
      " through this cha pter and sim ply skip the equa tions; hopefully , the\n",
      " text will be sufficien t to help you understand most of the concepts.\n",
      "Linear Regression\n",
      "In \n",
      " Cha pter 1\n",
      " , we looked a t a sim ple regression model of life sa tisfaction: \n",
      " l i f e_s a t i s f a c‡\n",
      " t i o n\n",
      " = \n",
      "–\n",
      "0\n",
      " + \n",
      "–\n",
      "1\n",
      " „ \n",
      " GD P_p er_c a p i t a\n",
      ".\n",
      " This model is just a linear function of the in put fea ture \n",
      "GDP_per_capita\n",
      ". \n",
      "–\n",
      "0\n",
      "   and \n",
      "–\n",
      "1\n",
      "   are\n",
      " the model ‡ s parameters.\n",
      " M ore generally , a linear model makes a prediction by sim ply com puting a weigh ted\n",
      " sum of the in put fea tures, plus a constan t called \n",
      "the \n",
      " b i as t er m\n",
      "   (also called the \n",
      " i n t er c e p t\n",
      " t er m\n",
      "), as shown in \n",
      " Equa tion 4-1\n",
      ".\n",
      " Eq u a t i o n 4-1. L i n e a r R e g r e s s i o n m o d e l p r e d i c t i o n\n",
      "y\n",
      "=\n",
      "–\n",
      "0\n",
      "+\n",
      "–\n",
      "1\n",
      "x\n",
      "1\n",
      "+\n",
      "–\n",
      "2\n",
      "x\n",
      "2\n",
      "+\n",
      "+\n",
      "–\n",
      "n\n",
      "x\n",
      "n\n",
      "⁄\n",
      "ƒ\n",
      " is the predicted value.\n",
      " 114  |  Chapter 4: Training Models\n",
      "\n",
      "⁄\n",
      "n\n",
      "  is the n umber of fea tures.\n",
      "⁄\n",
      "x\n",
      "i\n",
      " is the i\n",
      "th\n",
      "  fea ture value.\n",
      "⁄\n",
      "–\n",
      "j\n",
      " is the j\n",
      "th\n",
      " model parameter (including the bias term \n",
      "–\n",
      "0\n",
      "  and the fea ture weigh ts\n",
      "–\n",
      "1\n",
      ", \n",
      "–\n",
      "2\n",
      ", \n",
      ", \n",
      "–\n",
      "n\n",
      ").\n",
      " This can be written m uch more concisely using a vectorized form, as shown in \n",
      "Equaƒ\n",
      "tion 4-2\n",
      ".\n",
      " Eq u a t i o n 4-2. L i n e a r R e g r e s s i o n m o d e l p r e d i c t i o n (v e c t o r iz e d f o r m)\n",
      "y\n",
      "=\n",
      "h\n",
      "•\n",
      "x\n",
      "=\n",
      "•\n",
      "’\n",
      "x\n",
      "⁄\n",
      "•\n",
      "  is the model ‡ s \n",
      " p a r a m e t er v e c t o r\n",
      " , con taining the bias term \n",
      "–\n",
      "0\n",
      "  and the fea ture\n",
      " weigh ts \n",
      "–\n",
      "1\n",
      " to \n",
      "–\n",
      "n\n",
      ".\n",
      "⁄\n",
      "x\n",
      "  is the instance ‡ s \n",
      " f e a t u r e v e c t o r\n",
      " , con taining \n",
      "x\n",
      "0\n",
      " to \n",
      "x\n",
      "n\n",
      ", with \n",
      "x\n",
      "0\n",
      "  alwa ys equal to 1.\n",
      "⁄\n",
      "•\n",
      " ’ \n",
      "x\n",
      " is the dot product of the vectors \n",
      "•\n",
      " and \n",
      "x\n",
      ", which is of course equal to\n",
      "–\n",
      "0\n",
      "x\n",
      "0\n",
      "+\n",
      "–\n",
      "1\n",
      "x\n",
      "1\n",
      "+\n",
      "–\n",
      "2\n",
      "x\n",
      "2\n",
      "+\n",
      "+\n",
      "–\n",
      "n\n",
      "x\n",
      "n\n",
      ".\n",
      "⁄\n",
      "h\n",
      "•\n",
      "  is the h ypothesis function, using the model parameters \n",
      "•\n",
      ".\n",
      " In M achine Learning, vectors are often represen ted as \n",
      " c o l u m n v e c‡\n",
      " t o r s\n",
      " , which are 2D arra ys with a single column. If \n",
      "•\n",
      " and \n",
      "x\n",
      " are colƒ\n",
      "umn vectors, then the prediction is: \n",
      "y\n",
      "=\n",
      "•\n",
      "T\n",
      "x\n",
      ", where \n",
      "•\n",
      "T\n",
      " is the\n",
      " t r a ns p o s e\n",
      " of \n",
      "•\n",
      " (a row vector instead of a column vector) and \n",
      "•\n",
      "T\n",
      "x\n",
      " \n",
      "is\n",
      " the ma trix m ultiplica tion of \n",
      "•\n",
      "T\n",
      " and \n",
      "x\n",
      " . I t is of course the same preƒ\n",
      " diction, except it is now represen ted as a single cell ma trix ra ther\n",
      " than a scalar value. In this book we will use this nota tion to a void\n",
      " switching between dot products and ma trix m ultiplica tions.\n",
      " Oka y , tha t ‡ s the Linear Regression model, so now how do we train it? W ell, recall tha t\n",
      " training a model means setting its parameters so tha t the model best fits the training\n",
      " set. F or this purpose, we first need a measure of how well (or poorly) the model fits\n",
      " the training da ta. In \n",
      " Cha pter 2\n",
      "  we sa w tha t the most common performance measure\n",
      " of a regression model is the Root M ean Square Error (RMSE) (\n",
      " Equa tion 2-1\n",
      "). Thereƒ\n",
      "fore, to train a Linear Regression model, you need to find the value of \n",
      "•\n",
      "   tha t minimiƒ\n",
      " zes the RMSE. In practice, it is sim pler to minimize the M ean Square Error (MSE)\n",
      " Linear Regression  |  115\n",
      "\n",
      "1\n",
      " I t is often the case tha t a learning algorithm will tr y to optimize a differen t function than the performance\n",
      " measure used to evalua te the final model. This is generally beca use tha t function is easier to com pute, beca use\n",
      " it has useful differen tia tion properties tha t the performance measure lacks, or beca use we wan t to constrain\n",
      " the model during training, as we will see when we discuss regulariza tion.\n",
      "2\n",
      " The demonstra tion tha t this returns the value of \n",
      "•\n",
      "  tha t minimizes the cost function is outside the scope of this\n",
      "book.\n",
      " than the RMSE, and it leads to the same result (beca use the value tha t minimizes a\n",
      "function also minimizes its square root).\n",
      "1\n",
      " The MSE of a Linear Regression h ypothesis \n",
      "h\n",
      "•\n",
      " on a training set \n",
      "X\n",
      "  is calcula ted using\n",
      " Equa tion 4-3\n",
      ".\n",
      " Eq u a t i o n 4-3. MS E c o s t f u n c t i o n f o r a L i n e a r R e g r e s s i o n m o d e l\n",
      "MSE\n",
      "X\n",
      ",\n",
      "h\n",
      "•\n",
      "=\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "•\n",
      "T\n",
      "x\n",
      "i\n",
      "”\n",
      "y\n",
      "i\n",
      "2\n",
      " M ost of these nota tions were presen ted in \n",
      " Cha pter 2\n",
      " (see \n",
      " —N ota tions –\n",
      " on page \n",
      "43\n",
      ").\n",
      " The only difference is tha t we write \n",
      "h\n",
      "•\n",
      " instead of just \n",
      "h\n",
      "  in order to make it clear tha t\n",
      "the model is parametrized by the vector \n",
      "•\n",
      " . T o sim plif y nota tions, we will just write\n",
      "MSE(\n",
      "•\n",
      ") instead of MSE(\n",
      "X\n",
      ", \n",
      "h\n",
      "•\n",
      ").\n",
      "The Normal Equation\n",
      " T o \n",
      "find the value of \n",
      "•\n",
      " \n",
      " tha t minimizes the cost function, there is a \n",
      " c l o s e d-f o r m s o l u t i o n\n",
      " ›in other words, a ma thema tical equa tion tha t gives the result directly . This is called\n",
      "the \n",
      " N o r m a l Eq u a t i o n\n",
      " (\n",
      " Equa tion 4-4\n",
      ").\n",
      "2\n",
      " Eq u a t i o n 4-4. N o r m a l Eq u a t i o n\n",
      "•\n",
      "=\n",
      "X\n",
      "T\n",
      "X\n",
      " ” 1\n",
      "Ł\n",
      "X\n",
      "T\n",
      "Ł\n",
      "y\n",
      "⁄\n",
      "•\n",
      " is the value of \n",
      "•\n",
      "  tha t minimizes the cost function.\n",
      "⁄\n",
      "y\n",
      "  is the vector of target values con taining \n",
      "y\n",
      "(1)\n",
      " to \n",
      "y\n",
      "(\n",
      "m\n",
      ")\n",
      ".\n",
      " Let ‡ s genera te some linear -looking da ta to test this equa tion on (\n",
      "Figure 4-1\n",
      "):\n",
      "import\n",
      " \n",
      "numpy\n",
      " \n",
      "as\n",
      " \n",
      "np\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      " \n",
      "*\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "rand\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "1\n",
      ")\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "4\n",
      " \n",
      "+\n",
      " \n",
      "3\n",
      " \n",
      "*\n",
      " \n",
      "X\n",
      " \n",
      "+\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randn\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "1\n",
      ")\n",
      " 116  |  Chapter 4: Training Models\n",
      "\n",
      " F i g u r e 4-1. R a n d o m l y gen er a t e d l i n e a r d a t as e t\n",
      " N ow let ‡ s com pute \n",
      "•\n",
      " \n",
      " using the N ormal Equa tion. W e will use the \n",
      "inv()\n",
      "   function from\n",
      " N umPy‡ s Linear Algebra module (\n",
      "np.linalg\n",
      " ) to com pute the in verse of a ma trix, and\n",
      "the \n",
      "dot()\n",
      "  method for ma trix m ultiplica tion:\n",
      "X_b\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "np\n",
      ".\n",
      "ones\n",
      "((\n",
      "100\n",
      ",\n",
      " \n",
      "1\n",
      ")),\n",
      " \n",
      "X\n",
      "]\n",
      "  \n",
      "# add x0 = 1 to each instance\n",
      "theta_best\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "linalg\n",
      ".\n",
      "inv\n",
      "(\n",
      "X_b\n",
      ".\n",
      "T\n",
      ".\n",
      "dot\n",
      "(\n",
      "X_b\n",
      "))\n",
      ".\n",
      "dot\n",
      "(\n",
      "X_b\n",
      ".\n",
      "T\n",
      ")\n",
      ".\n",
      "dot\n",
      "(\n",
      "y\n",
      ")\n",
      " The actual function tha t we used to genera te the da ta is \n",
      "y\n",
      " \n",
      "= 4 + 3\n",
      "x\n",
      "1\n",
      " \n",
      " + Ga ussian noise.\n",
      " Let ‡ s see wha t the equa tion found:\n",
      ">>> \n",
      "theta_best\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      " W e would ha ve hoped for \n",
      "–\n",
      "0\n",
      " \n",
      "= 4 and \n",
      "–\n",
      "1\n",
      " \n",
      "= 3 instead of \n",
      "–\n",
      "0\n",
      " \n",
      "= 4.215 and \n",
      "–\n",
      "1\n",
      " \n",
      "= 2.770. Close\n",
      " enough, but the noise made it im possible to recover the exact parameters of the origiƒ\n",
      "nal function.\n",
      " N ow you can make predictions using \n",
      "•\n",
      ":\n",
      ">>> \n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([[\n",
      "0\n",
      "],\n",
      " \n",
      "[\n",
      "2\n",
      "]])\n",
      ">>> \n",
      "X_new_b\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "c_\n",
      "[\n",
      "np\n",
      ".\n",
      "ones\n",
      "((\n",
      "2\n",
      ",\n",
      " \n",
      "1\n",
      ")),\n",
      " \n",
      "X_new\n",
      "]\n",
      " \n",
      "# add x0 = 1 to each instance\n",
      ">>> \n",
      "y_predict\n",
      " \n",
      "=\n",
      " \n",
      "X_new_b\n",
      ".\n",
      "dot\n",
      "(\n",
      "theta_best\n",
      ")\n",
      ">>> \n",
      "y_predict\n",
      "array([[4.21509616],\n",
      "       [9.75532293]])\n",
      " Let ‡ s plot this model ‡ s predictions (\n",
      "Figure 4-2\n",
      "):\n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "X_new\n",
      ",\n",
      " \n",
      "y_predict\n",
      ",\n",
      " \n",
      "\"r-\"\n",
      ")\n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ",\n",
      " \n",
      "\"b.\"\n",
      ")\n",
      " Linear Regression  |  117\n",
      "\n",
      "3\n",
      " N ote tha t Scikit-Learn separa tes the bias term (\n",
      "intercept_\n",
      " ) from the fea ture weigh ts (\n",
      "coef_\n",
      ").\n",
      "plt\n",
      ".\n",
      "axis\n",
      "([\n",
      "0\n",
      ",\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "15\n",
      "])\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " F i g u r e 4-2. L i n e a r R e g r e s s i o n m o d e l p r e d i c t i o ns\n",
      " P erforming linear regression using Scikit-Learn is quite sim ple:\n",
      "3\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "LinearRegression\n",
      ">>> \n",
      "lin_reg\n",
      " \n",
      "=\n",
      " \n",
      "LinearRegression\n",
      "()\n",
      ">>> \n",
      "lin_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      ">>> \n",
      "lin_reg\n",
      ".\n",
      "intercept_\n",
      ",\n",
      " \n",
      "lin_reg\n",
      ".\n",
      "coef_\n",
      "(array([4.21509616]), array([[2.77011339]]))\n",
      ">>> \n",
      "lin_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      "array([[4.21509616],\n",
      "       [9.75532293]])\n",
      "The \n",
      "LinearRegression\n",
      " class is based on the \n",
      "scipy.linalg.lstsq()\n",
      " \n",
      "function (the\n",
      " name stands for —least squares –), which you could call directly :\n",
      ">>> \n",
      "theta_best_svd\n",
      ",\n",
      " \n",
      "residuals\n",
      ",\n",
      " \n",
      "rank\n",
      ",\n",
      " \n",
      "s\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "linalg\n",
      ".\n",
      "lstsq\n",
      "(\n",
      "X_b\n",
      ",\n",
      " \n",
      "y\n",
      ",\n",
      " \n",
      "rcond\n",
      "=\n",
      "1e-6\n",
      ")\n",
      ">>> \n",
      "theta_best_svd\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      " This function com putes \n",
      "•\n",
      "=\n",
      "X\n",
      "+\n",
      "y\n",
      ", where \n",
      "+\n",
      " is the \n",
      " p s eu d o i n v er s e\n",
      " \n",
      "of \n",
      "X\n",
      " \n",
      "(specifically the\n",
      " M oore-P enrose in verse). Y ou can use \n",
      "np.linalg.pinv()\n",
      "  to com pute the pseudoinƒ\n",
      " verse directly :\n",
      ">>> \n",
      "np\n",
      ".\n",
      "linalg\n",
      ".\n",
      "pinv\n",
      "(\n",
      "X_b\n",
      ")\n",
      ".\n",
      "dot\n",
      "(\n",
      "y\n",
      ")\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      " 118  |  Chapter 4: Training Models\n",
      "\n",
      " The pseudoin verse itself is com puted using a standard ma trix factoriza tion technique \n",
      "called \n",
      " S i n g u l a r V a l u e D e c o m p o s i t i o n\n",
      "  (SVD) tha t can decom pose the training set\n",
      " ma trix \n",
      "X\n",
      "  in to the ma trix m ultiplica tion of three ma trices \n",
      "U\n",
      " \n",
      "†\n",
      " \n",
      "V\n",
      "T\n",
      " \n",
      "(see\n",
      "numpy.linalg.svd()\n",
      " ). The pseudoin verse is com puted as \n",
      "X\n",
      "+\n",
      "=\n",
      "V†\n",
      "+\n",
      "U\n",
      "T\n",
      " . T o com pute\n",
      " the ma trix \n",
      "†\n",
      "+\n",
      ", the algorithm takes \n",
      "†\n",
      "  and sets to zero all values smaller than a tin y\n",
      " threshold value, then it replaces all the non-zero values with their in verse, and finally\n",
      " it transposes the resulting ma trix. This a pproach is more efficien t than com puting the\n",
      " N ormal Equa tion, plus it handles edge cases nicely : indeed, the N ormal Equa tion ma y\n",
      " not work if the ma trix \n",
      "X\n",
      "T\n",
      "X\n",
      " \n",
      " is not in vertible (i.e., singular), such as if \n",
      "m\n",
      "   < \n",
      "n\n",
      "   or if some\n",
      " fea tures are redundan t, but the pseudoin verse is alwa ys defined.\n",
      "Computational Complexity\n",
      " The N ormal Equa tion com putes the in verse of \n",
      "X\n",
      "T\n",
      " \n",
      "X\n",
      ", which is an (\n",
      "n\n",
      " + 1) „ (\n",
      "n\n",
      " + 1)\n",
      " ma trix (where \n",
      "n\n",
      " \n",
      " is the n umber of fea tures). The \n",
      " c o m p u t a t i o n a l c o m p l exi ty\n",
      "   of in verting\n",
      " such a ma trix is typically about \n",
      "O\n",
      "(\n",
      "n\n",
      "2.4\n",
      ") to \n",
      "O\n",
      "(\n",
      "n\n",
      "3\n",
      " ) (depending on the im plemen ta tion).\n",
      " In other words, if you double the n umber of fea tures, you m ultiply the com puta tion\n",
      "time by roughly 2\n",
      "2.4\n",
      " = 5.3 to 2\n",
      "3\n",
      " = 8.\n",
      " The SVD a pproach used by Scikit-Learn ‡ s \n",
      "LinearRegression\n",
      " class is about \n",
      "O\n",
      "(\n",
      "n\n",
      "2\n",
      "). If\n",
      " you double the n umber of fea tures, you m ultiply the com puta tion time by roughly 4.\n",
      " B oth the N ormal Equa tion and the SVD a pproach get ver y slow\n",
      " when the n umber of fea tures grows large (e.g., 100,000). On the\n",
      " positive side, both are linear with regards to the n umber of instanƒ\n",
      "ces in the training set (they are \n",
      "O\n",
      "(\n",
      "m\n",
      ")), so they handle large training\n",
      " sets efficien tly , provided they can fit in memor y .\n",
      " Also , once you ha ve trained your Linear Regression model (using the N ormal Equaƒ\n",
      " tion or an y other algorithm), predictions are ver y fast: the com puta tional com plexity\n",
      " is linear with regards to both the n umber of instances you wan t to make predictions\n",
      " on and the n umber of fea tures. In other words, making predictions on twice as man y\n",
      " instances (or twice as man y fea tures) will just take roughly twice as m uch time.\n",
      " N ow we will look a t ver y differen t wa ys to train a Linear Regression model, better\n",
      " suited for cases where there are a large n umber of fea tures, or too man y training\n",
      " instances to fit in memor y .\n",
      "Gradient Descent\n",
      " G r a d i en t D e s c en t\n",
      "  is a ver y generic optimiza tion algorithm ca pable of finding optimal\n",
      " solutions to a wide range of problems. The general idea of Gradien t Descen t is to\n",
      " tweak parameters itera tively in order to minimize a cost function.\n",
      " Gradient Descent  |  119\n",
      "\n",
      " Suppose you are lost in the moun tains in a dense fog; you can only feel the slope of\n",
      " the ground below your feet. A good stra teg y to get to the bottom of the valley quickly\n",
      " is to go downhill in the direction of the steepest slope. This is exactly wha t Gradien t\n",
      " Descen t does: it measures the local gradien t of the error function with regards to the \n",
      "parameter vector \n",
      "•\n",
      " , and it goes in the direction of descending gradien t. Once the graƒ\n",
      " dien t is zero , you ha ve reached a minim um!\n",
      " Concretely , you start by filling \n",
      "•\n",
      " with random values (this is called \n",
      " r a n d o m i n i t i a l iz a‡\n",
      " t i o n\n",
      " ), and then you im prove it gradually , taking one baby step a t a time, each step\n",
      " a ttem pting to decrease the cost function (e.g., the MSE), un til the algorithm \n",
      " c o n v er ge s\n",
      " to a minim um (see \n",
      "Figure 4-3\n",
      ").\n",
      " F i g u r e 4-3. G r a d i en t D e s c en t\n",
      " An im portan t parameter in Gradien t Descen t is the size of the steps, determined by \n",
      "the \n",
      " l e a r n i n g r a t e\n",
      "  h yperparameter . If the learning ra te is too small, then the algorithm\n",
      " will ha ve to go through man y itera tions to con verge, which will take a long time (see\n",
      "Figure 4-4\n",
      ").\n",
      " 120  |  Chapter 4: Training Models\n",
      "\n",
      " F i g u r e 4-4. L e a r n i n g r a t e t o o s m a l l\n",
      " On the other hand, if the learning ra te is too high, you migh t jum p across the valley\n",
      "and end up on the other side, possibly even higher up than you were before. This\n",
      " migh t make the algorithm diverge, with larger and larger values, failing to find a good\n",
      "solution (see \n",
      "Figure 4-5\n",
      ").\n",
      " F i g u r e 4-5. L e a r n i n g r a t e t o o l a r ge\n",
      " Finally , not all cost functions look like nice regular bowls. There ma y be holes, ridges,\n",
      " pla tea us, and all sorts of irregular terrains, making con vergence to the minim um ver y\n",
      "difficult. \n",
      "Figure 4-6\n",
      " \n",
      " shows the two main challenges with Gradien t Descen t: if the ranƒ\n",
      " dom initializa tion starts the algorithm on the left, then it will con verge to a \n",
      " l o c a l m i n i‡\n",
      " m u m\n",
      ", which is not as good as the \n",
      " gl o b a l m i n i m u m\n",
      " . If it starts on the righ t, then it will\n",
      " take a ver y long time to cross the pla tea u, and if you stop too early you will never\n",
      " reach the global minim um.\n",
      " Gradient Descent  |  121\n",
      "\n",
      "4\n",
      " T echnically speaking, its deriva tive is \n",
      " L i p s c h i tz c o n t i n u o u s\n",
      ".\n",
      "5\n",
      " Since fea ture 1 is smaller , it takes a larger change in \n",
      "–\n",
      "1\n",
      "  to affect the cost function, which is wh y the bowl is\n",
      " elonga ted along the \n",
      "–\n",
      "1\n",
      " axis.\n",
      " F i g u r e 4-6. G r a d i en t D e s c en t p i t f a l l s\n",
      " F ortuna tely , the MSE cost function for a Linear Regression model ha ppens to be a\n",
      " c o n v ex f u n c t i o n\n",
      " , which means tha t if you pick an y two poin ts on the cur ve, the line\n",
      " segmen t joining them never crosses the cur ve. This im plies tha t there are no local\n",
      " minima, just one global minim um. I t is also a con tin uous function with a slope tha t\n",
      " never changes abruptly .\n",
      "4\n",
      "  These two facts ha ve a grea t consequence: Gradien t Descen t\n",
      " is guaran teed to a pproach arbitrarily close the global minim um (if you wait long\n",
      " enough and if the learning ra te is not too high).\n",
      " In fact, the cost function has the sha pe of a bowl, but it can be an elonga ted bowl if\n",
      " the fea tures ha ve ver y differen t scales. \n",
      "Figure 4-7\n",
      "  shows Gradien t Descen t on a trainƒ\n",
      " ing set where fea tures 1 and 2 ha ve the same scale (on the left), and on a training set\n",
      " where fea ture 1 has m uch smaller values than fea ture 2 (on the righ t).\n",
      "5\n",
      " F i g u r e 4-7. G r a d i en t D e s c en t w i t h a n d w i t h o u t f e a t u r e s c a l i n g\n",
      " 122  |  Chapter 4: Training Models\n",
      "\n",
      " As you can see, on the left the Gradien t Descen t algorithm goes straigh t toward the\n",
      " minim um, thereby reaching it quickly , whereas on the righ t it first goes in a direction\n",
      " almost orthogonal to the direction of the global minim um, and it ends with a long\n",
      " march down an almost fla t valley . I t will even tually reach the minim um, but it will\n",
      "take a long time.\n",
      " When using Gradien t Descen t, you should ensure tha t all fea tures\n",
      " ha ve a similar scale (e.g., using Scikit-Learn ‡ s \n",
      "StandardScaler\n",
      " class), or else it will take m uch longer to con verge.\n",
      " This diagram also illustra tes the fact tha t training a model means searching for a\n",
      " combina tion of model parameters tha t minimizes a cost function (over the training\n",
      " set). I t is a search in the model ‡ s \n",
      " p a r a m e t er s p a c e\n",
      ": the more parameters a model has,\n",
      "the more dimensions this space has, and the harder the search is: searching for a neeƒ\n",
      " dle in a 300-dimensional ha ystack is m uch trickier than in three dimensions. F ortuƒ\n",
      " na tely , since the cost function is con vex in the case of Linear Regression, the needle is\n",
      " sim ply a t the bottom of the bowl.\n",
      "Batch Gradient Descent\n",
      " T o im plemen t Gradien t Descen t, you need to com pute the gradien t of the cost funcƒ\n",
      "tion with regards to each model parameter \n",
      "–\n",
      "j\n",
      " . In other words, you need to calcula te\n",
      " how m uch the cost function will change if you change \n",
      "–\n",
      "j\n",
      " \n",
      "just a little bit. This is called \n",
      "a \n",
      " p a r t i a l d er i v a t i v e\n",
      " . I t is like asking —wha t is the slope of the moun tain under m y feet\n",
      " if I face east? – and then asking the same question facing north (and so on for all other\n",
      "dimensions, if you can imagine a universe with more than three dimensions). \n",
      "Equaƒ\n",
      "tion 4-5\n",
      "  com putes the partial deriva tive of the cost function with regards to parameƒ\n",
      "ter \n",
      "–\n",
      "j\n",
      ", noted \n",
      "Œ\n",
      "Œ\n",
      "–\n",
      "j\n",
      " MSE(\n",
      "•\n",
      ").\n",
      " Eq u a t i o n 4-5. P a r t i a l d er i v a t i v e s o f t h e c o s t f u n c t i o n\n",
      "Œ\n",
      "Œ\n",
      "–\n",
      "j\n",
      "MSE\n",
      "•\n",
      "=\n",
      "2\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "•\n",
      "T\n",
      "x\n",
      "i\n",
      "”\n",
      "y\n",
      "i\n",
      "x\n",
      "j\n",
      "i\n",
      " Instead of com puting these partial deriva tives individually , you can use \n",
      " Equa tion 4-6\n",
      " to com pute them all in one go . The gradien t vector , noted \n",
      "•\n",
      "MSE(\n",
      "•\n",
      " ), con tains all the\n",
      " partial deriva tives of the cost function (one for each model parameter).\n",
      " Gradient Descent  |  123\n",
      "\n",
      "6\n",
      " E ta (\n",
      "−\n",
      ") is the 7\n",
      "th\n",
      " letter of the Greek alphabet.\n",
      " Eq u a t i o n 4-6. G r a d i en t v e c t o r o f t h e c o s t f u n c t i o n\n",
      "•\n",
      "MSE\n",
      "•\n",
      "=\n",
      "Œ\n",
      "Œ\n",
      "–\n",
      "0\n",
      "MSE\n",
      "•\n",
      "Œ\n",
      "Œ\n",
      "–\n",
      "1\n",
      "MSE\n",
      "•\n",
      "Œ\n",
      "Œ\n",
      "–\n",
      "n\n",
      "MSE\n",
      "•\n",
      "=\n",
      "2\n",
      "m\n",
      "X\n",
      "T\n",
      "X•\n",
      "”\n",
      "y\n",
      " N otice tha t this form ula in volves calcula tions over the full training\n",
      "set \n",
      "X\n",
      " , a t each Gradien t Descen t step! This is wh y the algorithm is\n",
      "called \n",
      " B a t c h G r a d i en t D e s c en t\n",
      " : it uses the whole ba tch of training\n",
      " da ta a t ever y step (actually , \n",
      " F u l l G r a d i en t D e s c en t\n",
      " would probably\n",
      " be a better name). As a result it is terribly slow on ver y large trainƒ\n",
      " ing sets (but we will see m uch faster Gradien t Descen t algorithms\n",
      " shortly). H owever , Gradien t Descen t scales well with the n umber of\n",
      " fea tures; training a Linear Regression model when there are h unƒ\n",
      " dreds of thousands of fea tures is m uch faster using Gradien t\n",
      " Descen t than using the N ormal Equa tion or SVD decom position.\n",
      " Once you ha ve the gradien t vector , which poin ts uphill, just go in the opposite direcƒ\n",
      "tion to go downhill. This means subtracting \n",
      "•\n",
      "MSE(\n",
      "•\n",
      ") from \n",
      "•\n",
      ". This is where the \n",
      " learning ra te \n",
      "−\n",
      "  comes in to pla y :\n",
      "6\n",
      "  m ultiply the gradien t vector by \n",
      "−\n",
      " to determine the\n",
      "size of the downhill step (\n",
      " Equa tion 4-7\n",
      ").\n",
      " Eq u a t i o n 4-7. G r a d i en t D e s c en t s t e p\n",
      "•\n",
      "next step\n",
      "=\n",
      "•\n",
      "”\n",
      "−\n",
      "•\n",
      "MSE\n",
      "•\n",
      " Let ‡ s look a t a quick im plemen ta tion of this algorithm:\n",
      "eta\n",
      " \n",
      "=\n",
      " \n",
      "0.1\n",
      "  \n",
      "# learning rate\n",
      "n_iterations\n",
      " \n",
      "=\n",
      " \n",
      "1000\n",
      "m\n",
      " \n",
      "=\n",
      " \n",
      "100\n",
      "theta\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randn\n",
      "(\n",
      "2\n",
      ",\n",
      "1\n",
      ")\n",
      "  \n",
      "# random initialization\n",
      "for\n",
      " \n",
      "iteration\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "n_iterations\n",
      "):\n",
      "    \n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      "/\n",
      "m\n",
      " \n",
      "*\n",
      " \n",
      "X_b\n",
      ".\n",
      "T\n",
      ".\n",
      "dot\n",
      "(\n",
      "X_b\n",
      ".\n",
      "dot\n",
      "(\n",
      "theta\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "y\n",
      ")\n",
      "    \n",
      "theta\n",
      " \n",
      "=\n",
      " \n",
      "theta\n",
      " \n",
      "-\n",
      " \n",
      "eta\n",
      " \n",
      "*\n",
      " \n",
      "gradients\n",
      " 124  |  Chapter 4: Training Models\n",
      "\n",
      " Tha t wasn ‡ t too hard! Let ‡ s look a t the resulting \n",
      "theta\n",
      ":\n",
      ">>> \n",
      "theta\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      " H ey , tha t ‡ s exactly wha t the N ormal Equa tion found! Gradien t Descen t worked perƒ\n",
      " fectly . But wha t if you had used a differen t learning ra te \n",
      "eta\n",
      "? \n",
      "Figure 4-8\n",
      " \n",
      "shows the\n",
      " first 10 steps of Gradien t Descen t using three differen t learning ra tes (the dashed line\n",
      " represen ts the starting poin t).\n",
      " F i g u r e 4-8. G r a d i en t D e s c en t w i t h v a r i o u s l e a r n i n g r a t e s\n",
      " On the left, the learning ra te is too low : the algorithm will even tually reach the soluƒ\n",
      " tion, but it will take a long time. In the middle, the learning ra te looks pretty good: in\n",
      " just a few itera tions, it has already con verged to the solution. On the righ t, the learnƒ\n",
      " ing ra te is too high: the algorithm diverges, jum ping all over the place and actually\n",
      " getting further and further a wa y from the solution a t ever y step .\n",
      " T o find a good learning ra te, you can use grid search (see \n",
      " Cha pter 2\n",
      " ). H owever , you\n",
      " ma y wan t to limit the n umber of itera tions so tha t grid search can elimina te models\n",
      " tha t take too long to con verge.\n",
      " Y ou ma y wonder how to set the n umber of itera tions. If it is too low , you will still be\n",
      " far a wa y from the optimal solution when the algorithm stops, but if it is too high, you\n",
      " will waste time while the model parameters do not change an ymore. A sim ple soluƒ\n",
      " tion is to set a ver y large n umber of itera tions but to in terrupt the algorithm when the\n",
      " gradien t vector becomes tin y›tha t is, when its norm becomes smaller than a tin y\n",
      " n umber \n",
      " (called the \n",
      " t o l er a n c e\n",
      " )›beca use this ha ppens when Gradien t Descen t has\n",
      " (almost) reached the minim um.\n",
      " Gradient Descent  |  125\n",
      "\n",
      "7\n",
      "Out-of-core algorithms are discussed in \n",
      " Cha pter 1\n",
      ".\n",
      "Convergence Rate\n",
      " When the cost function is con vex and its slope does not change abruptly (as is the\n",
      " case for the MSE cost function), Ba tch Gradien t Descen t with a fixed learning ra te\n",
      " will even tually con verge to the optimal solution, but you ma y ha ve to wait a while: it\n",
      "can take O(1/\n",
      " ) itera tions to reach the optim um within a range of \n",
      "   depending on the\n",
      " sha pe of the cost function. If you divide the tolerance by 10 to ha ve a more precise\n",
      " solution, then the algorithm ma y ha ve to run about 10 times longer .\n",
      "Stochastic Gradient Descent\n",
      " The main problem with Ba tch Gradien t Descen t is the fact tha t it uses the whole\n",
      " training set to com pute the gradien ts a t ever y step , which makes it ver y slow when\n",
      " the training set is large. A t the opposite extreme, \n",
      " S t o c h as t i c G r a d i en t D e s c en t\n",
      " \n",
      "just\n",
      " picks a random instance in the training set a t ever y step and com putes the gradien ts\n",
      " based only on tha t single instance. Obviously this makes the algorithm m uch faster\n",
      " since it has ver y little da ta to manipula te a t ever y itera tion. I t also makes it possible to\n",
      " train on h uge training sets, since only one instance needs to be in memor y a t each\n",
      " itera tion (SGD can be im plemen ted as an out-of-core algorithm.\n",
      "7\n",
      ")\n",
      " On the other hand, due to its stochastic (i.e., random) na ture, this algorithm is m uch\n",
      " less regular than Ba tch Gradien t Descen t: instead of gen tly decreasing un til it reaches\n",
      " the minim um, the cost function will bounce up and down, decreasing only on a verƒ\n",
      " age. O ver time it will end up ver y close to the minim um, but once it gets there it will\n",
      " con tin ue to bounce around, never settling down (see \n",
      "Figure 4-9\n",
      "). So once the algoƒ\n",
      "rithm stops, the final parameter values are good, but not optimal.\n",
      " F i g u r e 4-9. S t o c h as t i c G r a d i en t D e s c en t\n",
      " 126  |  Chapter 4: Training Models\n",
      "\n",
      " When the cost function is ver y irregular (as in \n",
      "Figure 4-6\n",
      "), this can actually help the\n",
      " algorithm jum p out of local minima, so Stochastic Gradien t Descen t has a better\n",
      " chance of finding the global minim um than Ba tch Gradien t Descen t does.\n",
      " Therefore randomness is good to esca pe from local optima, but bad beca use it means\n",
      " tha t the algorithm can never settle a t the minim um. One solution to this dilemma is\n",
      " to gradually reduce the learning ra te. The steps start out large (which helps make\n",
      " quick progress and esca pe local minima), then get smaller and smaller , allowing the\n",
      " algorithm to settle a t the global minim um. This process is akin to \n",
      " s i m u l a t e d a n n e a l‡\n",
      " i n g\n",
      " , an algorithm inspired from the process of annealing in metallurg y where molten\n",
      " metal is slowly cooled down. The function tha t determines the learning ra te a t each\n",
      " itera tion is called the \n",
      " l e a r n i n g s c h e d u l e\n",
      ". \n",
      " If the learning ra te is reduced too quickly , you\n",
      " ma y get stuck in a local minim um, or even end up frozen half wa y to the minim um. If\n",
      " the learning ra te is reduced too slowly , you ma y jum p around the minim um for a\n",
      " long time and end up with a suboptimal solution if you halt training too early .\n",
      " This code im plemen ts Stochastic Gradien t Descen t using a sim ple learning schedule:\n",
      "n_epochs\n",
      " \n",
      "=\n",
      " \n",
      "50\n",
      "t0\n",
      ",\n",
      " \n",
      "t1\n",
      " \n",
      "=\n",
      " \n",
      "5\n",
      ",\n",
      " \n",
      "50\n",
      "  \n",
      "# learning schedule hyperparameters\n",
      "def\n",
      " \n",
      "learning_schedule\n",
      "(\n",
      "t\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "t0\n",
      " \n",
      "/\n",
      " \n",
      "(\n",
      "t\n",
      " \n",
      "+\n",
      " \n",
      "t1\n",
      ")\n",
      "theta\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randn\n",
      "(\n",
      "2\n",
      ",\n",
      "1\n",
      ")\n",
      "  \n",
      "# random initialization\n",
      "for\n",
      " \n",
      "epoch\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "n_epochs\n",
      "):\n",
      "    \n",
      "for\n",
      " \n",
      "i\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "m\n",
      "):\n",
      "        \n",
      "random_index\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randint\n",
      "(\n",
      "m\n",
      ")\n",
      "        \n",
      "xi\n",
      " \n",
      "=\n",
      " \n",
      "X_b\n",
      "[\n",
      "random_index\n",
      ":\n",
      "random_index\n",
      "+\n",
      "1\n",
      "]\n",
      "        \n",
      "yi\n",
      " \n",
      "=\n",
      " \n",
      "y\n",
      "[\n",
      "random_index\n",
      ":\n",
      "random_index\n",
      "+\n",
      "1\n",
      "]\n",
      "        \n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      " \n",
      "*\n",
      " \n",
      "xi\n",
      ".\n",
      "T\n",
      ".\n",
      "dot\n",
      "(\n",
      "xi\n",
      ".\n",
      "dot\n",
      "(\n",
      "theta\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "yi\n",
      ")\n",
      "        \n",
      "eta\n",
      " \n",
      "=\n",
      " \n",
      "learning_schedule\n",
      "(\n",
      "epoch\n",
      " \n",
      "*\n",
      " \n",
      "m\n",
      " \n",
      "+\n",
      " \n",
      "i\n",
      ")\n",
      "        \n",
      "theta\n",
      " \n",
      "=\n",
      " \n",
      "theta\n",
      " \n",
      "-\n",
      " \n",
      "eta\n",
      " \n",
      "*\n",
      " \n",
      "gradients\n",
      " By con ven tion we itera te by rounds of \n",
      "m\n",
      "  itera tions; each round is called an \n",
      " e p o c h\n",
      ". \n",
      " While the Ba tch Gradien t Descen t code itera ted 1,000 times through the whole trainƒ\n",
      "ing set, this code goes through the training set only 50 times and reaches a fairly good\n",
      "solution:\n",
      ">>> \n",
      "theta\n",
      "array([[4.21076011],\n",
      "       [2.74856079]])\n",
      "Figure 4-10\n",
      " shows the first 20 steps of training (notice how irregular the steps are).\n",
      " Gradient Descent  |  127\n",
      "\n",
      " F i g u r e 4-10. S t o c h as t i c G r a d i en t D e s c en t \n",
      "†rst\n",
      "  20 s t e p s\n",
      " N ote tha t since instances are picked randomly , some instances ma y be picked several\n",
      " times per epoch while others ma y not be picked a t all. If you wan t to be sure tha t the\n",
      " algorithm goes through ever y instance a t each epoch, another a pproach is to sh uffle\n",
      " the training set (making sure to sh uffle the in put fea tures and the labels join tly), then\n",
      " go through it instance by instance, then sh uffle it again, and so on. H owever , this genƒ\n",
      " erally con verges more slowly .\n",
      " When using Stochastic Gradien t Descen t, the training instances\n",
      " m ust be independen t and iden tically distributed (IID), to ensure\n",
      " tha t the parameters get pulled towards the global optim um, on\n",
      " a verage. A sim ple wa y to ensure this is to sh uffle the instances durƒ\n",
      " ing training (e.g., pick each instance randomly , or sh uffle the trainƒ\n",
      " ing set a t the beginning of each epoch). If you do not do this, for\n",
      " exam ple if the instances are sorted by label, then SGD will start by\n",
      "optimizing for one label, then the next, and so on, and it will not\n",
      " settle close to the global minim um.\n",
      " T o perform Linear Regression using SGD with Scikit-Learn, you can use the \n",
      "SGDRe\n",
      "gressor\n",
      "  class, which defa ults to optimizing the squared error cost function. The folƒ\n",
      " lowing code runs for maxim um 1000 epochs (\n",
      "max_iter=1000\n",
      " ) or un til the loss drops\n",
      "by less than 1e-3 during one epoch (\n",
      "tol=1e-3\n",
      " ), starting with a learning ra te of 0.1\n",
      "(\n",
      "eta0=0.1\n",
      " ), using the defa ult learning schedule (differen t from the preceding one),\n",
      " and it does not use an y regulariza tion (\n",
      "penalty=None\n",
      "; more details on this shortly):\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "SGDRegressor\n",
      "sgd_reg\n",
      " \n",
      "=\n",
      " \n",
      "SGDRegressor\n",
      "(\n",
      "max_iter\n",
      "=\n",
      "1000\n",
      ",\n",
      " \n",
      "tol\n",
      "=\n",
      "1e-3\n",
      ",\n",
      " \n",
      "penalty\n",
      "=\n",
      "None\n",
      ",\n",
      " \n",
      "eta0\n",
      "=\n",
      "0.1\n",
      ")\n",
      "sgd_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ".\n",
      "ravel\n",
      "())\n",
      " 128  |  Chapter 4: Training Models\n",
      "\n",
      " Once again, you find a solution quite close to the one returned by the N ormal Equaƒ\n",
      "tion:\n",
      ">>> \n",
      "sgd_reg\n",
      ".\n",
      "intercept_\n",
      ",\n",
      " \n",
      "sgd_reg\n",
      ".\n",
      "coef_\n",
      "(array([4.24365286]), array([2.8250878]))\n",
      "Mini-batch Gradient Descent\n",
      " The last Gradien t Descen t algorithm we will look a t is called \n",
      " M i n i-b a t c h G r a d i en t\n",
      " D e s c en t\n",
      " . I t is quite sim ple to understand once you know Ba tch and Stochastic Gradiƒ\n",
      " en t Descen t: a t each step , instead of com puting the gradien ts based on the full trainƒ\n",
      " ing set (as in Ba tch GD) or based on just one instance (as in Stochastic GD), Mini-\n",
      " ba tch GD com putes the gradien ts on small random sets of instances called \n",
      " m i n i-\n",
      " b a t c h e s\n",
      " . The main advan tage of Mini-ba tch GD over Stochastic GD is tha t you can\n",
      " get a performance boost from hardware optimiza tion of ma trix opera tions, especially\n",
      " when using GPU s.\n",
      " The algorithm ‡ s progress in parameter space is less erra tic than with SGD , especially\n",
      " with fairly large mini-ba tches. As a result, Mini-ba tch GD will end up walking\n",
      " around a bit closer to the minim um than SGD . But, on the other hand, it ma y be\n",
      " harder for it to esca pe from local minima (in the case of problems tha t suffer from\n",
      " local minima, unlike Linear Regression as we sa w earlier). \n",
      "Figure 4-11\n",
      " \n",
      "shows the\n",
      " pa ths taken by the three Gradien t Descen t algorithms in parameter space during\n",
      " training. They all end up near the minim um, but Ba tch GD ‡ s pa th actually stops a t the\n",
      " minim um, while both Stochastic GD and Mini-ba tch GD con tin ue to walk around.\n",
      " H owever , don ‡ t forget tha t Ba tch GD takes a lot of time to take each step , and Stochasƒ\n",
      " tic GD and Mini-ba tch GD would also reach the minim um if you used a good learnƒ\n",
      "ing schedule.\n",
      " F i g u r e 4-11. G r a d i en t D e s c en t p a t hs i n p a r a m e t er s p a c e\n",
      " Gradient Descent  |  129\n",
      "\n",
      "8\n",
      " While the N ormal Equa tion can only perform Linear Regression, the Gradien t Descen t algorithms can be\n",
      " used to train man y other models, as we will see.\n",
      "9\n",
      " A quadra tic equa tion is of the form \n",
      "y\n",
      " = \n",
      "ax\n",
      "2\n",
      " + \n",
      " b x\n",
      " + \n",
      "c\n",
      ".\n",
      " Let ‡ s com pare the algorithms we ‡ ve discussed so far for Linear Regression\n",
      "8\n",
      "   (recall tha t\n",
      "m\n",
      "  is the n umber of training instances and \n",
      "n\n",
      "  is the n umber of fea tures); see \n",
      " T able 4-1\n",
      ".\n",
      " T a b l e 4-1. C o m p a r i s o n o f a l go r i t h ms f o r L i n e a r R e g r e s s i o n\n",
      "Algorithm\n",
      " Large  \n",
      "m\n",
      " Out-of-core   support\n",
      " Large  \n",
      "n\n",
      "Hyperparams\n",
      " Scaling   required\n",
      "Scikit-Learn\n",
      " Normal   Equation\n",
      "Fast\n",
      "No\n",
      "Slow\n",
      "0\n",
      "No\n",
      "n/a\n",
      "SVD\n",
      "Fast\n",
      "No\n",
      "Slow\n",
      "0\n",
      "No\n",
      "LinearRegression\n",
      " Batch   GD\n",
      "Slow\n",
      "No\n",
      "Fast\n",
      "2\n",
      "Yes\n",
      "SGDRegressor\n",
      " Stochastic   GD\n",
      "Fast\n",
      "Yes\n",
      "Fast\n",
      "†2\n",
      "Yes\n",
      "SGDRegressor\n",
      " Mini-batch   GD\n",
      "Fast\n",
      "Yes\n",
      "Fast\n",
      "†2\n",
      "Yes\n",
      "SGDRegressor\n",
      "There is almost no difference after training: all these algorithms\n",
      " end up with ver y similar models and make predictions in exactly \n",
      " the same wa y .\n",
      "Polynomial Regression\n",
      " Wha t if your da ta is actually more com plex than a sim ple straigh t line? Surprisingly ,\n",
      " you can actually use a linear model to fit nonlinear da ta. A sim ple wa y to do this is to\n",
      " add powers of each fea ture as new fea tures, then train a linear model on this extended\n",
      " set of fea tures. This technique is called \n",
      " P o l y n o m i a l R e g r e s s i o n\n",
      ".\n",
      " Let ‡ s look a t an exam ple. First, let ‡ s genera te some nonlinear da ta, based on a sim ple\n",
      " q u a d r a t i c e q u a t i o n\n",
      "9\n",
      " (plus some noise; see \n",
      "Figure 4-12\n",
      "):\n",
      "m\n",
      " \n",
      "=\n",
      " \n",
      "100\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "6\n",
      " \n",
      "*\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "rand\n",
      "(\n",
      "m\n",
      ",\n",
      " \n",
      "1\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "3\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "0.5\n",
      " \n",
      "*\n",
      " \n",
      "X\n",
      "**\n",
      "2\n",
      " \n",
      "+\n",
      " \n",
      "X\n",
      " \n",
      "+\n",
      " \n",
      "2\n",
      " \n",
      "+\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randn\n",
      "(\n",
      "m\n",
      ",\n",
      " \n",
      "1\n",
      ")\n",
      " 130  |  Chapter 4: Training Models\n",
      "\n",
      " F i g u r e 4-12. G en er a t e d n o n l i n e a r a n d n o i s y d a t as e t\n",
      " Clearly , a straigh t line will never fit this da ta properly . So let ‡ s use Scikit-Learn ‡ s \n",
      "Poly\n",
      "nomialFeatures\n",
      "  class to transform our training da ta, adding the square (2\n",
      "nd\n",
      "-degree\n",
      " polynomial) of each fea ture in the training set as new fea tures (in this case there is\n",
      " just one fea ture):\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "PolynomialFeatures\n",
      ">>> \n",
      "poly_features\n",
      " \n",
      "=\n",
      " \n",
      "PolynomialFeatures\n",
      "(\n",
      "degree\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "include_bias\n",
      "=\n",
      "False\n",
      ")\n",
      ">>> \n",
      "X_poly\n",
      " \n",
      "=\n",
      " \n",
      "poly_features\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X\n",
      ")\n",
      ">>> \n",
      "X\n",
      "[\n",
      "0\n",
      "]\n",
      "array([-0.75275929])\n",
      ">>> \n",
      "X_poly\n",
      "[\n",
      "0\n",
      "]\n",
      "array([-0.75275929, 0.56664654])\n",
      "X_poly\n",
      " \n",
      " now con tains the original fea ture of \n",
      "X\n",
      " \n",
      " plus the square of this fea ture. N ow you\n",
      "can fit a \n",
      "LinearRegression\n",
      "  model to this extended training da ta (\n",
      "Figure 4-13\n",
      "):\n",
      ">>> \n",
      "lin_reg\n",
      " \n",
      "=\n",
      " \n",
      "LinearRegression\n",
      "()\n",
      ">>> \n",
      "lin_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_poly\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      ">>> \n",
      "lin_reg\n",
      ".\n",
      "intercept_\n",
      ",\n",
      " \n",
      "lin_reg\n",
      ".\n",
      "coef_\n",
      "(array([1.78134581]), array([[0.93366893, 0.56456263]]))\n",
      " Polynomial Regression  |  131\n",
      "\n",
      " F i g u r e 4-13. P o l y n o m i a l R e g r e s s i o n m o d e l p r e d i c t i o ns\n",
      " N ot bad: the model estima tes \n",
      "y\n",
      " = 0 . 56\n",
      "x\n",
      "1\n",
      "2\n",
      " + 0 . 93\n",
      "x\n",
      "1\n",
      " + 1 . 78\n",
      " when in fact the original\n",
      "function was \n",
      "y\n",
      " = 0 . 5\n",
      "x\n",
      "1\n",
      "2\n",
      " + 1 . 0\n",
      "x\n",
      "1\n",
      " + 2 . 0 + Gaussian noise\n",
      ".\n",
      " N ote tha t when there are m ultiple fea tures, P olynomial Regression is ca pable of findƒ\n",
      " ing rela tionships between fea tures (which is something a plain Linear Regression\n",
      " model cannot do). This is made possible by the fact tha t \n",
      "PolynomialFeatures\n",
      " \n",
      "also\n",
      " adds all combina tions of fea tures up to the given degree. F or exam ple, if there were\n",
      " two fea tures \n",
      "a\n",
      " and \n",
      "b\n",
      ", \n",
      "PolynomialFeatures\n",
      " with \n",
      "degree=3\n",
      " would not only add the\n",
      " fea tures \n",
      "a\n",
      "2\n",
      ", \n",
      "a\n",
      "3\n",
      ", \n",
      "b\n",
      "2\n",
      ", and \n",
      "b\n",
      "3\n",
      " , but also the combina tions \n",
      " a b\n",
      ", \n",
      "a\n",
      "2\n",
      "b\n",
      ", and \n",
      " a b\n",
      "2\n",
      ".\n",
      "PolynomialFeatures(degree=d)\n",
      "  transforms an arra y con taining \n",
      "n\n",
      " fea tures in to an arra y con taining \n",
      "n\n",
      "+\n",
      "d\n",
      "!\n",
      "d\n",
      "!\n",
      "n\n",
      "!\n",
      "  fea tures, where \n",
      "n\n",
      "! is the\n",
      " f a c t o r i a l\n",
      "   of \n",
      "n\n",
      ", equal to 1 „ 2 „ 3 „ \n",
      " „ \n",
      "n\n",
      " . B eware of the combina toƒ\n",
      " rial explosion of the n umber of fea tures!\n",
      "Learning Curves\n",
      " If you perform high-degree P olynomial Regression, you will likely fit the training\n",
      " da ta m uch better than with plain Linear Regression. F or exam ple, \n",
      "Figure 4-14\n",
      "   a pplies\n",
      " a 300-degree polynomial model to the preceding training da ta, and com pares the\n",
      " result with a pure linear model and a quadra tic model (2\n",
      "nd\n",
      "-degree polynomial).\n",
      " N otice how the 300-degree polynomial model wiggles around to get as close as possiƒ\n",
      "ble to the training instances.\n",
      " 132  |  Chapter 4: Training Models\n",
      "\n",
      " F i g u r e 4-14. H i gh-d e g r e e P o l y n o m i a l R e g r e s s i o n\n",
      " Of course, this high-degree P olynomial Regression model is severely overfitting the\n",
      " training da ta, while the linear model is underfitting it. The model tha t will generalize\n",
      " best in this case is the quadra tic model. I t makes sense since the da ta was genera ted\n",
      " using a quadra tic model, but in general you won ‡ t know wha t function genera ted the\n",
      " da ta, so how can you decide how com plex your model should be? H ow can you tell\n",
      " tha t your model is overfitting or underfitting the da ta?\n",
      "In \n",
      " Cha pter 2\n",
      "  you used cross-valida tion to get an estima te of a model ‡ s generaliza tion\n",
      " performance. If a model performs well on the training da ta but generalizes poorly\n",
      " according to the cross-valida tion metrics, then your model is overfitting. If it perƒ\n",
      " forms poorly on both, then it is underfitting. This is one wa y to tell when a model is\n",
      " too sim ple or too com plex.\n",
      " Another wa y is to look a t the \n",
      " l e a r n i n g cu r v e s\n",
      " : these are plots of the model ‡ s perforƒ\n",
      " mance on the training set and the valida tion set as a function of the training set size\n",
      " (or the training itera tion). T o genera te the plots, sim ply train the model several times\n",
      " on differen t sized subsets of the training set. The following code defines a function\n",
      " tha t plots the learning cur ves of a model given some training da ta:\n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "mean_squared_error\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "train_test_split\n",
      "def\n",
      " \n",
      "plot_learning_curves\n",
      "(\n",
      "model\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      "):\n",
      "    \n",
      "X_train\n",
      ",\n",
      " \n",
      "X_val\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "y_val\n",
      " \n",
      "=\n",
      " \n",
      "train_test_split\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ",\n",
      " \n",
      "test_size\n",
      "=\n",
      "0.2\n",
      ")\n",
      "    \n",
      "train_errors\n",
      ",\n",
      " \n",
      "val_errors\n",
      " \n",
      "=\n",
      " \n",
      "[],\n",
      " \n",
      "[]\n",
      "    \n",
      "for\n",
      " \n",
      "m\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "len\n",
      "(\n",
      "X_train\n",
      ")):\n",
      "        \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      "[:\n",
      "m\n",
      "],\n",
      " \n",
      "y_train\n",
      "[:\n",
      "m\n",
      "])\n",
      "        \n",
      "y_train_predict\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_train\n",
      "[:\n",
      "m\n",
      "])\n",
      " Learning Curves  |  133\n",
      "\n",
      "        \n",
      "y_val_predict\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_val\n",
      ")\n",
      "        \n",
      "train_errors\n",
      ".\n",
      "append\n",
      "(\n",
      "mean_squared_error\n",
      "(\n",
      "y_train\n",
      "[:\n",
      "m\n",
      "],\n",
      " \n",
      "y_train_predict\n",
      "))\n",
      "        \n",
      "val_errors\n",
      ".\n",
      "append\n",
      "(\n",
      "mean_squared_error\n",
      "(\n",
      "y_val\n",
      ",\n",
      " \n",
      "y_val_predict\n",
      "))\n",
      "    \n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "train_errors\n",
      "),\n",
      " \n",
      "\"r-+\"\n",
      ",\n",
      " \n",
      "linewidth\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"train\"\n",
      ")\n",
      "    \n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "np\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "val_errors\n",
      "),\n",
      " \n",
      "\"b-\"\n",
      ",\n",
      " \n",
      "linewidth\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"val\"\n",
      ")\n",
      " Let ‡ s look a t the learning cur ves of the plain Linear Regression model (a straigh t line;\n",
      "Figure 4-15\n",
      "):\n",
      "lin_reg\n",
      " \n",
      "=\n",
      " \n",
      "LinearRegression\n",
      "()\n",
      "plot_learning_curves\n",
      "(\n",
      "lin_reg\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " F i g u r e 4-15. L e a r n i n g cu r v e s\n",
      " This deser ves a bit of explana tion. First, let ‡ s look a t the performance on the training\n",
      " da ta: when there are just one or two instances in the training set, the model can fit\n",
      " them perfectly , which is wh y the cur ve starts a t zero . But as new instances are added\n",
      " to the training set, it becomes im possible for the model to fit the training da ta perƒ\n",
      " fectly , both beca use the da ta is noisy and beca use it is not linear a t all. So the error on\n",
      " the training da ta goes up un til it reaches a pla tea u, a t which poin t adding new instanƒ\n",
      " ces to the training set doesn ‡ t make the a verage error m uch better or worse. N ow let ‡ s\n",
      " look a t the performance of the model on the valida tion da ta. When the model is\n",
      " trained on ver y few training instances, it is inca pable of generalizing properly , which\n",
      " is wh y the valida tion error is initially quite big. Then as the model is shown more\n",
      " training exam ples, it learns and th us the valida tion error slowly goes down. H owever ,\n",
      " once again a straigh t line cannot do a good job modeling the da ta, so the error ends\n",
      " up a t a pla tea u, ver y close to the other cur ve.\n",
      " These learning cur ves are typical of an underfitting model. B oth cur ves ha ve reached\n",
      " a pla tea u; they are close and fairly high.\n",
      " 134  |  Chapter 4: Training Models\n",
      "\n",
      " If your model is underfitting the training da ta, adding more trainƒ\n",
      " ing exam ples will not help . Y ou need to use a more com plex model\n",
      " or come up with better fea tures.\n",
      " N ow let ‡ s look a t the learning cur ves of a 10\n",
      "th\n",
      "-degree polynomial model on the same\n",
      " da ta (\n",
      "Figure 4-16\n",
      "):\n",
      "from\n",
      " \n",
      "sklearn.pipeline\n",
      " \n",
      "import\n",
      " \n",
      "Pipeline\n",
      "polynomial_regression\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"poly_features\"\n",
      ",\n",
      " \n",
      "PolynomialFeatures\n",
      "(\n",
      "degree\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "include_bias\n",
      "=\n",
      "False\n",
      ")),\n",
      "        \n",
      "(\n",
      "\"lin_reg\"\n",
      ",\n",
      " \n",
      "LinearRegression\n",
      "()),\n",
      "    \n",
      "])\n",
      "plot_learning_curves\n",
      "(\n",
      "polynomial_regression\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " These learning cur ves look a bit like the previous ones, but there are two ver y im porƒ\n",
      " tan t differences:\n",
      "⁄\n",
      " The error on the training da ta is m uch lower than with the Linear Regression\n",
      "model.\n",
      "⁄\n",
      " There is a ga p between the cur ves. This means tha t the model performs signifiƒ\n",
      " can tly better on the training da ta than on the valida tion da ta, which is the hallƒ\n",
      " mark of an overfitting model. H owever , if you used a m uch larger training set,\n",
      " the two cur ves would con tin ue to get closer .\n",
      " F i g u r e 4-16. L e a r n i n g cu r v e s f o r t h e p o l y n o m i a l m o d e l\n",
      " Learning Curves  |  135\n",
      "\n",
      "10\n",
      "This notion of bias is not to be confused with the bias term of linear models.\n",
      " One wa y to im prove an overfitting model is to feed it more training\n",
      " da ta un til the valida tion error reaches the training error .\n",
      "The Bias/Variance \n",
      "Tradeo…\n",
      " An im portan t theoretical result of sta tistics and M achine Learning is the fact tha t a\n",
      " model ‡ s generaliza tion error can be expressed as the sum of three ver y differen t\n",
      "errors:\n",
      " B i as\n",
      " This part of the generaliza tion error is due to wrong assum ptions, such as assumƒ\n",
      " ing tha t the da ta is linear when it is actually quadra tic. A high-bias model is most\n",
      " likely to underfit the training da ta.\n",
      "10\n",
      " V a r i a n c e\n",
      " This part is due to the model ‡ s excessive sensitivity to small varia tions in the\n",
      " training da ta. A model with man y degrees of freedom (such as a high-degree polƒ\n",
      " ynomial model) is likely to ha ve high variance, and th us to overfit the training\n",
      " da ta.\n",
      " I r r e d u ci b l e er r o r\n",
      " This part is due to the noisiness of the da ta itself. The only wa y to reduce this\n",
      " part of the error is to clean up the da ta (e.g., fix the da ta sources, such as broken\n",
      "sensors, or detect and remove outliers).\n",
      " Increasing a model ‡ s com plexity will typically increase its variance and reduce its bias.\n",
      " Con versely , reducing a model ‡ s com plexity increases its bias and reduces its variance. \n",
      " This is wh y it is called a tradeoff.\n",
      "Regularized Linear Models\n",
      " As we sa w in Cha pters \n",
      "1\n",
      " and \n",
      "2\n",
      " , a good wa y to reduce overfitting is to regularize the\n",
      "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
      " for it to overfit the da ta. F or exam ple, a sim ple wa y to regularize a polynomial model\n",
      " is to reduce the n umber of polynomial degrees.\n",
      " F or a linear model, regulariza tion is typically achieved by constraining the weigh ts of\n",
      " the model. W e will now look a t Ridge Regression, Lasso Regression, and Elastic N et,\n",
      " which im plemen t three differen t wa ys to constrain the weigh ts.\n",
      " 136  |  Chapter 4: Training Models\n",
      "\n",
      "11\n",
      " I t is common to use the nota tion \n",
      "J\n",
      "(\n",
      "•\n",
      " ) for cost functions tha t don ‡ t ha ve a short name; we will often use this\n",
      " nota tion throughout the rest of this book. The con text will make it clear which cost function is being disƒ\n",
      "cussed.\n",
      "12\n",
      " N orms are discussed in \n",
      " Cha pter 2\n",
      ".\n",
      "Ridge Regression\n",
      " R i d ge R e g r e s s i o n\n",
      " (also called \n",
      " T i k h o n o v r e g u l a r iz a t i o n\n",
      ") is a regularized version of Linƒ\n",
      "ear Regression: a \n",
      " r e g u l a r iz a t i o n t er m\n",
      " \n",
      "equal to \n",
      "‰\n",
      "“\n",
      "i\n",
      " = 1\n",
      "n\n",
      "–\n",
      "i\n",
      "2\n",
      " \n",
      "is added to the cost function. \n",
      " This forces the learning algorithm to not only fit the da ta but also keep the model\n",
      " weigh ts as small as possible. N ote tha t the regulariza tion term should only be added\n",
      " to the cost function during training. Once the model is trained, you wan t to evalua te\n",
      " the model ‡ s performance using the unregularized performance measure.\n",
      " I t is quite common for the cost function used during training to be\n",
      " differen t from the performance measure used for testing. A part\n",
      " from regulariza tion, another reason wh y they migh t be differen t is\n",
      " tha t a good training cost function should ha ve optimiza tion-\n",
      " friendly deriva tives, while the performance measure used for testƒ\n",
      "ing should be as close as possible to the final objective. A good\n",
      " exam ple of this is a classifier trained using a cost function such as\n",
      " the log loss (discussed in a momen t) but evalua ted using precision/\n",
      "recall.\n",
      " The h yperparameter \n",
      "‰\n",
      "  con trols how m uch you wan t to regularize the model. If \n",
      "‰\n",
      " = 0\n",
      "then Ridge Regression is just Linear Regression. If \n",
      "‰\n",
      " \n",
      " is ver y large, then all weigh ts end\n",
      " up ver y close to zero and the result is a fla t line going through the da ta ‡ s mean. \n",
      "Equaƒ\n",
      "tion 4-8\n",
      "  presen ts the Ridge Regression cost function.\n",
      "11\n",
      " Eq u a t i o n 4-8. R i d ge R e g r e s s i o n c o s t f u n c t i o n\n",
      "J\n",
      "•\n",
      " = MSE\n",
      "•\n",
      "+\n",
      "‰\n",
      "1\n",
      "2\n",
      "“\n",
      "i\n",
      " = 1\n",
      "n\n",
      "–\n",
      "i\n",
      "2\n",
      " N ote tha t the bias term \n",
      "–\n",
      "0\n",
      "  is not regularized (the sum starts a t \n",
      "i\n",
      " = 1, not 0). If we\n",
      "define \n",
      "w\n",
      "  as the vector of fea ture weigh ts (\n",
      "–\n",
      "1\n",
      " to \n",
      "–\n",
      "n\n",
      " ), then the regulariza tion term is\n",
      " sim ply equal to Š(\n",
      " \n",
      "w\n",
      " \n",
      "2\n",
      ")\n",
      "2\n",
      ", where \n",
      " \n",
      "w\n",
      " \n",
      "2\n",
      "   represen ts the …\n",
      "2\n",
      " \n",
      " norm of the weigh t vector .\n",
      "12\n",
      " F or Gradien t Descen t, just add \n",
      "‰\n",
      "w\n",
      "  to the MSE gradien t vector (\n",
      " Equa tion 4-6\n",
      ").\n",
      " I t is im portan t to scale the da ta (e.g., using a \n",
      "StandardScaler\n",
      ") \n",
      "before performing Ridge Regression, as it is sensitive to the scale of\n",
      " the in put fea tures. This is true of most regularized models.\n",
      " Regularized Linear Models  |  137\n",
      "\n",
      "13\n",
      " A square ma trix full of 0s except for 1s on the main diagonal (top-left to bottom-righ t).\n",
      "Figure 4-17\n",
      " \n",
      " shows several Ridge models trained on some linear da ta using differen t \n",
      "‰\n",
      "value. On the left, plain Ridge models are used, leading to linear predictions. On the\n",
      " righ t, the da ta is first expanded using \n",
      "PolynomialFeatures(degree=10)\n",
      ", then it is\n",
      "scaled using a \n",
      "StandardScaler\n",
      " , and finally the Ridge models are a pplied to the resultƒ\n",
      " ing fea tures: this is P olynomial Regression with Ridge regulariza tion. N ote how\n",
      "increasing \n",
      "‰\n",
      "  leads to fla tter (i.e., less extreme, more reasonable) predictions; this\n",
      " reduces the model ‡ s variance but increases its bias.\n",
      " As with Linear Regression, we can perform Ridge Regression either by com puting a \n",
      " closed-form equa tion or by performing Gradien t Descen t. The pros and cons are the\n",
      "same. \n",
      " Equa tion 4-9\n",
      " shows the closed-form solution (where \n",
      "A\n",
      " is the (\n",
      "n\n",
      " + 1) „ (\n",
      "n\n",
      " \n",
      "+ 1)\n",
      " i d en t i ty m a t r ix\n",
      "13\n",
      " except with a 0 in the top-left cell, corresponding to the bias term).\n",
      " F i g u r e 4-17. R i d ge R e g r e s s i o n\n",
      " Eq u a t i o n 4-9. R i d ge R e g r e s s i o n c l o s e d-f o r m s o l u t i o n\n",
      "•\n",
      "=\n",
      "X\n",
      "T\n",
      "X\n",
      "+\n",
      "‰\n",
      "A\n",
      " ” 1\n",
      "Ł\n",
      "X\n",
      "T\n",
      "Ł\n",
      "y\n",
      " H ere is how to perform Ridge Regression with Scikit-Learn using a closed-form soluƒ\n",
      " tion (a varian t of \n",
      " Equa tion 4-9\n",
      " \n",
      " using a ma trix factoriza tion technique by Andr•-Louis\n",
      "Cholesky):\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "Ridge\n",
      ">>> \n",
      "ridge_reg\n",
      " \n",
      "=\n",
      " \n",
      "Ridge\n",
      "(\n",
      "alpha\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "solver\n",
      "=\n",
      "\"cholesky\"\n",
      ")\n",
      ">>> \n",
      "ridge_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " 138  |  Chapter 4: Training Models\n",
      "\n",
      "14\n",
      " Alterna tively you can use the \n",
      "Ridge\n",
      " class with the \n",
      "\"sag\"\n",
      "  solver . Stochastic A verage GD is a varian t of SGD .\n",
      " F or more details, see the presen ta tion \n",
      " — Minimizing Finite Sums with the Stochastic A verage Gradien t Algoƒ\n",
      " rithm –\n",
      "  by M ark Schmidt et al. from the U niversity of British Columbia.\n",
      ">>> \n",
      "ridge_reg\n",
      ".\n",
      "predict\n",
      "([[\n",
      "1.5\n",
      "]])\n",
      "array([[1.55071465]])\n",
      " And using Stochastic Gradien t Descen t:\n",
      "14\n",
      ">>> \n",
      "sgd_reg\n",
      " \n",
      "=\n",
      " \n",
      "SGDRegressor\n",
      "(\n",
      "penalty\n",
      "=\n",
      "\"l2\"\n",
      ")\n",
      ">>> \n",
      "sgd_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ".\n",
      "ravel\n",
      "())\n",
      ">>> \n",
      "sgd_reg\n",
      ".\n",
      "predict\n",
      "([[\n",
      "1.5\n",
      "]])\n",
      "array([1.47012588])\n",
      "The \n",
      "penalty\n",
      "  h yperparameter sets the type of regulariza tion term to use. Specif ying\n",
      "\"l2\"\n",
      "  indica tes tha t you wan t SGD to add a regulariza tion term to the cost function \n",
      "equal to half the square of the …\n",
      "2\n",
      "  norm of the weigh t vector : this is sim ply \n",
      "Ridge\n",
      "Regression.\n",
      "Lasso Regression\n",
      " L e as t Ab s o l u t e S h r i n k a ge a n d S e l e c t i o n O p er a t o r R e g r e s s i o n\n",
      "  (sim ply called \n",
      " L as s o\n",
      " R e g r e s s i o n\n",
      ") is another regularized version of Linear Regression: just like Ridge\n",
      " Regression, it adds a regulariza tion term to the cost function, but it uses the …\n",
      "1\n",
      " \n",
      "norm\n",
      " of the weigh t vector instead of half the square of the …\n",
      "2\n",
      " norm (see \n",
      " Equa tion 4-10\n",
      ").\n",
      " Eq u a t i o n 4-10. L as s o R e g r e s s i o n c o s t f u n c t i o n\n",
      "J\n",
      "•\n",
      " = MSE\n",
      "•\n",
      "+\n",
      "‰\n",
      "“\n",
      "i\n",
      " = 1\n",
      "n\n",
      "–\n",
      "i\n",
      "Figure 4-18\n",
      " shows the same thing as \n",
      "Figure 4-17\n",
      " but replaces Ridge models with\n",
      "Lasso models and uses smaller \n",
      "‰\n",
      " values.\n",
      " Regularized Linear Models  |  139\n",
      "\n",
      " F i g u r e 4-18. L as s o R e g r e s s i o n\n",
      " An im portan t characteristic of Lasso Regression is tha t it tends to com pletely elimiƒ\n",
      " na te the weigh ts of the least im portan t fea tures (i.e., set them to zero). F or exam ple,\n",
      " the dashed line in the righ t plot on \n",
      "Figure 4-18\n",
      "   (with \n",
      "‰\n",
      "   = 10\n",
      "-7\n",
      " ) looks quadra tic, almost\n",
      " linear : all the weigh ts for the high-degree polynomial fea tures are equal to zero . In\n",
      " other words, Lasso Regression a utoma tically performs fea ture selection and outputs \n",
      "a\n",
      " s p a r s e m o d e l\n",
      "  (i.e., with few nonzero fea ture weigh ts).\n",
      " Y ou can get a sense of wh y this is the case by looking a t \n",
      "Figure 4-19\n",
      ": on the top-left\n",
      " plot, the background con tours (ellipses) represen t an unregularized MSE cost funcƒ\n",
      "tion (\n",
      "‰\n",
      "  = 0), and the white circles show the Ba tch Gradien t Descen t pa th with tha t\n",
      " cost function. The foreground con tours (diamonds) represen t the …\n",
      "1\n",
      " \n",
      " penalty , and the\n",
      " triangles show the B GD pa th for this penalty only (\n",
      "‰\n",
      "   Ÿ  ‚\n",
      " ). N otice how the pa th first\n",
      "reaches \n",
      "–\n",
      "1\n",
      "  = 0, then rolls down a gutter un til it reaches \n",
      "–\n",
      "2\n",
      "  = 0. On the top-righ t plot,\n",
      " the con tours represen t the same cost function plus an …\n",
      "1\n",
      " penalty with \n",
      "‰\n",
      " = 0.5. The\n",
      " global minim um is on the \n",
      "–\n",
      "2\n",
      "  = 0 axis. B GD first reaches \n",
      "–\n",
      "2\n",
      " \n",
      "= 0, then rolls down the\n",
      " gutter un til it reaches the global minim um. The two bottom plots show the same\n",
      "thing but uses an …\n",
      "2\n",
      " \n",
      " penalty instead. The regularized minim um is closer to \n",
      "•\n",
      "   = \n",
      "0\n",
      "   than\n",
      " the unregularized minim um, but the weigh ts do not get fully elimina ted.\n",
      " 140  |  Chapter 4: Training Models\n",
      "\n",
      "15\n",
      " Y ou can think of a subgradien t vector a t a nondifferen tiable poin t as an in termedia te vector between the graƒ\n",
      " dien t vectors around tha t poin t.\n",
      " F i g u r e 4-19. L as s o v er s u s R i d ge r e g u l a r iz a t i o n\n",
      " On the Lasso cost function, the B GD pa th tends to bounce across\n",
      " the gutter toward the end. This is beca use the slope changes\n",
      " abruptly a t \n",
      "–\n",
      "2\n",
      " \n",
      " = 0. Y ou need to gradually reduce the learning ra te in\n",
      " order to actually con verge to the global minim um.\n",
      " The Lasso cost function is not differen tiable a t \n",
      "–\n",
      "i\n",
      " \n",
      "= 0 (for \n",
      "i\n",
      " \n",
      "= 1, 2, \n",
      ", \n",
      "n\n",
      " ), but Gradien t\n",
      " Descen t still works fine if you use a \n",
      " s u bg r a d i en t v e c t o r\n",
      " \n",
      "g\n",
      "15\n",
      "  instead when an y \n",
      "–\n",
      "i\n",
      " \n",
      "= 0.\n",
      " Equa tion 4-11\n",
      " \n",
      " shows a subgradien t vector equa tion you can use for Gradien t Descen t\n",
      "with the Lasso cost function.\n",
      " Eq u a t i o n 4-11. L as s o R e g r e s s i o n s u bg r a d i en t v e c t o r\n",
      "g\n",
      "•\n",
      ",\n",
      "J\n",
      "=\n",
      "•\n",
      "MSE\n",
      "•\n",
      "+\n",
      "‰\n",
      "sign\n",
      "–\n",
      "1\n",
      "sign\n",
      "–\n",
      "2\n",
      "sign\n",
      "–\n",
      "n\n",
      " ŁŁ whereŁ sign\n",
      "–\n",
      "i\n",
      "=\n",
      " ” 1 ifŁ\n",
      "–\n",
      "i\n",
      " < 0\n",
      " 0 ifŁ\n",
      "–\n",
      "i\n",
      " = 0\n",
      " + 1 ifŁ\n",
      "–\n",
      "i\n",
      " > 0\n",
      " Regularized Linear Models  |  141\n",
      "\n",
      " H ere is a small Scikit-Learn exam ple using the \n",
      "Lasso\n",
      "  class. N ote tha t you could\n",
      "instead use an \n",
      "SGDRegressor(penalty=\"l1\")\n",
      ".\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "Lasso\n",
      ">>> \n",
      "lasso_reg\n",
      " \n",
      "=\n",
      " \n",
      "Lasso\n",
      "(\n",
      "alpha\n",
      "=\n",
      "0.1\n",
      ")\n",
      ">>> \n",
      "lasso_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      ">>> \n",
      "lasso_reg\n",
      ".\n",
      "predict\n",
      "([[\n",
      "1.5\n",
      "]])\n",
      "array([1.53788174])\n",
      "Elastic Net\n",
      " Elastic N et is a middle ground between Ridge Regression and Lasso Regression. The\n",
      " regulariza tion term is a sim ple mix of both Ridge and Lasso ‡ s regulariza tion terms,\n",
      " and you can con trol the mix ra tio \n",
      "r\n",
      ". When \n",
      "r\n",
      "  = 0, Elastic N et is equivalen t to Ridge\n",
      "Regression, and when \n",
      "r\n",
      "  = 1, it is equivalen t to Lasso Regression (see \n",
      " Equa tion 4-12\n",
      ").\n",
      " Eq u a t i o n 4-12. E l as t i c N e t c o s t f u n c t i o n\n",
      "J\n",
      "•\n",
      " = MSE\n",
      "•\n",
      "+\n",
      " r ‰\n",
      "“\n",
      "i\n",
      " = 1\n",
      "n\n",
      "–\n",
      "i\n",
      "+\n",
      " 1 ”\n",
      "r\n",
      "2\n",
      "‰\n",
      "“\n",
      "i\n",
      " = 1\n",
      "n\n",
      "–\n",
      "i\n",
      "2\n",
      " So when should you use plain Linear Regression (i.e., without an y regulariza tion),\n",
      " Ridge, Lasso , or Elastic N et? I t is almost alwa ys preferable to ha ve a t least a little bit of\n",
      " regulariza tion, so generally you should a void plain Linear Regression. Ridge is a good\n",
      " defa ult, but if you suspect tha t only a few fea tures are actually useful, you should preƒ\n",
      " fer Lasso or Elastic N et since they tend to reduce the useless fea tures ‡ weigh ts down to\n",
      " zero as we ha ve discussed. In general, Elastic N et is preferred over Lasso since Lasso\n",
      " ma y beha ve erra tically when the n umber of fea tures is grea ter than the n umber of\n",
      " training instances or when several fea tures are strongly correla ted.\n",
      " H ere is a short exam ple using Scikit-Learn ‡ s \n",
      "ElasticNet\n",
      " \n",
      "(\n",
      "l1_ratio\n",
      " \n",
      "corresponds to\n",
      " the mix ra tio \n",
      "r\n",
      "):\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "ElasticNet\n",
      ">>> \n",
      "elastic_net\n",
      " \n",
      "=\n",
      " \n",
      "ElasticNet\n",
      "(\n",
      "alpha\n",
      "=\n",
      "0.1\n",
      ",\n",
      " \n",
      "l1_ratio\n",
      "=\n",
      "0.5\n",
      ")\n",
      ">>> \n",
      "elastic_net\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      ">>> \n",
      "elastic_net\n",
      ".\n",
      "predict\n",
      "([[\n",
      "1.5\n",
      "]])\n",
      "array([1.54333232])\n",
      "Early Stopping\n",
      " A ver y differen t wa y to regularize itera tive learning algorithms such as Gradien t\n",
      " Descen t is to stop training as soon as the valida tion error reaches a minim um. This is\n",
      "called \n",
      " e a r l y s t o p p i n g\n",
      ". \n",
      "Figure 4-20\n",
      "  shows a com plex model (in this case a high-degree\n",
      " P olynomial Regression model) being trained using Ba tch Gradien t Descen t. As the\n",
      " epochs go by , the algorithm learns and its prediction error (RMSE) on the training set\n",
      " na turally goes down, and so does its prediction error on the valida tion set. H owever ,\n",
      " 142  |  Chapter 4: Training Models\n",
      "\n",
      " after a while the valida tion error stops decreasing and actually starts to go back up .\n",
      " This indica tes tha t the model has started to overfit the training da ta. W ith early stopƒ\n",
      " ping you just stop training as soon as the valida tion error reaches the minim um. I t is\n",
      " such a sim ple and efficien t regulariza tion technique tha t Geoffrey Hin ton called it a\n",
      " —bea utiful free lunch. –\n",
      " F i g u r e 4-20. Ea r l y s t o p p i n g r e g u l a r iz a t i o n\n",
      " W ith Stochastic and Mini-ba tch Gradien t Descen t, the cur ves are\n",
      " not so smooth, and it ma y be hard to know whether you ha ve\n",
      " reached the minim um or not. One solution is to stop only after the\n",
      " valida tion error has been above the minim um for some time (when\n",
      " you are confiden t tha t the model will not do an y better), then roll\n",
      " back the model parameters to the poin t where the valida tion error\n",
      " was a t a minim um.\n",
      " H ere is a basic im plemen ta tion of early stopping:\n",
      "from\n",
      " \n",
      "sklearn.base\n",
      " \n",
      "import\n",
      " \n",
      "clone\n",
      "# prepare the data\n",
      "poly_scaler\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"poly_features\"\n",
      ",\n",
      " \n",
      "PolynomialFeatures\n",
      "(\n",
      "degree\n",
      "=\n",
      "90\n",
      ",\n",
      " \n",
      "include_bias\n",
      "=\n",
      "False\n",
      ")),\n",
      "        \n",
      "(\n",
      "\"std_scaler\"\n",
      ",\n",
      " \n",
      "StandardScaler\n",
      "())\n",
      "    \n",
      "])\n",
      "X_train_poly_scaled\n",
      " \n",
      "=\n",
      " \n",
      "poly_scaler\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      "X_val_poly_scaled\n",
      " \n",
      "=\n",
      " \n",
      "poly_scaler\n",
      ".\n",
      "transform\n",
      "(\n",
      "X_val\n",
      ")\n",
      "sgd_reg\n",
      " \n",
      "=\n",
      " \n",
      "SGDRegressor\n",
      "(\n",
      "max_iter\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "tol\n",
      "=-\n",
      "np\n",
      ".\n",
      "infty\n",
      ",\n",
      " \n",
      "warm_start\n",
      "=\n",
      "True\n",
      ",\n",
      "                       \n",
      "penalty\n",
      "=\n",
      "None\n",
      ",\n",
      " \n",
      "learning_rate\n",
      "=\n",
      "\"constant\"\n",
      ",\n",
      " \n",
      "eta0\n",
      "=\n",
      "0.0005\n",
      ")\n",
      " Regularized Linear Models  |  143\n",
      "\n",
      "minimum_val_error\n",
      " \n",
      "=\n",
      " \n",
      "float\n",
      "(\n",
      "\"inf\"\n",
      ")\n",
      "best_epoch\n",
      " \n",
      "=\n",
      " \n",
      "None\n",
      "best_model\n",
      " \n",
      "=\n",
      " \n",
      "None\n",
      "for\n",
      " \n",
      "epoch\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "1000\n",
      "):\n",
      "    \n",
      "sgd_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_poly_scaled\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "  \n",
      "# continues where it left off\n",
      "    \n",
      "y_val_predict\n",
      " \n",
      "=\n",
      " \n",
      "sgd_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_val_poly_scaled\n",
      ")\n",
      "    \n",
      "val_error\n",
      " \n",
      "=\n",
      " \n",
      "mean_squared_error\n",
      "(\n",
      "y_val\n",
      ",\n",
      " \n",
      "y_val_predict\n",
      ")\n",
      "    \n",
      "if\n",
      " \n",
      "val_error\n",
      " \n",
      "<\n",
      " \n",
      "minimum_val_error\n",
      ":\n",
      "        \n",
      "minimum_val_error\n",
      " \n",
      "=\n",
      " \n",
      "val_error\n",
      "        \n",
      "best_epoch\n",
      " \n",
      "=\n",
      " \n",
      "epoch\n",
      "        \n",
      "best_model\n",
      " \n",
      "=\n",
      " \n",
      "clone\n",
      "(\n",
      "sgd_reg\n",
      ")\n",
      " N ote tha t with \n",
      "warm_start=True\n",
      ", when the \n",
      "fit()\n",
      "  method is called, it just con tin ues\n",
      " training where it left off instead of restarting from scra tch.\n",
      "Logistic Regression\n",
      "As we discussed in \n",
      " Cha pter 1\n",
      ", some regression algorithms can be used for classificaƒ\n",
      "tion as well (and vice versa). \n",
      " L og i s t i c R e g r e s s i o n\n",
      " (also called \n",
      " L og i t R e g r e s s i o n\n",
      ") is comƒ\n",
      " monly used to estima te the probability tha t an instance belongs to a particular class\n",
      " (e.g., wha t is the probability tha t this email is spam?). If the estima ted probability is\n",
      " grea ter than 50%, then the model predicts tha t the instance belongs to tha t class\n",
      " (called the positive class, labeled —1–), or else it predicts tha t it does not (i.e., it\n",
      " belongs to the nega tive class, labeled —0–). This makes it a binar y classifier .\n",
      "Estimating Probabilities\n",
      " So how does it work? J ust like a Linear Regression model, a Logistic Regression\n",
      " model com putes a weigh ted sum of the in put fea tures (plus a bias term), but instead\n",
      "of outputting the result directly like the Linear Regression model does, it outputs the\n",
      " l og i s t i c\n",
      " of this result (see \n",
      " Equa tion 4-13\n",
      ").\n",
      " Eq u a t i o n 4-13. L og i s t i c R e g r e s s i o n m o d e l e s t i m a t e d p r o b a b i l i ty (v e c t o r iz e d f o r m)\n",
      "p\n",
      "=\n",
      "h\n",
      "•\n",
      "x\n",
      "=\n",
      "„\n",
      "x\n",
      "T\n",
      "•\n",
      "The logistic›noted \n",
      "„\n",
      "(’)›is a \n",
      " s i g m o i d f u n c t i o n\n",
      " (i.e., \n",
      "S\n",
      " -sha ped) tha t outputs \n",
      " a n umber\n",
      " between 0 and 1. I t is defined as shown in \n",
      " Equa tion 4-14\n",
      " and \n",
      "Figure 4-21\n",
      ".\n",
      " Eq u a t i o n 4-14. L og i s t i c f u n c t i o n\n",
      "„\n",
      "t\n",
      "=\n",
      "1\n",
      " 1 + exp\n",
      "”\n",
      "t\n",
      " 144  |  Chapter 4: Training Models\n",
      "\n",
      " F i g u r e 4-21. L og i s t i c f u n c t i o n\n",
      " Once the Logistic Regression model has estima ted the probability \n",
      "p\n",
      " = \n",
      "h\n",
      "•\n",
      "(\n",
      "x\n",
      " ) tha t an\n",
      "instance \n",
      "x\n",
      " belongs to the positive class, it can make its prediction \n",
      "ƒ\n",
      " easily (see \n",
      "Equaƒ\n",
      "tion 4-15\n",
      ").\n",
      " Eq u a t i o n 4-15. L og i s t i c R e g r e s s i o n m o d e l p r e d i c t i o n\n",
      "y\n",
      "=\n",
      " 0 if\n",
      "p\n",
      " < 0 . 5\n",
      " 1 if\n",
      "p\n",
      " Ž 0 . 5\n",
      " N otice tha t \n",
      "„\n",
      "(\n",
      "t\n",
      ") < 0.5 when \n",
      "t\n",
      " < 0, and \n",
      "„\n",
      "(\n",
      "t\n",
      ") \n",
      "Ž\n",
      " 0.5 when \n",
      "t\n",
      " \n",
      "Ž\n",
      " 0, so a Logistic Regression\n",
      "model predicts 1 if \n",
      "x\n",
      "T\n",
      " \n",
      "•\n",
      "  is positive, and 0 if it is nega tive.\n",
      "The score \n",
      "t\n",
      " is often called the \n",
      " l og i t\n",
      ": this name comes from the fact\n",
      " tha t the logit function, defined as logit(\n",
      "p\n",
      ") = log(\n",
      "p\n",
      " / (1 - \n",
      "p\n",
      ")), is the\n",
      " in verse of the logistic function. Indeed, if you com pute the logit of\n",
      " the estima ted probability \n",
      "p\n",
      " , you will find tha t the result is \n",
      "t\n",
      ". The\n",
      "logit is also called the \n",
      " l og-o d d s\n",
      " , since it is the log of the ra tio\n",
      " between the estima ted probability for the positive class and the\n",
      " estima ted probability for the nega tive class.\n",
      "Training and Cost Function\n",
      " Good, now you know how a Logistic Regression model estima tes probabilities and\n",
      " makes predictions. But how is it trained? The objective of training is to set the  paramƒ\n",
      "eter vector \n",
      "•\n",
      " \n",
      " so tha t the model estima tes high probabilities for positive instances (\n",
      "y\n",
      "   =\n",
      " 1) and low probabilities for nega tive instances (\n",
      "y\n",
      "  = 0). This idea is ca ptured by the\n",
      "cost function shown in \n",
      " Equa tion 4-16\n",
      " for a single training instance \n",
      "x\n",
      ".\n",
      " Eq u a t i o n 4-16. C o s t f u n c t i o n o f a s i n gl e t r a i n i n g i ns t a n c e\n",
      "c\n",
      "•\n",
      "=\n",
      " ” log\n",
      "p\n",
      "ifŁ\n",
      "y\n",
      " = 1\n",
      " ” log\n",
      " 1 ”\n",
      "p\n",
      "ifŁ\n",
      "y\n",
      " = 0\n",
      " Logistic Regression  |  145\n",
      "\n",
      " This cost function makes sense beca use − log(\n",
      "t\n",
      " ) grows ver y large when \n",
      "t\n",
      " \n",
      " a pproaches\n",
      " 0, so the cost will be large if the model estima tes a probability close to 0 for a positive\n",
      " instance, and it will also be ver y large if the model estima tes a probability close to 1\n",
      " for a nega tive instance. On the other hand, − log(\n",
      "t\n",
      ") is close to 0 when \n",
      "t\n",
      "   is close to 1, so\n",
      " the cost will be close to 0 if the estima ted probability is close to 0 for a nega tive\n",
      " instance or close to 1 for a positive instance, which is precisely wha t we wan t.\n",
      " The cost function over the whole training set is sim ply the a verage cost over all trainƒ\n",
      " ing instances. I t can be written in a single expression (as you can verif y easily), called \n",
      "the \n",
      " l og l o s s\n",
      ", shown in \n",
      " Equa tion 4-17\n",
      ".\n",
      " Eq u a t i o n 4-17. L og i s t i c R e g r e s s i o n c o s t f u n c t i o n (l og l o s s)\n",
      "J\n",
      "•\n",
      " = ”\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "y\n",
      "i\n",
      " l o g\n",
      "p\n",
      "i\n",
      "+\n",
      " 1 ”\n",
      "y\n",
      "i\n",
      " l o g\n",
      " 1 ”\n",
      "p\n",
      "i\n",
      " The bad news is tha t there is no known closed-form equa tion to com pute the value of\n",
      "•\n",
      "  tha t minimizes this cost function (there is no equivalen t of the N ormal Equa tion).\n",
      " But the good news is tha t this cost function is con vex, so Gradien t Descen t (or an y\n",
      " other optimiza tion algorithm) is guaran teed to find the global minim um (if the learnƒ\n",
      " ing ra te is not too large and you wait long enough). The partial deriva tives of the cost\n",
      "function with regards to the j\n",
      "th\n",
      " model parameter \n",
      "–\n",
      "j\n",
      " is given by \n",
      " Equa tion 4-18\n",
      ".\n",
      " Eq u a t i o n 4-18. L og i s t i c c o s t f u n c t i o n p a r t i a l d er i v a t i v e s\n",
      "Œ\n",
      "Œ\n",
      "–\n",
      "j\n",
      "J\n",
      "•\n",
      "=\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "„\n",
      "•\n",
      "T\n",
      "x\n",
      "i\n",
      "”\n",
      "y\n",
      "i\n",
      "x\n",
      "j\n",
      "i\n",
      " This equa tion looks ver y m uch like \n",
      " Equa tion 4-5\n",
      " : for each instance it com putes the\n",
      " prediction error and m ultiplies it by the j\n",
      "th\n",
      "  fea ture value, and then it com putes the\n",
      " a verage over all training instances. Once you ha ve the gradien t vector con taining all\n",
      " the partial deriva tives you can use it in the Ba tch Gradien t Descen t algorithm. Tha t ‡ s\n",
      " it: you now know how to train a Logistic Regression model. F or Stochastic GD you\n",
      " would of course just take one instance a t a time, and for Mini-ba tch GD you would\n",
      " use a mini-ba tch a t a time.\n",
      "Decision Boundaries\n",
      " Let ‡ s use the iris da taset to illustra te Logistic Regression. This is a famous da taset tha t\n",
      " con tains the sepal and petal length and width of 150 iris flowers of three differen t\n",
      " species: Iris-Setosa, Iris-V ersicolor , and Iris-V irginica (see \n",
      "Figure 4-22\n",
      ").\n",
      " 146  |  Chapter 4: Training Models\n",
      "\n",
      "16\n",
      " Photos reproduced from the corresponding W ikipedia pages. Iris-V irginica photo by Frank M a yfield (\n",
      "Creaƒ\n",
      " tive Commons BY -SA 2.0\n",
      " ), Iris-V ersicolor photo by D . Gordon E. Robertson (\n",
      " Crea tive Commons BY -SA 3.0\n",
      "),\n",
      "and Iris-Setosa photo is public domain.\n",
      "17\n",
      " N umPy‡ s \n",
      "reshape()\n",
      "  function allows one dimension to be −1, which means — unspecified –: the value is inferred\n",
      " from the length of the arra y and the remaining dimensions.\n",
      " F i g u r e 4-22. F l o w er s o f t h r e e i r i s p l a n t s p e ci e s\n",
      "16\n",
      " Let ‡ s tr y to build a classifier to detect the Iris-V irginica type based only on the petal\n",
      " width fea ture. First let ‡ s load the da ta:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn\n",
      " \n",
      "import\n",
      " \n",
      "datasets\n",
      ">>> \n",
      "iris\n",
      " \n",
      "=\n",
      " \n",
      "datasets\n",
      ".\n",
      "load_iris\n",
      "()\n",
      ">>> \n",
      "list\n",
      "(\n",
      "iris\n",
      ".\n",
      "keys\n",
      "())\n",
      "[•data•, •target•, •target_names•, •DESCR•, •feature_names•, •filename•]\n",
      ">>> \n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      "[\n",
      "\"data\"\n",
      "][:,\n",
      " \n",
      "3\n",
      ":]\n",
      "  \n",
      "# petal width\n",
      ">>> \n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "iris\n",
      "[\n",
      "\"target\"\n",
      "]\n",
      " \n",
      "==\n",
      " \n",
      "2\n",
      ")\n",
      ".\n",
      "astype\n",
      "(\n",
      "np\n",
      ".\n",
      "int\n",
      ")\n",
      "  \n",
      "# 1 if Iris-Virginica, else 0\n",
      " N ow let ‡ s train a Logistic Regression model:\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "LogisticRegression\n",
      "log_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "()\n",
      "log_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " Let ‡ s look a t the model ‡ s estima ted probabilities for flowers with petal widths var ying\n",
      "from 0 to 3 cm (\n",
      "Figure 4-23\n",
      ")\n",
      "17\n",
      ":\n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "linspace\n",
      "(\n",
      "0\n",
      ",\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "1000\n",
      ")\n",
      ".\n",
      "reshape\n",
      "(\n",
      "-\n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ")\n",
      "y_proba\n",
      " \n",
      "=\n",
      " \n",
      "log_reg\n",
      ".\n",
      "predict_proba\n",
      "(\n",
      "X_new\n",
      ")\n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "X_new\n",
      ",\n",
      " \n",
      "y_proba\n",
      "[:,\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "\"g-\"\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"Iris-Virginica\"\n",
      ")\n",
      " Logistic Regression  |  147\n",
      "\n",
      "18\n",
      " I t is the the set of poin ts \n",
      "x\n",
      "  such tha t \n",
      "–\n",
      "0\n",
      " + \n",
      "–\n",
      "1\n",
      "x\n",
      "1\n",
      " + \n",
      "–\n",
      "2\n",
      "x\n",
      "2\n",
      "  = 0, which defines a straigh t line.\n",
      "plt\n",
      ".\n",
      "plot\n",
      "(\n",
      "X_new\n",
      ",\n",
      " \n",
      "y_proba\n",
      "[:,\n",
      " \n",
      "0\n",
      "],\n",
      " \n",
      "\"b--\"\n",
      ",\n",
      " \n",
      "label\n",
      "=\n",
      "\"Not Iris-Virginica\"\n",
      ")\n",
      "# + more Matplotlib code to make the image look pretty\n",
      " F i g u r e 4-23. E s t i m a t e d p r o b a b i l i t i e s a n d d e ci s i o n b o u n d a r y\n",
      " The petal width of Iris-V irginica flowers (represen ted by triangles) ranges from 1.4\n",
      " cm to 2.5 cm, while the other iris flowers (represen ted by squares) generally ha ve a\n",
      " smaller petal width, ranging from 0.1 cm to 1.8 cm. N otice tha t there is a bit of overƒ\n",
      " la p . A bove about 2 cm the classifier is highly confiden t tha t the flower is an Iris-\n",
      " V irginica (it outputs a high probability to tha t class), while below 1 cm it is highly\n",
      " confiden t tha t it is not an Iris-V irginica (high probability for the —N ot Iris-V irginica –\n",
      " class). In between these extremes, the classifier is unsure. H owever , if you ask it to\n",
      "predict the class (using the \n",
      "predict()\n",
      "  method ra ther than the \n",
      "predict_proba()\n",
      " method), it will return whichever class is the most likely . Therefore, there is a \n",
      " d e ci s i o n\n",
      " b o u n d a r y\n",
      "  a t around 1.6 cm where both probabilities are equal to 50%: if the petal\n",
      " width is higher than 1.6 cm, the classifier will predict tha t the flower is an Iris-\n",
      " V irginica, or else it will predict tha t it is not (even if it is not ver y confiden t):\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "predict\n",
      "([[\n",
      "1.7\n",
      "],\n",
      " \n",
      "[\n",
      "1.5\n",
      "]])\n",
      "array([1, 0])\n",
      "Figure 4-24\n",
      "  shows the same da taset but this time displa ying two fea tures: petal width\n",
      " and length. Once trained, the Logistic Regression classifier can estima te the probabilƒ\n",
      " ity tha t a new flower is an Iris-V irginica based on these two fea tures. The dashed line\n",
      " represen ts the poin ts where the model estima tes a 50% probability : this is the model ‡ s\n",
      " decision boundar y . N ote tha t it is a linear boundar y .\n",
      "18\n",
      "   Each parallel line represen ts the\n",
      " poin ts where the model outputs a specific probability , from 15% (bottom left) to 90%\n",
      " (top righ t). All the flowers beyond the top-righ t line ha ve an over 90% chance of\n",
      " being Iris-V irginica according to the model.\n",
      " 148  |  Chapter 4: Training Models\n",
      "\n",
      " F i g u r e 4-24. L i n e a r d e ci s i o n b o u n d a r y\n",
      " J ust like the other linear models, Logistic Regression models can be regularized using \n",
      "…\n",
      "1\n",
      " or …\n",
      "2\n",
      " penalties. Scitkit-Learn actually adds an …\n",
      "2\n",
      "  penalty by defa ult.\n",
      " The h yperparameter con trolling the regulariza tion strength of a\n",
      "Scikit-Learn \n",
      "LogisticRegression\n",
      " model is not \n",
      "alpha\n",
      " (as in other\n",
      " linear models), but its in verse: \n",
      "C\n",
      ". The higher the value of \n",
      "C\n",
      ", the \n",
      " l e s s\n",
      "the model is regularized.\n",
      "Softmax Regression\n",
      "The \n",
      " Logistic Regression model can be generalized to support m ultiple classes directly ,\n",
      " without ha ving to train and combine m ultiple binar y classifiers (as discussed in\n",
      " Cha pter 3\n",
      "). This is called \n",
      "So“max\n",
      "  R e g r e s s i o n\n",
      ", or \n",
      " M u l t i n o m i a l L og i s t i c R e g r e s s i o n\n",
      ".\n",
      " The idea is quite sim ple: when given an instance \n",
      "x\n",
      ", the Softmax Regression model\n",
      " first com putes a score \n",
      "s\n",
      "k\n",
      "(\n",
      "x\n",
      ") for each class \n",
      "k\n",
      " , then estima tes the probability of each\n",
      " class by a pplying the \n",
      "so“max\n",
      "  f u n c t i o n\n",
      " (also called the \n",
      " n o r m a l iz e d exp o n en t i a l\n",
      ") to the\n",
      " scores. The equa tion to com pute \n",
      "s\n",
      "k\n",
      "(\n",
      "x\n",
      " ) should look familiar , as it is just like the equaƒ\n",
      "tion for Linear Regression prediction (see \n",
      " Equa tion 4-19\n",
      ").\n",
      " Eq u a t i o n 4-19. \n",
      "So“max\n",
      "  s c o r e f o r c l as s k\n",
      "s\n",
      "k\n",
      "x\n",
      "=\n",
      "x\n",
      "T\n",
      "•\n",
      "k\n",
      " N ote tha t each class has its own dedica ted parameter vector \n",
      "•\n",
      "(k)\n",
      ". All these vectors are\n",
      "typically stored as rows in a \n",
      " p a r a m e t er m a t r ix\n",
      " \n",
      "‡\n",
      ".\n",
      " Once you ha ve com puted the score of ever y class for the instance \n",
      "x\n",
      " , you can estima te\n",
      "the probability \n",
      "p\n",
      "k\n",
      "  tha t the instance belongs to class \n",
      "k\n",
      " by running the scores through\n",
      "the softmax function (\n",
      " Equa tion 4-20\n",
      " ): it com putes the exponen tial of ever y score,\n",
      " Logistic Regression  |  149\n",
      "\n",
      " then normalizes them (dividing by the sum of all the exponen tials). The scores are\n",
      "generally called logits or log-odds (although they are actually unnormalized log-\n",
      "odds).\n",
      " Eq u a t i o n 4-20. \n",
      "So“max\n",
      "  f u n c t i o n\n",
      "p\n",
      "k\n",
      "=\n",
      "„\n",
      "s\n",
      "x\n",
      "k\n",
      "=\n",
      "exp\n",
      "s\n",
      "k\n",
      "x\n",
      "“\n",
      "j\n",
      " = 1\n",
      "K\n",
      "exp\n",
      "s\n",
      "j\n",
      "x\n",
      "⁄\n",
      "K\n",
      "  is the n umber of classes.\n",
      "⁄\n",
      "s\n",
      "(\n",
      "x\n",
      " ) is a vector con taining the scores of each class for the instance \n",
      "x\n",
      ".\n",
      "⁄\n",
      "„\n",
      "(\n",
      "s\n",
      "(\n",
      "x\n",
      "))\n",
      "k\n",
      "  is the estima ted probability tha t the instance \n",
      "x\n",
      " belongs to class \n",
      "k\n",
      " \n",
      "given\n",
      " the scores of each class for tha t instance.\n",
      " J ust like the Logistic Regression classifier , the Softmax Regression classifier predicts\n",
      " the class with the highest estima ted probability (which is sim ply the class with the\n",
      "highest score), as shown in \n",
      " Equa tion 4-21\n",
      ".\n",
      " Eq u a t i o n 4-21. \n",
      "So“max\n",
      "  R e g r e s s i o n \n",
      "classi†er\n",
      "  p r e d i c t i o n\n",
      "y\n",
      " = argmax\n",
      "k\n",
      "„\n",
      "s\n",
      "x\n",
      "k\n",
      " = argmax\n",
      "k\n",
      "s\n",
      "k\n",
      "x\n",
      " = argmax\n",
      "k\n",
      "•\n",
      "k\n",
      "T\n",
      "x\n",
      "⁄\n",
      "The \n",
      " a r g m ax\n",
      " \n",
      " opera tor returns the value of a variable tha t maximizes a function. In\n",
      " this equa tion, it returns the value of \n",
      "k\n",
      "  tha t maximizes the estima ted probability\n",
      "„\n",
      "(\n",
      "s\n",
      "(\n",
      "x\n",
      "))\n",
      "k\n",
      ".\n",
      " The Softmax Regression classifier predicts only one class a t a time\n",
      " (i.e., it is m ulticlass, not m ultioutput) so it should be used only with\n",
      " m utually exclusive classes such as differen t types of plan ts. Y ou\n",
      " cannot use it to recognize m ultiple people in one picture.\n",
      " N ow tha t you know how the model estima tes probabilities and makes predictions,\n",
      " let ‡ s take a look a t training. The objective is to ha ve a model tha t estima tes a high\n",
      " probability for the target class (and consequen tly a low probability for the other\n",
      "classes). Minimizing the cost function shown in \n",
      " Equa tion 4-22\n",
      ", called the \n",
      " cr o s s\n",
      " en t r o p y\n",
      " , should lead to this objective beca use it penalizes the model when it estima tes\n",
      " a low probability for a target class. Cross en tropy is frequen tly used to measure how\n",
      " 150  |  Chapter 4: Training Models\n",
      "\n",
      " well a set of estima ted class probabilities ma tch the target classes (we will use it again\n",
      " several times in the following cha pters).\n",
      " Eq u a t i o n 4-22. C r o s s en t r o p y c o s t f u n c t i o n\n",
      "J\n",
      "‡\n",
      " = ”\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "“\n",
      "k\n",
      " = 1\n",
      "K\n",
      "y\n",
      "k\n",
      "i\n",
      "log\n",
      "p\n",
      "k\n",
      "i\n",
      "⁄\n",
      "y\n",
      "k\n",
      "i\n",
      " \n",
      " is the target probability tha t the i\n",
      "th\n",
      " \n",
      "instance belongs to class \n",
      "k\n",
      ". In general, it is\n",
      "either equal to 1 or 0, depending on whether the instance belongs to the class or\n",
      "not.\n",
      " N otice tha t when there are just two classes (\n",
      "K\n",
      "  = 2), this cost function is equivalen t to\n",
      " the Logistic Regression ‡ s cost function (log loss; see \n",
      " Equa tion 4-17\n",
      ").\n",
      "Cross Entropy\n",
      " Cross en tropy origina ted from informa tion theor y . Suppose you wan t to efficien tly\n",
      " transmit informa tion about the wea ther ever y da y . If there are eigh t options (sunn y ,\n",
      " rain y , etc.), you could encode each option using 3 bits since 2\n",
      "3\n",
      " \n",
      " = 8. H owever , if you\n",
      " think it will be sunn y almost ever y da y , it would be m uch more efficien t to code\n",
      " — sunn y– on just one bit (0) and the other seven options on 4 bits (starting with a 1).\n",
      " Cross en tropy measures the a verage n umber of bits you actually send per option. If\n",
      " your assum ption about the wea ther is perfect, cross en tropy will just be equal to the\n",
      " en tropy of the wea ther itself (i.e., its in trinsic un predictability). But if your assum pƒ\n",
      " tions are wrong (e.g., if it rains often), cross en tropy will be grea ter by an amoun t \n",
      "called the \n",
      " K u l l b a c k”L ei b l er d i v er gen c e\n",
      ".\n",
      " The cross en tropy between two probability distributions \n",
      "p\n",
      " and \n",
      "q\n",
      " is defined as\n",
      "H\n",
      "p\n",
      ",\n",
      "q\n",
      " = ” “\n",
      "x\n",
      "p\n",
      "x\n",
      "log\n",
      "q\n",
      "x\n",
      "  (a t least when the distributions are discrete). F or more\n",
      "details, check out \n",
      "this video\n",
      ".\n",
      " The gradien t vector of this cost function with regards to \n",
      "•\n",
      "(k)\n",
      " \n",
      "is given by \n",
      " Equa tion\n",
      "4-23\n",
      ":\n",
      " Eq u a t i o n 4-23. C r o s s en t r o p y g r a d i en t v e c t o r f o r c l as s k\n",
      "•\n",
      "k\n",
      "J\n",
      "‡\n",
      "=\n",
      "1\n",
      "m\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "p\n",
      "k\n",
      "i\n",
      "”\n",
      "y\n",
      "k\n",
      "i\n",
      "x\n",
      "i\n",
      " N ow you can com pute the gradien t vector for ever y class, then use Gradien t Descen t\n",
      " (or an y other optimiza tion algorithm) to find the parameter ma trix \n",
      "‡\n",
      "   tha t minimizes\n",
      "the cost function.\n",
      " Logistic Regression  |  151\n",
      "\n",
      " Let ‡ s use Softmax Regression to classif y the iris flowers in to all three classes. Scikit-\n",
      " Learn ‡ s \n",
      "LogisticRegression\n",
      "   uses \n",
      " one-versus-all by defa ult when you train it on more\n",
      "than two classes, but you can set the \n",
      "multi_class\n",
      " \n",
      " h yperparameter to \n",
      "\"multinomial\"\n",
      " to switch it to Softmax Regression instead. Y ou m ust also specif y a solver tha t supƒ\n",
      "ports Softmax Regression, such as the \n",
      "\"lbfgs\"\n",
      "  solver (see Scikit-Learn ‡ s documen taƒ\n",
      " tion for more details). I t also a pplies …\n",
      "2\n",
      "  regulariza tion by defa ult, which you can\n",
      " con trol using the h yperparameter \n",
      "C\n",
      ".\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      "[\n",
      "\"data\"\n",
      "][:,\n",
      " \n",
      "(\n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      ")]\n",
      "  \n",
      "# petal length, petal width\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      "[\n",
      "\"target\"\n",
      "]\n",
      "softmax_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "(\n",
      "multi_class\n",
      "=\n",
      "\"multinomial\"\n",
      ",\n",
      "solver\n",
      "=\n",
      "\"lbfgs\"\n",
      ",\n",
      " \n",
      "C\n",
      "=\n",
      "10\n",
      ")\n",
      "softmax_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      "So the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask\n",
      " your model to tell you wha t type of iris it is, and it will answer Iris-V irginica (class 2)\n",
      " with 94.2% probability (or Iris-V ersicolor with 5.8% probability):\n",
      ">>> \n",
      "softmax_reg\n",
      ".\n",
      "predict\n",
      "([[\n",
      "5\n",
      ",\n",
      " \n",
      "2\n",
      "]])\n",
      "array([2])\n",
      ">>> \n",
      "softmax_reg\n",
      ".\n",
      "predict_proba\n",
      "([[\n",
      "5\n",
      ",\n",
      " \n",
      "2\n",
      "]])\n",
      "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])\n",
      "Figure 4-25\n",
      "  shows the resulting decision boundaries, represen ted by the background\n",
      " colors. N otice tha t the decision boundaries between an y two classes are linear . The\n",
      " figure also shows the probabilities for the Iris-V ersicolor class, represen ted by the\n",
      " cur ved lines (e.g., the line labeled with 0.450 represen ts the 45% probability boundƒ\n",
      " ar y). N otice tha t the model can predict a class tha t has an estima ted probability below\n",
      " 50%. F or exam ple, a t the poin t where all decision boundaries meet, all classes ha ve an\n",
      " equal estima ted probability of 33%.\n",
      " F i g u r e 4-25. \n",
      "So“max\n",
      "  R e g r e s s i o n d e ci s i o n b o u n d a r i e s\n",
      " 152  |  Chapter 4: Training Models\n",
      "\n",
      "Exercises\n",
      "1.\n",
      " Wha t Linear Regression training algorithm can you use if you ha ve a training set\n",
      " with millions of fea tures?\n",
      "2.\n",
      " Suppose the fea tures in your training set ha ve ver y differen t scales. Wha t algoƒ\n",
      " rithms migh t suffer from this, and how? Wha t can you do about it?\n",
      "3.\n",
      " Can Gradien t Descen t get stuck in a local minim um when training a Logistic\n",
      "Regression model?\n",
      "4.\n",
      " Do all Gradien t Descen t algorithms lead to the same model provided you let\n",
      "them run long enough?\n",
      "5.\n",
      " Suppose you use Ba tch Gradien t Descen t and you plot the valida tion error a t\n",
      " ever y epoch. If you notice tha t the valida tion error consisten tly goes up , wha t is\n",
      " likely going on? H ow can you fix this?\n",
      "6.\n",
      " I s it a good idea to stop Mini-ba tch Gradien t Descen t immedia tely when the valiƒ\n",
      " da tion error goes up?\n",
      "7.\n",
      " Which Gradien t Descen t algorithm (among those we discussed) will reach the\n",
      " vicinity of the optimal solution the fastest? Which will actually con verge? H ow\n",
      " can you make the others con verge as well?\n",
      "8.\n",
      " Suppose you are using P olynomial Regression. Y ou plot the learning cur ves and\n",
      " you notice tha t there is a large ga p between the training error and the valida tion\n",
      " error . Wha t is ha ppening? Wha t are three wa ys to solve this?\n",
      "9.\n",
      " Suppose you are using Ridge Regression and you notice tha t the training error\n",
      " and the valida tion error are almost equal and fairly high. W ould you sa y tha t the\n",
      "model suffers from high bias or high variance? Should you increase the regulariƒ\n",
      " za tion h yperparameter \n",
      "‰\n",
      " or reduce it?\n",
      "10.\n",
      " Wh y would you wan t to use:\n",
      "⁄\n",
      " Ridge Regression instead of plain Linear Regression (i.e., without an y regulariƒ\n",
      " za tion)?\n",
      "⁄\n",
      "Lasso instead of Ridge Regression?\n",
      "⁄\n",
      " Elastic N et instead of Lasso?\n",
      "11.\n",
      " Suppose you wan t to classif y pictures as outdoor/indoor and da ytime/nigh ttime.\n",
      " Should you im plemen t two Logistic Regression classifiers or one Softmax Regresƒ\n",
      "sion classifier?\n",
      "12.\n",
      " Im plemen t Ba tch Gradien t Descen t with early stopping for Softmax Regression \n",
      "(without using Scikit-Learn).\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " Exercises  |  153\n",
      "\n",
      "\n",
      "CHAPTER 5\n",
      "Support Vector Machines\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 5 in the final\n",
      "release of the book.\n",
      "A \n",
      " S u p p o r t V e c t o r M a c h i n e\n",
      "  (SVM) is a ver y powerful and versa tile M achine Learning\n",
      " model, ca pable of performing linear or nonlinear classifica tion, regression, and even\n",
      " outlier detection. I t is one of the most popular models in M achine Learning, and an yƒ\n",
      " one in terested in M achine Learning should ha ve it in their toolbox. SVMs are particƒ\n",
      " ularly well suited for classifica tion of com plex but small- or medium-sized da tasets.\n",
      " This cha pter will explain the core concepts of SVMs, how to use them, and how they\n",
      "work.\n",
      "Linear SVM \n",
      "Classi•cation\n",
      " The fundamen tal idea behind SVMs is best explained with some pictures. \n",
      "Figure 5-1\n",
      " shows part of the iris da taset tha t was in troduced a t the end of \n",
      " Cha pter 4\n",
      ". The two\n",
      " classes can clearly be separa ted easily with a straigh t line (they are \n",
      " l i n e a r l y s e p a r a b l e\n",
      ").\n",
      "The left plot shows the decision boundaries of three possible linear classifiers. The\n",
      " model whose decision boundar y is represen ted by the dashed line is so bad tha t it\n",
      " does not even separa te the classes properly . The other two models work perfectly on\n",
      " this training set, but their decision boundaries come so close to the instances tha t\n",
      " these models will probably not perform as well on new instances. In con trast, the\n",
      " solid line in the plot on the righ t represen ts the decision boundar y of an SVM classiƒ\n",
      " fier ; this line not only separa tes the two classes but also sta ys as far a wa y from the\n",
      " closest training instances as possible. Y ou can think of an SVM classifier as fitting the\n",
      "155\n",
      "\n",
      " widest possible street (represen ted by the parallel dashed lines) between the classes.\n",
      "This is called \n",
      " l a r ge m a r g i n \n",
      "classi†cation\n",
      ".\n",
      " F i g u r e 5-1. L a r ge m a r g i n \n",
      "classi†cation\n",
      " N otice tha t adding more training instances — off the street – will not affect the decision\n",
      " boundar y a t all: it is fully determined (or — supported –) by the instances loca ted on the\n",
      "edge of the street. These instances are called the \n",
      " s u p p o r t v e c t o r s\n",
      " (they are circled in\n",
      "Figure 5-1\n",
      ").\n",
      " SVMs are sensitive to the fea ture scales, as you can see in\n",
      "Figure 5-2\n",
      " : on the left plot, the vertical scale is m uch larger than the\n",
      " horizon tal scale, so the widest possible street is close to horizon tal.\n",
      " After fea ture scaling (e.g., using Scikit-Learn ‡ s \n",
      "StandardScaler\n",
      "), \n",
      " the decision boundar y looks m uch better (on the righ t plot).\n",
      " F i g u r e 5-2. S ens i t i v i ty t o f e a t u r e s c a l e s\n",
      "Soft Margin \n",
      "Classi•cation\n",
      " If we strictly im pose tha t all instances be off the street and on the righ t side, this is\n",
      "called \n",
      " h a r d m a r g i n \n",
      "classi†cation\n",
      ". \n",
      "There are two main issues with hard margin classifiƒ\n",
      " ca tion. First, it only works if the da ta is linearly separable, and second it is quite sensiƒ\n",
      "tive to outliers. \n",
      "Figure 5-3\n",
      "  shows the iris da taset with just one additional outlier : on\n",
      " the left, it is im possible to find a hard margin, and on the righ t the decision boundar y\n",
      " ends up ver y differen t from the one we sa w in \n",
      "Figure 5-1\n",
      "  without the outlier , and it\n",
      "will probably not generalize as well.\n",
      " 156  |  Chapter 5: Support Vector Machines\n",
      "\n",
      " F i g u r e 5-3. H a r d m a r g i n s ens i t i v i ty t o o u t l i er s\n",
      " T o a void these issues it is preferable to use a more flexible model. The objective is to\n",
      "find a good balance between keeping the street as large as possible and limiting the\n",
      " m a r g i n v i o l a t i o ns\n",
      " \n",
      " (i.e., instances tha t end up in the middle of the street or even on the\n",
      "wrong side). This is called \n",
      "so“\n",
      "  m a r g i n \n",
      "classi†cation\n",
      ".\n",
      " In Scikit-Learn ‡ s SVM classes, you can con trol this balance using the \n",
      "C\n",
      " \n",
      " h yperparameƒ\n",
      " ter : a smaller \n",
      "C\n",
      "  value leads to a wider street but more margin viola tions. \n",
      "Figure 5-4\n",
      "shows the decision boundaries and margins of two soft margin SVM classifiers on a\n",
      " nonlinearly separable da taset. On the left, using a low \n",
      "C\n",
      " value the margin is quite\n",
      " large, but man y instances end up on the street. On the righ t, using a high \n",
      "C\n",
      " \n",
      "value the\n",
      " classifier makes fewer margin viola tions but ends up with a smaller margin. H owever ,\n",
      " it seems likely tha t the first classifier will generalize better : in fact even on this trainƒ\n",
      " ing set it makes fewer prediction errors, since most of the margin viola tions are\n",
      " actually on the correct side of the decision boundar y .\n",
      " F i g u r e 5-4. L a r ge m a r g i n \n",
      "(le“)\n",
      "  v er s u s f e w er m a r g i n v i o l a t i o ns (r i gh t)\n",
      " If your SVM model is overfitting, you can tr y regularizing it by\n",
      "reducing \n",
      "C\n",
      ".\n",
      " The following Scikit-Learn code loads the iris da taset, scales the fea tures, and then\n",
      "trains a linear SVM model (using the \n",
      "LinearSVC\n",
      " \n",
      "class with \n",
      "C\n",
      " = 1 and the \n",
      " h i n ge l o s s\n",
      " function, described shortly) to detect Iris-V irginica flowers. The resulting model is\n",
      " represen ted on the left of \n",
      "Figure 5-4\n",
      ".\n",
      "Linear SVM \n",
      "Classi•cation\n",
      "   |  157\n",
      "\n",
      "import\n",
      " \n",
      "numpy\n",
      " \n",
      "as\n",
      " \n",
      "np\n",
      "from\n",
      " \n",
      "sklearn\n",
      " \n",
      "import\n",
      " \n",
      "datasets\n",
      "from\n",
      " \n",
      "sklearn.pipeline\n",
      " \n",
      "import\n",
      " \n",
      "Pipeline\n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "StandardScaler\n",
      "from\n",
      " \n",
      "sklearn.svm\n",
      " \n",
      "import\n",
      " \n",
      "LinearSVC\n",
      "iris\n",
      " \n",
      "=\n",
      " \n",
      "datasets\n",
      ".\n",
      "load_iris\n",
      "()\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      "[\n",
      "\"data\"\n",
      "][:,\n",
      " \n",
      "(\n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      ")]\n",
      "  \n",
      "# petal length, petal width\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "iris\n",
      "[\n",
      "\"target\"\n",
      "]\n",
      " \n",
      "==\n",
      " \n",
      "2\n",
      ")\n",
      ".\n",
      "astype\n",
      "(\n",
      "np\n",
      ".\n",
      "float64\n",
      ")\n",
      "  \n",
      "# Iris-Virginica\n",
      "svm_clf\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"scaler\"\n",
      ",\n",
      " \n",
      "StandardScaler\n",
      "()),\n",
      "        \n",
      "(\n",
      "\"linear_svc\"\n",
      ",\n",
      " \n",
      "LinearSVC\n",
      "(\n",
      "C\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "loss\n",
      "=\n",
      "\"hinge\"\n",
      ")),\n",
      "    \n",
      "])\n",
      "svm_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      "Then, as usual, you can use the model to make predictions:\n",
      ">>> \n",
      "svm_clf\n",
      ".\n",
      "predict\n",
      "([[\n",
      "5.5\n",
      ",\n",
      " \n",
      "1.7\n",
      "]])\n",
      "array([1.])\n",
      " U nlike Logistic Regression classifiers, SVM classifiers do not outƒ\n",
      "put probabilities for each class.\n",
      " Alterna tively , you could use the \n",
      "SVC\n",
      " class, using \n",
      "SVC(kernel=\"linear\", C=1)\n",
      ", but it\n",
      " is m uch slower , especially with large training sets, so it is not recommended. Another\n",
      "option is to use the \n",
      "SGDClassifier\n",
      " class, with \n",
      "SGDClassifier(loss=\"hinge\",\n",
      "alpha=1/(m*C))\n",
      " . This a pplies regular Stochastic Gradien t Descen t (see \n",
      " Cha pter 4\n",
      ") to\n",
      " train a linear SVM classifier . I t does not con verge as fast as the \n",
      "LinearSVC\n",
      " \n",
      "class, but it\n",
      " can be useful to handle h uge da tasets tha t do not fit in memor y (out-of-core trainƒ\n",
      " ing), or to handle online classifica tion tasks.\n",
      "The \n",
      "LinearSVC\n",
      " \n",
      " class regularizes the bias term, so you should cen ter\n",
      " the training set first by subtracting its mean. This is a utoma tic if\n",
      " you scale the da ta using the \n",
      "StandardScaler\n",
      " . M oreover , make sure\n",
      "you set the \n",
      "loss\n",
      " \n",
      " h yperparameter to \n",
      "\"hinge\"\n",
      " , as it is not the defa ult\n",
      " value. Finally , for better performance you should set the \n",
      "dual\n",
      " h yperparameter to \n",
      "False\n",
      " , unless there are more fea tures than\n",
      " training instances (we will discuss duality la ter in the cha pter).\n",
      " 158  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "Nonlinear SVM \n",
      "Classi•cation\n",
      " Although linear SVM classifiers are efficien t and work surprisingly well in man y\n",
      " cases, man y da tasets are not even close to being linearly separable. One a pproach to\n",
      " handling nonlinear da tasets is to add more fea tures, such as \n",
      " polynomial fea tures (as\n",
      "you did in \n",
      " Cha pter 4\n",
      " ); in some cases this can result in a linearly separable da taset.\n",
      "Consider the left plot in \n",
      "Figure 5-5\n",
      " : it represen ts a sim ple da taset with just one fea ture\n",
      "x\n",
      "1\n",
      " . This da taset is not linearly separable, as you can see. But if you add a second feaƒ\n",
      "ture \n",
      "x\n",
      "2\n",
      " = (\n",
      "x\n",
      "1\n",
      ")\n",
      "2\n",
      " , the resulting 2D da taset is perfectly linearly separable.\n",
      " F i g u r e 5-5. Ad d i n g f e a t u r e s t o m a k e a d a t as e t l i n e a r l y s e p a r a b l e\n",
      " T o im plemen t this idea using Scikit-Learn, you can crea te a \n",
      "Pipeline\n",
      " \n",
      " con taining a\n",
      "PolynomialFeatures\n",
      " transformer (discussed in \n",
      " —P olynomial Regression – on page\n",
      "130\n",
      "), followed by a \n",
      "StandardScaler\n",
      " and a \n",
      "LinearSVC\n",
      " . Let ‡ s test this on the moons\n",
      " da taset: this is a toy da taset for binar y classifica tion in which the da ta poin ts are shaƒ\n",
      " ped as two in terlea ving half circles (see \n",
      "Figure 5-6\n",
      " ). Y ou can genera te this da taset\n",
      "using the \n",
      "make_moons()\n",
      " function:\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "make_moons\n",
      "from\n",
      " \n",
      "sklearn.pipeline\n",
      " \n",
      "import\n",
      " \n",
      "Pipeline\n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "PolynomialFeatures\n",
      "polynomial_svm_clf\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"poly_features\"\n",
      ",\n",
      " \n",
      "PolynomialFeatures\n",
      "(\n",
      "degree\n",
      "=\n",
      "3\n",
      ")),\n",
      "        \n",
      "(\n",
      "\"scaler\"\n",
      ",\n",
      " \n",
      "StandardScaler\n",
      "()),\n",
      "        \n",
      "(\n",
      "\"svm_clf\"\n",
      ",\n",
      " \n",
      "LinearSVC\n",
      "(\n",
      "C\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "loss\n",
      "=\n",
      "\"hinge\"\n",
      "))\n",
      "    \n",
      "])\n",
      "polynomial_svm_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      "Nonlinear SVM \n",
      "Classi•cation\n",
      "   |  159\n",
      "\n",
      " F i g u r e 5-6. L i n e a r SVM \n",
      "classi†er\n",
      "  u s i n g p o l y n o m i a l f e a t u r e s\n",
      "Polynomial Kernel\n",
      " A dding polynomial fea tures is sim ple to im plemen t and can work grea t with all sorts\n",
      " of M achine Learning algorithms (not just SVMs), but a t a low polynomial degree it\n",
      " cannot deal with ver y com plex da tasets, and with a high polynomial degree it crea tes\n",
      " a h uge n umber of fea tures, making the model too slow .\n",
      " F ortuna tely , when using SVMs you can a pply an almost miraculous ma thema tical\n",
      "technique called the \n",
      " k er n e l t r i c k\n",
      "  (it is explained in a momen t). I t makes it possible to\n",
      " get the same result as if you added man y polynomial fea tures, even with ver y high-\n",
      " degree polynomials, without actually ha ving to add them. So there is no combina toƒ\n",
      " rial explosion of the n umber of fea tures since you don ‡ t actually add an y fea tures. This\n",
      " trick is im plemen ted by the \n",
      "SVC\n",
      "  class. Let ‡ s test it on the moons da taset:\n",
      "from\n",
      " \n",
      "sklearn.svm\n",
      " \n",
      "import\n",
      " \n",
      "SVC\n",
      "poly_kernel_svm_clf\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"scaler\"\n",
      ",\n",
      " \n",
      "StandardScaler\n",
      "()),\n",
      "        \n",
      "(\n",
      "\"svm_clf\"\n",
      ",\n",
      " \n",
      "SVC\n",
      "(\n",
      "kernel\n",
      "=\n",
      "\"poly\"\n",
      ",\n",
      " \n",
      "degree\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "coef0\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "C\n",
      "=\n",
      "5\n",
      "))\n",
      "    \n",
      "])\n",
      "poly_kernel_svm_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      "This code trains an SVM classifier using a 3\n",
      "rd\n",
      " -degree polynomial kernel. I t is repreƒ\n",
      " sen ted on the left of \n",
      "Figure 5-7\n",
      " . On the righ t is another SVM classifier using a 10\n",
      "th\n",
      "-\n",
      " degree polynomial kernel. Obviously , if your model is overfitting, you migh t wan t to\n",
      " 160  |  Chapter 5: Support Vector Machines\n",
      "\n",
      " reduce the polynomial degree. Con versely , if it is underfitting, you can tr y increasing\n",
      " it. The h yperparameter \n",
      "coef0\n",
      "  con trols how m uch the model is influenced by high-\n",
      "degree polynomials versus low-degree polynomials.\n",
      " F i g u r e 5-7. SVM \n",
      "classi†ers\n",
      "  w i t h a p o l y n o m i a l k er n e l\n",
      " A common a pproach to find the righ t h yperparameter values is to\n",
      "use grid search (see \n",
      " Cha pter 2\n",
      " ). I t is often faster to first do a ver y\n",
      "coarse grid search, then a finer grid search around the best values\n",
      " found. H a ving a good sense of wha t each h yperparameter actually\n",
      " does can also help you search in the righ t part of the \n",
      " h yperparameƒ\n",
      "ter space.\n",
      "Adding Similarity Features\n",
      " Another technique to tackle nonlinear problems is to add fea tures com puted using a\n",
      " s i m i l a r i ty f u n c t i o n\n",
      "  tha t measures how m uch each instance resembles a particular\n",
      " l a n d m a r k\n",
      " . F or exam ple, let ‡ s take the one-dimensional da taset discussed earlier and\n",
      " add two landmarks to it a t \n",
      "x\n",
      "1\n",
      " = −2 and \n",
      "x\n",
      "1\n",
      " = 1 (see the left plot in \n",
      "Figure 5-8\n",
      " ). N ext,\n",
      " let ‡ s define the similarity function to be the Ga ussian \n",
      " R a d i a l B as i s F u n c t i o n\n",
      " \n",
      "(\n",
      "RBF\n",
      ")\n",
      "with \n",
      "‘\n",
      " = 0.3 (see \n",
      " Equa tion 5-1\n",
      ").\n",
      " Eq u a t i o n 5-1. G a u s s i a n RBF\n",
      "’\n",
      "‘\n",
      "x\n",
      " , …\n",
      " = exp\n",
      "”\n",
      "‘\n",
      "x\n",
      " ” …\n",
      "2\n",
      " I t is a bell-sha ped function var ying from 0 (ver y far a wa y from the landmark) to 1 (a t\n",
      " the landmark). N ow we are ready to com pute the new fea tures. F or exam ple, let ‡ s look\n",
      " a t the instance \n",
      "x\n",
      "1\n",
      "  = −1: it is loca ted a t a distance of 1 from the first landmark, and 2\n",
      " from the second landmark. Therefore its new fea tures are \n",
      "x\n",
      "2\n",
      " \n",
      "= exp (−0.3 „ 1\n",
      "2\n",
      ") \n",
      "ı\n",
      " 0.74\n",
      "and \n",
      "x\n",
      "3\n",
      " = exp (−0.3 „ 2\n",
      "2\n",
      ") \n",
      "ı\n",
      "  0.30. The plot on the righ t of \n",
      "Figure 5-8\n",
      " \n",
      "shows the transƒ\n",
      " formed da taset (dropping the original fea tures). As you can see, it is now linearly\n",
      "separable.\n",
      "Nonlinear SVM \n",
      "Classi•cation\n",
      "   |  161\n",
      "\n",
      " F i g u r e 5-8. S i m i l a r i ty f e a t u r e s u s i n g t h e G a u s s i a n RBF\n",
      " Y ou ma y wonder how to select the landmarks. The sim plest a pproach is to crea te a\n",
      " landmark a t the loca tion of each and ever y instance in the da taset. This crea tes man y\n",
      " dimensions and th us increases the chances tha t the transformed training set will be\n",
      " linearly separable. The downside is tha t a training set with \n",
      "m\n",
      " \n",
      "instances and \n",
      "n\n",
      "   fea tures\n",
      " gets transformed in to a training set with \n",
      "m\n",
      " instances and \n",
      "m\n",
      "  fea tures (assuming you\n",
      " drop the original fea tures). If your training set is ver y large, you end up with an\n",
      " equally large n umber of fea tures.\n",
      "Gaussian RBF Kernel\n",
      " J ust \n",
      " like the polynomial fea tures method, the similarity fea tures method can be useful\n",
      " with an y M achine Learning algorithm, but it ma y be com puta tionally expensive to\n",
      " com pute all the additional fea tures, especially on large training sets. H owever , once\n",
      "again the kernel trick does its SVM magic: it makes it possible to obtain a similar\n",
      " result as if you had added man y similarity fea tures, without actually ha ving to add\n",
      " them. Let ‡ s tr y the Ga ussian RBF kernel using the \n",
      "SVC\n",
      " class:\n",
      "rbf_kernel_svm_clf\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"scaler\"\n",
      ",\n",
      " \n",
      "StandardScaler\n",
      "()),\n",
      "        \n",
      "(\n",
      "\"svm_clf\"\n",
      ",\n",
      " \n",
      "SVC\n",
      "(\n",
      "kernel\n",
      "=\n",
      "\"rbf\"\n",
      ",\n",
      " \n",
      "gamma\n",
      "=\n",
      "5\n",
      ",\n",
      " \n",
      "C\n",
      "=\n",
      "0.001\n",
      "))\n",
      "    \n",
      "])\n",
      "rbf_kernel_svm_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " This model is represen ted on the bottom left of \n",
      "Figure 5-9\n",
      ". The other plots show\n",
      " models trained with differen t values of h yperparameters \n",
      "gamma\n",
      " \n",
      "(\n",
      "‘\n",
      ") and \n",
      "C\n",
      ". Increasing\n",
      "gamma\n",
      "  makes the bell-sha pe cur ve narrower (see the left plot of \n",
      "Figure 5-8\n",
      "), and as a\n",
      " result each instance ‡ s range of influence is smaller : the decision boundar y ends up\n",
      " being more irregular , wiggling around individual instances. Con versely , a small \n",
      "gamma\n",
      " \n",
      " value makes the bell-sha ped cur ve wider , so instances ha ve a larger range of influƒ\n",
      " ence, and the decision boundar y ends up smoother . So \n",
      "‘\n",
      "  acts like a regulariza tion\n",
      " h yperparameter : if your model is overfitting, you should reduce it, and if it is \n",
      "underƒ\n",
      "fitting, you should increase it (similar to the \n",
      "C\n",
      "  h yperparameter).\n",
      " 162  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "1\n",
      " — A Dual Coordina te Descen t M ethod for Large-scale Linear SVM, – Lin et al. (2008).\n",
      " F i g u r e 5-9. SVM \n",
      "classi†ers\n",
      "  u s i n g a n RBF k er n e l\n",
      " Other kernels exist but are used m uch more rarely . F or exam ple, some kernels are\n",
      " specialized for specific da ta structures. \n",
      " S t r i n g k er n e l s\n",
      " \n",
      "are sometimes used when classiƒ\n",
      " f ying text documen ts or DN A sequences (e.g., using the \n",
      " s t r i n g s u b s e q u en c e k er n e l\n",
      " \n",
      "or\n",
      "kernels based on the \n",
      " L e v ens h t ei n d i s t a n c e\n",
      ").\n",
      " W ith so man y kernels to choose from, how can you decide which\n",
      " one to use? As a rule of th umb , you should alwa ys tr y the linear\n",
      " kernel first (remember tha t \n",
      "LinearSVC\n",
      " \n",
      " is m uch faster than \n",
      "SVC(ker\n",
      "nel=\"linear\")\n",
      " ), especially if the training set is ver y large or if it\n",
      " has plen ty of fea tures. If the training set is not too large, you should\n",
      " tr y the Ga ussian RBF kernel as well; it works well in most cases.\n",
      " Then if you ha ve spare time and com puting power , you can also\n",
      " experimen t with a few other kernels using cross-valida tion and grid\n",
      "search, especially if there are kernels specialized for your training\n",
      " set ‡ s da ta structure.\n",
      "Computational Complexity\n",
      "The \n",
      "LinearSVC\n",
      " \n",
      "class is based on the \n",
      " l i b l i n e a r\n",
      "   librar y , \n",
      " which im plemen ts an \n",
      "optimized\n",
      "algorithm\n",
      " for linear SVMs.\n",
      "1\n",
      "  I t does not support the kernel trick, but it scales almost\n",
      "Nonlinear SVM \n",
      "Classi•cation\n",
      "   |  163\n",
      "\n",
      "2\n",
      " — Sequen tial Minimal Optimiza tion (SMO), – J . Pla tt (1998).\n",
      " linearly with the n umber of training instances and the n umber of fea tures: its training\n",
      " time com plexity is roughly \n",
      "O\n",
      "(\n",
      "m\n",
      " „ \n",
      "n\n",
      ").\n",
      " The algorithm takes longer if you require a ver y high precision. This is con trolled by\n",
      " the tolerance h yperparameter \n",
      " (called \n",
      "tol\n",
      "  in Scikit-Learn). In most classifica tion\n",
      " tasks, the defa ult tolerance is fine.\n",
      "The \n",
      "SVC\n",
      " \n",
      "class is based on \n",
      "the \n",
      " l i b s v m\n",
      " \n",
      " librar y , which im plemen ts \n",
      "an algorithm\n",
      "   tha t supƒ\n",
      "ports the kernel trick.\n",
      "2\n",
      "  The training time com plexity is usually between \n",
      "O\n",
      "(\n",
      "m\n",
      "2\n",
      " „ \n",
      "n\n",
      ")\n",
      "and \n",
      "O\n",
      "(\n",
      "m\n",
      "3\n",
      " „ \n",
      "n\n",
      " ). U nfortuna tely , this means tha t it gets dreadfully slow when the n umƒ\n",
      " ber of training instances gets large (e.g., h undreds of thousands of instances). This\n",
      " algorithm is perfect for com plex but small or medium training sets. H owever , it scales\n",
      " well with the n umber of fea tures, especially with \n",
      " s p a r s e f e a t u r e s\n",
      " (i.e., when each\n",
      " instance has few nonzero fea tures). In this case, the algorithm scales roughly with the\n",
      " a verage n umber of nonzero fea tures per instance. \n",
      " T able 5-1\n",
      "  com pares \n",
      " Scikit-Learn ‡ s\n",
      " SVM classifica tion classes.\n",
      " T a b l e 5-1. C o m p a r i s o n o f S ci k i t-L e a r n c l as s e s f o r SVM \n",
      "classi†cation\n",
      "Class\n",
      " T ime   complexity\n",
      " Out-of-core   support\n",
      " Scaling   required\n",
      " Kernel   trick\n",
      "LinearSVC\n",
      "O(\n",
      "m\n",
      "   ‡  \n",
      "n\n",
      ")\n",
      "No\n",
      "Yes\n",
      "No\n",
      "SGDClassifier\n",
      "O(\n",
      "m\n",
      "   ‡  \n",
      "n\n",
      ")\n",
      "Yes\n",
      "Yes\n",
      "No\n",
      "SVC\n",
      "O(\n",
      "m\n",
      " …   ‡  \n",
      "n\n",
      " )   to   O(\n",
      "m\n",
      " —   ‡  \n",
      "n\n",
      ")\n",
      "No\n",
      "Yes\n",
      "Yes\n",
      "SVM Regression\n",
      " As we men tioned earlier , the SVM algorithm is quite versa tile: not only does it supƒ\n",
      " port linear and nonlinear classifica tion, but it also supports linear and nonlinear\n",
      " regression. The trick is to reverse the objective: instead of tr ying to fit the largest posƒ\n",
      " sible street between two classes while limiting margin viola tions, SVM Regression\n",
      " tries to fit as man y instances as possible \n",
      " o n\n",
      " \n",
      " the street while limiting margin viola tions\n",
      "(i.e., instances \n",
      "o›\n",
      "  the street). The width of the street is con trolled by a h yperparameƒ\n",
      "ter \n",
      ". \n",
      "Figure 5-10\n",
      " \n",
      "shows two linear SVM Regression models trained on some random\n",
      " linear da ta, one with a large margin (\n",
      " = 1.5) and the other with a small margin (\n",
      " \n",
      "=\n",
      "0.5).\n",
      " 164  |  Chapter 5: Support Vector Machines\n",
      "\n",
      " F i g u r e 5-10. SVM R e g r e s s i o n\n",
      " A dding more training instances within the margin does not affect the model ‡ s predicƒ\n",
      " tions; th us, the model is said to be \n",
      " -i ns ens i t i v e\n",
      ".\n",
      " Y ou can use Scikit-Learn ‡ s \n",
      "LinearSVR\n",
      " class to perform linear SVM Regression. The\n",
      " following code produces the model represen ted on the left of \n",
      "Figure 5-10\n",
      " \n",
      "(the trainƒ\n",
      " ing da ta should be scaled and cen tered first):\n",
      "from\n",
      " \n",
      "sklearn.svm\n",
      " \n",
      "import\n",
      " \n",
      "LinearSVR\n",
      "svm_reg\n",
      " \n",
      "=\n",
      " \n",
      "LinearSVR\n",
      "(\n",
      "epsilon\n",
      "=\n",
      "1.5\n",
      ")\n",
      "svm_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " T o tackle nonlinear regression tasks, you can use a kernelized SVM model. F or examƒ\n",
      "ple, \n",
      "Figure 5-11\n",
      "  shows SVM Regression on a random quadra tic training set, using a\n",
      "2\n",
      "nd\n",
      " -degree polynomial kernel. There is little regulariza tion on the left plot (i.e., a large\n",
      "C\n",
      "  value), and m uch more regulariza tion on the righ t plot (i.e., a small \n",
      "C\n",
      " value).\n",
      " F i g u r e 5-11. SVM r e g r e s s i o n u s i n g a 2\n",
      " n d\n",
      " -d e g r e e p o l y n o m i a l k er n e l\n",
      " SVM Regression  |  165\n",
      "\n",
      " The following code produces the model represen ted on the left of \n",
      "Figure 5-11\n",
      " \n",
      "using\n",
      " Scikit-Learn ‡ s \n",
      "SVR\n",
      " class (which supports the kernel trick). The \n",
      "SVR\n",
      " class is the regresƒ\n",
      " sion equivalen t of the \n",
      "SVC\n",
      " class, and the \n",
      "LinearSVR\n",
      "  class is the regression equivalen t\n",
      "of the \n",
      "LinearSVC\n",
      " class. The \n",
      "LinearSVR\n",
      " class scales linearly with the size of the trainƒ\n",
      "ing set (just like the \n",
      "LinearSVC\n",
      " class), while the \n",
      "SVR\n",
      "  class gets m uch too slow when\n",
      "the training set grows large (just like the \n",
      "SVC\n",
      " class).\n",
      "from\n",
      " \n",
      "sklearn.svm\n",
      " \n",
      "import\n",
      " \n",
      "SVR\n",
      "svm_poly_reg\n",
      " \n",
      "=\n",
      " \n",
      "SVR\n",
      "(\n",
      "kernel\n",
      "=\n",
      "\"poly\"\n",
      ",\n",
      " \n",
      "degree\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "C\n",
      "=\n",
      "100\n",
      ",\n",
      " \n",
      "epsilon\n",
      "=\n",
      "0.1\n",
      ")\n",
      "svm_poly_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " SVMs can also be used for outlier detection; see Scikit-Learn ‡ s docƒ\n",
      " umen ta tion for more details.\n",
      "Under the Hood\n",
      "This \n",
      "section explains how SVMs make predictions and how their training algorithms\n",
      " work, starting with linear SVM classifiers. Y ou can safely skip it and go straigh t to the\n",
      " exercises a t the end of this cha pter if you are just getting started with M achine Learnƒ\n",
      " ing, and come back la ter when you wan t to get a deeper understanding of SVMs.\n",
      " First, a word about nota tions: in \n",
      " Cha pter 4\n",
      " \n",
      " we used the con ven tion of putting all the \n",
      "model parameters in one vector \n",
      "•\n",
      ", including the bias term \n",
      "–\n",
      "0\n",
      "  and the in put fea ture\n",
      " weigh ts \n",
      "–\n",
      "1\n",
      "   to \n",
      "–\n",
      "n\n",
      " , and adding a bias in put \n",
      "x\n",
      "0\n",
      " \n",
      " = 1 to all instances. In this cha pter , we will\n",
      " use a differen t con ven tion, which is more con venien t (and more common) when you\n",
      "are dealing with SVMs: the bias term will be called \n",
      "b\n",
      "  and the fea ture weigh ts vector\n",
      "will be called \n",
      "w\n",
      " . N o bias fea ture will be added to the in put fea ture vectors.\n",
      "Decision Function and Predictions\n",
      "The \n",
      "linear SVM classifier model predicts the class of a new instance \n",
      "x\n",
      "   by sim ply comƒ\n",
      "puting the decision function \n",
      "w\n",
      "T\n",
      " \n",
      "x\n",
      "   + \n",
      "b\n",
      "   = \n",
      "w\n",
      "1\n",
      " \n",
      "x\n",
      "1\n",
      "   + \n",
      " + \n",
      "w\n",
      "n\n",
      " \n",
      "x\n",
      "n\n",
      "   + \n",
      "b\n",
      ": if the result is positive,\n",
      "the predicted class \n",
      "ƒ\n",
      "  is the positive class (1), or else it is the nega tive class (0); see\n",
      " Equa tion 5-2\n",
      ".\n",
      " Eq u a t i o n 5-2. L i n e a r SVM \n",
      "classi†er\n",
      "  p r e d i c t i o n\n",
      "y\n",
      "=\n",
      " 0 if\n",
      "w\n",
      "T\n",
      "x\n",
      "+\n",
      "b\n",
      " < 0,\n",
      " 1 if\n",
      "w\n",
      "T\n",
      "x\n",
      "+\n",
      "b\n",
      " Ž 0\n",
      " 166  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "3\n",
      " M ore generally , when there are \n",
      "n\n",
      "  fea tures, the decision function is an \n",
      "n\n",
      "-dimensional \n",
      " h y p er p l a n e\n",
      ", and the deciƒ\n",
      " sion boundar y is an (\n",
      "n\n",
      "  − 1)-dimensional h yperplane.\n",
      "Figure 5-12\n",
      "  shows the decision function tha t corresponds to the model on the left of\n",
      "Figure 5-4\n",
      " : it is a two-dimensional plane since this da taset has two fea tures (petal\n",
      " width and petal length). The decision boundar y is the set of poin ts where the decision\n",
      " function is equal to 0: it is the in tersection of two planes, which is a straigh t line (repƒ\n",
      " resen ted by the thick solid line).\n",
      "3\n",
      " F i g u r e 5-12. D e ci s i o n f u n c t i o n f o r t h e i r i s d a t as e t\n",
      " The dashed lines represen t the poin ts where the decision function is equal to 1 or −1:\n",
      " they are parallel and a t equal distance to the decision boundar y , forming a margin\n",
      " around it. T raining a linear SVM classifier means finding the value of \n",
      "w\n",
      " and \n",
      "b\n",
      " \n",
      " tha t\n",
      " make this margin as wide as possible while a voiding margin viola tions (hard margin)\n",
      "or limiting them (soft margin).\n",
      "Training Objective\n",
      "Consider \n",
      " the slope of the decision function: it is equal to the norm of the weigh t vecƒ\n",
      " tor , \n",
      " \n",
      "w\n",
      " \n",
      " . If we divide this slope by 2, the poin ts where the decision function is equal\n",
      " to ﬂ1 are going to be twice as far a wa y from the decision boundar y . In other words,\n",
      " dividing the slope by 2 will m ultiply the margin by 2. P erha ps this is easier to visualƒ\n",
      "ize in 2D in \n",
      "Figure 5-13\n",
      " . The smaller the weigh t vector \n",
      "w\n",
      ", the larger the margin.\n",
      " Under the Hood  |  167\n",
      "\n",
      "4\n",
      "Zeta (\n",
      "‚\n",
      ") is the 6\n",
      "th\n",
      " letter of the Greek alphabet.\n",
      " F i g u r e 5-13. A s m a l l er w ei gh t v e c t o r r e s u l ts i n a l a r ger m a r g i n\n",
      " So we wan t to minimize \n",
      " \n",
      "w\n",
      " \n",
      "  to get a large margin. H owever , if we also wan t to a void\n",
      " an y margin viola tion (hard margin), then we need the decision function to be grea ter\n",
      " than 1 for all positive training instances, and lower than −1 for nega tive training\n",
      "instances. If we define \n",
      "t\n",
      "(i)\n",
      "   = −1 for nega tive instances (if \n",
      "y\n",
      "(i)\n",
      "   = 0) and \n",
      "t\n",
      "(i)\n",
      "   = 1 for positive\n",
      "instances (if \n",
      "y\n",
      "(i)\n",
      "  = 1), then we can express this constrain t as \n",
      "t\n",
      "(i)\n",
      "(\n",
      "w\n",
      "T\n",
      " \n",
      "x\n",
      "(i)\n",
      " + \n",
      "b\n",
      ") \n",
      "Ž\n",
      " 1 for all\n",
      "instances.\n",
      " W e can therefore express the hard margin linear SVM classifier objective as the \n",
      " c o n‡\n",
      " s t r a i n e d o p t i m iz a t i o n\n",
      " problem in \n",
      " Equa tion 5-3\n",
      ".\n",
      " Eq u a t i o n 5-3. H a r d m a r g i n l i n e a r SVM \n",
      "classi†er\n",
      "  o b je c t i v e\n",
      "minimize\n",
      "w\n",
      ",\n",
      "b\n",
      "1\n",
      "2\n",
      "w\n",
      "T\n",
      "w\n",
      " subject to\n",
      "t\n",
      "i\n",
      "w\n",
      "T\n",
      "x\n",
      "i\n",
      "+\n",
      "b\n",
      " Ž 1 for\n",
      "i\n",
      " = 1, 2,\n",
      ",\n",
      "m\n",
      " W e are minimizing \n",
      "1\n",
      "2\n",
      "w\n",
      "T\n",
      " \n",
      "w\n",
      ", which is equal to \n",
      "1\n",
      "2\n",
      " \n",
      "w\n",
      " \n",
      "2\n",
      " , ra ther than\n",
      "minimizing \n",
      " \n",
      "w\n",
      " \n",
      ". Indeed, \n",
      "1\n",
      "2\n",
      " \n",
      "w\n",
      " \n",
      "2\n",
      " \n",
      " has a nice and sim ple deriva tive\n",
      "(it is just \n",
      "w\n",
      ") while \n",
      " \n",
      "w\n",
      " \n",
      "  is not differen tiable a t \n",
      "w\n",
      "   = \n",
      "0\n",
      " . Optimiza tion\n",
      " algorithms work m uch better on differen tiable functions.\n",
      " T o get the soft margin objective, we need to in troduce \n",
      "a \n",
      " s l a c k v a r i a b l e\n",
      " \n",
      "‚\n",
      "(i)\n",
      "   Ž\n",
      " 0 for each\n",
      "instance:\n",
      "4\n",
      " \n",
      "‚\n",
      "(i)\n",
      " \n",
      " measures how m uch the i\n",
      "th\n",
      " \n",
      " instance is allowed to viola te the margin. W e\n",
      " now ha ve two conflicting objectives: making the slack variables as small as possible to\n",
      " reduce the margin viola tions, and making \n",
      "1\n",
      "2\n",
      "w\n",
      "T\n",
      " \n",
      "w\n",
      " as small as possible to increase the\n",
      "margin. This is where the \n",
      "C\n",
      " \n",
      " h yperparameter comes in: it allows us to define the tradeƒ\n",
      " 168  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "5\n",
      " T o learn more about Quadra tic Programming, you can start by reading Stephen B oyd and Lieven V andenƒ\n",
      "berghe, \n",
      " C o n v ex O p t i m iz a t i o n\n",
      "  (Cambridge, UK: Cambridge U niversity Press, 2004) or wa tch Richard Brown ‡ s\n",
      "series of video lectures\n",
      ".\n",
      " off between these two objectives. This gives us the constrained optimiza tion problem\n",
      "in \n",
      " Equa tion 5-4\n",
      ".\n",
      " Eq u a t i o n 5-4. \n",
      "So“\n",
      "  m a r g i n l i n e a r SVM \n",
      "classi†er\n",
      "  o b je c t i v e\n",
      "minimize\n",
      "w\n",
      ",\n",
      "b\n",
      ",\n",
      "‚\n",
      "1\n",
      "2\n",
      "w\n",
      "T\n",
      "w\n",
      "+\n",
      "C\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "‚\n",
      "i\n",
      " subject to\n",
      "t\n",
      "i\n",
      "w\n",
      "T\n",
      "x\n",
      "i\n",
      "+\n",
      "b\n",
      " Ž 1 ”\n",
      "‚\n",
      "i\n",
      "and\n",
      "‚\n",
      "i\n",
      " Ž 0 for\n",
      "i\n",
      " = 1, 2,\n",
      ",\n",
      "m\n",
      "Quadratic Programming\n",
      " The hard margin and soft margin problems are both con vex quadra tic optimiza tion\n",
      " problems with linear constrain ts. Such problems are known as \n",
      " Q u a d r a t i c P r og r a m‡\n",
      " m i n g\n",
      "  (QP) problems. M an y off-the-shelf solvers are a vailable to solve QP problems\n",
      " using a variety of techniques tha t are outside the scope of this book.\n",
      "5\n",
      " The general\n",
      " problem form ula tion is given by \n",
      " Equa tion 5-5\n",
      ".\n",
      " Eq u a t i o n 5-5. Q u a d r a t i c P r og r a m m i n g p r o b l em\n",
      "Minimize\n",
      "p\n",
      "1\n",
      "2\n",
      "p\n",
      "T\n",
      "Hp\n",
      "+\n",
      "f\n",
      "T\n",
      "p\n",
      " subject to\n",
      "Ap\n",
      "ł\n",
      "b\n",
      "where\n",
      "p\n",
      " is an\n",
      "n\n",
      "p\n",
      " ƒdimensional vector (\n",
      "n\n",
      "p\n",
      " = number of parameters),\n",
      "H\n",
      " is an\n",
      "n\n",
      "p\n",
      "„\n",
      "n\n",
      "p\n",
      "matrix,\n",
      "f\n",
      " is an\n",
      "n\n",
      "p\n",
      " ƒdimensional vector,\n",
      "A\n",
      " is an\n",
      "n\n",
      "c\n",
      "„\n",
      "n\n",
      "p\n",
      " matrix (\n",
      "n\n",
      "c\n",
      " = number of constraints),\n",
      "b\n",
      " is an\n",
      "n\n",
      "c\n",
      " ƒdimensional vector.\n",
      " N ote tha t the expression \n",
      "A\n",
      " \n",
      "p\n",
      " \n",
      "ł\n",
      " \n",
      "b\n",
      " actually defines \n",
      "n\n",
      "c\n",
      " \n",
      " constrain ts: \n",
      "p\n",
      "T\n",
      " \n",
      "a\n",
      "(i)\n",
      " \n",
      "ł\n",
      " \n",
      "b\n",
      "(i)\n",
      " \n",
      "for \n",
      "i\n",
      " \n",
      "= 1,\n",
      "2, \n",
      ", \n",
      "n\n",
      "c\n",
      ", where \n",
      "a\n",
      "(i)\n",
      "  is the vector con taining the elemen ts of the i\n",
      "th\n",
      " \n",
      "row of \n",
      "A\n",
      " and \n",
      "b\n",
      "(i)\n",
      " \n",
      "is\n",
      "the i\n",
      "th\n",
      "  elemen t of \n",
      "b\n",
      ".\n",
      " Y ou can easily verif y tha t if you set the QP parameters in the following wa y , you get\n",
      "the hard margin linear SVM classifier objective:\n",
      "⁄\n",
      "n\n",
      "p\n",
      " = \n",
      "n\n",
      " + 1, where \n",
      "n\n",
      "  is the n umber of fea tures (the +1 is for the bias term).\n",
      " Under the Hood  |  169\n",
      "\n",
      "6\n",
      " The objective function is con vex, and the inequality constrain ts are con tin uously differen tiable and con vex\n",
      "functions.\n",
      "⁄\n",
      "n\n",
      "c\n",
      " = \n",
      "m\n",
      ", where \n",
      "m\n",
      "  is the n umber of training instances.\n",
      "⁄\n",
      "H\n",
      " is the \n",
      "n\n",
      "p\n",
      " „ \n",
      "n\n",
      "p\n",
      "  iden tity ma trix, except with a zero in the top-left cell (to ignore\n",
      "the bias term).\n",
      "⁄\n",
      "f\n",
      " = \n",
      "0\n",
      ", an \n",
      "n\n",
      "p\n",
      "-dimensional vector full of 0s.\n",
      "⁄\n",
      "b\n",
      " = \n",
      "…1\n",
      ", an \n",
      "n\n",
      "c\n",
      "-dimensional vector full of −1s.\n",
      "⁄\n",
      "a\n",
      "(i)\n",
      " = −\n",
      "t\n",
      "(i)\n",
      " \n",
      "x\n",
      "œ\n",
      " \n",
      "(i)\n",
      ", where \n",
      "x\n",
      "œ\n",
      " \n",
      "(i)\n",
      " is equal to \n",
      "x\n",
      "(i)\n",
      "  with an extra bias fea ture \n",
      "x\n",
      "œ\n",
      " \n",
      "0\n",
      " = 1.\n",
      " So one wa y to train a hard margin linear SVM classifier is just to use an off-the-shelf\n",
      "QP solver by passing it the preceding parameters. The resulting vector \n",
      "p\n",
      " \n",
      " will con tain\n",
      "the bias term \n",
      "b\n",
      " = \n",
      "p\n",
      "0\n",
      "  and the fea ture weigh ts \n",
      "w\n",
      "i\n",
      " \n",
      "= \n",
      "p\n",
      "i\n",
      " for \n",
      "i\n",
      " = 1, 2, \n",
      ", \n",
      "n\n",
      " . Similarly , you\n",
      " can use a QP solver to solve the soft margin problem (see the exercises a t the end of\n",
      " the cha pter).\n",
      " H owever , to use the kernel trick we are going to look a t a differen t constrained optiƒ\n",
      " miza tion problem.\n",
      "The Dual Problem\n",
      " Given a constrained optimiza tion problem, known as the \n",
      " p r i m a l p r o b l em\n",
      ", it is possiƒ\n",
      " ble to express a differen t but closely rela ted problem, called \n",
      "its \n",
      " d u a l p r o b l em\n",
      ". The solƒ\n",
      "ution to the dual problem typically gives a lower bound to the solution of the primal\n",
      " problem, but under some conditions it can even ha ve the same solutions as the priƒ\n",
      " mal problem. L uckily , the SVM problem ha ppens to meet these conditions,\n",
      "6\n",
      " so you\n",
      " can choose to solve the primal problem or the dual problem; both will ha ve the same\n",
      "solution. \n",
      " Equa tion 5-6\n",
      " shows the dual form of the linear SVM objective (if you are\n",
      " in terested in knowing how to derive the dual problem from the primal problem,\n",
      "see \n",
      "???\n",
      ").\n",
      " Eq u a t i o n 5-6. D u a l f o r m o f t h e l i n e a r SVM o b je c t i v e\n",
      "minimize\n",
      "‰\n",
      "1\n",
      "2\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "‰\n",
      "i\n",
      "‰\n",
      "j\n",
      "t\n",
      "i\n",
      "t\n",
      "j\n",
      "x\n",
      "i\n",
      "T\n",
      "x\n",
      "j\n",
      "”\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "‰\n",
      "i\n",
      " subject to\n",
      "‰\n",
      "i\n",
      " Ž 0 for\n",
      "i\n",
      " = 1, 2,\n",
      ",\n",
      "m\n",
      " 170  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "7\n",
      "As explained in \n",
      " Cha pter 4\n",
      ", the dot product of two vectors \n",
      "a\n",
      " and \n",
      "b\n",
      " is normally noted \n",
      "a\n",
      " ’ \n",
      "b\n",
      " . H owever , in\n",
      " M achine Learning, vectors are frequen tly represen ted as column vectors (i.e., single-column ma trices), so the\n",
      " dot product is achieved by com puting \n",
      "a\n",
      "T\n",
      "b\n",
      " . T o remain consisten t with the rest of the book, we will use this\n",
      " nota tion here, ignoring the fact tha t this technically results in a single-cell ma trix ra ther than a scalar value.\n",
      "Once you find the vector \n",
      "‰\n",
      " \n",
      " tha t minimizes this equa tion (using a QP solver), you can\n",
      " com pute \n",
      "w\n",
      " and \n",
      "b\n",
      "  tha t minimize the primal problem by using \n",
      " Equa tion 5-7\n",
      ".\n",
      " Eq u a t i o n 5-7. F r o m t h e d u a l s o l u t i o n t o t h e p r i m a l s o l u t i o n\n",
      "w\n",
      "=\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "‰\n",
      "i\n",
      "t\n",
      "i\n",
      "x\n",
      "i\n",
      "b\n",
      "=\n",
      "1\n",
      "n\n",
      "s\n",
      "“\n",
      "i\n",
      " = 1\n",
      "‰\n",
      "i\n",
      " > 0\n",
      "m\n",
      "t\n",
      "i\n",
      "”\n",
      "w\n",
      "T\n",
      "x\n",
      "i\n",
      " The dual problem is faster to solve than the primal when the n umber of training\n",
      " instances is smaller than the n umber of fea tures. M ore im portan tly , it makes the kerƒ\n",
      " nel trick possible, while the primal does not. So wha t is this kernel trick an ywa y?\n",
      "Kernelized SVM\n",
      " Suppose you wan t to a pply a 2\n",
      "nd\n",
      " -degree polynomial transforma tion to a two-\n",
      "dimensional training set (such as the moons training set), then train a linear SVM\n",
      "classifier on the transformed training set. \n",
      " Equa tion 5-8\n",
      " \n",
      "shows the 2\n",
      "nd\n",
      "-degree polynoƒ\n",
      " mial ma pping function \n",
      "’\n",
      "  tha t you wan t to a pply .\n",
      " Eq u a t i o n 5-8. S e c o n d-d e g r e e p o l y n o m i a l m a p p i n g\n",
      "’\n",
      "x\n",
      "=\n",
      "’\n",
      "x\n",
      "1\n",
      "x\n",
      "2\n",
      "=\n",
      "x\n",
      "1\n",
      "2\n",
      "2\n",
      "x\n",
      "1\n",
      "x\n",
      "2\n",
      "x\n",
      "2\n",
      "2\n",
      " N otice tha t the transformed vector is three-dimensional instead of two-dimensional.\n",
      " N ow let ‡ s look a t wha t ha ppens to a couple of two-dimensional vectors, \n",
      "a\n",
      "   and \n",
      "b\n",
      ", if we\n",
      " a pply this 2\n",
      "nd\n",
      " -degree polynomial ma pping and then com pute the dot product\n",
      "7\n",
      " \n",
      "of the\n",
      "transformed vectors (See \n",
      " Equa tion 5-9\n",
      ").\n",
      " Under the Hood  |  171\n",
      "\n",
      " Eq u a t i o n 5-9. K er n e l t r i c k f o r a 2\n",
      " n d\n",
      " -d e g r e e p o l y n o m i a l m a p p i n g\n",
      "’\n",
      "a\n",
      "T\n",
      "’\n",
      "b\n",
      "=\n",
      "a\n",
      "1\n",
      "2\n",
      "2\n",
      "a\n",
      "1\n",
      "a\n",
      "2\n",
      "a\n",
      "2\n",
      "2\n",
      "T\n",
      "b\n",
      "1\n",
      "2\n",
      "2\n",
      "b\n",
      "1\n",
      "b\n",
      "2\n",
      "b\n",
      "2\n",
      "2\n",
      "=\n",
      "a\n",
      "1\n",
      "2\n",
      "b\n",
      "1\n",
      "2\n",
      " + 2\n",
      "a\n",
      "1\n",
      "b\n",
      "1\n",
      "a\n",
      "2\n",
      "b\n",
      "2\n",
      "+\n",
      "a\n",
      "2\n",
      "2\n",
      "b\n",
      "2\n",
      "2\n",
      "=\n",
      "a\n",
      "1\n",
      "b\n",
      "1\n",
      "+\n",
      "a\n",
      "2\n",
      "b\n",
      "2\n",
      "2\n",
      "=\n",
      "a\n",
      "1\n",
      "a\n",
      "2\n",
      "T\n",
      "b\n",
      "1\n",
      "b\n",
      "2\n",
      "2\n",
      "=\n",
      "a\n",
      "T\n",
      "b\n",
      "2\n",
      " H ow about tha t? The dot product of the transformed vectors is equal to the square of\n",
      "the dot product of the original vectors: \n",
      "’\n",
      "(\n",
      "a\n",
      ")\n",
      "T\n",
      " \n",
      "’\n",
      "(\n",
      "b\n",
      ") = (\n",
      "a\n",
      "T\n",
      " \n",
      "b\n",
      ")\n",
      "2\n",
      ".\n",
      " N ow here is the key insigh t: if you a pply the transforma tion \n",
      "’\n",
      " to all training instanƒ\n",
      "ces, then the dual problem (see \n",
      " Equa tion 5-6\n",
      " ) will con tain the dot product \n",
      "’\n",
      "(\n",
      "x\n",
      "(i)\n",
      ")\n",
      "T\n",
      "’\n",
      "(\n",
      "x\n",
      "(j)\n",
      "). But if \n",
      "’\n",
      " is the 2\n",
      "nd\n",
      " -degree polynomial transforma tion defined in \n",
      " Equa tion 5-8\n",
      ",\n",
      " then you can replace this dot product of transformed vectors sim ply by \n",
      "x\n",
      "i\n",
      "T\n",
      "x\n",
      "j\n",
      "2\n",
      ". So\n",
      " you don ‡ t actually need to transform the training instances a t all: just replace the dot\n",
      "product by its square in \n",
      " Equa tion 5-6\n",
      ". The result will be strictly the same as if you\n",
      " wen t through the trouble of actually transforming the training set then fitting a linear\n",
      " SVM algorithm, but this trick makes the whole process m uch more com puta tionally\n",
      " efficien t. This is the essence of the kernel trick.\n",
      "The function \n",
      "K\n",
      "(\n",
      "a\n",
      ", \n",
      "b\n",
      ") = (\n",
      "a\n",
      "T\n",
      " \n",
      "b\n",
      ")\n",
      "2\n",
      " is called a 2\n",
      "nd\n",
      "-degree \n",
      " p o l y n o m i a l k er n e l\n",
      ". \n",
      " In M achine\n",
      "Learning, a \n",
      " k er n e l\n",
      "  is a function ca pable of com puting the dot product \n",
      "’\n",
      "(\n",
      "a\n",
      ")\n",
      "T\n",
      " \n",
      "’\n",
      "(\n",
      "b\n",
      ")\n",
      "based only on the original vectors \n",
      "a\n",
      " and \n",
      "b\n",
      " , without ha ving to com pute (or even to\n",
      " know about) the transforma tion \n",
      "’\n",
      ". \n",
      " Equa tion 5-10\n",
      " lists some of the most commonly\n",
      "used kernels.\n",
      " Eq u a t i o n 5-10. C o m m o n k er n e l s\n",
      "Linear:\n",
      "K\n",
      "a\n",
      ",\n",
      "b\n",
      "=\n",
      "a\n",
      "T\n",
      "b\n",
      "Polynomial:\n",
      "K\n",
      "a\n",
      ",\n",
      "b\n",
      "=\n",
      "‘\n",
      "a\n",
      "T\n",
      "b\n",
      "+\n",
      "r\n",
      "d\n",
      " Gaussian RBF:\n",
      "K\n",
      "a\n",
      ",\n",
      "b\n",
      " = exp\n",
      "”\n",
      "‘\n",
      "a\n",
      "”\n",
      "b\n",
      "2\n",
      "Sigmoid:\n",
      "K\n",
      "a\n",
      ",\n",
      "b\n",
      " = tanh\n",
      "‘\n",
      "a\n",
      "T\n",
      "b\n",
      "+\n",
      "r\n",
      " 172  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "Mercer†s Theorem\n",
      " A ccording  to \n",
      " M er c er ‹ s t h e o r em\n",
      ", if a function \n",
      "K\n",
      "(\n",
      "a\n",
      ", \n",
      "b\n",
      " ) respects a few ma thema tical conƒ\n",
      "ditions called \n",
      " M er c er ‹ s c o n d i t i o ns\n",
      " \n",
      "(\n",
      "K\n",
      "  m ust be con tin uous, symmetric in its argumen ts\n",
      "so \n",
      "K\n",
      "(\n",
      "a\n",
      ", \n",
      "b\n",
      ") = \n",
      "K\n",
      "(\n",
      "b\n",
      ", \n",
      "a\n",
      "), etc.), then there exists a function \n",
      "’\n",
      "  tha t ma ps \n",
      "a\n",
      " \n",
      "and \n",
      "b\n",
      " \n",
      " in to\n",
      " another space (possibly with m uch higher dimensions) such tha t \n",
      "K\n",
      "(\n",
      "a\n",
      ", \n",
      "b\n",
      ") = \n",
      "’\n",
      "(\n",
      "a\n",
      ")\n",
      "T\n",
      " \n",
      "’\n",
      "(\n",
      "b\n",
      ").\n",
      "So you can use \n",
      "K\n",
      " as a kernel since you know \n",
      "’\n",
      "  exists, even if you don ‡ t know wha t \n",
      "’\n",
      " is. In the case of the Ga ussian RBF kernel, it can be shown tha t \n",
      "’\n",
      " \n",
      " actually ma ps each\n",
      " training instance to an infinite-dimensional space, so it ‡ s a good thing you don ‡ t need\n",
      " to actually perform the ma pping!\n",
      " N ote tha t some frequen tly used kernels (such as the Sigmoid kernel) don ‡ t respect all\n",
      " of M ercer‡ s conditions, yet they generally work well in practice.\n",
      " There is still one loose end we m ust tie. \n",
      " Equa tion 5-7\n",
      " shows how to go from the dual\n",
      " solution to the primal solution in the case of a linear SVM classifier , but if you a pply\n",
      " the kernel trick you end up with equa tions tha t include \n",
      "’\n",
      "(\n",
      "x\n",
      "(i)\n",
      "). In fact, \n",
      "w\n",
      "  m ust ha ve\n",
      " the same n umber of dimensions as \n",
      "’\n",
      "(\n",
      "x\n",
      "(i)\n",
      " ), which ma y be h uge or even infinite, so you\n",
      " can ‡ t com pute it. But how can you make predictions without knowing \n",
      "w\n",
      " ? W ell, the\n",
      " good news is tha t you can plug in the form ula for \n",
      "w\n",
      "   from \n",
      " Equa tion 5-7\n",
      " \n",
      " in to the deciƒ\n",
      "sion function for a new instance \n",
      "x\n",
      "(n)\n",
      " , and you get an equa tion with only dot products\n",
      " between in put vectors. This makes it possible to use the kernel trick, once again\n",
      "(\n",
      " Equa tion 5-11\n",
      ").\n",
      " Eq u a t i o n 5-11. M a k i n g p r e d i c t i o ns w i t h a k er n e l iz e d SVM\n",
      "h\n",
      "w\n",
      ",\n",
      "b\n",
      "’\n",
      "x\n",
      "n\n",
      "=\n",
      "w\n",
      "T\n",
      "’\n",
      "x\n",
      "n\n",
      "+\n",
      "b\n",
      "=\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "‰\n",
      "i\n",
      "t\n",
      "i\n",
      "’\n",
      "x\n",
      "i\n",
      "T\n",
      "’\n",
      "x\n",
      "n\n",
      "+\n",
      "b\n",
      "=\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "‰\n",
      "i\n",
      "t\n",
      "i\n",
      "’\n",
      "x\n",
      "i\n",
      "T\n",
      "’\n",
      "x\n",
      "n\n",
      "+\n",
      "b\n",
      "=\n",
      "“\n",
      "i\n",
      " = 1\n",
      "‰\n",
      "i\n",
      " > 0\n",
      "m\n",
      "‰\n",
      "i\n",
      "t\n",
      "i\n",
      "K\n",
      "x\n",
      "i\n",
      ",\n",
      "x\n",
      "n\n",
      "+\n",
      "b\n",
      " N ote tha t since \n",
      "‰\n",
      "(i)\n",
      "   š\n",
      "  0 only for support vectors, making predictions in volves com putƒ\n",
      " ing the dot product of the new in put vector \n",
      "x\n",
      "(n)\n",
      " with only the support vectors, not all\n",
      " the training instances. Of course, you also need to com pute the bias term \n",
      "b\n",
      ", using the\n",
      "same trick (\n",
      " Equa tion 5-12\n",
      ").\n",
      " Under the Hood  |  173\n",
      "\n",
      " Eq u a t i o n 5-12. C o m p u t i n g t h e b i as t er m u s i n g t h e k er n e l t r i c k\n",
      "b\n",
      "=\n",
      "1\n",
      "n\n",
      "s\n",
      "“\n",
      "i\n",
      " = 1\n",
      "‰\n",
      "i\n",
      " > 0\n",
      "m\n",
      "t\n",
      "i\n",
      "”\n",
      "w\n",
      "T\n",
      "’\n",
      "x\n",
      "i\n",
      "=\n",
      "1\n",
      "n\n",
      "s\n",
      "“\n",
      "i\n",
      " = 1\n",
      "‰\n",
      "i\n",
      " > 0\n",
      "m\n",
      "t\n",
      "i\n",
      "”\n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "‰\n",
      "j\n",
      "t\n",
      "j\n",
      "’\n",
      "x\n",
      "j\n",
      "T\n",
      "’\n",
      "x\n",
      "i\n",
      "=\n",
      "1\n",
      "n\n",
      "s\n",
      "“\n",
      "i\n",
      " = 1\n",
      "‰\n",
      "i\n",
      " > 0\n",
      "m\n",
      "t\n",
      "i\n",
      "”\n",
      "“\n",
      "j\n",
      " = 1\n",
      "‰\n",
      "j\n",
      " > 0\n",
      "m\n",
      "‰\n",
      "j\n",
      "t\n",
      "j\n",
      "K\n",
      "x\n",
      "i\n",
      ",\n",
      "x\n",
      "j\n",
      " If you are starting to get a headache, it ‡ s perfectly normal: it ‡ s an unfortuna te side\n",
      "effect of the kernel trick.\n",
      "Online SVMs\n",
      " B efore \n",
      " concluding this cha pter , let ‡ s take a quick look a t online SVM classifiers (recall\n",
      " tha t online learning means learning incremen tally , typically as new instances arrive).\n",
      " F or linear SVM classifiers, one method is to use Gradien t Descen t (e.g., using\n",
      "SGDClassifier\n",
      ") to minimize the cost function in \n",
      " Equa tion 5-13\n",
      ", which is derived\n",
      " from the primal problem. U nfortuna tely it con verges m uch more slowly than the\n",
      " methods based on QP .\n",
      " Eq u a t i o n 5-13. L i n e a r SVM \n",
      "classi†er\n",
      "  c o s t f u n c t i o n\n",
      "J\n",
      "w\n",
      ",\n",
      "b\n",
      "=\n",
      "1\n",
      "2\n",
      "w\n",
      "T\n",
      "w\n",
      "+\n",
      "C\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "max\n",
      " 0, 1 ”\n",
      "t\n",
      "i\n",
      "w\n",
      "T\n",
      "x\n",
      "i\n",
      "+\n",
      "b\n",
      " The first sum in the cost function will push the model to ha ve a small weigh t vector\n",
      "w\n",
      " , leading to a larger margin. The second sum com putes the total of all margin violaƒ\n",
      " tions. An instance ‡ s margin viola tion is equal to 0 if it is loca ted off the street and on\n",
      "the correct side, or else it is proportional to the distance to the correct side of the\n",
      " street. Minimizing this term ensures tha t the model makes the margin viola tions as\n",
      "small and as few as possible\n",
      "Hinge Loss\n",
      "The function \n",
      " m ax\n",
      "(0, 1 − \n",
      "t\n",
      ") is called the \n",
      " h i n ge l o s s\n",
      "  function (represen ted below). I t is\n",
      "equal to 0 when \n",
      "t\n",
      "   Ž\n",
      "  1. I ts deriva tive (slope) is equal to −1 if \n",
      "t\n",
      " \n",
      "< 1 and 0 if \n",
      "t\n",
      " \n",
      " > 1. I t is not\n",
      " differen tiable a t \n",
      "t\n",
      " = 1, but just like for Lasso Regression (see \n",
      " —Lasso Regression –\n",
      " \n",
      "on\n",
      "page \n",
      "139\n",
      " ) you can still use Gradien t Descen t using an y \n",
      " s u b d er i v a t i v e\n",
      "  a t \n",
      "t\n",
      " \n",
      " = 1 (i.e., an y\n",
      "value between −1 and 0).\n",
      " 174  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "8\n",
      " —Incremen tal and Decremen tal Support V ector M achine Learning, – G. Ca uwenberghs, T . P oggio (2001).\n",
      "9\n",
      " —F ast K ernel Classifiers with Online and A ctive Learning,— A. B ordes, S. Ertekin, J . W eston, L. B ottou (2005).\n",
      " I t is also possible to im plemen t online kernelized SVMs›for exam ple, using \n",
      "—Increƒ\n",
      " men tal and Decremen tal SVM Learning –\n",
      "8\n",
      "   or \n",
      " —F ast K ernel Classifiers with Online and\n",
      " A ctive Learning. –\n",
      "9\n",
      "  H owever , these are im plemen ted in M a tlab and C++. F or large-\n",
      " scale nonlinear problems, you ma y wan t to consider using neural networks instead \n",
      "(see \n",
      " P art II\n",
      ").\n",
      "Exercises\n",
      "1.\n",
      " Wha t is the fundamen tal idea behind Support V ector M achines?\n",
      "2.\n",
      " Wha t is a support vector?\n",
      "3.\n",
      " Wh y is it im portan t to scale the in puts when using SVMs?\n",
      "4.\n",
      "Can an SVM classifier output a confidence score when it classifies an instance?\n",
      " Wha t about a probability?\n",
      "5.\n",
      "Should you use the primal or the dual form of the SVM problem to train a model\n",
      " on a training set with millions of instances and h undreds of fea tures?\n",
      "6.\n",
      " Sa y you trained an SVM classifier with an RBF kernel. I t seems to underfit the\n",
      "training set: should you increase or decrease \n",
      "‘\n",
      " (\n",
      "gamma\n",
      " )? Wha t about \n",
      "C\n",
      "?\n",
      "7.\n",
      " H ow should you set the QP parameters (\n",
      "H\n",
      ", \n",
      "f\n",
      ", \n",
      "A\n",
      ", and \n",
      "b\n",
      ") to solve the soft margin\n",
      "linear SVM classifier problem using an off-the-shelf QP solver?\n",
      "8.\n",
      " T rain a \n",
      "LinearSVC\n",
      "  on a linearly separable da taset. Then train an \n",
      "SVC\n",
      " \n",
      "and a\n",
      "SGDClassifier\n",
      "  on the same da taset. See if you can get them to produce roughly\n",
      "the same model.\n",
      "9.\n",
      " T rain an SVM classifier on the MNIST da taset. Since SVM classifiers are binar y\n",
      " classifiers, you will need to use one-versus-all to classif y all 10 digits. Y ou ma y\n",
      " Exercises  |  175\n",
      "\n",
      " wan t to tune the h yperparameters using small valida tion sets to speed up the proƒ\n",
      " cess. Wha t accuracy can you reach?\n",
      "10.\n",
      " T rain an SVM regressor on the California housing da taset.\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " 176  |  Chapter 5: Support Vector Machines\n",
      "\n",
      "CHAPTER 6\n",
      "Decision Trees\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 6 in the final\n",
      "release of the book.\n",
      "Like SVMs, \n",
      " D e ci s i o n T r e e s\n",
      "  are versa tile M achine Learning algorithms tha t can perƒ\n",
      " form both classifica tion and regression tasks, and even m ultioutput tasks. They are\n",
      " ver y powerful algorithms, ca pable of fitting com plex da tasets. F or exam ple, in \n",
      " Cha pƒ\n",
      "ter 2\n",
      " \n",
      "you trained a \n",
      "DecisionTreeRegressor\n",
      " \n",
      " model on the California housing da taset,\n",
      "fitting it perfectly (actually overfitting it).\n",
      " Decision T rees are also the fundamen tal com ponen ts of R andom F orests (see \n",
      " Cha pƒ\n",
      "ter 7\n",
      " ), which are among the most powerful M achine Learning algorithms a vailable\n",
      " toda y .\n",
      " In this cha pter we will start by discussing how to train, visualize, and make predicƒ\n",
      " tions with Decision T rees. Then we will go through the CAR T training algorithm\n",
      "used by Scikit-Learn, and we will discuss how to regularize trees and use them for\n",
      " regression tasks. Finally , we will discuss some of the limita tions of Decision T rees.\n",
      "Training and Visualizing a Decision Tree\n",
      " T o  understand Decision T rees, let ‡ s just build one and take a look a t how it makes preƒ\n",
      "dictions. The following code trains a \n",
      "DecisionTreeClassifier\n",
      "  on the iris da taset\n",
      "(see \n",
      " Cha pter 4\n",
      "):\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "load_iris\n",
      "from\n",
      " \n",
      "sklearn.tree\n",
      " \n",
      "import\n",
      " \n",
      "DecisionTreeClassifier\n",
      "177\n",
      "\n",
      "1\n",
      " Gra ph viz is an open source gra ph visualiza tion software package, a vailable a t \n",
      " h ttp://w w w .g r a p h v iz.o r g/\n",
      ".\n",
      "iris\n",
      " \n",
      "=\n",
      " \n",
      "load_iris\n",
      "()\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      ".\n",
      "data\n",
      "[:,\n",
      " \n",
      "2\n",
      ":]\n",
      " \n",
      "# petal length and width\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      ".\n",
      "target\n",
      "tree_clf\n",
      " \n",
      "=\n",
      " \n",
      "DecisionTreeClassifier\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ")\n",
      "tree_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " Y ou can visualize the trained Decision T ree by first using the \n",
      "export_graphviz()\n",
      " \n",
      " method to output a gra ph definition file called \n",
      " i r i s_t r e e.d o t\n",
      ":\n",
      "from\n",
      " \n",
      "sklearn.tree\n",
      " \n",
      "import\n",
      " \n",
      "export_graphviz\n",
      "export_graphviz\n",
      "(\n",
      "        \n",
      "tree_clf\n",
      ",\n",
      "        \n",
      "out_file\n",
      "=\n",
      "image_path\n",
      "(\n",
      "\"iris_tree.dot\"\n",
      "),\n",
      "        \n",
      "feature_names\n",
      "=\n",
      "iris\n",
      ".\n",
      "feature_names\n",
      "[\n",
      "2\n",
      ":],\n",
      "        \n",
      "class_names\n",
      "=\n",
      "iris\n",
      ".\n",
      "target_names\n",
      ",\n",
      "        \n",
      "rounded\n",
      "=\n",
      "True\n",
      ",\n",
      "        \n",
      "filled\n",
      "=\n",
      "True\n",
      "    \n",
      ")\n",
      " Then you can con vert this \n",
      " .d o t\n",
      "  file to a variety of forma ts such as PDF or PNG using\n",
      "the \n",
      "dot\n",
      " command-line tool from the \n",
      " g r a p h v iz\n",
      " \n",
      "package.\n",
      "1\n",
      "  This command line con verts\n",
      "the \n",
      " .d o t\n",
      " file to a \n",
      " .p n g\n",
      " image file:\n",
      "$ dot -Tpng iris_tree.dot -o iris_tree.png\n",
      " Y our first decision tree looks like \n",
      "Figure 6-1\n",
      ".\n",
      " 178  |  Chapter 6: Decision Trees\n",
      "\n",
      " F i g u r e 6-1. I r i s D e ci s i o n T r e e\n",
      "Making Predictions\n",
      " Let ‡ s see how the tree represen ted in \n",
      "Figure 6-1\n",
      " makes predictions. Suppose you find\n",
      " an iris flower and you wan t to classif y it. Y ou start a t the \n",
      " r o o t n o d e\n",
      "  (depth 0, a t the\n",
      " top): this node asks whether the flower‡ s petal length is smaller than 2.45 cm. If it is,\n",
      " then you move down to the root ‡ s left child node (depth 1, left). In this case, it is a \n",
      " l e a f\n",
      " n o d e\n",
      "  (i.e., it does not ha ve an y children nodes), so it does not ask an y questions: you\n",
      " can sim ply look a t the predicted class for tha t node and the Decision T ree predicts\n",
      " tha t your flower is an Iris-Setosa (\n",
      "class=setosa\n",
      ").\n",
      " N ow suppose you find another flower , but this time the petal length is grea ter than\n",
      " 2.45 cm. Y ou m ust move down to the root ‡ s righ t child node (depth 1, righ t), which is\n",
      "not a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\n",
      " it is, then your flower is most likely an Iris-V ersicolor (depth 2, left). If not, it is likely\n",
      " an Iris-V irginica (depth 2, righ t). I t ‡ s really tha t sim ple.\n",
      " One of the man y qualities of Decision T rees is tha t they require\n",
      " ver y little da ta prepara tion. In particular , they don ‡ t require fea ture\n",
      " scaling or cen tering a t all.\n",
      " Making Predictions  |  179\n",
      "\n",
      " A node ‡ s \n",
      "samples\n",
      "  a ttribute coun ts how man y training instances it a pplies to . F or\n",
      " exam ple, 100 training instances ha ve a petal length grea ter than 2.45 cm (depth 1,\n",
      " righ t), among which 54 ha ve a petal width smaller than 1.75 cm (depth 2, left). A\n",
      " node ‡ s \n",
      "value\n",
      "  a ttribute tells you how man y training instances of each class this node\n",
      " a pplies to: for exam ple, the bottom-righ t node a pplies to 0 Iris-Setosa, 1 Iris-\n",
      " V ersicolor , and 45 Iris-V irginica. Finally , a node ‡ s \n",
      "gini\n",
      "  a ttribute measures its \n",
      " i m p u r‡\n",
      " i ty\n",
      " : a node is — pure – (\n",
      "gini=0\n",
      " ) if all training instances it a pplies to belong to the same\n",
      " class. F or exam ple, since the depth-1 left node a pplies only to Iris-Setosa training\n",
      "instances, it is pure and its \n",
      "gini\n",
      " \n",
      "score is 0. \n",
      " Equa tion 6-1\n",
      " \n",
      "shows how the training algoƒ\n",
      " rithm com putes the gini score \n",
      "G\n",
      "i\n",
      " of the i\n",
      "th\n",
      "  node. F or exam ple, the depth-2 left node\n",
      "has a \n",
      "gini\n",
      " score equal to 1 − (0/54)\n",
      "2\n",
      " − (49/54)\n",
      "2\n",
      " − (5/54)\n",
      "2\n",
      " \n",
      "ı\n",
      " 0.168. Another \n",
      " i m p u r i ty\n",
      " m e as u r e\n",
      "  is discussed shortly .\n",
      " Eq u a t i o n 6-1. G i n i i m p u r i ty\n",
      "G\n",
      "i\n",
      " = 1 ”\n",
      "“\n",
      "k\n",
      " = 1\n",
      "n\n",
      "p\n",
      "i\n",
      ",\n",
      "k\n",
      "2\n",
      "⁄\n",
      "p\n",
      "i\n",
      ",\n",
      "k\n",
      "  is the ra tio of class \n",
      "k\n",
      " instances among the training instances in the \n",
      "i\n",
      "th\n",
      " node.\n",
      " Scikit-Learn  uses the \n",
      " CAR T algorithm, which produces \n",
      "only \n",
      " b i n a r y\n",
      " t r e e s\n",
      " : nonleaf nodes alwa ys ha ve two children (i.e., questions only\n",
      " ha ve yes/no answers). H owever , other algorithms such as ID3 can\n",
      " produce Decision T rees with nodes tha t ha ve more than two chilƒ\n",
      "dren.\n",
      "Figure 6-2\n",
      " \n",
      " shows this Decision T ree ‡ s decision boundaries. The thick vertical line repƒ\n",
      " resen ts the decision boundar y of the root node (depth 0): petal length = 2.45 cm.\n",
      " Since the left area is pure (only Iris-Setosa), it cannot be split an y further . H owever ,\n",
      " the righ t area is im pure, so the depth-1 righ t node splits it a t petal width = 1.75 cm\n",
      " (represen ted by the dashed line). Since \n",
      "max_depth\n",
      "  was set to 2, the Decision T ree\n",
      " stops righ t there. H owever , if you set \n",
      "max_depth\n",
      " to 3, then the two depth-2 nodes\n",
      " would each add another decision boundar y (represen ted by the dotted lines).\n",
      " 180  |  Chapter 6: Decision Trees\n",
      "\n",
      " F i g u r e 6-2. D e ci s i o n T r e e d e ci s i o n b o u n d a r i e s\n",
      "Model Interpretation: White Box Versus Black Box\n",
      " As you can see Decision T rees are fairly in tuitive and their decisions are easy to in terƒ\n",
      "pret. Such models are often called \n",
      " w h i t e b o x m o d e l s\n",
      " . In con trast, as we will see, R anƒ\n",
      " dom F orests or neural networks are generally considered \n",
      " b l a c k b o x m o d e l s\n",
      ". They\n",
      " make grea t predictions, and you can easily check the calcula tions tha t they performed\n",
      " to make these predictions; nevertheless, it is usually hard to explain in sim ple terms\n",
      " wh y the predictions were made. F or exam ple, if a neural network sa ys tha t a particuƒ\n",
      " lar person a ppears on a picture, it is hard to know wha t actually con tributed to this\n",
      " prediction: did the model recognize tha t person ‡ s eyes? H er mouth? H er nose? H er\n",
      " shoes? Or even the couch tha t she was sitting on? Con versely , Decision T rees provide\n",
      " nice and sim ple classifica tion rules tha t can even be a pplied man ually if need be (e.g.,\n",
      " for flower classifica tion).\n",
      "Estimating Class Probabilities\n",
      "A \n",
      " Decision T ree can also estima te the probability tha t an instance belongs to a particƒ\n",
      "ular class \n",
      "k\n",
      " : first it tra verses the tree to find the leaf node for this instance, and then it\n",
      " returns the ra tio of training instances of class \n",
      "k\n",
      "  in this node. F or exam ple, suppose\n",
      " you ha ve found a flower whose petals are 5 cm long and 1.5 cm wide. The correƒ\n",
      " sponding leaf node is the depth-2 left node, so the Decision T ree should output the\n",
      " following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-V ersicolor (49/54),\n",
      " and 9.3% for Iris-V irginica (5/54). And of course if you ask it to predict the class, it\n",
      " should output Iris-V ersicolor (class 1) since it has the highest probability . Let ‡ s check\n",
      "this:\n",
      ">>> \n",
      "tree_clf\n",
      ".\n",
      "predict_proba\n",
      "([[\n",
      "5\n",
      ",\n",
      " \n",
      "1.5\n",
      "]])\n",
      "array([[0.        , 0.90740741, 0.09259259]])\n",
      " Estimating Class Probabilities  |  181\n",
      "\n",
      ">>> \n",
      "tree_clf\n",
      ".\n",
      "predict\n",
      "([[\n",
      "5\n",
      ",\n",
      " \n",
      "1.5\n",
      "]])\n",
      "array([1])\n",
      " P erfect! N otice tha t the estima ted probabilities would be iden tical an ywhere else in\n",
      " the bottom-righ t rectangle of \n",
      "Figure 6-2\n",
      " ›for exam ple, if the petals were 6 cm long\n",
      " and 1.5 cm wide (even though it seems obvious tha t it would most likely be an Iris-\n",
      " V irginica in this case).\n",
      "The CART Training Algorithm\n",
      "Scikit-Learn uses the \n",
      "Classi†cation\n",
      "  An d R e g r e s s i o n T r e e\n",
      "  (CAR T) algorithm to train\n",
      " Decision T rees (also called — growing – trees). The idea is really quite sim ple: the algoƒ\n",
      " rithm first splits the training set in two subsets using a single fea ture \n",
      "k\n",
      " \n",
      "and a thresƒ\n",
      "hold \n",
      "t\n",
      "k\n",
      "   (e.g., — petal length  ł\n",
      "  2.45 cm –). H ow does it choose \n",
      "k\n",
      "   and \n",
      "t\n",
      "k\n",
      " ? I t searches for the\n",
      "pair (\n",
      "k\n",
      ", \n",
      "t\n",
      "k\n",
      " ) tha t produces the purest subsets (weigh ted by their size). The cost function\n",
      " tha t the algorithm tries to minimize is given by \n",
      " Equa tion 6-2\n",
      ".\n",
      " Eq u a t i o n 6-2. CAR T c o s t f u n c t i o n f o r \n",
      "classi†cation\n",
      "J\n",
      "k\n",
      ",\n",
      "t\n",
      "k\n",
      "=\n",
      "m\n",
      "left\n",
      "m\n",
      "G\n",
      "left\n",
      "+\n",
      "m\n",
      "right\n",
      "m\n",
      "G\n",
      "right\n",
      "where\n",
      "G\n",
      "left/right\n",
      " measures the impurity of the left/right subset,\n",
      "m\n",
      "left/right\n",
      " is the number of instances in the left/right subset.\n",
      " Once it has successfully split the training set in two , it splits the subsets using the\n",
      " same logic, then the sub-subsets and so on, recursively . I t stops recursing once it reaƒ\n",
      " ches the maxim um depth (defined by the \n",
      "max_depth\n",
      "  h yperparameter), or if it cannot\n",
      " find a split tha t will reduce im purity . A few other h yperparameters (described in a\n",
      " momen t) con trol additional stopping conditions (\n",
      "min_samples_split\n",
      ", \n",
      "min_sam\n",
      "ples_leaf\n",
      ", \n",
      "min_weight_fraction_leaf\n",
      ", and \n",
      "max_leaf_nodes\n",
      ").\n",
      " 182  |  Chapter 6: Decision Trees\n",
      "\n",
      "2\n",
      " P is the set of problems tha t can be solved in polynomial time. NP is the set of problems whose solutions can\n",
      " be verified in polynomial time. An NP -H ard problem is a problem to which an y NP problem can be reduced\n",
      " in polynomial time. An NP -Com plete problem is both NP and NP -H ard. A ma jor open ma thema tical quesƒ\n",
      " tion is whether or not P = NP . If P š NP (which seems likely), then no polynomial algorithm will ever be\n",
      " found for an y NP -Com plete problem (except perha ps on a quan tum com puter).\n",
      "3\n",
      " l og\n",
      "2\n",
      "  is the binar y logarithm. I t is equal to \n",
      " l og\n",
      "2\n",
      "(\n",
      "m\n",
      ") = \n",
      " l og\n",
      "(\n",
      "m\n",
      ") / \n",
      " l og\n",
      "(2).\n",
      "4\n",
      " A reduction of en tropy is often called an \n",
      " i n f o r m a t i o n ga i n\n",
      ".\n",
      " As you can see, the CAR T algorithm is \n",
      "a \n",
      " g r e e d y a l go r i t h m\n",
      ": it greedƒ\n",
      " ily searches for an optim um split a t the top level, then repea ts the\n",
      " process a t each level. I t does not check whether or not the split will\n",
      " lead to the lowest possible im purity several levels down. A greedy\n",
      "algorithm often produces a reasonably good solution, but it is not\n",
      " guaran teed to be the optimal solution.\n",
      " U nfortuna tely , finding the optimal tree is known to be an \n",
      " NP -\n",
      " C o m p l e t e\n",
      " \n",
      "problem:\n",
      "2\n",
      " it requires \n",
      "O\n",
      "(exp(\n",
      "m\n",
      ")) time, making the probƒ\n",
      " lem in tractable even for fairly small training sets. This is wh y we\n",
      " m ust settle for a — reasonably good – solution.\n",
      "Computational Complexity\n",
      " M aking predictions requires tra versing the Decision T ree from the root to a leaf.\n",
      " Decision T rees are generally a pproxima tely balanced, so tra versing the Decision T ree\n",
      "requires going through roughly \n",
      "O\n",
      "(\n",
      " l og\n",
      "2\n",
      "(\n",
      "m\n",
      ")) nodes.\n",
      "3\n",
      " Since each node only requires\n",
      " checking the value of one fea ture, the overall prediction com plexity is just \n",
      "O\n",
      "(\n",
      " l og\n",
      "2\n",
      "(\n",
      "m\n",
      ")),\n",
      " independen t of the n umber of fea tures. So predictions are ver y fast, even when dealƒ\n",
      "ing with large training sets.\n",
      " H owever , the training algorithm com pares all fea tures (or less if \n",
      "max_features\n",
      " is set)\n",
      " on all sam ples a t each node. This results in a training com plexity of \n",
      "O\n",
      "(\n",
      "n\n",
      "   „ \n",
      "m\n",
      " \n",
      " l og\n",
      "(\n",
      "m\n",
      ")).\n",
      " F or small training sets (less than a few thousand instances), Scikit-Learn can speed up\n",
      " training by presorting the da ta (set \n",
      "presort=True\n",
      "), but this slows down training conƒ\n",
      "siderably for larger training sets.\n",
      "Gini Impurity or Entropy?\n",
      " By defa ult, the Gini im purity measure is used, but you can select the \n",
      " en t r o p y\n",
      "   im purity\n",
      "measure instead by setting the \n",
      "criterion\n",
      "   h yperparameter to \n",
      "\"entropy\"\n",
      ". The concept\n",
      " of en tropy origina ted in thermodynamics as a measure of molecular disorder :\n",
      " en tropy a pproaches zero when molecules are still and well ordered. I t la ter spread to a\n",
      " wide variety of domains, including Shannon ‡ s \n",
      " i n f o r m a t i o n t h e o r y\n",
      ", where it measures\n",
      " the a verage informa tion con ten t of a message:\n",
      "4\n",
      "  en tropy is zero when all messages are\n",
      " iden tical. In M achine Learning, it is frequen tly used as an im purity measure: a set ‡ s\n",
      " Computational Complexity  |  183\n",
      "\n",
      "5\n",
      " See Sebastian R aschka ‡ s \n",
      " in teresting analysis for more details\n",
      ".\n",
      " en tropy is zero when it con tains instances of only one class. \n",
      " Equa tion 6-3\n",
      " \n",
      "shows the\n",
      " definition of the en tropy of the i\n",
      "th\n",
      "  node. F or exam ple, the depth-2 left node in\n",
      "Figure 6-1\n",
      "  has an en tropy equal to \n",
      "”\n",
      "49\n",
      "54\n",
      "log\n",
      "2\n",
      "49\n",
      "54\n",
      "”\n",
      "5\n",
      "54\n",
      "log\n",
      "2\n",
      "5\n",
      "54\n",
      " ı 0.445.\n",
      " Eq u a t i o n 6-3. E n t r o p y\n",
      "H\n",
      "i\n",
      " = ”\n",
      "“\n",
      "k\n",
      " = 1\n",
      "p\n",
      "i\n",
      ",\n",
      "k\n",
      " š 0\n",
      "n\n",
      "p\n",
      "i\n",
      ",\n",
      "k\n",
      "log\n",
      "2\n",
      "p\n",
      "i\n",
      ",\n",
      "k\n",
      " So should you use Gini im purity or en tropy? The truth is, most of the time it does not\n",
      " make a big difference: they lead to similar trees. Gini im purity is sligh tly faster to\n",
      " com pute, so it is a good defa ult. H owever , when they differ , Gini im purity tends to\n",
      " isola te the most frequen t class in its own branch of the tree, while en tropy tends to\n",
      " produce sligh tly more balanced trees.\n",
      "5\n",
      "Regularization Hyperparameters\n",
      " Decision T rees \n",
      " make ver y few assum ptions about the training da ta (as opposed to linƒ\n",
      " ear models, which obviously assume tha t the da ta is linear , for exam ple). If left\n",
      " unconstrained, the tree structure will ada pt itself to the training da ta, fitting it ver y\n",
      " closely , and most likely overfitting it. Such a model is often called a \n",
      " n o n p a r a m e t r i c\n",
      " m o d e l\n",
      ", \n",
      " not beca use it does not ha ve an y parameters (it often has a lot) but beca use the\n",
      " n umber of parameters is not determined prior to training, so the model structure is\n",
      " free to stick closely to the da ta. In con trast, \n",
      "a \n",
      " p a r a m e t r i c m o d e l\n",
      " \n",
      "such as a linear model\n",
      " has a predetermined n umber of parameters, so its degree of freedom is limited,\n",
      "reducing the risk of overfitting (but increasing the risk of underfitting).\n",
      " T o a void \n",
      " overfitting the training da ta, you need to restrict the Decision T ree ‡ s freedom\n",
      " during training. As you know by now , this is called regulariza tion. The regulariza tion\n",
      " h yperparameters depend on the algorithm used, but generally you can a t least restrict\n",
      " the maxim um depth of the Decision T ree. In Scikit-Learn, this is con trolled by the\n",
      "max_depth\n",
      "  h yperparameter (the defa ult value is \n",
      "None\n",
      ", which means unlimited).\n",
      "Reducing \n",
      "max_depth\n",
      "  will regularize the model and th us reduce the risk of overfitting.\n",
      "The \n",
      "DecisionTreeClassifier\n",
      "  class has a few other parameters tha t similarly restrict\n",
      " the sha pe of the Decision T ree: \n",
      "min_samples_split\n",
      "  (the minim um n umber of samƒ\n",
      " ples a node m ust ha ve before it can be split), \n",
      "min_samples_leaf\n",
      " \n",
      " (the minim um n umƒ\n",
      " ber of sam ples a leaf node m ust ha ve), \n",
      "min_weight_fraction_leaf\n",
      " \n",
      "(same as\n",
      "min_samples_leaf\n",
      "  but expressed as a fraction of the total n umber of weigh ted\n",
      " 184  |  Chapter 6: Decision Trees\n",
      "\n",
      "instances), \n",
      "max_leaf_nodes\n",
      "  (maxim um n umber of leaf nodes), and \n",
      "max_features\n",
      " (maxim um n umber of fea tures tha t are evalua ted for splitting a t each node). Increasƒ\n",
      "ing \n",
      "min_*\n",
      "  h yperparameters or reducing \n",
      "max_*\n",
      "  h yperparameters will regularize the\n",
      "model.\n",
      " Other algorithms work by first training the Decision T ree without\n",
      "restrictions, then \n",
      " p r u n i n g\n",
      "  (deleting) unnecessar y nodes. A node\n",
      " whose children are all leaf nodes is considered unnecessar y if the\n",
      " purity im provemen t it provides is not \n",
      " s t a t i s t i c a l l y \n",
      "signi†cant\n",
      ". Stanƒ\n",
      " dard sta tistical tests, such as the \n",
      "™\n",
      "2\n",
      " \n",
      " t e s t\n",
      " , are used to estima te the\n",
      " probability tha t the im provemen t is purely the result of chance\n",
      "(which is called \n",
      "the \n",
      " n u l l h y p o t h e s i s\n",
      " ). If this probability , called the \n",
      "p-\n",
      " v a l u e\n",
      " , is higher than a given threshold (typically 5%, con trolled by\n",
      " a h yperparameter), then the node is considered unnecessar y and its\n",
      " children are deleted. The pruning con tin ues un til all unnecessar y\n",
      " nodes ha ve been pruned.\n",
      "Figure 6-3\n",
      "  shows two Decision T rees trained on the moons da taset (in troduced in\n",
      " Cha pter 5\n",
      " ). On the left, the Decision T ree is trained with the defa ult h yperparameters\n",
      " (i.e., no restrictions), and on the righ t the Decision T ree is trained with \n",
      "min_sam\n",
      "ples_leaf=4\n",
      " . I t is quite obvious tha t the model on the left is overfitting, and the\n",
      " model on the righ t will probably generalize better .\n",
      " F i g u r e 6-3. R e g u l a r iz a t i o n u s i n g m i n_s a m p l e s_l e a f\n",
      "Regression\n",
      " Decision T rees are also ca pable of performing regression tasks. Let ‡ s build a regresƒ\n",
      " sion tree using Scikit-Learn ‡ s \n",
      "DecisionTreeRegressor\n",
      " class, training it on a noisy\n",
      " quadra tic da taset with \n",
      "max_depth=2\n",
      ":\n",
      "from\n",
      " \n",
      "sklearn.tree\n",
      " \n",
      "import\n",
      " \n",
      "DecisionTreeRegressor\n",
      " Regression  |  185\n",
      "\n",
      "tree_reg\n",
      " \n",
      "=\n",
      " \n",
      "DecisionTreeRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ")\n",
      "tree_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " The resulting tree is represen ted on \n",
      "Figure 6-4\n",
      ".\n",
      " F i g u r e 6-4. A D e ci s i o n T r e e f o r r e g r e s s i o n\n",
      " This tree looks ver y similar to the classifica tion tree you built earlier . The main differƒ\n",
      " ence is tha t instead of predicting a class in each node, it predicts a value. F or exam ple,\n",
      " suppose you wan t to make a prediction for a new instance with \n",
      "x\n",
      "1\n",
      " \n",
      " = 0.6. Y ou tra verse\n",
      " the tree starting a t the root, and you even tually reach the leaf node tha t predicts\n",
      "value=0.1106\n",
      " . This prediction is sim ply the a verage target value of the 110 training\n",
      " instances associa ted to this leaf node. This prediction results in a M ean Squared Error\n",
      "(MSE) equal to 0.0151 over these 110 instances.\n",
      " This model ‡ s predictions are represen ted on the left of \n",
      "Figure 6-5\n",
      ". If you set\n",
      "max_depth=3\n",
      " , you get the predictions represen ted on the righ t. N otice how the preƒ\n",
      " dicted value for each region is alwa ys the a verage target value of the instances in tha t\n",
      " region. The algorithm splits each region in a wa y tha t makes most training instances\n",
      " as close as possible to tha t predicted value.\n",
      " 186  |  Chapter 6: Decision Trees\n",
      "\n",
      " F i g u r e 6-5. P r e d i c t i o ns o f tw o D e ci s i o n T r e e r e g r e s s i o n m o d e l s\n",
      "The \n",
      " CAR T algorithm works mostly the same wa y as earlier , except tha t instead of tr yƒ\n",
      " ing to split the training set in a wa y tha t minimizes im purity , it now tries to split the\n",
      " training set in a wa y tha t minimizes the MSE. \n",
      " Equa tion 6-4\n",
      " \n",
      "shows the cost function\n",
      " tha t the algorithm tries to minimize.\n",
      " Eq u a t i o n 6-4. CAR T c o s t f u n c t i o n f o r r e g r e s s i o n\n",
      "J\n",
      "k\n",
      ",\n",
      "t\n",
      "k\n",
      "=\n",
      "m\n",
      "left\n",
      "m\n",
      "MSE\n",
      "left\n",
      "+\n",
      "m\n",
      "right\n",
      "m\n",
      "MSE\n",
      "right\n",
      "where\n",
      "MSE\n",
      "node\n",
      "=\n",
      "“\n",
      "i\n",
      "node\n",
      "y\n",
      "node\n",
      "”\n",
      "y\n",
      "i\n",
      "2\n",
      "y\n",
      "node\n",
      "=\n",
      "1\n",
      "m\n",
      "node\n",
      "“\n",
      "i\n",
      "node\n",
      "y\n",
      "i\n",
      " J ust like for classifica tion tasks, Decision T rees are prone to overfitting when dealing\n",
      " with regression tasks. W ithout an y regulariza tion (i.e., using the defa ult h yperparaƒ\n",
      "meters), you get the predictions on the left of \n",
      "Figure 6-6\n",
      " . I t is obviously overfitting\n",
      " the training set ver y badly . J ust setting \n",
      "min_samples_leaf=10\n",
      "  results in a m uch more\n",
      " reasonable model, represen ted on the righ t of \n",
      "Figure 6-6\n",
      ".\n",
      " F i g u r e 6-6. R e g u l a r izi n g a D e ci s i o n T r e e r e g r e s s o r\n",
      " Regression  |  187\n",
      "\n",
      "6\n",
      " I t randomly selects the set of fea tures to evalua te a t each node.\n",
      "Instability\n",
      " H opefully by now you are con vinced tha t Decision T rees ha ve a lot going for them:\n",
      " they are sim ple to understand and in terpret, easy to use, versa tile, and powerful.\n",
      " H owever they do ha ve a few limita tions. First, as you ma y ha ve noticed, Decision\n",
      " T rees love orthogonal decision boundaries (all splits are perpendicular to an axis),\n",
      " which makes them sensitive to training set rota tion. F or exam ple, \n",
      "Figure 6-7\n",
      " \n",
      "shows a\n",
      " sim ple linearly separable da taset: on the left, a Decision T ree can split it easily , while\n",
      " on the righ t, after the da taset is rota ted by 45‘, the decision boundar y looks unnecesƒ\n",
      " sarily con voluted. Although both Decision T rees fit the training set perfectly , it is ver y\n",
      " likely tha t the model on the righ t will not generalize well. One wa y to limit this probƒ\n",
      "lem is to use PCA (see \n",
      " Cha pter 8\n",
      " ), which often results in a better orien ta tion of the\n",
      " training da ta.\n",
      " F i g u r e 6-7. S ens i t i v i ty t o t r a i n i n g s e t r o t a t i o n\n",
      " M ore generally , the main issue with Decision T rees is tha t they are ver y sensitive to\n",
      " small varia tions in the training da ta. F or exam ple, if you just remove the widest Iris-\n",
      " V ersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\n",
      " and train a new Decision T ree, you ma y get the model represen ted in \n",
      "Figure 6-8\n",
      ". As\n",
      " you can see, it looks ver y differen t from the previous Decision T ree (\n",
      "Figure 6-2\n",
      ").\n",
      " A ctually , since the training algorithm used by Scikit-Learn is stochastic\n",
      "6\n",
      " \n",
      " you ma y\n",
      " get ver y differen t models even on the same training da ta (unless you set the\n",
      "random_state\n",
      "  h yperparameter).\n",
      " 188  |  Chapter 6: Decision Trees\n",
      "\n",
      " F i g u r e 6-8. S ens i t i v i ty t o t r a i n i n g s e t d e t a i l s\n",
      " R andom F orests \n",
      " can limit this instability by a veraging predictions over man y trees, as\n",
      " we will see in the next cha pter .\n",
      "Exercises\n",
      "1.\n",
      " Wha t is the a pproxima te depth of a Decision T ree trained (without restrictions)\n",
      "on a training set with 1 million instances?\n",
      "2.\n",
      " I s a node ‡ s Gini im purity generally lower or grea ter than its paren t ‡ s? I s it \n",
      " gen er‡\n",
      " a l l y\n",
      "  lower/grea ter , or \n",
      " a l w a ys\n",
      "  lower/grea ter?\n",
      "3.\n",
      " If a Decision T ree is overfitting the training set, is it a good idea to tr y decreasing\n",
      "max_depth\n",
      "?\n",
      "4.\n",
      " If a Decision T ree is underfitting the training set, is it a good idea to tr y scaling\n",
      " the in put fea tures?\n",
      "5.\n",
      " If it takes one hour to train a Decision T ree on a training set con taining 1 million\n",
      " instances, roughly how m uch time will it take to train another Decision T ree on a\n",
      " training set con taining 10 million instances?\n",
      "6.\n",
      " If your training set con tains 100,000 instances, will setting \n",
      "presort=True\n",
      " \n",
      "speed\n",
      "up training?\n",
      "7.\n",
      " T rain and fine-tune a Decision T ree for the moons da taset.\n",
      "a.\n",
      " Genera te a moons da taset using \n",
      "make_moons(n_samples=10000, noise=0.4)\n",
      ".\n",
      " b .\n",
      " Split it in to a training set and a test set using \n",
      "train_test_split()\n",
      ".\n",
      " Exercises  |  189\n",
      "\n",
      "c.\n",
      " U se grid search with cross-valida tion (with the help of the \n",
      "GridSearchCV\n",
      " class) to find good h yperparameter values for a \n",
      "DecisionTreeClassifier\n",
      ". \n",
      " Hin t: tr y various values for \n",
      "max_leaf_nodes\n",
      ".\n",
      "d.\n",
      " T rain it on the full training set using these h yperparameters, and measure\n",
      " your model ‡ s performance on the test set. Y ou should get roughly 85% to 87%\n",
      " accuracy .\n",
      "8.\n",
      "Grow a forest.\n",
      "a.\n",
      " Con tin uing the previous exercise, genera te 1,000 subsets of the training set,\n",
      " each con taining 100 instances selected randomly . Hin t: you can use Scikit-\n",
      " Learn ‡ s \n",
      "ShuffleSplit\n",
      " class for this.\n",
      " b .\n",
      " T rain one Decision T ree on each subset, using the best h yperparameter values\n",
      " found above. E valua te these 1,000 Decision T rees on the test set. Since they\n",
      " were trained on smaller sets, these Decision T rees will likely perform worse\n",
      " than the first Decision T ree, achieving only about 80% accuracy .\n",
      "c.\n",
      " N ow comes the magic. F or each test set instance, genera te the predictions of\n",
      " the 1,000 Decision T rees, and keep only the most frequen t prediction (you can\n",
      " use SciPy‡ s \n",
      "mode()\n",
      " function for this). This gives you \n",
      " m a jo r i ty-v o t e p r e d i c t i o ns\n",
      "over the test set.\n",
      "d.\n",
      " E valua te these predictions on the test set: you should obtain a sligh tly higher\n",
      " accuracy than your first model (about 0.5 to 1.5% higher). Congra tula tions,\n",
      " you ha ve trained a R andom F orest classifier!\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " 190  |  Chapter 6: Decision Trees\n",
      "\n",
      "CHAPTER 7\n",
      "Ensemble Learning and Random Forests\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 7 in the final\n",
      "release of the book.\n",
      " Suppose you ask a com plex question to thousands of random people, then aggrega te\n",
      " their answers. In man y cases you will find tha t this aggrega ted answer is better than\n",
      " an expert ‡ s answer . This is called the \n",
      " w i s d o m o f t h e cr o w d\n",
      " . Similarly , if you aggrega te\n",
      "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
      " often get better predictions than with the best individual predictor . A group of preƒ\n",
      "dictors is called an \n",
      " ens em b l e\n",
      " ; th us, this technique is called \n",
      " E ns em b l e L e a r n i n g\n",
      ", and an\n",
      "Ensemble Learning algorithm is called an \n",
      " E ns em b l e m e t h o d\n",
      ".\n",
      " F or exam ple, you can train a group of Decision T ree classifiers, each on a differen t\n",
      " random subset of the training set. T o make predictions, you just obtain the predicƒ\n",
      " tions of all individual trees, then predict the class tha t gets the most votes (see the last\n",
      "exercise in \n",
      " Cha pter 6\n",
      " ). Such an ensemble of Decision T rees is called a \n",
      " R a n d o m F o r e s t\n",
      ", \n",
      " and despite its sim plicity , this is one of the most powerful M achine Learning algoƒ\n",
      " rithms a vailable toda y .\n",
      " M oreover , as we discussed in \n",
      " Cha pter 2\n",
      ", you will often use Ensemble methods near\n",
      " the end of a project, once you ha ve already built a few good predictors, to combine\n",
      " them in to an even better predictor . In fact, the winning solutions in M achine Learnƒ\n",
      " ing com petitions often in volve several Ensemble methods (most famously in the \n",
      " N etƒ\n",
      " flix Prize com petition\n",
      ").\n",
      " In this cha pter we will discuss the most popular Ensemble methods, including \n",
      " b a g‡\n",
      " g i n g\n",
      ", \n",
      " b o o s t i n g\n",
      ", \n",
      " s t a c k i n g\n",
      " , and a few others. W e will also explore R andom F orests.\n",
      "191\n",
      "\n",
      "Voting \n",
      "Classi•ers\n",
      " Suppose you ha ve trained a few classifiers, each one achieving about 80% accuracy .\n",
      " Y ou ma y ha ve a Logistic Regression classifier , an SVM classifier , a R andom F orest\n",
      " classifier , a K-N earest N eighbors classifier , and perha ps a few more (see \n",
      "Figure 7-1\n",
      ").\n",
      " F i g u r e 7-1. T r a i n i n g d i v er s e \n",
      "classi†ers\n",
      " A ver y sim ple wa y to crea te an even better classifier is to aggrega te the predictions of\n",
      " each classifier and predict the class tha t gets the most votes. This ma jority-vote classiƒ\n",
      "fier is called a \n",
      " h a r d v o t i n g\n",
      " classifier (see \n",
      "Figure 7-2\n",
      ").\n",
      " F i g u r e 7-2. H a r d v o t i n g \n",
      "classi†er\n",
      "  p r e d i c t i o ns\n",
      " 192  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      " Somewha t surprisingly , this voting classifier often achieves a higher accuracy than the\n",
      "best classifier in the ensemble. In fact, even if each classifier is a \n",
      " w e a k l e a r n er\n",
      " \n",
      "(meanƒ\n",
      " ing it does only sligh tly better than random guessing), the ensemble can still be a\n",
      " s t r o n g l e a r n er\n",
      "  (achieving high accuracy), provided there are a sufficien t n umber of\n",
      " weak learners and they are sufficien tly diverse.\n",
      " H ow is this possible? The following analog y can help shed some ligh t on this m yster y .\n",
      " Suppose you ha ve a sligh tly biased coin tha t has a 51% chance of coming up heads,\n",
      "and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\n",
      " more or less 510 heads and 490 tails, and hence a ma jority of heads. If you do the\n",
      " ma th, you will find tha t the probability of obtaining a ma jority of heads after 1,000\n",
      "tosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\n",
      "with 10,000 tosses, the probability climbs over 97%). This is due to the \n",
      " l a w o f l a r ge\n",
      " n u m b er s\n",
      " : as you keep tossing the coin, the ra tio of heads gets closer and closer to the\n",
      "probability of heads (51%). \n",
      "Figure 7-3\n",
      "  shows 10 series of biased coin tosses. Y ou can\n",
      " see tha t as the n umber of tosses increases, the ra tio of heads a pproaches 51%. E ven tuƒ\n",
      " ally all 10 series end up so close to 51% tha t they are consisten tly above 50%.\n",
      " F i g u r e 7-3. \n",
      "•e\n",
      "  l a w o f l a r ge n u m b er s\n",
      " Similarly , suppose you build an ensemble con taining 1,000 classifiers tha t are individƒ\n",
      "ually correct only 51% of the time (barely better than random guessing). If you preƒ\n",
      " dict the ma jority voted class, you can hope for up to 75% accuracy! H owever , this is\n",
      " only true if all classifiers are perfectly independen t, making uncorrela ted errors,\n",
      " which is clearly not the case since they are trained on the same da ta. They are likely to\n",
      " make the same types of errors, so there will be man y ma jority votes for the wrong\n",
      " class, reducing the ensemble ‡ s accuracy .\n",
      "Voting \n",
      "Classi•ers\n",
      "   |  193\n",
      "\n",
      "Ensemble methods work best when the predictors are as independƒ\n",
      " en t from one another as possible. One wa y to get diverse classifiers\n",
      " is to train them using ver y differen t algorithms. This increases the\n",
      " chance tha t they will make ver y differen t types of errors, im proving\n",
      " the ensemble ‡ s accuracy .\n",
      " The following code crea tes and trains a voting classifier in Scikit-Learn, com posed of\n",
      " three diverse classifiers (the training set is the moons da taset, \n",
      " in troduced in \n",
      " Cha pƒ\n",
      "ter 5\n",
      "):\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "RandomForestClassifier\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "VotingClassifier\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "LogisticRegression\n",
      "from\n",
      " \n",
      "sklearn.svm\n",
      " \n",
      "import\n",
      " \n",
      "SVC\n",
      "log_clf\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "()\n",
      "rnd_clf\n",
      " \n",
      "=\n",
      " \n",
      "RandomForestClassifier\n",
      "()\n",
      "svm_clf\n",
      " \n",
      "=\n",
      " \n",
      "SVC\n",
      "()\n",
      "voting_clf\n",
      " \n",
      "=\n",
      " \n",
      "VotingClassifier\n",
      "(\n",
      "    \n",
      "estimators\n",
      "=\n",
      "[(\n",
      "•lr•\n",
      ",\n",
      " \n",
      "log_clf\n",
      "),\n",
      " \n",
      "(\n",
      "•rf•\n",
      ",\n",
      " \n",
      "rnd_clf\n",
      "),\n",
      " \n",
      "(\n",
      "•svc•\n",
      ",\n",
      " \n",
      "svm_clf\n",
      ")],\n",
      "    \n",
      "voting\n",
      "=\n",
      "•hard•\n",
      ")\n",
      "voting_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " Let ‡ s look a t each classifier‡ s accuracy on the test set:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "accuracy_score\n",
      ">>> \n",
      "for\n",
      " \n",
      "clf\n",
      " \n",
      "in\n",
      " \n",
      "(\n",
      "log_clf\n",
      ",\n",
      " \n",
      "rnd_clf\n",
      ",\n",
      " \n",
      "svm_clf\n",
      ",\n",
      " \n",
      "voting_clf\n",
      "):\n",
      "... \n",
      "    \n",
      "clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "... \n",
      "    \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "clf\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test\n",
      ")\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "clf\n",
      ".\n",
      "__class__\n",
      ".\n",
      "__name__\n",
      ",\n",
      " \n",
      "accuracy_score\n",
      "(\n",
      "y_test\n",
      ",\n",
      " \n",
      "y_pred\n",
      "))\n",
      "...\n",
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.888\n",
      "VotingClassifier 0.904\n",
      " There you ha ve it! The voting classifier sligh tly outperforms all the individual classifiƒ\n",
      "ers.\n",
      " If all classifiers are able to estima te class probabilities (i.e., they ha ve a \n",
      "pre\n",
      "dict_proba()\n",
      " method), then you can tell Scikit-Learn to predict the class with the\n",
      " highest class probability , a veraged over all the individual classifiers. This is called \n",
      "so“\n",
      " v o t i n g\n",
      " . I t often achieves higher performance than hard voting beca use it gives more\n",
      " weigh t to highly confiden t votes. All you need to do is replace \n",
      "voting=\"hard\"\n",
      " \n",
      "with\n",
      "voting=\"soft\"\n",
      "  and ensure tha t all classifiers can estima te class probabilities. This is\n",
      "not the case of the \n",
      "SVC\n",
      " \n",
      " class by defa ult, so you need to set its \n",
      "probability\n",
      "   h yperparaƒ\n",
      "meter to \n",
      "True\n",
      "   (this will make the \n",
      "SVC\n",
      "   class use cross-valida tion to estima te class probƒ\n",
      "abilities, slowing down training, and it will add a \n",
      "predict_proba()\n",
      " \n",
      "method). If you\n",
      " 194  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "1\n",
      " —Bagging Predictors, – L. Breiman (1996).\n",
      "2\n",
      " In sta tistics, resam pling with replacemen t is called \n",
      " b o o ts t r a p p i n g\n",
      ".\n",
      "3\n",
      " —P asting small votes for classifica tion in large da tabases and on-line, – L. Breiman (1999).\n",
      " modif y the preceding code to use soft voting, you will find tha t the voting classifier\n",
      "achieves over 91.2% accuracy!\n",
      "Bagging and Pasting\n",
      " One wa y to get a diverse set of classifiers is to use ver y differen t training algorithms,\n",
      " as just discussed. Another a pproach is to use the same training algorithm for ever y\n",
      " predictor , but to train them on differen t random subsets of the training set. When\n",
      " sam pling is performed \n",
      " w i t h\n",
      "  replacemen t, this method is called \n",
      " b a g g i n g\n",
      "1\n",
      " \n",
      "(short for\n",
      " b o o ts t r a p a g g r e ga t i n g\n",
      "2\n",
      " ). When sam pling is performed \n",
      " w i t h o u t\n",
      " \n",
      " replacemen t, it is called\n",
      " p as t i n g\n",
      ".\n",
      "3\n",
      " In other words, both bagging and pasting allow training instances to be sam pled sevƒ\n",
      " eral times across m ultiple predictors, but only bagging allows training instances to be\n",
      " sam pled several times for the same predictor . This sam pling and training process is\n",
      " represen ted in \n",
      "Figure 7-4\n",
      ".\n",
      " F i g u r e 7-4. P as t i n g/b a g g i n g t r a i n i n g s e t s a m p l i n g a n d t r a i n i n g\n",
      "Once all predictors are trained, the ensemble can make a prediction for a new\n",
      " instance by sim ply aggrega ting the predictions of all predictors. The aggrega tion\n",
      "function is typically the \n",
      " s t a t i s t i c a l m o d e\n",
      "  (i.e., the most frequen t prediction, just like a\n",
      " hard voting classifier) for classifica tion, or the a verage for regression. Each individual\n",
      " Bagging and Pasting  |  195\n",
      "\n",
      "4\n",
      " Bias and variance were in troduced in \n",
      " Cha pter 4\n",
      ".\n",
      "5\n",
      "max_samples\n",
      "  can alterna tively be set to a floa t between 0.0 and 1.0, in which case the max n umber of instances\n",
      " to sam ple is equal to the size of the training set times \n",
      "max_samples\n",
      ".\n",
      "predictor has a higher bias than if it were trained on the original training set, but\n",
      " aggrega tion reduces both bias and variance.\n",
      "4\n",
      "  Generally , the net result is tha t the\n",
      "ensemble has a similar bias but a lower variance than a single predictor trained on the\n",
      "original training set.\n",
      "As you can see in \n",
      "Figure 7-4\n",
      " , predictors can all be trained in parallel, via differen t\n",
      " CPU cores or even differen t ser vers. Similarly , predictions can be made in parallel.\n",
      " This is one of the reasons wh y bagging and pasting are such popular methods: they\n",
      " scale ver y well.\n",
      "Bagging and Pasting in Scikit-Learn\n",
      " Scikit-Learn offers a sim ple API for both bagging and pasting with the \n",
      "BaggingClas\n",
      "sifier\n",
      " class (or \n",
      "BaggingRegressor\n",
      " for regression). The following code trains an\n",
      " ensemble of  500 Decision T ree classifiers,\n",
      "5\n",
      " \n",
      "each trained on 100 training instances ranƒ\n",
      " domly sam pled from the training set with replacemen t (this is an exam ple of bagging,\n",
      " but if you wan t to use pasting instead, just set \n",
      "bootstrap=False\n",
      "). The \n",
      "n_jobs\n",
      "   paramƒ\n",
      " eter tells Scikit-Learn the n umber of CPU cores to use for training and predictions\n",
      " (−1 tells Scikit-Learn to use all a vailable cores):\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "BaggingClassifier\n",
      "from\n",
      " \n",
      "sklearn.tree\n",
      " \n",
      "import\n",
      " \n",
      "DecisionTreeClassifier\n",
      "bag_clf\n",
      " \n",
      "=\n",
      " \n",
      "BaggingClassifier\n",
      "(\n",
      "    \n",
      "DecisionTreeClassifier\n",
      "(),\n",
      " \n",
      "n_estimators\n",
      "=\n",
      "500\n",
      ",\n",
      "    \n",
      "max_samples\n",
      "=\n",
      "100\n",
      ",\n",
      " \n",
      "bootstrap\n",
      "=\n",
      "True\n",
      ",\n",
      " \n",
      "n_jobs\n",
      "=-\n",
      "1\n",
      ")\n",
      "bag_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "bag_clf\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test\n",
      ")\n",
      "The \n",
      "BaggingClassifier\n",
      "  a utoma tically performs soft voting\n",
      " instead of hard voting if the base classifier can estima te class probaƒ\n",
      "bilities (i.e., if it has a \n",
      "predict_proba()\n",
      " method), which is the case\n",
      " with Decision T rees classifiers.\n",
      "Figure 7-5\n",
      "  com pares the decision boundar y of a single Decision T ree with the deciƒ\n",
      " sion boundar y of a bagging ensemble of 500 trees (from the preceding code), both\n",
      " trained on the moons da taset. As you can see, the ensemble ‡ s predictions will likely\n",
      " generalize m uch better than the single Decision T ree ‡ s predictions: the ensemble has a\n",
      " com parable bias but a smaller variance (it makes roughly the same n umber of errors\n",
      " on the training set, but the decision boundar y is less irregular).\n",
      " 196  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "6\n",
      "As \n",
      "m\n",
      "  grows, this ra tio a pproaches 1 − exp(−1) ı 63.212%.\n",
      " F i g u r e 7-5. A s i n gl e D e ci s i o n T r e e v er s u s a b a g g i n g ens em b l e o f 500 t r e e s\n",
      " B ootstra pping in troduces a bit more diversity in the subsets tha t each predictor is\n",
      " trained on, so bagging ends up with a sligh tly higher bias than pasting, but this also\n",
      " means tha t predictors end up being less correla ted so the ensemble ‡ s variance is\n",
      " reduced. O verall, bagging often results in better models, which explains wh y it is genƒ\n",
      " erally preferred. H owever , if you ha ve spare time and CPU power you can use cross-\n",
      " valida tion to evalua te both bagging and pasting and select the one tha t works best.\n",
      "Out-of-Bag Evaluation\n",
      " W ith bagging, some instances ma y be sam pled several times for an y given predictor ,\n",
      " while others ma y not be sam pled a t all. By defa ult a \n",
      "BaggingClassifier\n",
      "  sam ples \n",
      "m\n",
      " training instances with replacemen t (\n",
      "bootstrap=True\n",
      "), where \n",
      "m\n",
      " \n",
      "is the size of the\n",
      " training set. This means tha t only about 63% of the training instances are sam pled on\n",
      " a verage for each predictor .\n",
      "6\n",
      "  The remaining 37% of the training instances tha t are not\n",
      " sam pled are called \n",
      " o u t-o f-b a g\n",
      "  (oob) instances. N ote tha t they are not the same 37%\n",
      "for all predictors.\n",
      " Since a predictor never sees the oob instances during training, it can be evalua ted on\n",
      " these instances, without the need for a separa te valida tion set. Y ou can evalua te the\n",
      " ensemble itself by a veraging out the oob evalua tions of each predictor .\n",
      "In Scikit-Learn, you can set \n",
      "oob_score=True\n",
      "  when crea ting a \n",
      "BaggingClassifier\n",
      " \n",
      "to\n",
      " request an a utoma tic oob evalua tion after training. The following code demonstra tes\n",
      " this. The resulting evalua tion score is a vailable through the \n",
      "oob_score_\n",
      " variable:\n",
      ">>> \n",
      "bag_clf\n",
      " \n",
      "=\n",
      " \n",
      "BaggingClassifier\n",
      "(\n",
      "... \n",
      "    \n",
      "DecisionTreeClassifier\n",
      "(),\n",
      " \n",
      "n_estimators\n",
      "=\n",
      "500\n",
      ",\n",
      "... \n",
      "    \n",
      "bootstrap\n",
      "=\n",
      "True\n",
      ",\n",
      " \n",
      "n_jobs\n",
      "=-\n",
      "1\n",
      ",\n",
      " \n",
      "oob_score\n",
      "=\n",
      "True\n",
      ")\n",
      "...\n",
      ">>> \n",
      "bag_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " Bagging and Pasting  |  197\n",
      "\n",
      "7\n",
      " —Ensembles on R andom P a tches, – G. Louppe and P . Geurts (2012).\n",
      "8\n",
      " — The random subspace method for constructing decision forests, – T in Kam H o (1998).\n",
      ">>> \n",
      "bag_clf\n",
      ".\n",
      "oob_score_\n",
      "0.90133333333333332\n",
      " A ccording to this oob evalua tion, this \n",
      "BaggingClassifier\n",
      " is likely to achieve about\n",
      " 90.1% accuracy on the test set. Let ‡ s verif y this:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "accuracy_score\n",
      ">>> \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "bag_clf\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test\n",
      ")\n",
      ">>> \n",
      "accuracy_score\n",
      "(\n",
      "y_test\n",
      ",\n",
      " \n",
      "y_pred\n",
      ")\n",
      "0.91200000000000003\n",
      " W e get 91.2% accuracy on the test set›close enough!\n",
      " The oob decision function for each training instance is also a vailable through the\n",
      "oob_decision_function_\n",
      "  variable. In this case (since the base estima tor has a \n",
      "pre\n",
      "dict_proba()\n",
      " method) the decision function returns the class probabilities for each\n",
      " training instance. F or exam ple, the oob evalua tion estima tes tha t the first training\n",
      "instance has a 68.25% probability of belonging to the positive class (and 31.75% of\n",
      " belonging to the nega tive class):\n",
      ">>> \n",
      "bag_clf\n",
      ".\n",
      "oob_decision_function_\n",
      "array([[0.31746032, 0.68253968],\n",
      "       [0.34117647, 0.65882353],\n",
      "       [1.        , 0.        ],\n",
      "       ...\n",
      "       [1.        , 0.        ],\n",
      "       [0.03108808, 0.96891192],\n",
      "       [0.57291667, 0.42708333]])\n",
      "Random Patches and Random Subspaces\n",
      "The \n",
      "BaggingClassifier\n",
      "  class supports sam pling the fea tures as well. This is conƒ\n",
      " trolled by two h yperparameters: \n",
      "max_features\n",
      "   and \n",
      "bootstrap_features\n",
      ". They work\n",
      " the same wa y as \n",
      "max_samples\n",
      " and \n",
      "bootstrap\n",
      " , but for fea ture sam pling instead of\n",
      " instance sam pling. Th us, each predictor will be trained on a random subset of the\n",
      " in put fea tures.\n",
      " This is particularly useful when you are dealing with high-dimensional in puts (such\n",
      " as images). Sam pling both training instances and fea tures is called the \n",
      " R a n d o m\n",
      " P a t c h e s\n",
      " \n",
      "method\n",
      ".\n",
      "7\n",
      "  K eeping all training instances (i.e., \n",
      "bootstrap=False\n",
      " and \n",
      "max_sam\n",
      "ples=1.0\n",
      " ) but sam pling fea tures (i.e., \n",
      "bootstrap_features=True\n",
      " \n",
      "and/or \n",
      "max_fea\n",
      "tures\n",
      " smaller than 1.0) is called the \n",
      " R a n d o m S u b s p a c e s\n",
      " method\n",
      ".\n",
      "8\n",
      " 198  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "9\n",
      " —R andom Decision F orests, – T . H o (1995).\n",
      "10\n",
      "The \n",
      "BaggingClassifier\n",
      "  class remains useful if you wan t a bag of something other than Decision T rees.\n",
      "11\n",
      "There are a few notable exceptions: \n",
      "splitter\n",
      "  is absen t (forced to \n",
      "\"random\"\n",
      "), \n",
      "presort\n",
      "  is absen t (forced to\n",
      "False\n",
      "), \n",
      "max_samples\n",
      "  is absen t (forced to \n",
      "1.0\n",
      "), and \n",
      "base_estimator\n",
      "  is absen t (forced to \n",
      "DecisionTreeClassi\n",
      "fier\n",
      "  with the provided h yperparameters).\n",
      " Sam pling fea tures results in even more predictor diversity , trading a bit more bias for\n",
      "a lower variance.\n",
      "Random Forests\n",
      " As we ha ve discussed, a \n",
      " R andom F orest\n",
      "9\n",
      "  is an ensemble of Decision T rees, generally\n",
      "trained via the bagging method (or sometimes pasting), typically with \n",
      "max_samples\n",
      "set to the size of the training set. Instead of building a \n",
      "BaggingClassifier\n",
      " and passƒ\n",
      "ing it a \n",
      "DecisionTreeClassifier\n",
      ", you can instead use the \n",
      "RandomForestClassifier\n",
      " class, which is more con venien t and optimized for Decision T rees\n",
      "10\n",
      "   (similarly , there is\n",
      "a \n",
      "RandomForestRegressor\n",
      " class for regression tasks). The following code trains a\n",
      " R andom F orest classifier with 500 trees (each limited to maxim um 16 nodes), using\n",
      " all a vailable CPU cores:\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "RandomForestClassifier\n",
      "rnd_clf\n",
      " \n",
      "=\n",
      " \n",
      "RandomForestClassifier\n",
      "(\n",
      "n_estimators\n",
      "=\n",
      "500\n",
      ",\n",
      " \n",
      "max_leaf_nodes\n",
      "=\n",
      "16\n",
      ",\n",
      " \n",
      "n_jobs\n",
      "=-\n",
      "1\n",
      ")\n",
      "rnd_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "y_pred_rf\n",
      " \n",
      "=\n",
      " \n",
      "rnd_clf\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test\n",
      ")\n",
      " W ith a few exceptions, a \n",
      "RandomForestClassifier\n",
      "  has all the h yperparameters of a\n",
      "DecisionTreeClassifier\n",
      "  (to con trol how trees are grown), plus all the h yperparaƒ\n",
      "meters of a \n",
      "BaggingClassifier\n",
      "  to con trol the ensemble itself.\n",
      "11\n",
      " The R andom F orest algorithm in troduces extra randomness when growing trees;\n",
      " instead of searching for the ver y best fea ture when splitting a node (see \n",
      " Cha pter 6\n",
      "), it\n",
      " searches for the best fea ture among a random subset of fea tures. This results in a\n",
      " grea ter tree diversity , which (once again) trades a higher bias for a lower variance,\n",
      "generally yielding an overall better model. The following \n",
      "BaggingClassifier\n",
      " \n",
      "is\n",
      " roughly equivalen t to the previous \n",
      "RandomForestClassifier\n",
      ":\n",
      "bag_clf\n",
      " \n",
      "=\n",
      " \n",
      "BaggingClassifier\n",
      "(\n",
      "    \n",
      "DecisionTreeClassifier\n",
      "(\n",
      "splitter\n",
      "=\n",
      "\"random\"\n",
      ",\n",
      " \n",
      "max_leaf_nodes\n",
      "=\n",
      "16\n",
      "),\n",
      "    \n",
      "n_estimators\n",
      "=\n",
      "500\n",
      ",\n",
      " \n",
      "max_samples\n",
      "=\n",
      "1.0\n",
      ",\n",
      " \n",
      "bootstrap\n",
      "=\n",
      "True\n",
      ",\n",
      " \n",
      "n_jobs\n",
      "=-\n",
      "1\n",
      ")\n",
      " Random Forests  |  199\n",
      "\n",
      "12\n",
      " —Extremely randomized trees, – P . Geurts, D . Ernst, L. W ehenkel (2005).\n",
      "Extra-Trees\n",
      "When \n",
      " you are growing a tree in a R andom F orest, a t each node only a random subset\n",
      " of the fea tures is considered for splitting (as discussed earlier). I t is possible to make\n",
      " trees even more random by also using random thresholds for each fea ture ra ther than\n",
      " searching for the best possible thresholds (like regular Decision T rees do).\n",
      " A forest of such extremely random trees is sim ply called an \n",
      " E xt r em e l y R a n d o m iz e d\n",
      " T r e e s\n",
      " \n",
      "ensemble\n",
      "12\n",
      " (or \n",
      " E xt r a-T r e e s\n",
      " for short). Once again, this trades more bias for a\n",
      " lower variance. I t also makes Extra-T rees m uch faster to train than regular R andom\n",
      " F orests since finding the best possible threshold for each fea ture a t ever y node is one\n",
      "of the most time-consuming tasks of growing a tree.\n",
      " Y ou can crea te an Extra-T rees classifier using Scikit-Learn ‡ s \n",
      "ExtraTreesClassifier\n",
      " class. I ts API is iden tical to the \n",
      "RandomForestClassifier\n",
      "  class. Similarly , the \n",
      "Extra\n",
      "TreesRegressor\n",
      " class has the same API as the \n",
      "RandomForestRegressor\n",
      " class.\n",
      " I t is hard to tell in advance whether a \n",
      "RandomForestClassifier\n",
      "will perform better or worse than an \n",
      "ExtraTreesClassifier\n",
      ". Genƒ\n",
      " erally , the only wa y to know is to tr y both and com pare them using\n",
      " cross-valida tion (and tuning the h yperparameters using grid\n",
      "search).\n",
      "Feature Importance\n",
      " Y et another grea t quality of R andom F orests is tha t they make it easy to measure the \n",
      " rela tive im portance of each fea ture. Scikit-Learn measures a fea ture ‡ s im portance by\n",
      " looking a t how m uch the tree nodes tha t use tha t fea ture reduce im purity on a verage\n",
      " (across all trees in the forest). M ore precisely , it is a weigh ted a verage, where each\n",
      " node ‡ s weigh t is equal to the n umber of training sam ples tha t are associa ted with it\n",
      "(see \n",
      " Cha pter 6\n",
      ").\n",
      " Scikit-Learn com putes this score a utoma tically for each fea ture after training, then it\n",
      " scales the results so tha t the sum of all im portances is equal to 1. Y ou can access the\n",
      "result using the \n",
      "feature_importances_\n",
      "  variable. F or exam ple, the following code\n",
      "trains a \n",
      "RandomForestClassifier\n",
      "  on the iris da taset (in troduced in \n",
      " Cha pter 4\n",
      ") and\n",
      " outputs each fea ture ‡ s im portance. I t seems tha t the most im portan t fea tures are the\n",
      " petal length (44%) and width (42%), while sepal length and width are ra ther unimƒ\n",
      " portan t in com parison (11% and 2%, respectively).\n",
      " 200  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "load_iris\n",
      ">>> \n",
      "iris\n",
      " \n",
      "=\n",
      " \n",
      "load_iris\n",
      "()\n",
      ">>> \n",
      "rnd_clf\n",
      " \n",
      "=\n",
      " \n",
      "RandomForestClassifier\n",
      "(\n",
      "n_estimators\n",
      "=\n",
      "500\n",
      ",\n",
      " \n",
      "n_jobs\n",
      "=-\n",
      "1\n",
      ")\n",
      ">>> \n",
      "rnd_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "iris\n",
      "[\n",
      "\"data\"\n",
      "],\n",
      " \n",
      "iris\n",
      "[\n",
      "\"target\"\n",
      "])\n",
      ">>> \n",
      "for\n",
      " \n",
      "name\n",
      ",\n",
      " \n",
      "score\n",
      " \n",
      "in\n",
      " \n",
      "zip\n",
      "(\n",
      "iris\n",
      "[\n",
      "\"feature_names\"\n",
      "],\n",
      " \n",
      "rnd_clf\n",
      ".\n",
      "feature_importances_\n",
      "):\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "name\n",
      ",\n",
      " \n",
      "score\n",
      ")\n",
      "...\n",
      "sepal length (cm) 0.112492250999\n",
      "sepal width (cm) 0.0231192882825\n",
      "petal length (cm) 0.441030464364\n",
      "petal width (cm) 0.423357996355\n",
      " Similarly , if you train a R andom F orest classifier on the MNIST da taset (in troduced\n",
      "in \n",
      " Cha pter 3\n",
      " ) and plot each pixel ‡ s im portance, you get the image represen ted in\n",
      "Figure 7-6\n",
      ".\n",
      " F i g u r e 7-6. MNIS T p ix e l i m p o r t a n c e (a c c o r d i n g t o a R a n d o m F o r e s t \n",
      "classi†er)\n",
      " R andom F orests are ver y handy to get a quick understanding of wha t fea tures\n",
      " actually ma tter , in particular if you need to perform fea ture selection.\n",
      "Boosting\n",
      " B o o s t i n g\n",
      " (originally called \n",
      " h y p o t h e s i s b o o s t i n g\n",
      " ) refers to an y Ensemble method tha t\n",
      " can combine several weak learners in to a strong learner . The general idea of most\n",
      " boosting methods is to train predictors sequen tially , each tr ying to correct its predeƒ\n",
      " cessor . There are man y boosting methods a vailable, but by far the most popular are\n",
      " Boosting  |  201\n",
      "\n",
      "13\n",
      " — A Decision-Theoretic Generaliza tion of On-Line Learning and an A pplica tion to B oosting, – Y oa v Freund,\n",
      " Robert E. Scha pire (1997).\n",
      "14\n",
      " This is just for illustra tive purposes. SVMs are generally not good base predictors for A daB oost, beca use they\n",
      " are slow and tend to be unstable with A daB oost.\n",
      " Ad aB o o s t\n",
      "13\n",
      " (short for \n",
      " Ad a p t i v e B o o s t i n g\n",
      ") and \n",
      " G r a d i en t B o o s t i n g\n",
      " . Let ‡ s start with A daƒ\n",
      " B oost.\n",
      "AdaBoost\n",
      " One wa y for a new predictor to correct its predecessor is to pa y a bit more a tten tion\n",
      " to the training instances tha t the predecessor underfitted. This results in new predicƒ\n",
      " tors focusing more and more on the hard cases. This is the technique used by A daƒ\n",
      " B oost.\n",
      " F or exam ple, to build an A daB oost classifier , a first base classifier (such as a Decision\n",
      " T ree) is trained and used to make predictions on the training set. The rela tive weigh t\n",
      "of misclassified training instances is then increased. A second classifier is trained\n",
      " using the upda ted weigh ts and again it makes predictions on the training set, weigh ts\n",
      " are upda ted, and so on (see \n",
      "Figure 7-7\n",
      ").\n",
      " F i g u r e 7-7. Ad aB o o s t s e q u en t i a l t r a i n i n g w i t h i ns t a n c e w ei gh t u p d a t e s\n",
      "Figure 7-8\n",
      " shows the decision boundaries of five consecutive predictors on the\n",
      " moons da taset (in this exam ple, each predictor is a highly regularized SVM classifier\n",
      "with an RBF kernel\n",
      "14\n",
      " ). The first classifier gets man y instances wrong, so their weigh ts\n",
      " 202  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "get boosted. The second classifier therefore does a better job on these instances, and\n",
      " so on. The plot on the righ t represen ts the same sequence of predictors except tha t\n",
      " the learning ra te is halved (i.e., the misclassified instance weigh ts are boosted half as\n",
      " m uch a t ever y itera tion). As you can see, this sequen tial learning technique has some\n",
      " similarities with Gradien t Descen t, except tha t instead of tweaking a single predictor‡ s\n",
      " parameters to minimize a cost function, A daB oost adds predictors to the ensemble,\n",
      " gradually making it better .\n",
      " F i g u r e 7-8. D e ci s i o n b o u n d a r i e s o f c o ns e cu t i v e p r e d i c t o r s\n",
      " Once all predictors are trained, the ensemble makes predictions ver y m uch like bagƒ\n",
      " ging or pasting, except tha t predictors ha ve differen t weigh ts depending on their\n",
      " overall accuracy on the weigh ted training set.\n",
      " There is one im portan t dra wback to this sequen tial learning techniƒ\n",
      "que: it cannot be parallelized (or only partially), since each predicƒ\n",
      "tor can only be trained after the previous predictor has been\n",
      " trained and evalua ted. As a result, it does not scale as well as bagƒ\n",
      "ging or pasting.\n",
      " Let ‡ s take a closer look a t the A daB oost algorithm. Each instance weigh t \n",
      "w\n",
      "(i)\n",
      "   is initially\n",
      "set to \n",
      "1\n",
      "m\n",
      " . A first predictor is trained and its weigh ted error ra te \n",
      "r\n",
      "1\n",
      " \n",
      " is com puted on the\n",
      "training set; see \n",
      " Equa tion 7-1\n",
      ".\n",
      " Eq u a t i o n 7-1. W ei gh t e d er r o r r a t e o f t h e j\n",
      " t h\n",
      "  p r e d i c t o r\n",
      "r\n",
      "j\n",
      "=\n",
      "“\n",
      "i\n",
      " = 1\n",
      "y\n",
      "j\n",
      "i\n",
      "š\n",
      "y\n",
      "i\n",
      "m\n",
      "w\n",
      "i\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      "where\n",
      "y\n",
      "j\n",
      "i\n",
      " is the\n",
      "j\n",
      "th\n",
      " predictor‡s prediction for the\n",
      "i\n",
      "th\n",
      "instance.\n",
      " Boosting  |  203\n",
      "\n",
      "15\n",
      " The original A daB oost algorithm does not use a learning ra te h yperparameter .\n",
      " The predictor‡ s weigh t \n",
      "‰\n",
      "j\n",
      "  is then com puted using \n",
      " Equa tion 7-2\n",
      ", where \n",
      "−\n",
      " is the learnƒ\n",
      " ing ra te h yperparameter (defa ults to 1).\n",
      "15\n",
      "  The more accura te the predictor is, the\n",
      " higher its weigh t will be. If it is just guessing randomly , then its weigh t will be close to\n",
      " zero . H owever , if it is most often wrong (i.e., less accura te than random guessing),\n",
      " then its weigh t will be nega tive.\n",
      " Eq u a t i o n 7-2. P r e d i c t o r w ei gh t\n",
      "‰\n",
      "j\n",
      "=\n",
      "−\n",
      "log\n",
      " 1 ”\n",
      "r\n",
      "j\n",
      "r\n",
      "j\n",
      " N ext the instance weigh ts are upda ted using \n",
      " Equa tion 7-3\n",
      ": the misclassified instances\n",
      "are boosted.\n",
      " Eq u a t i o n 7-3. W ei gh t u p d a t e r u l e\n",
      "for\n",
      "i\n",
      " = 1, 2,\n",
      ",\n",
      "m\n",
      "w\n",
      "i\n",
      "w\n",
      "i\n",
      "if\n",
      "y\n",
      "j\n",
      "i\n",
      "=\n",
      "y\n",
      "i\n",
      "w\n",
      "i\n",
      "exp\n",
      "‰\n",
      "j\n",
      "if\n",
      "y\n",
      "j\n",
      "i\n",
      "š\n",
      "y\n",
      "i\n",
      " Then all the instance weigh ts are normalized (i.e., divided by \n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      ").\n",
      " Finally , a new predictor is trained using the upda ted weigh ts, and the whole process is\n",
      " repea ted (the new predictor‡ s weigh t is com puted, the instance weigh ts are upda ted,\n",
      "then another predictor is trained, and so on). The algorithm stops when the desired\n",
      " n umber of predictors is reached, or when a perfect predictor is found.\n",
      " T o make predictions, A daB oost sim ply com putes the predictions of all the predictors\n",
      " and weighs them using the predictor weigh ts \n",
      "‰\n",
      "j\n",
      " . The predicted class is the one tha t\n",
      " receives the ma jority of weigh ted votes (see \n",
      " Equa tion 7-4\n",
      ").\n",
      " Eq u a t i o n 7-4. Ad aB o o s t p r e d i c t i o ns\n",
      "y\n",
      "x\n",
      " = argmax\n",
      "k\n",
      "“\n",
      "j\n",
      " = 1\n",
      "y\n",
      "j\n",
      "x\n",
      "=\n",
      "k\n",
      "N\n",
      "‰\n",
      "j\n",
      "where\n",
      "N\n",
      " is the number of predictors.\n",
      " 204  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "16\n",
      " F or more details, see — M ulti-Class A daB oost, – J . Zh u et al. (2006).\n",
      "17\n",
      " First in troduced in — Arcing the Edge, – L. Breiman (1997), and further developed in the pa per — Greedy F uncƒ\n",
      " tion A pproxima tion: A Gradien t B oosting M achine, – J erome H. Friedman (1999).\n",
      " Scikit-Learn actually uses a m ulticlass version of A daB oost called \n",
      "SAMME\n",
      "16\n",
      " \n",
      "(which\n",
      "stands for \n",
      " S t a ge w i s e Ad d i t i v e M o d e l i n g u s i n g a M u l t i c l as s E xp o n en t i a l l o s s f u n c t i o n\n",
      ").\n",
      " When there are just two classes, SAMME is equivalen t to A daB oost. M oreover , if the\n",
      " predictors can estima te class probabilities (i.e., if they ha ve a \n",
      "predict_proba()\n",
      " method), Scikit-Learn can use a varian t of SAMME called \n",
      "SAMME.R\n",
      " (the \n",
      "R\n",
      " \n",
      "stands\n",
      " for —Real –), which relies on class probabilities ra ther than predictions and generally\n",
      " performs better .\n",
      " The following code trains an A daB oost classifier based on 200 \n",
      " D e ci s i o n S t u m p s\n",
      " \n",
      "using\n",
      " Scikit-Learn ‡ s \n",
      "AdaBoostClassifier\n",
      "  class (as you migh t expect, there is also an \n",
      "Ada\n",
      "BoostRegressor\n",
      "  class). A Decision Stum p is a Decision T ree with \n",
      "max_depth=1\n",
      "›in\n",
      " other words, a tree com posed of a single decision node plus two leaf nodes. This is\n",
      " the defa ult base estima tor for the \n",
      "AdaBoostClassifier\n",
      " class:\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "AdaBoostClassifier\n",
      "ada_clf\n",
      " \n",
      "=\n",
      " \n",
      "AdaBoostClassifier\n",
      "(\n",
      "    \n",
      "DecisionTreeClassifier\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "1\n",
      "),\n",
      " \n",
      "n_estimators\n",
      "=\n",
      "200\n",
      ",\n",
      "    \n",
      "algorithm\n",
      "=\n",
      "\"SAMME.R\"\n",
      ",\n",
      " \n",
      "learning_rate\n",
      "=\n",
      "0.5\n",
      ")\n",
      "ada_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " If your A daB oost ensemble is overfitting the training set, you can\n",
      " tr y reducing the n umber of estima tors or more strongly regularizƒ\n",
      " ing the base estima tor .\n",
      "Gradient Boosting\n",
      " Another ver y popular B oosting algorithm is \n",
      " G r a d i en t B o o s t i n g\n",
      ".\n",
      "17\n",
      "  J ust like A daB oost,\n",
      " Gradien t B oosting works by sequen tially adding predictors to an ensemble, each one\n",
      " correcting its predecessor . H owever , instead of tweaking the instance weigh ts a t ever y\n",
      " itera tion like A daB oost does, this method tries to fit the new predictor to the \n",
      " r e s i d u a l\n",
      " er r o r s\n",
      "  made by the previous predictor .\n",
      " Let ‡ s go through a sim ple regression exam ple using Decision T rees as the base predicƒ\n",
      " tors (of course Gradien t B oosting also works grea t with regression tasks). This is\n",
      "called\n",
      " \n",
      " G r a d i en t T r e e B o o s t i n g\n",
      ", or \n",
      " G r a d i en t B o o s t e d R e g r e s s i o n T r e e s\n",
      " \n",
      "(\n",
      " GBR T\n",
      " ). First, let ‡ s\n",
      "fit a \n",
      "DecisionTreeRegressor\n",
      " \n",
      " to the training set (for exam ple, a noisy quadra tic trainƒ\n",
      "ing set):\n",
      " Boosting  |  205\n",
      "\n",
      "from\n",
      " \n",
      "sklearn.tree\n",
      " \n",
      "import\n",
      " \n",
      "DecisionTreeRegressor\n",
      "tree_reg1\n",
      " \n",
      "=\n",
      " \n",
      "DecisionTreeRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ")\n",
      "tree_reg1\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " N ow train a second \n",
      "DecisionTreeRegressor\n",
      " on the residual errors made by the first\n",
      " predictor :\n",
      "y2\n",
      " \n",
      "=\n",
      " \n",
      "y\n",
      " \n",
      "-\n",
      " \n",
      "tree_reg1\n",
      ".\n",
      "predict\n",
      "(\n",
      "X\n",
      ")\n",
      "tree_reg2\n",
      " \n",
      "=\n",
      " \n",
      "DecisionTreeRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ")\n",
      "tree_reg2\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y2\n",
      ")\n",
      " Then we train a third regressor on the residual errors made by the second predictor :\n",
      "y3\n",
      " \n",
      "=\n",
      " \n",
      "y2\n",
      " \n",
      "-\n",
      " \n",
      "tree_reg2\n",
      ".\n",
      "predict\n",
      "(\n",
      "X\n",
      ")\n",
      "tree_reg3\n",
      " \n",
      "=\n",
      " \n",
      "DecisionTreeRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ")\n",
      "tree_reg3\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y3\n",
      ")\n",
      " N ow we ha ve an ensemble con taining three trees. I t can make predictions on a new\n",
      " instance sim ply by adding up the predictions of all the trees:\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "sum\n",
      "(\n",
      "tree\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      " \n",
      "for\n",
      " \n",
      "tree\n",
      " \n",
      "in\n",
      " \n",
      "(\n",
      "tree_reg1\n",
      ",\n",
      " \n",
      "tree_reg2\n",
      ",\n",
      " \n",
      "tree_reg3\n",
      "))\n",
      "Figure 7-9\n",
      "  represen ts the predictions of these three trees in the left column, and the\n",
      " ensemble ‡ s predictions in the righ t column. In the first row , the ensemble has just one\n",
      " tree, so its predictions are exactly the same as the first tree ‡ s predictions. In the second\n",
      " row , a new tree is trained on the residual errors of the first tree. On the righ t you can\n",
      " see tha t the ensemble ‡ s predictions are equal to the sum of the predictions of the first\n",
      " two trees. Similarly , in the third row another tree is trained on the residual errors of\n",
      " the second tree. Y ou can see tha t the ensemble ‡ s predictions gradually get better as\n",
      "trees are added to the ensemble.\n",
      " A sim pler wa y to train GBR T ensembles is to use Scikit-Learn ‡ s \n",
      "GradientBoostingRe\n",
      "gressor\n",
      "   class. M uch like the \n",
      "RandomForestRegressor\n",
      "  class, it has h yperparameters to\n",
      " con trol the growth of Decision T rees (e.g., \n",
      "max_depth\n",
      ", \n",
      "min_samples_leaf\n",
      ", and so on),\n",
      " as well as h yperparameters to con trol the ensemble training, such as the n umber of\n",
      "trees (\n",
      "n_estimators\n",
      " ). The following code crea tes the same ensemble as the previous\n",
      "one:\n",
      "from\n",
      " \n",
      "sklearn.ensemble\n",
      " \n",
      "import\n",
      " \n",
      "GradientBoostingRegressor\n",
      "gbrt\n",
      " \n",
      "=\n",
      " \n",
      "GradientBoostingRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "n_estimators\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "learning_rate\n",
      "=\n",
      "1.0\n",
      ")\n",
      "gbrt\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " 206  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      " F i g u r e 7-9. G r a d i en t B o o s t i n g\n",
      "The \n",
      "learning_rate\n",
      "  h yperparameter scales the con tribution of each tree. If you set it\n",
      "to a low value, such as \n",
      "0.1\n",
      ", you will need more trees in the ensemble to fit the trainƒ\n",
      " ing set, but the predictions will usually generalize better . This is a regulariza tion techƒ\n",
      "nique called \n",
      " s h r i n k a ge\n",
      ". \n",
      "Figure 7-10\n",
      "  shows two GBR T ensembles trained with a low\n",
      " learning ra te: the one on the left does not ha ve enough trees to fit the training set,\n",
      " while the one on the righ t has too man y trees and overfits the training set.\n",
      " Boosting  |  207\n",
      "\n",
      " F i g u r e 7-10. GBR T ens em b l e s w i t h n o t en o u gh p r e d i c t o r s \n",
      "(le“)\n",
      "  a n d t o o m a n y (r i gh t)\n",
      " In order to find the optimal n umber of trees, you can use early stopping (see \n",
      " Cha pƒ\n",
      "ter 4\n",
      " ). A sim ple wa y to im plemen t this is to use the \n",
      "staged_predict()\n",
      " method: it\n",
      " returns an itera tor over the predictions made by the ensemble a t each stage of trainƒ\n",
      " ing (with one tree, two trees, etc.). The following code trains a GBR T ensemble with\n",
      " 120 trees, then measures the valida tion error a t each stage of training to find the optiƒ\n",
      " mal n umber of trees, and finally trains another GBR T ensemble using the optimal\n",
      " n umber of trees:\n",
      "import\n",
      " \n",
      "numpy\n",
      " \n",
      "as\n",
      " \n",
      "np\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "train_test_split\n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "mean_squared_error\n",
      "X_train\n",
      ",\n",
      " \n",
      "X_val\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "y_val\n",
      " \n",
      "=\n",
      " \n",
      "train_test_split\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      "gbrt\n",
      " \n",
      "=\n",
      " \n",
      "GradientBoostingRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "n_estimators\n",
      "=\n",
      "120\n",
      ")\n",
      "gbrt\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "errors\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "mean_squared_error\n",
      "(\n",
      "y_val\n",
      ",\n",
      " \n",
      "y_pred\n",
      ")\n",
      "          \n",
      "for\n",
      " \n",
      "y_pred\n",
      " \n",
      "in\n",
      " \n",
      "gbrt\n",
      ".\n",
      "staged_predict\n",
      "(\n",
      "X_val\n",
      ")]\n",
      "bst_n_estimators\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "argmin\n",
      "(\n",
      "errors\n",
      ")\n",
      "gbrt_best\n",
      " \n",
      "=\n",
      " \n",
      "GradientBoostingRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ",\n",
      "n_estimators\n",
      "=\n",
      "bst_n_estimators\n",
      ")\n",
      "gbrt_best\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " The valida tion errors are represen ted on the left of \n",
      "Figure 7-11\n",
      " , and the best model ‡ s\n",
      " predictions are represen ted on the righ t.\n",
      " 208  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      " F i g u r e 7-11. T u n i n g t h e n u m b er o f t r e e s u s i n g e a r l y s t o p p i n g\n",
      " I t is also possible to im plemen t early stopping by actually stopping training early\n",
      " (instead of training a large n umber of trees first and then looking back to find the\n",
      " optimal n umber). Y ou can do so by setting \n",
      "warm_start=True\n",
      ", which makes Scikit-\n",
      "Learn keep existing trees when the \n",
      "fit()\n",
      "  method is called, allowing incremen tal\n",
      " training. The following code stops training when the valida tion error does not\n",
      " im prove for five itera tions in a row :\n",
      "gbrt\n",
      " \n",
      "=\n",
      " \n",
      "GradientBoostingRegressor\n",
      "(\n",
      "max_depth\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "warm_start\n",
      "=\n",
      "True\n",
      ")\n",
      "min_val_error\n",
      " \n",
      "=\n",
      " \n",
      "float\n",
      "(\n",
      "\"inf\"\n",
      ")\n",
      "error_going_up\n",
      " \n",
      "=\n",
      " \n",
      "0\n",
      "for\n",
      " \n",
      "n_estimators\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "120\n",
      "):\n",
      "    \n",
      "gbrt\n",
      ".\n",
      "n_estimators\n",
      " \n",
      "=\n",
      " \n",
      "n_estimators\n",
      "    \n",
      "gbrt\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "    \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "gbrt\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_val\n",
      ")\n",
      "    \n",
      "val_error\n",
      " \n",
      "=\n",
      " \n",
      "mean_squared_error\n",
      "(\n",
      "y_val\n",
      ",\n",
      " \n",
      "y_pred\n",
      ")\n",
      "    \n",
      "if\n",
      " \n",
      "val_error\n",
      " \n",
      "<\n",
      " \n",
      "min_val_error\n",
      ":\n",
      "        \n",
      "min_val_error\n",
      " \n",
      "=\n",
      " \n",
      "val_error\n",
      "        \n",
      "error_going_up\n",
      " \n",
      "=\n",
      " \n",
      "0\n",
      "    \n",
      "else\n",
      ":\n",
      "        \n",
      "error_going_up\n",
      " \n",
      "+=\n",
      " \n",
      "1\n",
      "        \n",
      "if\n",
      " \n",
      "error_going_up\n",
      " \n",
      "==\n",
      " \n",
      "5\n",
      ":\n",
      "            \n",
      "break\n",
      "  \n",
      "# early stopping\n",
      "The \n",
      "GradientBoostingRegressor\n",
      " class also supports a \n",
      "subsample\n",
      " \n",
      " h yperparameter ,\n",
      " which specifies the fraction of training instances to be used for training each tree. F or\n",
      " exam ple, if \n",
      "subsample=0.25\n",
      ", then each tree is trained on 25% of the training instanƒ\n",
      " ces, selected randomly . As you can probably guess by now , this trades a higher bias\n",
      " for a lower variance. I t also speeds up training considerably . This technique is called\n",
      " S t o c h as t i c G r a d i en t B o o s t i n g\n",
      ".\n",
      " Boosting  |  209\n",
      "\n",
      "18\n",
      " — Stacked Generaliza tion, – D . W olpert (1992).\n",
      " I t is possible to use Gradien t B oosting with other cost functions.\n",
      " This is con trolled by the \n",
      "loss\n",
      " \n",
      " h yperparameter (see Scikit-Learn ‡ s\n",
      " documen ta tion for more details).\n",
      " I t is worth noting tha t an optimized im plemen ta tion of Gradien t B oosting is a vailable\n",
      " in the popular python librar y \n",
      " X GB o o s t\n",
      " , which stands for Extreme Gradien t B oosting.\n",
      " This package was initially developed by T ianqi Chen as part of the Distributed (Deep)\n",
      " M achine Learning Comm unity (\n",
      "DMLC\n",
      " ), and it aims a t being extremely fast, scalable\n",
      " and portable. In fact, X GB oost is often an im portan t com ponen t of the winning\n",
      " en tries in ML com petitions. X GB oost ‡ s API is quite similar to Scikit-Learn ‡ s:\n",
      "import\n",
      " \n",
      "xgboost\n",
      "xgb_reg\n",
      " \n",
      "=\n",
      " \n",
      "xgboost\n",
      ".\n",
      "XGBRegressor\n",
      "()\n",
      "xgb_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "xgb_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_val\n",
      ")\n",
      " X GB oost also offers several nice fea tures, such as a utoma tically taking care of early\n",
      "stopping:\n",
      "xgb_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      "            \n",
      "eval_set\n",
      "=\n",
      "[(\n",
      "X_val\n",
      ",\n",
      " \n",
      "y_val\n",
      ")],\n",
      " \n",
      "early_stopping_rounds\n",
      "=\n",
      "2\n",
      ")\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "xgb_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_val\n",
      ")\n",
      " Y ou should definitely check it out!\n",
      "Stacking\n",
      " The last Ensemble method we will discuss in this cha pter is called \n",
      " s t a c k i n g\n",
      " \n",
      "(short for\n",
      " s t a c k e d gen er a l iz a t i o n\n",
      ").\n",
      "18\n",
      " \n",
      " I t is based on a sim ple idea: instead of using trivial functions\n",
      " (such as hard voting) to aggrega te the predictions of all predictors in an ensemble,\n",
      " wh y don ‡ t we train a model to perform this aggrega tion? \n",
      "Figure 7-12\n",
      " shows such an\n",
      "ensemble performing a regression task on a new instance. Each of the bottom three\n",
      " predictors predicts a differen t value (3.1, 2.7, and 2.9), and then the final predictor \n",
      "(called a \n",
      " b l en d er\n",
      ", or a \n",
      " m e t a l e a r n er\n",
      " ) takes these predictions as in puts and makes the\n",
      "final prediction (3.0).\n",
      " 210  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "19\n",
      " Alterna tively , it is possible to use out-of-fold predictions. In some con texts this is called \n",
      " s t a c k i n g\n",
      ", while using a\n",
      "hold-out set is called \n",
      " b l en d i n g\n",
      " . H owever , for man y people these terms are synon ymous.\n",
      " F i g u r e 7-12. Ag g r e ga t i n g p r e d i c t i o ns u s i n g a b l en d i n g p r e d i c t o r\n",
      " T o train the blender , a common a pproach is to use a \n",
      "hold-out set.\n",
      "19\n",
      "  Let ‡ s see how it\n",
      "works. First, the training set is split in two subsets. The first subset is used to train the\n",
      " predictors in the first la yer (see \n",
      "Figure 7-13\n",
      ").\n",
      " F i g u r e 7-13. T r a i n i n g t h e \n",
      "†rst\n",
      "  l a y er\n",
      " N ext, the first la yer predictors are used to make predictions on the second (held-out)\n",
      "set (see \n",
      "Figure 7-14\n",
      " ). This ensures tha t the predictions are — clean, – since the predictors\n",
      " never sa w these instances during training. N ow for each instance in the hold-out set\n",
      " Stacking  |  211\n",
      "\n",
      " there are three predicted values. W e can crea te a new training set using these predicƒ\n",
      " ted values as in put fea tures (which makes this new training set three-dimensional),\n",
      "and keeping the target values. The blender is trained on this new training set, so it\n",
      " learns to predict the target value given the first la yer‡ s predictions.\n",
      " F i g u r e 7-14. T r a i n i n g t h e b l en d er\n",
      " I t is actually possible to train several differen t blenders this wa y (e.g., one using Linƒ\n",
      " ear Regression, another using R andom F orest Regression, and so on): we get a whole\n",
      " la yer of blenders. The trick is to split the training set in to three subsets: the first one is\n",
      " used to train the first la yer , the second one is used to crea te the training set used to\n",
      " train the second la yer (using predictions made by the predictors of the first la yer),\n",
      " and the third one is used to crea te the training set to train the third la yer (using preƒ\n",
      " dictions made by the predictors of the second la yer). Once this is done, we can make\n",
      " a prediction for a new instance by going through each la yer sequen tially , as shown in\n",
      "Figure 7-15\n",
      ".\n",
      " 212  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      " F i g u r e 7-15. P r e d i c t i o ns i n a m u l t i l a y er s t a c k i n g ens em b l e\n",
      " U nfortuna tely , Scikit-Learn does not support stacking directly , but it is not too hard\n",
      " to roll out your own im plemen ta tion (see the following exercises). Alterna tively , you\n",
      " can use an open source im plemen ta tion such as \n",
      "brew\n",
      "  (a vailable a t \n",
      " h ttp s://g i t h u b .c o m/\n",
      " v i i s a r/b r e w\n",
      ").\n",
      "Exercises\n",
      "1.\n",
      " If you ha ve trained five differen t models on the exact same training da ta, and\n",
      " they all achieve 95% precision, is there an y chance tha t you can combine these\n",
      " models to get better results? If so , how? If not, wh y?\n",
      "2.\n",
      " Wha t is the difference between hard and soft voting classifiers?\n",
      "3.\n",
      " I s it possible to speed up training of a bagging ensemble by distributing it across\n",
      " m ultiple ser vers? Wha t about pasting ensembles, boosting ensembles, random\n",
      "forests, or stacking ensembles?\n",
      "4.\n",
      " Wha t is the benefit of out-of-bag evalua tion?\n",
      "5.\n",
      " Wha t makes Extra-T rees more random than regular R andom F orests? H ow can\n",
      " this extra randomness help? Are Extra-T rees slower or faster than regular R anƒ\n",
      " dom F orests?\n",
      "6.\n",
      " If your A daB oost ensemble underfits the training da ta, wha t h yperparameters\n",
      "should you tweak and how?\n",
      " Exercises  |  213\n",
      "\n",
      "7.\n",
      " If your Gradien t B oosting ensemble overfits the training set, should you increase\n",
      " or decrease the learning ra te?\n",
      "8.\n",
      " Load the MNIST da ta (in troduced in \n",
      " Cha pter 3\n",
      " ), and split it in to a training set, a\n",
      " valida tion set, and a test set (e.g., use 50,000 instances for training, 10,000 for valƒ\n",
      " ida tion, and 10,000 for testing). Then train various classifiers, such as a R andom\n",
      " F orest classifier , an Extra-T rees classifier , and an SVM. N ext, tr y to combine\n",
      " them in to an ensemble tha t outperforms them all on the valida tion set, using a\n",
      " soft or hard voting classifier . Once you ha ve found one, tr y it on the test set. H ow\n",
      " m uch better does it perform com pared to the individual classifiers?\n",
      "9.\n",
      " R un the individual classifiers from the previous exercise to make predictions on\n",
      " the valida tion set, and crea te a new training set with the resulting predictions:\n",
      " each training instance is a vector con taining the set of predictions from all your\n",
      " classifiers for an image, and the target is the image ‡ s class. T rain a classifier on\n",
      " this new training set. Congra tula tions, you ha ve just trained a blender , and\n",
      " together with the classifiers they form a stacking ensemble! N ow let ‡ s evalua te the\n",
      " ensemble on the test set. F or each image in the test set, make predictions with all\n",
      " your classifiers, then feed the predictions to the blender to get the ensemble ‡ s preƒ\n",
      " dictions. H ow does it com pare to the voting classifier you trained earlier?\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " 214  |  Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "CHAPTER 8\n",
      "Dimensionality Reduction\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 8 in the final\n",
      "release of the book.\n",
      " M an y M achine Learning problems in volve thousands or even millions of fea tures for\n",
      " each training instance. N ot only does this make training extremely slow , it can also\n",
      " make it m uch harder to find a good solution, as we will see. This problem is often\n",
      "referred to as the \n",
      " cu r s e o f d i m ens i o n a l i ty\n",
      ".\n",
      " F ortuna tely , in real-world problems, it is often possible to reduce the n umber of feaƒ\n",
      " tures considerably , turning an in tractable problem in to a tractable one. F or exam ple,\n",
      " consider the MNIST images (in troduced in \n",
      " Cha pter 3\n",
      "): the pixels on the image borƒ\n",
      " ders are almost alwa ys white, so you could com pletely drop these pixels from the\n",
      " training set without losing m uch informa tion. \n",
      "Figure 7-6\n",
      "  confirms tha t these pixels\n",
      " are utterly unim portan t for the classifica tion task. M oreover , two neighboring pixels\n",
      " are often highly correla ted: if you merge them in to a single pixel (e.g., by taking the\n",
      " mean of the two pixel in tensities), you will not lose m uch informa tion.\n",
      "215\n",
      "\n",
      "1\n",
      " W ell, four dimensions if you coun t time, and a few more if you are a string theorist.\n",
      " Reducing dimensionality does lose some informa tion (just like\n",
      " com pressing an image to JPEG can degrade its quality), so even\n",
      " though it will speed up training, it ma y also make your system perƒ\n",
      " form sligh tly worse. I t also makes your pipelines a bit more comƒ\n",
      " plex and th us harder to main tain. So you should first tr y to train\n",
      " your system with the original da ta before considering using dimenƒ\n",
      " sionality reduction if training is too slow . In some cases, however ,\n",
      " reducing the dimensionality of the training da ta ma y filter out\n",
      " some noise and unnecessar y details and th us result in higher perƒ\n",
      " formance (but in general it won ‡ t; it will just speed up training).\n",
      " A part from speeding up training, dimensionality reduction is also extremely useful\n",
      " for da ta visualiza tion (or \n",
      " D a t aV iz\n",
      " ). Reducing the n umber of dimensions down to two\n",
      "(or three) makes it possible to plot a condensed view of a high-dimensional training\n",
      " set on a gra ph and often gain some im portan t insigh ts by visually detecting pa tterns,\n",
      " such as clusters. M oreover , Da taV iz is essen tial to comm unica te your conclusions to\n",
      " people who are not da ta scien tists, in particular decision makers who will use your\n",
      "results.\n",
      " In this cha pter we will discuss the curse of dimensionality and get a sense of wha t\n",
      " goes on in high-dimensional space. Then, we will presen t the two main a pproaches to\n",
      " dimensionality reduction (projection and M anifold Learning), and we will go\n",
      " through three of the most popular dimensionality reduction techniques: PCA, K ernel\n",
      "PCA, and LLE.\n",
      "The Curse of Dimensionality\n",
      " W e are so used to living in three dimensions\n",
      "1\n",
      "  tha t our in tuition fails us when we tr y\n",
      " to imagine a high-dimensional space. E ven a basic 4D h ypercube is incredibly hard to\n",
      "picture in our mind (see \n",
      "Figure 8-1\n",
      " ), let alone a 200-dimensional ellipsoid ben t in a\n",
      "1,000-dimensional space.\n",
      " 216  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      "2\n",
      " W a tch a rota ting tesseract projected in to 3D space a t \n",
      " h ttp s://h o m l.i n f o/30\n",
      " . Image by W ikipedia user N erdƒ\n",
      " B oy1392 (\n",
      " Crea tive Commons BY -SA 3.0\n",
      "). Reproduced from \n",
      " h ttp s://en.w i k i p e d i a.o r g/w i k i/T e s s er a c t\n",
      ".\n",
      "3\n",
      " F un fact: an yone you know is probably an extremist in a t least one dimension (e.g., how m uch sugar they put\n",
      "in their coffee), if you consider enough dimensions.\n",
      " F i g u r e 8-1. P o i n t , s e g m en t , s q u a r e, cu b e, a n d t e s s er a c t (0D t o 4D h y p er cu b e s)\n",
      "2\n",
      " I t turns out tha t man y things beha ve ver y differen tly in high-dimensional space. F or\n",
      " exam ple, if you pick a random poin t in a unit square (a 1 „ 1 square), it will ha ve only\n",
      " about a 0.4% chance of being loca ted less than 0.001 from a border (in other words, it\n",
      " is ver y unlikely tha t a random poin t will be — extreme – along an y dimension). But in a\n",
      " 10,000-dimensional unit h ypercube (a 1 „ 1 „ \n",
      " „ 1 cube, with ten thousand 1s), this\n",
      " probability is grea ter than 99.999999%. M ost poin ts in a high-dimensional h ypercube\n",
      " are ver y close to the border .\n",
      "3\n",
      " H ere is a more troublesome difference: if you pick two poin ts randomly in a unit\n",
      " square, the distance between these two poin ts will be, on a verage, roughly 0.52. If you\n",
      " pick two random poin ts in a unit 3D cube, the a verage distance will be roughly 0.66.\n",
      " But wha t about two poin ts picked randomly in a 1,000,000-dimensional h ypercube?\n",
      " W ell, the a verage distance, believe it or not, will be about 408.25 (roughly\n",
      " 1, 000, 000 / 6\n",
      " )! This is quite coun terin tuitive: how can two poin ts be so far a part\n",
      " when they both lie within the same unit h ypercube? This fact im plies tha t high-\n",
      " dimensional da tasets are a t risk of being ver y sparse: most training instances are\n",
      " likely to be far a wa y from each other . Of course, this also means tha t a new instance\n",
      " will likely be far a wa y from an y training instance, making predictions m uch less reliaƒ\n",
      " ble than in lower dimensions, since they will be based on m uch larger extra pola tions.\n",
      " In short, the more dimensions the training set has, the grea ter the risk of overfitting\n",
      "it.\n",
      " In theor y , one solution to the curse of dimensionality could be to increase the size of\n",
      " the training set to reach a sufficien t density of training instances. U nfortuna tely , in\n",
      " practice, the n umber of training instances required to reach a given density grows\n",
      " exponen tially with the n umber of dimensions. W ith just 100 fea tures (m uch less than\n",
      " The Curse of Dimensionality  |  217\n",
      "\n",
      " in the MNIST problem), you would need more training instances than a toms in the\n",
      " obser vable universe in order for training instances to be within 0.1 of each other on\n",
      " a verage, assuming they were spread out uniformly across all dimensions.\n",
      "Main Approaches for Dimensionality Reduction\n",
      " B efore we dive in to specific dimensionality reduction algorithms, let ‡ s take a look a t\n",
      " the two main a pproaches to reducing dimensionality : projection and M anifold\n",
      "Learning.\n",
      "Projection\n",
      "In most real-world problems, training instances are \n",
      " n o t\n",
      " spread out uniformly across\n",
      " all dimensions. M an y fea tures are almost constan t, while others are highly correla ted\n",
      "(as discussed earlier for MNIST). As a result, all training instances actually lie within\n",
      " (or close to) a m uch lower -dimensional \n",
      " s u b s p a c e\n",
      " \n",
      "of the high-dimensional space. This\n",
      " sounds ver y abstract, so let ‡ s look a t an exam ple. In \n",
      "Figure 8-2\n",
      " \n",
      " you can see a 3D da taƒ\n",
      " set represen ted by the circles.\n",
      " F i g u r e 8-2. A 3D d a t as e t l y i n g c l o s e t o a 2D s u b s p a c e\n",
      " N otice tha t all training instances lie close to a plane: this is a lower -dimensional (2D)\n",
      " subspace of the high-dimensional (3D) space. N ow if we project ever y training\n",
      " instance perpendicularly on to this subspace (as represen ted by the short lines conƒ\n",
      " necting the instances to the plane), we get the new 2D da taset shown in \n",
      "Figure 8-3\n",
      ".\n",
      " T a-da! W e ha ve just reduced the da taset ‡ s dimensionality from 3D to 2D . N ote tha t\n",
      " the axes correspond to new fea tures \n",
      "z\n",
      "1\n",
      " \n",
      "and \n",
      "z\n",
      "2\n",
      "  (the coordina tes of the projections on\n",
      "the plane).\n",
      " 218  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      " F i g u r e 8-3. \n",
      "•e\n",
      "  n e w 2D d a t as e t \n",
      "a“er\n",
      "  p r o je c t i o n\n",
      " H owever , projection is not alwa ys the best a pproach to dimensionality reduction. In\n",
      " man y cases the subspace ma y twist and turn, such as in the famous \n",
      " S w i s s r o l l\n",
      "   toy da taƒ\n",
      " set represen ted in \n",
      "Figure 8-4\n",
      ".\n",
      " F i g u r e 8-4. S w i s s r o l l d a t as e t\n",
      " Main Approaches for Dimensionality Reduction  |  219\n",
      "\n",
      " Sim ply projecting on to a plane (e.g., by dropping \n",
      "x\n",
      "3\n",
      " ) would squash differen t la yers of\n",
      " the Swiss roll together , as shown on the left of \n",
      "Figure 8-5\n",
      " . H owever , wha t you really\n",
      " wan t is to unroll the Swiss roll to obtain the 2D da taset on the righ t of \n",
      "Figure 8-5\n",
      ".\n",
      " F i g u r e 8-5. S q u as h i n g b y p r o je c t i n g o n t o a p l a n e \n",
      "(le“)\n",
      "  v er s u s u n r o l l i n g t h e S w i s s r o l l\n",
      " (r i gh t)\n",
      "Manifold Learning\n",
      " The Swiss roll is an exam ple of a 2D \n",
      " m a n i f o l d\n",
      " . Put sim ply , a 2D manifold is a 2D\n",
      " sha pe tha t can be ben t and twisted in a higher -dimensional space. M ore generally , a\n",
      "d\n",
      "-dimensional manifold is a part of an \n",
      "n\n",
      "-dimensional space (where \n",
      "d\n",
      "   < \n",
      "n\n",
      " ) tha t locally\n",
      "resembles a \n",
      "d\n",
      "-dimensional \n",
      " h yperplane. In the case of the Swiss roll, \n",
      "d\n",
      "   = 2 and \n",
      "n\n",
      "   = 3: it\n",
      "locally resembles a 2D plane, but it is rolled in the third dimension.\n",
      " M an y dimensionality reduction algorithms work by modeling the \n",
      " m a n i f o l d\n",
      " on which\n",
      "the training instances lie; this is called \n",
      " M a n i f o l d L e a r n i n g\n",
      " . I t relies on the \n",
      " m a n i f o l d\n",
      " as s u m p t i o n\n",
      ", also called the \n",
      " m a n i f o l d h y p o t h e s i s\n",
      " , which holds tha t most real-world\n",
      " high-dimensional da tasets lie close to a m uch lower -dimensional manifold. This\n",
      " assum ption is ver y often em pirically obser ved.\n",
      " Once again, think about the MNIST da taset: all handwritten digit images ha ve some\n",
      "similarities. They are made of connected lines, the borders are white, they are more\n",
      " or less cen tered, and so on. If you randomly genera ted images, only a ridiculously\n",
      " tin y fraction of them would look like handwritten digits. In other words, the degrees\n",
      " of freedom a vailable to you if you tr y to crea te a digit image are drama tically lower\n",
      " than the degrees of freedom you would ha ve if you were allowed to genera te an y\n",
      " image you wan ted. These constrain ts tend to squeeze the da taset in to a lower -\n",
      "dimensional manifold.\n",
      " The manifold assum ption is often accom panied by another im plicit assum ption: tha t\n",
      " the task a t hand (e.g., classifica tion or regression) will be sim pler if expressed in the\n",
      " lower -dimensional space of the manifold. F or exam ple, in the top row of \n",
      "Figure 8-6\n",
      " the Swiss roll is split in to two classes: in the 3D space (on the left), the decision\n",
      " 220  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      " boundar y would be fairly com plex, but in the 2D unrolled manifold space (on the\n",
      " righ t), the decision boundar y is a sim ple straigh t line.\n",
      " H owever , this assum ption does not alwa ys hold. F or exam ple, in the bottom row of\n",
      "Figure 8-6\n",
      " , the decision boundar y is loca ted a t \n",
      "x\n",
      "1\n",
      "  = 5. This decision boundar y looks\n",
      " ver y sim ple in the original 3D space (a vertical plane), but it looks more com plex in\n",
      " the unrolled manifold (a collection of four independen t line segmen ts).\n",
      "In short, if you reduce the dimensionality of your training set before training a\n",
      " model, it will usually speed up training, but it ma y not alwa ys lead to a better or simƒ\n",
      " pler solution; it all depends on the da taset.\n",
      " H opefully you now ha ve a good sense of wha t the curse of dimensionality is and how\n",
      " dimensionality reduction algorithms can figh t it, especially when the manifold\n",
      " assum ption holds. The rest of this cha pter will go through some of the most popular\n",
      "algorithms.\n",
      " F i g u r e 8-6. \n",
      "•e\n",
      "  d e ci s i o n b o u n d a r y m a y n o t a l w a ys b e s i m p l er w i t h l o w er d i m ens i o ns\n",
      " Main Approaches for Dimensionality Reduction  |  221\n",
      "\n",
      "4\n",
      " — On Lines and Planes of Closest Fit to Systems of P oin ts in Space, – K. P earson (1901).\n",
      "PCA\n",
      " P r i n ci p a l C o m p o n en t An a l ys i s\n",
      "  (PCA)  is by far the most popular dimensionality reducƒ\n",
      " tion algorithm. First it iden tifies the h yperplane tha t lies closest to the da ta, and then\n",
      " it projects the da ta on to it, just like in \n",
      "Figure 8-2\n",
      ".\n",
      "Preserving the Variance\n",
      " B efore you can project the training set on to a lower -dimensional h yperplane, you\n",
      " first need to choose the righ t h yperplane. F or exam ple, a sim ple 2D da taset is repreƒ\n",
      " sen ted on the left of \n",
      "Figure 8-7\n",
      " , along with three differen t axes (i.e., one-dimensional\n",
      " h yperplanes). On the righ t is the result of the projection of the da taset on to each of\n",
      " these axes. As you can see, the projection on to the solid line preser ves the maxim um\n",
      " variance, while the projection on to the dotted line preser ves ver y little variance, and\n",
      " the projection on to the dashed line preser ves an in termedia te amoun t of variance.\n",
      " F i g u r e 8-7. S e l e c t i n g t h e s u b s p a c e o n t o w h i c h t o p r o je c t\n",
      " I t seems reasonable to select the axis tha t preser ves the maxim um amoun t of varƒ\n",
      " iance, as it will most likely lose less informa tion than the other projections. Another\n",
      " wa y to justif y this choice is tha t it is the axis tha t minimizes the mean squared disƒ\n",
      " tance between the original da taset and its projection on to tha t axis. This is the ra ther\n",
      " sim ple idea behind \n",
      "PCA\n",
      ".\n",
      "4\n",
      " 222  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      "Principal Components\n",
      " PCA iden tifies the axis tha t accoun ts for the largest amoun t of variance in the trainƒ\n",
      "ing set. In \n",
      "Figure 8-7\n",
      " , it is the solid line. I t also finds a second axis, orthogonal to the\n",
      " first one, tha t accoun ts for the largest amoun t of remaining variance. In this 2D\n",
      " exam ple there is no choice: it is the dotted line. If it were a higher -dimensional da taƒ\n",
      "set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\n",
      " a fifth, and so on›as man y axes as the n umber of dimensions in the da taset.\n",
      " The unit vector tha t defines the i\n",
      "th\n",
      " axis is called the i\n",
      "th\n",
      " \n",
      " p r i n ci p a l c o m p o n en t\n",
      " \n",
      "(PC). In\n",
      "Figure 8-7\n",
      ", the 1\n",
      "st\n",
      " PC is \n",
      "c\n",
      "1\n",
      " and the 2\n",
      "nd\n",
      " PC is \n",
      "c\n",
      "2\n",
      ". In \n",
      "Figure 8-2\n",
      " the first two PCs are\n",
      " represen ted by the orthogonal arrows in the plane, and the third PC would be\n",
      " orthogonal to the plane (poin ting up or down).\n",
      " The direction of the principal com ponen ts is not stable: if you perƒ\n",
      " turb the training set sligh tly and run PCA again, some of the new\n",
      " PCs ma y poin t in the opposite direction of the original PCs. H owƒ\n",
      " ever , they will generally still lie on the same axes. In some cases, a\n",
      " pair of PCs ma y even rota te or swa p , but the plane they define will\n",
      "generally remain the same.\n",
      " So how can you find the principal com ponen ts of a training set? L uckily , there is a\n",
      " standard ma trix factoriza tion technique called \n",
      " S i n g u l a r V a l u e D e c o m p o s i t i o n\n",
      " \n",
      "(SVD)\n",
      " tha t can decom pose the training set ma trix \n",
      "X\n",
      " \n",
      " in to the ma trix m ultiplica tion of three\n",
      " ma trices \n",
      "U\n",
      " ž \n",
      "V\n",
      "T\n",
      ", where \n",
      "V\n",
      "  con tains all the principal com ponen ts tha t we are looking\n",
      " for , as shown in \n",
      " Equa tion 8-1\n",
      ".\n",
      " Eq u a t i o n 8-1. P r i n ci p a l c o m p o n en ts m a t r ix\n",
      "V\n",
      "=\n",
      "\n",
      "c\n",
      "1\n",
      "c\n",
      "2\n",
      "c\n",
      "n\n",
      "\n",
      " The following Python code uses N umPy‡ s \n",
      "svd()\n",
      " function \n",
      "to obtain all the principal\n",
      " com ponen ts of the training set, then extracts the first two PCs:\n",
      "X_centered\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      " \n",
      "-\n",
      " \n",
      "X\n",
      ".\n",
      "mean\n",
      "(\n",
      "axis\n",
      "=\n",
      "0\n",
      ")\n",
      "U\n",
      ",\n",
      " \n",
      "s\n",
      ",\n",
      " \n",
      "Vt\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "linalg\n",
      ".\n",
      "svd\n",
      "(\n",
      "X_centered\n",
      ")\n",
      "c1\n",
      " \n",
      "=\n",
      " \n",
      "Vt\n",
      ".\n",
      "T\n",
      "[:,\n",
      " \n",
      "0\n",
      "]\n",
      "c2\n",
      " \n",
      "=\n",
      " \n",
      "Vt\n",
      ".\n",
      "T\n",
      "[:,\n",
      " \n",
      "1\n",
      "]\n",
      " PCA  |  223\n",
      "\n",
      " PCA assumes tha t the da taset is cen tered around the origin. As we\n",
      " will see, Scikit-Learn ‡ s PCA classes take care of cen tering the da ta\n",
      " for you. H owever , if you im plemen t PCA yourself (as in the preƒ\n",
      " ceding exam ple), or if you use other libraries, don ‡ t forget to cen ter\n",
      " the da ta first.\n",
      "Projecting Down to \n",
      "d\n",
      " Dimensions\n",
      " Once you ha ve iden tified all the principal com ponen ts, you can reduce the dimenƒ\n",
      " sionality of the da taset down to \n",
      "d\n",
      "  dimensions by projecting it on to the \n",
      " h yperplane\n",
      "defined by the first \n",
      "d\n",
      "   principal com ponen ts. Selecting this h yperplane ensures tha t the\n",
      " projection will preser ve as m uch variance as possible. F or exam ple, in \n",
      "Figure 8-2\n",
      " \n",
      "the\n",
      " 3D da taset is projected down to the 2D plane defined by the first two principal comƒ\n",
      " ponen ts, preser ving a large part of the da taset ‡ s variance. As a result, the 2D projecƒ\n",
      " tion looks ver y m uch like the original 3D da taset.\n",
      " T o project the training set on to the h yperplane, you can sim ply com pute the ma trix\n",
      " m ultiplica tion of the training set ma trix \n",
      "X\n",
      "  by the ma trix \n",
      "W\n",
      "d\n",
      " , defined as the ma trix\n",
      " con taining the first \n",
      "d\n",
      "  principal com ponen ts (i.e., the ma trix com posed of the first \n",
      "d\n",
      "columns of \n",
      "V\n",
      "), as shown in \n",
      " Equa tion 8-2\n",
      ".\n",
      " Eq u a t i o n 8-2. P r o je c t i n g t h e t r a i n i n g s e t d o w n t o d d i m ens i o ns\n",
      "X\n",
      "d\n",
      "ƒproj\n",
      "=\n",
      "XW\n",
      "d\n",
      " The following Python code projects the training set on to the plane defined by the first\n",
      " two principal com ponen ts:\n",
      "W2\n",
      " \n",
      "=\n",
      " \n",
      "Vt\n",
      ".\n",
      "T\n",
      "[:,\n",
      " \n",
      ":\n",
      "2\n",
      "]\n",
      "X2D\n",
      " \n",
      "=\n",
      " \n",
      "X_centered\n",
      ".\n",
      "dot\n",
      "(\n",
      "W2\n",
      ")\n",
      " There you ha ve it! Y ou now know how to reduce the dimensionality of an y da taset\n",
      " down to an y n umber of dimensions, while preser ving as m uch variance as possible.\n",
      "Using Scikit-Learn\n",
      " Scikit-Learn ‡ s \n",
      "PCA\n",
      "  class im plemen ts PCA using SVD decom position just like we did\n",
      " before. The following code a pplies PCA to reduce the dimensionality of the da taset\n",
      " down to two dimensions (note tha t it a utoma tically takes care of cen tering the da ta):\n",
      "from\n",
      " \n",
      "sklearn.decomposition\n",
      " \n",
      "import\n",
      " \n",
      "PCA\n",
      "pca\n",
      " \n",
      "=\n",
      " \n",
      "PCA\n",
      "(\n",
      "n_components\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      ")\n",
      "X2D\n",
      " \n",
      "=\n",
      " \n",
      "pca\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X\n",
      ")\n",
      "After fitting the \n",
      "PCA\n",
      "  transformer to the da taset, you can access the principal com poƒ\n",
      " nen ts using the \n",
      "components_\n",
      " \n",
      " variable (note tha t it con tains the PCs as horizon tal vecƒ\n",
      " 224  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      " tors, so , for exam ple, the first principal com ponen t is equal to \n",
      "pca.components_.T[:,\n",
      "0]\n",
      ").\n",
      "Explained Variance Ratio\n",
      " Another ver y useful piece of informa tion is the \n",
      " exp l a i n e d v a r i a n c e r a t i o\n",
      " \n",
      "of each prinƒ\n",
      " cipal com ponen t, a vailable via the \n",
      "explained_variance_ratio_\n",
      "  variable. I t indica tes\n",
      " the proportion of the da taset ‡ s variance tha t lies along the axis of each principal comƒ\n",
      " ponen t. F or exam ple, let ‡ s look a t the explained variance ra tios of the first two com poƒ\n",
      " nen ts of the 3D da taset represen ted in \n",
      "Figure 8-2\n",
      ":\n",
      ">>> \n",
      "pca\n",
      ".\n",
      "explained_variance_ratio_\n",
      "array([0.84248607, 0.14631839])\n",
      " This tells you tha t 84.2% of the da taset ‡ s variance lies along the first axis, and 14.6%\n",
      " lies along the second axis. This lea ves less than 1.2% for the third axis, so it is reasonƒ\n",
      " able to assume tha t it probably carries little informa tion.\n",
      "Choosing the Right Number of Dimensions\n",
      " Instead of arbitrarily choosing the n umber of dimensions to reduce down to , it is\n",
      " generally preferable to choose the n umber of dimensions tha t add up to a sufficien tly\n",
      " large portion of the variance (e.g., 95%). U nless, of course, you are reducing dimenƒ\n",
      " sionality for da ta visualiza tion›in tha t case you will generally wan t to reduce the\n",
      "dimensionality down to 2 or 3.\n",
      " The following code com putes PCA without reducing dimensionality , then com putes\n",
      " the minim um n umber of dimensions required to preser ve 95% of the training set ‡ s\n",
      "variance:\n",
      "pca\n",
      " \n",
      "=\n",
      " \n",
      "PCA\n",
      "()\n",
      "pca\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ")\n",
      "cumsum\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "cumsum\n",
      "(\n",
      "pca\n",
      ".\n",
      "explained_variance_ratio_\n",
      ")\n",
      "d\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "argmax\n",
      "(\n",
      "cumsum\n",
      " \n",
      ">=\n",
      " \n",
      "0.95\n",
      ")\n",
      " \n",
      "+\n",
      " \n",
      "1\n",
      " Y ou could then set \n",
      "n_components=d\n",
      "  and run PCA again. H owever , there is a m uch\n",
      " better option: instead of specif ying the n umber of principal com ponen ts you wan t to\n",
      " preser ve, you can set \n",
      "n_components\n",
      "  to be a floa t between \n",
      "0.0\n",
      " \n",
      "and \n",
      "1.0\n",
      " , indica ting the\n",
      " ra tio of variance you wish to preser ve:\n",
      "pca\n",
      " \n",
      "=\n",
      " \n",
      "PCA\n",
      "(\n",
      "n_components\n",
      "=\n",
      "0.95\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "pca\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      " Y et another option is to plot the explained variance as a function of the n umber of\n",
      " dimensions (sim ply plot \n",
      "cumsum\n",
      "; see \n",
      "Figure 8-8\n",
      "). There will usually be an elbow in the\n",
      " cur ve, where the explained variance stops growing fast. Y ou can think of this as the\n",
      " in trinsic dimensionality of the da taset. In this case, you can see tha t reducing the\n",
      " PCA  |  225\n",
      "\n",
      " dimensionality down to about 100 dimensions wouldn ‡ t lose too m uch explained varƒ\n",
      "iance.\n",
      " F i g u r e 8-8. E xp l a i n e d v a r i a n c e as a f u n c t i o n o f t h e n u m b er o f d i m ens i o ns\n",
      "PCA for Compression\n",
      " Obviously after dimensionality reduction, the training set takes up m uch less space.\n",
      " F or exam ple, tr y a pplying PCA to the MNIST da taset while preser ving 95% of its varƒ\n",
      " iance. Y ou should find tha t each instance will ha ve just over 150 fea tures, instead of\n",
      " the original 784 fea tures. So while most of the variance is preser ved, the da taset is\n",
      " now less than 20% of its original size! This is a reasonable com pression ra tio , and you\n",
      " can see how this can speed up a classifica tion algorithm (such as an SVM classifier)\n",
      " tremendously .\n",
      " I t is also possible to decom press the reduced da taset back to 784 dimensions by\n",
      " a pplying the in verse transforma tion of the PCA projection. Of course this won ‡ t give\n",
      " you back the original da ta, since the projection lost a bit of informa tion (within the\n",
      " 5% variance tha t was dropped), but it will likely be quite close to the original da ta.\n",
      " The mean squared distance between the original da ta and the reconstructed da ta\n",
      " (com pressed and then decom pressed) is called the \n",
      " r e c o ns t r u c t i o n er r o r\n",
      " . F or exam ple,\n",
      " the following code com presses the MNIST da taset down to 154 dimensions, then uses\n",
      "the \n",
      "inverse_transform()\n",
      "  method to decom press it back to 784 dimensions.\n",
      "Figure 8-9\n",
      " shows a few digits from the original training set (on the left), and the corƒ\n",
      " responding digits after com pression and decom pression. Y ou can see tha t there is a\n",
      " sligh t image quality loss, but the digits are still mostly in tact.\n",
      "pca\n",
      " \n",
      "=\n",
      " \n",
      "PCA\n",
      "(\n",
      "n_components\n",
      " \n",
      "=\n",
      " \n",
      "154\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "pca\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      "X_recovered\n",
      " \n",
      "=\n",
      " \n",
      "pca\n",
      ".\n",
      "inverse_transform\n",
      "(\n",
      "X_reduced\n",
      ")\n",
      " 226  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      " F i g u r e 8-9. MNIS T c o m p r e s s i o n p r e s er v i n g 95% o f t h e v a r i a n c e\n",
      " The equa tion of the in verse transforma tion is shown in \n",
      " Equa tion 8-3\n",
      ".\n",
      " Eq u a t i o n 8-3. PCA i n v er s e t r a ns f o r m a t i o n, b a c k t o t h e o r i g i n a l n u m b er o f\n",
      " d i m ens i o ns\n",
      "X\n",
      "recovered\n",
      "=\n",
      "X\n",
      "d\n",
      "ƒproj\n",
      "W\n",
      "d\n",
      "T\n",
      "Randomized PCA\n",
      "If you set the \n",
      "svd_solver\n",
      "  h yperparameter to \n",
      "\"randomized\"\n",
      ", Scikit-Learn uses a stoƒ\n",
      "chastic algorithm called \n",
      " R a n d o m iz e d PCA\n",
      "  tha t quickly finds an a pproxima tion of the\n",
      "first \n",
      "d\n",
      "  principal com ponen ts. I ts com puta tional com plexity is \n",
      "O\n",
      "(\n",
      "m\n",
      " „ \n",
      "d\n",
      "2\n",
      ") + \n",
      "O\n",
      "(\n",
      "d\n",
      "3\n",
      "),\n",
      "instead of \n",
      "O\n",
      "(\n",
      "m\n",
      " „ \n",
      "n\n",
      "2\n",
      ") + \n",
      "O\n",
      "(\n",
      "n\n",
      "3\n",
      " ) for the full SVD a pproach, so it is drama tically faster\n",
      "than full SVD when \n",
      "d\n",
      "  is m uch smaller than \n",
      "n\n",
      ":\n",
      "rnd_pca\n",
      " \n",
      "=\n",
      " \n",
      "PCA\n",
      "(\n",
      "n_components\n",
      "=\n",
      "154\n",
      ",\n",
      " \n",
      "svd_solver\n",
      "=\n",
      "\"randomized\"\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "rnd_pca\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      " By defa ult, \n",
      "svd_solver\n",
      " is actually set to \n",
      "\"auto\"\n",
      " : Scikit-Learn a utoma tically uses the\n",
      "randomized PCA algorithm if \n",
      "m\n",
      " or \n",
      "n\n",
      "  is grea ter than 500 and \n",
      "d\n",
      " \n",
      "is less than 80% of \n",
      "m\n",
      "or \n",
      "n\n",
      " , or else it uses the full SVD a pproach. If you wan t to force Scikit-Learn to use full\n",
      " SVD , you can set the \n",
      "svd_solver\n",
      "  h yperparameter to \n",
      "\"full\"\n",
      ".\n",
      "Incremental PCA\n",
      " One problem with the preceding im plemen ta tions of PCA is tha t they require the\n",
      " whole training set to fit in memor y in order for the algorithm to run. F ortuna tely ,\n",
      " I n cr em en t a l PCA\n",
      "  (IPCA) algorithms ha ve been developed: you can split the training\n",
      " set in to mini-ba tches and feed an IPCA algorithm one mini-ba tch a t a time. This is\n",
      " PCA  |  227\n",
      "\n",
      "5\n",
      " Scikit-Learn uses the algorithm described in —Incremen tal Learning for Robust V isual T racking, – D . Ross et al.\n",
      "(2007).\n",
      " useful for large training sets, and also to a pply PCA online (i.e., on the fly , as new\n",
      "instances arrive).\n",
      " The following code splits the MNIST da taset in to 100 mini-ba tches (using N umPy‡ s\n",
      "array_split()\n",
      "  function) and feeds them to Scikit-Learn ‡ s \n",
      "IncrementalPCA\n",
      " \n",
      "class\n",
      "5\n",
      " to \n",
      " reduce the dimensionality of the MNIST da taset down to 154 dimensions (just like\n",
      " before). N ote tha t you m ust call the \n",
      "partial_fit()\n",
      "  method with each mini-ba tch\n",
      " ra ther than the \n",
      "fit()\n",
      " method with the whole training set:\n",
      "from\n",
      " \n",
      "sklearn.decomposition\n",
      " \n",
      "import\n",
      " \n",
      "IncrementalPCA\n",
      "n_batches\n",
      " \n",
      "=\n",
      " \n",
      "100\n",
      "inc_pca\n",
      " \n",
      "=\n",
      " \n",
      "IncrementalPCA\n",
      "(\n",
      "n_components\n",
      "=\n",
      "154\n",
      ")\n",
      "for\n",
      " \n",
      "X_batch\n",
      " \n",
      "in\n",
      " \n",
      "np\n",
      ".\n",
      "array_split\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "n_batches\n",
      "):\n",
      "    \n",
      "inc_pca\n",
      ".\n",
      "partial_fit\n",
      "(\n",
      "X_batch\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "inc_pca\n",
      ".\n",
      "transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      " Alterna tively , you can use N umPy‡ s \n",
      "memmap\n",
      "  class, which allows you to manipula te a\n",
      " large arra y stored in a binar y file on disk as if it were en tirely in memor y ; the class\n",
      " loads only the da ta it needs in memor y , when it needs it. Since the \n",
      "IncrementalPCA\n",
      " class uses only a small part of the arra y a t an y given time, the memor y usage remains\n",
      " under con trol. This makes it possible to call the usual \n",
      "fit()\n",
      " \n",
      "method, as you can see\n",
      "in the following code:\n",
      "X_mm\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "memmap\n",
      "(\n",
      "filename\n",
      ",\n",
      " \n",
      "dtype\n",
      "=\n",
      "\"float32\"\n",
      ",\n",
      " \n",
      "mode\n",
      "=\n",
      "\"readonly\"\n",
      ",\n",
      " \n",
      "shape\n",
      "=\n",
      "(\n",
      "m\n",
      ",\n",
      " \n",
      "n\n",
      "))\n",
      "batch_size\n",
      " \n",
      "=\n",
      " \n",
      "m\n",
      " \n",
      "//\n",
      " \n",
      "n_batches\n",
      "inc_pca\n",
      " \n",
      "=\n",
      " \n",
      "IncrementalPCA\n",
      "(\n",
      "n_components\n",
      "=\n",
      "154\n",
      ",\n",
      " \n",
      "batch_size\n",
      "=\n",
      "batch_size\n",
      ")\n",
      "inc_pca\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_mm\n",
      ")\n",
      "Kernel PCA\n",
      "In \n",
      " Cha pter 5\n",
      "  we discussed the kernel trick, a ma thema tical technique tha t im plicitly\n",
      " ma ps instances in to a ver y high-dimensional space (called the \n",
      " f e a t u r e s p a c e\n",
      "), enabling\n",
      " nonlinear classifica tion and regression with Support V ector M achines. Recall tha t a\n",
      " linear decision boundar y in the high-dimensional fea ture space corresponds to a\n",
      " com plex nonlinear decision boundar y in the \n",
      " o r i g i n a l s p a c e\n",
      ".\n",
      " I t turns out tha t the same trick can be a pplied to PCA, making it possible to perform\n",
      " com plex nonlinear projections for dimensionality reduction. This is called \n",
      " K er n e l\n",
      " 228  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      "6\n",
      " —K ernel Principal Com ponen t Analysis, – B ﬁller (1999).\n",
      "PCA\n",
      " \n",
      "(kPCA)\n",
      ".\n",
      "6\n",
      "  I t is often good a t preser ving clusters of instances after projection, or\n",
      " sometimes even unrolling da tasets tha t lie close to a twisted manifold.\n",
      " F or exam ple, the following code uses Scikit-Learn ‡ s \n",
      "KernelPCA\n",
      " \n",
      "class to perform kPCA\n",
      "with an RBF kernel (see \n",
      " Cha pter 5\n",
      " for more details about the RBF kernel and the\n",
      "other kernels):\n",
      "from\n",
      " \n",
      "sklearn.decomposition\n",
      " \n",
      "import\n",
      " \n",
      "KernelPCA\n",
      "rbf_pca\n",
      " \n",
      "=\n",
      " \n",
      "KernelPCA\n",
      "(\n",
      "n_components\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "kernel\n",
      "=\n",
      "\"rbf\"\n",
      ",\n",
      " \n",
      "gamma\n",
      "=\n",
      "0.04\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "rbf_pca\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X\n",
      ")\n",
      "Figure 8-10\n",
      " shows the Swiss roll, reduced to two dimensions using a linear kernel\n",
      " (equivalen t to sim ply using the \n",
      "PCA\n",
      " class), an RBF kernel, and a sigmoid kernel\n",
      "(Logistic).\n",
      " F i g u r e 8-10. S w i s s r o l l r e d u c e d t o 2D u s i n g kPCA w i t h v a r i o u s k er n e l s\n",
      "Selecting a Kernel and Tuning Hyperparameters\n",
      " As kPCA is an unsuper vised learning algorithm, there is no obvious performance\n",
      " measure to help you select the best kernel and h yperparameter values. H owever ,\n",
      " dimensionality reduction is often a prepara tion step for a super vised learning task\n",
      " (e.g., classifica tion), so you can sim ply use grid search to select the kernel and h yperƒ\n",
      " parameters tha t lead to the best performance on tha t task. F or exam ple, the following\n",
      " code crea tes a two-step pipeline, first reducing dimensionality to two dimensions\n",
      " using kPCA, then a pplying Logistic Regression for classifica tion. Then it uses \n",
      "Grid\n",
      "SearchCV\n",
      " to find the best kernel and gamma value for kPCA in order to get the best\n",
      " classifica tion accuracy a t the end of the pipeline:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "GridSearchCV\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "LogisticRegression\n",
      "from\n",
      " \n",
      "sklearn.pipeline\n",
      " \n",
      "import\n",
      " \n",
      "Pipeline\n",
      " Kernel PCA  |  229\n",
      "\n",
      "clf\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "        \n",
      "(\n",
      "\"kpca\"\n",
      ",\n",
      " \n",
      "KernelPCA\n",
      "(\n",
      "n_components\n",
      "=\n",
      "2\n",
      ")),\n",
      "        \n",
      "(\n",
      "\"log_reg\"\n",
      ",\n",
      " \n",
      "LogisticRegression\n",
      "())\n",
      "    \n",
      "])\n",
      "param_grid\n",
      " \n",
      "=\n",
      " \n",
      "[{\n",
      "        \n",
      "\"kpca__gamma\"\n",
      ":\n",
      " \n",
      "np\n",
      ".\n",
      "linspace\n",
      "(\n",
      "0.03\n",
      ",\n",
      " \n",
      "0.05\n",
      ",\n",
      " \n",
      "10\n",
      "),\n",
      "        \n",
      "\"kpca__kernel\"\n",
      ":\n",
      " \n",
      "[\n",
      "\"rbf\"\n",
      ",\n",
      " \n",
      "\"sigmoid\"\n",
      "]\n",
      "    \n",
      "}]\n",
      "grid_search\n",
      " \n",
      "=\n",
      " \n",
      "GridSearchCV\n",
      "(\n",
      "clf\n",
      ",\n",
      " \n",
      "param_grid\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ")\n",
      "grid_search\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " The best kernel and h yperparameters are then a vailable through the \n",
      "best_params_\n",
      "variable:\n",
      ">>> \n",
      "print\n",
      "(\n",
      "grid_search\n",
      ".\n",
      "best_params_\n",
      ")\n",
      "{•kpca__gamma•: 0.043333333333333335, •kpca__kernel•: •rbf•}\n",
      " Another a pproach, this time en tirely unsuper vised, is to select the kernel and h yperƒ\n",
      " parameters tha t yield the lowest reconstruction error . H owever , reconstruction is not\n",
      " as easy as with linear PCA. H ere ‡ s wh y . \n",
      "Figure 8-11\n",
      " shows the original Swiss roll 3D\n",
      " da taset (top left), and the resulting 2D da taset after kPCA is a pplied using an RBF\n",
      " kernel (top righ t). Thanks to the kernel trick, this is ma thema tically equivalen t to\n",
      " ma pping the training set to an infinite-dimensional fea ture space (bottom righ t)\n",
      "using the \n",
      " f e a t u r e m a p\n",
      " \n",
      "€\n",
      ", then projecting the transformed training set down to 2D\n",
      " using linear PCA. N otice tha t if we could in vert the linear PCA step for a given\n",
      " instance in the reduced space, the reconstructed poin t would lie in fea ture space, not\n",
      " in the original space (e.g., like the one represen ted by an x in the diagram). Since the\n",
      " fea ture space is infinite-dimensional, we cannot com pute the reconstructed poin t,\n",
      " and therefore we cannot com pute the true reconstruction error . F ortuna tely , it is posƒ\n",
      " sible to find a poin t in the original space tha t would ma p close to the reconstructed\n",
      " poin t. This is called the \n",
      "reconstruction \n",
      " p r e-i m a ge\n",
      " . Once you ha ve this pre-image, you\n",
      " can measure its squared distance to the original instance. Y ou can then select the kerƒ\n",
      " nel and h yperparameters tha t minimize this reconstruction pre-image error .\n",
      " 230  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      "7\n",
      " Scikit-Learn uses the algorithm based on K ernel Ridge Regression described in Gokhan H. Bak°r , J ason\n",
      " W eston, and B ernhard Scholkopf, \n",
      " —Learning to Find Pre-images –\n",
      "  (T ubingen, German y : M ax Planck Institute\n",
      "for Biological Cybernetics, 2004).\n",
      " F i g u r e 8-11. K er n e l PCA a n d t h e r e c o ns t r u c t i o n p r e-i m a ge er r o r\n",
      " Y ou ma y be wondering how to perform this reconstruction. One solution is to train a\n",
      " super vised regression model, with the projected instances as the training set and the\n",
      " original instances as the targets. Scikit-Learn will do this a utoma tically if you \n",
      "set\n",
      "fit_inverse_transform=True\n",
      ", as shown in the following code:\n",
      "7\n",
      "rbf_pca\n",
      " \n",
      "=\n",
      " \n",
      "KernelPCA\n",
      "(\n",
      "n_components\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "kernel\n",
      "=\n",
      "\"rbf\"\n",
      ",\n",
      " \n",
      "gamma\n",
      "=\n",
      "0.0433\n",
      ",\n",
      "                    \n",
      "fit_inverse_transform\n",
      "=\n",
      "True\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "rbf_pca\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X\n",
      ")\n",
      "X_preimage\n",
      " \n",
      "=\n",
      " \n",
      "rbf_pca\n",
      ".\n",
      "inverse_transform\n",
      "(\n",
      "X_reduced\n",
      ")\n",
      " By defa ult, \n",
      "fit_inverse_transform=False\n",
      " and \n",
      "KernelPCA\n",
      " has no\n",
      "inverse_transform()\n",
      "  method. This method only gets crea ted\n",
      "when you set \n",
      "fit_inverse_transform=True\n",
      ".\n",
      " Kernel PCA  |  231\n",
      "\n",
      "8\n",
      " —N onlinear Dimensionality Reduction by Locally Linear Embedding, – S. Roweis, L. Sa ul (2000).\n",
      " Y ou can then com pute the reconstruction pre-image error :\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "mean_squared_error\n",
      ">>> \n",
      "mean_squared_error\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "X_preimage\n",
      ")\n",
      "32.786308795766132\n",
      " N ow you can use grid search with cross-valida tion to find the kernel and h yperparaƒ\n",
      " meters tha t minimize this pre-image reconstruction error .\n",
      "LLE\n",
      " L o c a l l y L i n e a r E m b e d d i n g\n",
      " \n",
      "(LLE)\n",
      "8\n",
      " \n",
      " is another ver y powerful \n",
      " n o n l i n e a r d i m ens i o n a l i ty\n",
      " r e d u c t i o n\n",
      "  (NLDR) technique. I t is a M anifold Learning technique tha t does not rely\n",
      " on projections like the previous algorithms. In a n utshell, LLE works by first measurƒ\n",
      " ing how each training instance linearly rela tes to its closest neighbors (c.n.), and then\n",
      " looking for a low-dimensional represen ta tion of the training set where these local\n",
      " rela tionships are best preser ved (more details shortly). This makes it particularly\n",
      " good a t unrolling twisted manifolds, especially when there is not too m uch noise.\n",
      " F or exam ple, the following code uses Scikit-Learn ‡ s \n",
      "LocallyLinearEmbedding\n",
      "   class  to\n",
      " unroll the Swiss roll. The resulting 2D da taset is shown in \n",
      "Figure 8-12\n",
      ". As you can\n",
      " see, the Swiss roll is com pletely unrolled and the distances between instances are\n",
      " locally well preser ved. H owever , distances are not preser ved on a larger scale: the left\n",
      " part of the unrolled Swiss roll is stretched, while the righ t part is squeezed. N evertheƒ\n",
      " less, LLE did a pretty good job a t modeling the manifold.\n",
      "from\n",
      " \n",
      "sklearn.manifold\n",
      " \n",
      "import\n",
      " \n",
      "LocallyLinearEmbedding\n",
      "lle\n",
      " \n",
      "=\n",
      " \n",
      "LocallyLinearEmbedding\n",
      "(\n",
      "n_components\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "n_neighbors\n",
      "=\n",
      "10\n",
      ")\n",
      "X_reduced\n",
      " \n",
      "=\n",
      " \n",
      "lle\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X\n",
      ")\n",
      " 232  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      " F i g u r e 8-12. U n r o l l e d S w i s s r o l l u s i n g LLE\n",
      " H ere ‡ s how LLE works: first, for each training instance \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " , the algorithm iden tifies its\n",
      "k\n",
      " closest neighbors (in the preceding code \n",
      "k\n",
      " = 10), then tries to reconstruct \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " as a\n",
      " linear function of these neighbors. M ore specifically , it finds the weigh ts \n",
      "w\n",
      "i,j\n",
      "   such tha t\n",
      "the squared distance between \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      "   and \n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      "x\n",
      "j\n",
      "   is as small as possible, assuming \n",
      "w\n",
      "i,j\n",
      "= 0 if \n",
      "x\n",
      "(\n",
      "j\n",
      ")\n",
      " is not one of the \n",
      "k\n",
      " closest neighbors of \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " . Th us the first step of LLE is the\n",
      " constrained optimiza tion problem described in \n",
      " Equa tion 8-4\n",
      ", where \n",
      "W\n",
      " \n",
      " is the weigh t\n",
      " ma trix con taining all the weigh ts \n",
      "w\n",
      "i,j\n",
      " . The second constrain t sim ply normalizes the\n",
      " weigh ts for each training instance \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      ".\n",
      " LLE  |  233\n",
      "\n",
      " Eq u a t i o n 8-4. LLE s t e p 1: l i n e a r l y m o d e l i n g l o c a l r e l a t i o ns h i p s\n",
      "W\n",
      " = argmin\n",
      "W\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "x\n",
      "i\n",
      "”\n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      "x\n",
      "j\n",
      "2\n",
      " subject to\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      " = 0 if\n",
      "x\n",
      "j\n",
      " is not one of the\n",
      "k\n",
      " c.n. of\n",
      "x\n",
      "i\n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      " = 1 for\n",
      "i\n",
      " = 1, 2,\n",
      ",\n",
      "m\n",
      " After this step , the weigh t ma trix \n",
      "W\n",
      "  (con taining the weigh ts \n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      ") encodes the local\n",
      " linear rela tionships between the training instances. N ow the second step is to ma p the\n",
      " training instances in to a \n",
      "d\n",
      "-dimensional space (where \n",
      "d\n",
      " < \n",
      "n\n",
      " ) while preser ving these\n",
      " local rela tionships as m uch as possible. If \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      " is the image of \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " in this \n",
      "d\n",
      "-dimensional\n",
      " space, then we wan t the squared distance between \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      " and \n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      "z\n",
      "j\n",
      " to be as small\n",
      " as possible. This idea leads to the unconstrained optimiza tion problem described in\n",
      " Equa tion 8-5\n",
      " . I t looks ver y similar to the first step , but instead of keeping the instanƒ\n",
      " ces fixed and finding the optimal weigh ts, we are doing the reverse: keeping the\n",
      " weigh ts fixed and finding the optimal position of the instances ‡ images in the low-\n",
      " dimensional space. N ote tha t \n",
      "Z\n",
      "  is the ma trix con taining all \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      ".\n",
      " Eq u a t i o n 8-5. LLE s t e p 2: r e d u ci n g d i m ens i o n a l i ty w h i l e p r e s er v i n g r e l a t i o ns h i p s\n",
      "Z\n",
      " = argmin\n",
      "Z\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "z\n",
      "i\n",
      "”\n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      "z\n",
      "j\n",
      "2\n",
      " Scikit-Learn ‡ s LLE im plemen ta tion has the following com puta tional com plexity :\n",
      "O\n",
      "(\n",
      "m\n",
      " \n",
      "log(\n",
      "m\n",
      ")\n",
      "n\n",
      " \n",
      "log(\n",
      "k\n",
      "))\n",
      " for finding the \n",
      "k\n",
      " nearest neighbors, \n",
      "O\n",
      "(\n",
      " m n k\n",
      "3\n",
      ") for optimizing the\n",
      " weigh ts, and \n",
      "O\n",
      "(\n",
      " d m\n",
      "2\n",
      " ) for constructing the low-dimensional represen ta tions. U nfortuƒ\n",
      " na tely , the \n",
      "m\n",
      "2\n",
      "  in the last term makes this algorithm scale poorly to ver y large da tasets.\n",
      "Other Dimensionality Reduction Techniques\n",
      " There are man y other dimensionality reduction techniques, several of which are\n",
      " a vailable in Scikit-Learn. H ere are some of the most popular :\n",
      "⁄\n",
      " M u l t i d i m ens i o n a l S c a l i n g\n",
      "  (MDS) reduces dimensionality while tr ying to preser ve\n",
      "the distances between the instances (see \n",
      "Figure 8-13\n",
      ").\n",
      " 234  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      "9\n",
      " The geodesic distance between two nodes in a gra ph is the n umber of nodes on the shortest pa th between\n",
      "these nodes.\n",
      "⁄\n",
      " I s o m a p\n",
      "   crea tes \n",
      " a gra ph by connecting each instance to its nearest neighbors, then\n",
      " reduces dimensionality while tr ying to preser ve the \n",
      " ge o d e s i c d i s t a n c e s\n",
      "9\n",
      " \n",
      "between\n",
      "the instances.\n",
      "⁄\n",
      " t-D i s t r i b u t e d S t o c h as t i c N ei gh b o r E m b e d d i n g\n",
      " (t-SNE) reduces dimensionality\n",
      " while tr ying to keep similar instances close and dissimilar instances a part. I t is\n",
      " mostly used for visualiza tion, in particular to visualize clusters of instances in\n",
      "high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
      "⁄\n",
      " L i n e a r D i s cr i m i n a n t An a l ys i s\n",
      "   (LD A) is  actually a classifica tion algorithm, but durƒ\n",
      " ing training it learns the most discrimina tive axes between the classes, and these\n",
      " axes can then be used to define a h yperplane on to which to project the da ta. The\n",
      " benefit is tha t the projection will keep classes as far a part as possible, so LD A is a\n",
      " good technique to reduce dimensionality before running another classifica tion\n",
      " algorithm such as an SVM classifier .\n",
      " F i g u r e 8-13. R e d u ci n g t h e S w i s s r o l l t o 2D u s i n g v a r i o u s t e c h n i q u e s\n",
      "Exercises\n",
      "1.\n",
      " Wha t are the main motiva tions for reducing a da taset ‡ s dimensionality? Wha t are\n",
      " the main dra wbacks?\n",
      "2.\n",
      " Wha t is the curse of dimensionality?\n",
      "3.\n",
      " Once a da taset ‡ s dimensionality has been reduced, is it possible to reverse the\n",
      " opera tion? If so , how? If not, wh y?\n",
      "4.\n",
      " Can PCA be used to reduce the dimensionality of a highly nonlinear da taset?\n",
      "5.\n",
      " Suppose you perform PCA on a 1,000-dimensional da taset, setting the explained\n",
      " variance ra tio to 95%. H ow man y dimensions will the resulting da taset ha ve?\n",
      " Exercises  |  235\n",
      "\n",
      "6.\n",
      " In wha t cases would you use vanilla PCA, Incremen tal PCA, R andomized PCA,\n",
      " or K ernel PCA?\n",
      "7.\n",
      " H ow can you evalua te the performance of a dimensionality reduction algorithm\n",
      " on your da taset?\n",
      "8.\n",
      " Does it make an y sense to chain two differen t dimensionality reduction algoƒ\n",
      "rithms?\n",
      "9.\n",
      " Load the MNIST da taset (in troduced in \n",
      " Cha pter 3\n",
      " ) and split it in to a training set\n",
      "and a test set (take the first 60,000 instances for training, and the remaining\n",
      " 10,000 for testing). T rain a R andom F orest classifier on the da taset and time how\n",
      " long it takes, then evalua te the resulting model on the test set. N ext, use PCA to\n",
      " reduce the da taset ‡ s dimensionality , with an explained variance ra tio of 95%.\n",
      " T rain a new R andom F orest classifier on the reduced da taset and see how long it\n",
      " takes. W as training m uch faster? N ext evalua te the classifier on the test set: how\n",
      " does it com pare to the previous classifier?\n",
      "10.\n",
      " U se t-SNE to reduce the MNIST da taset down to two dimensions and plot the\n",
      " result using M a tplotlib . Y ou can use a sca tterplot using 10 differen t colors to repƒ\n",
      " resen t each image ‡ s target class. Alterna tively , you can write colored digits a t the\n",
      " loca tion of each instance, or even plot scaled-down versions of the digit images\n",
      " themselves (if you plot all digits, the visualiza tion will be too cluttered, so you\n",
      " should either dra w a random sam ple or plot an instance only if no other instance\n",
      " has already been plotted a t a close distance). Y ou should get a nice visualiza tion\n",
      " with well-separa ted clusters of digits. T r y using other dimensionality reduction\n",
      " algorithms such as PCA, LLE, or MDS and com pare the resulting visualiza tions.\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " 236  |  Chapter 8: Dimensionality Reduction\n",
      "\n",
      "CHAPTER 9\n",
      "Unsupervised Learning Techniques\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 9 in the final\n",
      "release of the book.\n",
      " Although most of the a pplica tions of M achine Learning toda y are based on superƒ\n",
      " vised learning (and as a result, this is where most of the in vestmen ts go to), the vast\n",
      " ma jority of the a vailable da ta is actually unlabeled: we ha ve the in put fea tures \n",
      "X\n",
      ", but\n",
      " we do not ha ve the labels \n",
      "y\n",
      " . Y ann LeCun famously said tha t — if in telligence was a cake,\n",
      " unsuper vised learning would be the cake, super vised learning would be the icing on\n",
      " the cake, and reinforcemen t learning would be the cherr y on the cake – . In other\n",
      " words, there is a h uge poten tial in unsuper vised learning tha t we ha ve only barely\n",
      " started to sink our teeth in to .\n",
      " F or exam ple, sa y you wan t to crea te a system tha t will take a few pictures of each item\n",
      " on a man ufacturing production line and detect which items are defective. Y ou can\n",
      " fairly easily crea te a system tha t will take pictures a utoma tically , and this migh t give\n",
      " you thousands of pictures ever y da y . Y ou can then build a reasonably large da taset in\n",
      " just a few weeks. But wait, there are no labels! If you wan t to train a regular binar y\n",
      " classifier tha t will predict whether an item is defective or not, you will need to label\n",
      " ever y single picture as — defective – or — normal – . This will generally require h uman\n",
      " experts to sit down and man ually go through all the pictures. This is a long, costly\n",
      " and tedious task, so it will usually only be done on a small subset of the a vailable picƒ\n",
      " tures. As a result, the labeled da taset will be quite small, and the classifier‡ s perforƒ\n",
      " mance will be disa ppoin ting. M oreover , ever y time the com pan y makes an y change to\n",
      " its products, the whole process will need to be started over from scra tch. W ouldn ‡ t it\n",
      "237\n",
      "\n",
      " be grea t if the algorithm could just exploit the unlabeled da ta without needing\n",
      " h umans to label ever y picture? En ter unsuper vised learning.\n",
      "In \n",
      " Cha pter 8\n",
      " , we looked a t the most common unsuper vised learning task: dimensionƒ\n",
      " ality reduction. In this cha pter , we will look a t a few more unsuper vised learning tasks\n",
      "and algorithms:\n",
      "⁄\n",
      " Cl u s t er i n g\n",
      " : the goal is to group similar instances together in to \n",
      " c l u s t er s\n",
      ". This is a\n",
      " grea t tool for da ta analysis, customer segmen ta tion, recommender systems,\n",
      " search engines, image segmen ta tion, semi-super vised learning, dimensionality\n",
      "reduction, and more.\n",
      "⁄\n",
      " An o m a l y d e t e c t i o n\n",
      " : the objective is to learn wha t — normal – da ta looks like, and\n",
      "use this to detect abnormal instances, such as defective items on a production\n",
      "line or a new trend in a time series.\n",
      "⁄\n",
      " D ens i ty e s t i m a t i o n\n",
      " : this is the task of estima ting the \n",
      " p r o b a b i l i ty d ens i ty f u n c t i o n\n",
      " (PDF) of the random process tha t genera ted the da taset. This is commonly used\n",
      " for anomaly detection: instances loca ted in ver y low-density regions are likely to\n",
      " be anomalies. I t is also useful for da ta analysis and visualiza tion.\n",
      " Ready for some cake? W e will start with clustering, using K-M eans and DBSCAN,\n",
      " and then we will discuss Ga ussian mixture models and see how they can be used for\n",
      " density estima tion, clustering, and anomaly detection.\n",
      "Clustering\n",
      " As you en joy a hike in the moun tains, you stumble upon a plan t you ha ve never seen\n",
      " before. Y ou look around and you notice a few more. They are not perfectly iden tical,\n",
      " yet they are sufficien tly similar for you to know tha t they most likely belong to the\n",
      " same species (or a t least the same gen us). Y ou ma y need a botanist to tell you wha t\n",
      " species tha t is, but you certainly don ‡ t need an expert to iden tif y groups of similar -\n",
      "looking objects. This is called \n",
      " c l u s t er i n g\n",
      " : it is the task of iden tif ying similar instances\n",
      "and assigning them to \n",
      " c l u s t er s\n",
      ", i.e., groups of similar instances.\n",
      " J ust like in classifica tion, each instance gets assigned to a group . H owever , this is an\n",
      " unsuper vised task. Consider \n",
      "Figure 9-1\n",
      " : on the left is the iris da taset (in troduced in\n",
      " Cha pter 4\n",
      " ), where each instance ‡ s species (i.e., its class) is represen ted with a differen t\n",
      " marker . I t is a labeled da taset, for which classifica tion algorithms such as Logistic\n",
      " Regression, SVMs or R andom F orest classifiers are well suited. On the righ t is the\n",
      " same da taset, but without the labels, so you cannot use a classifica tion algorithm an yƒ\n",
      " more. This is where clustering algorithms step in: man y of them can easily detect the\n",
      " top left cluster . I t is also quite easy to see with our own eyes, but it is not so obvious\n",
      " tha t the lower righ t cluster is actually com posed of two distinct sub-clusters. Tha t\n",
      " said, the da taset actually has two additional fea tures (sepal length and width), not\n",
      " 238  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " represen ted here, and clustering algorithms can make good use of all fea tures, so in\n",
      " fact they iden tif y the three clusters fairly well (e.g., using a Ga ussian mixture model,\n",
      "only 5 instances out of 150 are assigned to the wrong cluster).\n",
      " F i g u r e 9-1. \n",
      "Classi†cation\n",
      " \n",
      "(le“)\n",
      "  v er s u s c l u s t er i n g (r i gh t)\n",
      " Clustering is used in a wide variety of a pplica tions, including:\n",
      "⁄\n",
      " F or customer segmen ta tion: you can cluster your customers based on their purƒ\n",
      "chases, their activity on your website, and so on. This is useful to understand who\n",
      " your customers are and wha t they need, so you can ada pt your products and\n",
      " marketing cam paigns to each segmen t. F or exam ple, this can be useful in \n",
      " r e c o m‡\n",
      " m en d er s ys t ems\n",
      "  to suggest con ten t tha t other users in the same cluster en joyed.\n",
      "⁄\n",
      " F or da ta analysis: when analyzing a new da taset, it is often useful to first discover\n",
      " clusters of similar instances, as it is often easier to analyze clusters separa tely .\n",
      "⁄\n",
      " As a dimensionality reduction technique: once a da taset has been clustered, it is\n",
      " usually possible to measure each instance ‡ s \n",
      "a⁄nity\n",
      " with each cluster (affinity is\n",
      " an y measure of how well an instance fits in to a cluster). Each instance ‡ s fea ture\n",
      "vector \n",
      "x\n",
      " \n",
      "can then be replaced with the vector of its cluster affinities. If there are \n",
      "k\n",
      "clusters, then this vector is \n",
      "k\n",
      "  dimensional. This is typically m uch lower dimenƒ\n",
      " sional than the original fea ture vector , but it can preser ve enough informa tion for\n",
      "further processing.\n",
      "⁄\n",
      " F or \n",
      " a n o m a l y d e t e c t i o n\n",
      " (also called \n",
      " o u t l i er d e t e c t i o n\n",
      " ): an y instance tha t has a low\n",
      " affinity to all the clusters is likely to be an anomaly . F or exam ple, if you ha ve clusƒ\n",
      " tered the users of your website based on their beha vior , you can detect users with\n",
      " un usual beha vior , such as an un usual n umber of requests per second, and so on.\n",
      " Anomaly detection is particularly useful in detecting defects in man ufacturing, or\n",
      "for \n",
      " f r a u d d e t e c t i o n\n",
      ".\n",
      "⁄\n",
      " F or semi-super vised learning: if you only ha ve a few labels, you could perform\n",
      " clustering and propaga te the labels to all the instances in the same cluster . This\n",
      " can grea tly increase the amoun t of labels a vailable for a subsequen t super vised\n",
      " learning algorithm, and th us im prove its performance.\n",
      " Clustering  |  239\n",
      "\n",
      "1\n",
      " —Least square quan tiza tion in PCM, – Stuart P . Lloyd. (1982).\n",
      "⁄\n",
      " F or search engines: for exam ple, some search engines let you search for images\n",
      " tha t are similar to a reference image. T o build such a system, you would first\n",
      " a pply a clustering algorithm to all the images in your da tabase: similar images\n",
      " would end up in the same cluster . Then when a user provides a reference image,\n",
      " all you need to do is to find this image ‡ s cluster using the trained clustering\n",
      " model, and you can then sim ply return all the images from this cluster .\n",
      "⁄\n",
      " T o segmen t an image: by clustering pixels according to their color , then replacing\n",
      " each pixel ‡ s color with the mean color of its cluster , it is possible to reduce the\n",
      " n umber of differen t colors in the image considerably . This technique is used in\n",
      " man y object detection and tracking systems, as it makes it easier to detect the\n",
      " con tour of each object.\n",
      " There is no universal definition of wha t a cluster is: it really depends on the con text,\n",
      " and differen t algorithms will ca pture differen t kinds of clusters. F or exam ple, some\n",
      " algorithms look for instances cen tered around a particular poin t, called a \n",
      " c en t r o i d\n",
      ".\n",
      " Others look for con tin uous regions of densely packed instances: these clusters can\n",
      " take on an y sha pe. Some algorithms are hierarchical, looking for clusters of clusters.\n",
      "And the list goes on.\n",
      " In this section, we will look a t two popular clustering algorithms: K-M eans and\n",
      " DBSCAN, and we will show some of their a pplica tions, such as non-linear dimenƒ\n",
      " sionality reduction, semi-super vised learning and anomaly detection.\n",
      "K-Means\n",
      " Consider the unlabeled da taset represen ted in \n",
      "Figure 9-2\n",
      ": you can clearly see 5 blobs\n",
      " of instances. The K-M eans algorithm is a sim ple algorithm ca pable of clustering this\n",
      " kind of da taset ver y quickly and efficien tly , often in just a few itera tions. I t was proƒ\n",
      " posed by Stuart Lloyd a t the B ell Labs in 1957 as a technique for pulse-code modulaƒ\n",
      " tion, but it was only published outside of the com pan y in 1982, in a pa per titled\n",
      " —Least square quan tiza tion in PCM –\n",
      ".\n",
      "1\n",
      "  By then, in 1965, Edward W . F org y had pubƒ\n",
      " lished virtually the same algorithm, so K-M eans is sometimes referred to as Lloyd-\n",
      " F org y .\n",
      " 240  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " F i g u r e 9-2. An u n l a b e l e d d a t as e t c o m p o s e d o f \n",
      "†ve\n",
      "  b l o b s o f i ns t a n c e s\n",
      " Let ‡ s train a K-M eans clusterer on this da taset. I t will tr y to find each blob ‡ s cen ter and\n",
      "assign each instance to the closest blob:\n",
      "from\n",
      " \n",
      "sklearn.cluster\n",
      " \n",
      "import\n",
      " \n",
      "KMeans\n",
      "k\n",
      " \n",
      "=\n",
      " \n",
      "5\n",
      "kmeans\n",
      " \n",
      "=\n",
      " \n",
      "KMeans\n",
      "(\n",
      "n_clusters\n",
      "=\n",
      "k\n",
      ")\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "kmeans\n",
      ".\n",
      "fit_predict\n",
      "(\n",
      "X\n",
      ")\n",
      " N ote tha t you ha ve to specif y the n umber of clusters \n",
      "k\n",
      "  tha t the algorithm m ust find.\n",
      " In this exam ple, it is pretty obvious from looking a t the da ta tha t \n",
      "k\n",
      " \n",
      "should be set to 5,\n",
      " but in general it is not tha t easy . W e will discuss this shortly .\n",
      " Each instance was assigned to one of the 5 clusters. In the con text of clustering, an\n",
      " instance ‡ s \n",
      " l a b e l\n",
      "  is the index of the cluster tha t this instance gets assigned to by the\n",
      " algorithm: this is not to be confused with the class labels in classifica tion (remember\n",
      " tha t clustering is an unsuper vised learning task). The \n",
      "KMeans\n",
      "  instance preser ves a\n",
      " copy of the labels of the instances it was trained on, a vailable via the \n",
      "labels_\n",
      "   instance\n",
      "variable:\n",
      ">>> \n",
      "y_pred\n",
      "array([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n",
      ">>> \n",
      "y_pred\n",
      " \n",
      "is\n",
      " \n",
      "kmeans\n",
      ".\n",
      "labels_\n",
      "True\n",
      " W e can also take a look a t the 5 cen troids tha t the algorithm found:\n",
      ">>> \n",
      "kmeans\n",
      ".\n",
      "cluster_centers_\n",
      "array([[-2.80389616,  1.80117999],\n",
      "       [ 0.20876306,  2.25551336],\n",
      "       [-2.79290307,  2.79641063],\n",
      "       [-1.46679593,  2.28585348],\n",
      "       [-2.80037642,  1.30082566]])\n",
      " Of course, you can easily assign new instances to the cluster whose cen troid is closest:\n",
      " Clustering  |  241\n",
      "\n",
      ">>> \n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([[\n",
      "0\n",
      ",\n",
      " \n",
      "2\n",
      "],\n",
      " \n",
      "[\n",
      "3\n",
      ",\n",
      " \n",
      "2\n",
      "],\n",
      " \n",
      "[\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "3\n",
      "],\n",
      " \n",
      "[\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "2.5\n",
      "]])\n",
      ">>> \n",
      "kmeans\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      "array([1, 1, 2, 2], dtype=int32)\n",
      " If you plot the cluster‡ s decision boundaries, you get a V oronoi tessella tion (see\n",
      "Figure 9-3\n",
      " , where each cen troid is represen ted with an X):\n",
      " F i g u r e 9-3. K-M e a ns d e ci s i o n b o u n d a r i e s ( V o r o n o i t e s s e l l a t i o n)\n",
      " The vast ma jority of the instances were clearly assigned to the a ppropria te cluster , but\n",
      " a few instances were probably mislabeled (especially near the boundar y between the\n",
      " top left cluster and the cen tral cluster). Indeed, the K-M eans algorithm does not\n",
      " beha ve ver y well when the blobs ha ve ver y differen t diameters since all it cares about\n",
      " when assigning an instance to a cluster is the distance to the cen troid.\n",
      " Instead of assigning each instance to a single cluster , which is called \n",
      " h a r d c l u s t er i n g\n",
      ", it\n",
      " can be useful to just give each instance a score per cluster : this is called \n",
      "so“\n",
      "  c l u s t er i n g\n",
      ".\n",
      " F or exam ple, the score can be the distance between the instance and the cen troid, or\n",
      " con versely it can be a similarity score (or affinity) such as the Ga ussian R adial Basis\n",
      " F unction (in troduced in \n",
      " Cha pter 5\n",
      "). In the \n",
      "KMeans\n",
      " \n",
      "class, the \n",
      "transform()\n",
      " \n",
      "method\n",
      " measures the distance from each instance to ever y cen troid:\n",
      ">>> \n",
      "kmeans\n",
      ".\n",
      "transform\n",
      "(\n",
      "X_new\n",
      ")\n",
      "array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n",
      "       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n",
      "       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n",
      "       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\n",
      " In this exam ple, the first instance in \n",
      "X_new\n",
      "  is loca ted a t a distance of 2.81 from the\n",
      " first cen troid, 0.33 from the second cen troid, 2.90 from the third cen troid, 1.49 from\n",
      " the fourth cen troid and 2.87 from the fifth cen troid. If you ha ve a high-dimensional\n",
      " da taset and you transform it this wa y , you end up with a \n",
      "k\n",
      " -dimensional da taset: this\n",
      " can be a ver y efficien t non-linear dimensionality reduction technique.\n",
      " 242  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "2\n",
      " This can be proven by poin ting out tha t the mean squared distance between the instances and their closest\n",
      " cen troid can only go down a t each step .\n",
      "The K-Means Algorithm\n",
      " So how does the algorithm work? W ell it is really quite sim ple. Suppose you were\n",
      " given the cen troids: you could easily label all the instances in the da taset by assigning\n",
      " each of them to the cluster whose cen troid is closest. Con versely , if you were given all\n",
      " the instance labels, you could easily loca te all the cen troids by com puting the mean of\n",
      " the instances for each cluster . But you are given neither the labels nor the cen troids,\n",
      " so how can you proceed? W ell, just start by placing the cen troids randomly (e.g., by\n",
      "picking \n",
      "k\n",
      "  instances a t random and using their loca tions as cen troids). Then label the\n",
      " instances, upda te the cen troids, label the instances, upda te the cen troids, and so on\n",
      " un til the cen troids stop moving. The algorithm is guaran teed to con verge in a finite\n",
      " n umber of steps (usually quite small), it will not oscilla te forever\n",
      "2\n",
      " . Y ou can see the\n",
      "algorithm in action in \n",
      "Figure 9-4\n",
      " : the cen troids are initialized randomly (top left),\n",
      " then the instances are labeled (top righ t), then the cen troids are upda ted (cen ter left),\n",
      " the instances are relabeled (cen ter righ t), and so on. As you can see, in just 3 iteraƒ\n",
      " tions the algorithm has reached a clustering tha t seems close to optimal.\n",
      " F i g u r e 9-4. \n",
      "•e\n",
      "  K-M e a ns a l go r i t h m\n",
      " Clustering  |  243\n",
      "\n",
      " The com puta tional com plexity of the algorithm is generally linear\n",
      " with regards to the n umber of instances \n",
      "m\n",
      " , the n umber of clusters\n",
      "k\n",
      " \n",
      " and the n umber of dimensions \n",
      "n\n",
      " . H owever , this is only true when\n",
      " the da ta has a clustering structure. If it does not, then in the worst\n",
      " case scenario the com plexity can increase exponen tially with the\n",
      " n umber of instances. In practice, however , this rarely ha ppens, and\n",
      " K-M eans is generally one of the fastest clustering algorithms.\n",
      " U nfortuna tely , although the algorithm is guaran teed to con verge, it ma y not con verge\n",
      " to the righ t solution (i.e., it ma y con verge to a local optim um): this depends on the\n",
      " cen troid initializa tion. F or exam ple, \n",
      "Figure 9-5\n",
      "  shows two sub-optimal solutions tha t\n",
      " the algorithm can con verge to if you are not lucky with the random initializa tion step:\n",
      " F i g u r e 9-5. S u b-o p t i m a l s o l u t i o ns d u e t o u n l u c k y c en t r o i d i n i t i a l iz a t i o ns\n",
      " Let ‡ s look a t a few wa ys you can mitiga te this risk by im proving the cen troid initializaƒ\n",
      "tion.\n",
      "Centroid Initialization Methods\n",
      " If you ha ppen to know a pproxima tely where the cen troids should be (e.g., if you ran\n",
      "another clustering algorithm earlier), then you can set the \n",
      "init\n",
      "  h yperparameter to a\n",
      " N umPy arra y con taining the list of cen troids, and set \n",
      "n_init\n",
      " to 1:\n",
      "good_init\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([[\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "3\n",
      "],\n",
      " \n",
      "[\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "2\n",
      "],\n",
      " \n",
      "[\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "[\n",
      "-\n",
      "1\n",
      ",\n",
      " \n",
      "2\n",
      "],\n",
      " \n",
      "[\n",
      "0\n",
      ",\n",
      " \n",
      "2\n",
      "]])\n",
      "kmeans\n",
      " \n",
      "=\n",
      " \n",
      "KMeans\n",
      "(\n",
      "n_clusters\n",
      "=\n",
      "5\n",
      ",\n",
      " \n",
      "init\n",
      "=\n",
      "good_init\n",
      ",\n",
      " \n",
      "n_init\n",
      "=\n",
      "1\n",
      ")\n",
      " Another solution is to run the algorithm m ultiple times with differen t random initialƒ\n",
      " iza tions and keep the best solution. This is con trolled by the \n",
      "n_init\n",
      " \n",
      " h yperparameter :\n",
      " by defa ult, it is equal to 10, which means tha t the whole algorithm described earlier\n",
      "actually runs 10 times when you call \n",
      "fit()\n",
      ", and Scikit-Learn keeps the best solution.\n",
      " But how exactly does it know which solution is the best? W ell of course it uses a perƒ\n",
      " formance metric! I t is called the model ‡ s \n",
      " i n er t i a\n",
      ": this is the mean squared distance\n",
      " between each instance and its closest cen troid. I t is roughly equal to 223.3 for the\n",
      "model on the left of \n",
      "Figure 9-5\n",
      " , 237.5 for the model on the righ t of \n",
      "Figure 9-5\n",
      ", and\n",
      "211.6 for the model in \n",
      "Figure 9-3\n",
      ". The \n",
      "KMeans\n",
      " class runs the algorithm \n",
      "n_init\n",
      " \n",
      "times\n",
      " and keeps the model with the lowest inertia: in this exam ple, the model in \n",
      "Figure 9-3\n",
      " will be selected (unless we are ver y unlucky with \n",
      "n_init\n",
      " \n",
      "consecutive random initialiƒ\n",
      " 244  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "3\n",
      " —k-means\\++: The advan tages of careful seeding, – Da vid Arth ur and Sergei V assilvitskii (2006).\n",
      "4\n",
      " —U sing the T riangle Inequality to A ccelera te k-M eans, – Charles Elkan (2003).\n",
      " za tions). If you are curious, a model ‡ s inertia is accessible via the \n",
      "inertia_\n",
      " \n",
      "instance\n",
      "variable:\n",
      ">>> \n",
      "kmeans\n",
      ".\n",
      "inertia_\n",
      "211.59853725816856\n",
      "The \n",
      "score()\n",
      "  method returns the nega tive inertia. Wh y nega tive? W ell, it is beca use a\n",
      " predictor‡ s \n",
      "score()\n",
      "  method m ust alwa ys respect the \"\n",
      " g r e a t i s b e tt er\n",
      "\" rule.\n",
      ">>> \n",
      "kmeans\n",
      ".\n",
      "score\n",
      "(\n",
      "X\n",
      ")\n",
      "-211.59853725816856\n",
      " An im portan t im provemen t to the K-M eans algorithm, called \n",
      " K-M e a ns+\\+\n",
      ", was proƒ\n",
      "posed in a \n",
      " 2006 pa per\n",
      "  by Da vid Arth ur and Sergei V assilvitskii:\n",
      "3\n",
      "  they in troduced a\n",
      " smarter initializa tion step tha t tends to select cen troids tha t are distan t from one\n",
      " another , and this makes the K-M eans algorithm m uch less likely to con verge to a sub-\n",
      " optimal solution. They showed tha t the additional com puta tion required for the\n",
      " smarter initializa tion step is well worth it since it makes it possible to drastically\n",
      " reduce the n umber of times the algorithm needs to be run to find the optimal soluƒ\n",
      " tion. H ere is the K-M eans++ initializa tion algorithm:\n",
      "⁄\n",
      " T ake one cen troid \n",
      "c\n",
      "(1)\n",
      " , chosen uniformly a t random from the da taset.\n",
      "⁄\n",
      " T ake a new cen troid \n",
      "c\n",
      "(\n",
      "i\n",
      ")\n",
      ", choosing an instance \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      "  with probability : \n",
      "D\n",
      "i\n",
      "2\n",
      "“\n",
      "j\n",
      " = 1\n",
      "m\n",
      "D\n",
      "j\n",
      "2\n",
      "   where D(\n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      ") is the distance between the instance \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      "   and the closest\n",
      " cen troid tha t was already chosen. This probability distribution ensures tha t\n",
      " instances further a wa y from already chosen cen troids are m uch more likely be\n",
      " selected as cen troids.\n",
      "⁄\n",
      " Repea t the previous step un til all \n",
      "k\n",
      "  cen troids ha ve been chosen.\n",
      "The \n",
      "KMeans\n",
      "  class actually uses this initializa tion method by defa ult. If you wan t to\n",
      "force it to use the original method (i.e., picking \n",
      "k\n",
      " instances randomly to define the\n",
      " initial cen troids), then you can set the \n",
      "init\n",
      "  h yperparameter to \n",
      "\"random\"\n",
      " . Y ou will\n",
      "rarely need to do this.\n",
      "Accelerated K-Means and Mini-batch K-Means\n",
      " Another im portan t im provemen t to the K-M eans algorithm was proposed in a \n",
      "2003\n",
      " pa per\n",
      " by Charles Elkan.\n",
      "4\n",
      "  I t considerably accelera tes the algorithm by a voiding man y\n",
      " unnecessar y distance calcula tions: this is achieved by exploiting the triangle inequalƒ\n",
      " Clustering  |  245\n",
      "\n",
      "5\n",
      " The triangle inequality is A C ł AB + B C where A, B and C are three poin ts, and AB , A C and B C are the\n",
      " distances between these poin ts.\n",
      "6\n",
      " —W eb-Scale K-M eans Clustering, – Da vid Sculley (2010).\n",
      " ity (i.e., the straigh t line is alwa ys the shortest\n",
      "5\n",
      ") and by keeping track of lower and\n",
      " upper bounds for distances between instances and cen troids. This is the algorithm\n",
      " used by defa ult by the \n",
      "KMeans\n",
      " \n",
      "class (but you can force it to use the original algorithm\n",
      "by setting the \n",
      "algorithm\n",
      "  h yperparameter to \n",
      "\"full\"\n",
      ", although you probably will\n",
      "never need to).\n",
      " Y et another im portan t varian t of the K-M eans algorithm was proposed in a \n",
      "2010\n",
      " pa per\n",
      "  by Da vid Sculley .\n",
      "6\n",
      "  Instead of using the full da taset a t each itera tion, the algoƒ\n",
      " rithm is ca pable of using mini-ba tches, moving the cen troids just sligh tly a t each iterƒ\n",
      " a tion. This speeds up the algorithm typically by a factor of 3 or 4 and makes it\n",
      " possible to cluster h uge da tasets tha t do not fit in memor y . Scikit-Learn im plemen ts\n",
      "this algorithm in the \n",
      "MiniBatchKMeans\n",
      "  class. Y ou can just use this class like the\n",
      "KMeans\n",
      " class:\n",
      "from\n",
      " \n",
      "sklearn.cluster\n",
      " \n",
      "import\n",
      " \n",
      "MiniBatchKMeans\n",
      "minibatch_kmeans\n",
      " \n",
      "=\n",
      " \n",
      "MiniBatchKMeans\n",
      "(\n",
      "n_clusters\n",
      "=\n",
      "5\n",
      ")\n",
      "minibatch_kmeans\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ")\n",
      " If the da taset does not fit in memor y , the sim plest option is to use the \n",
      "memmap\n",
      "   class, as\n",
      " we did for incremen tal PCA in \n",
      " Cha pter 8\n",
      " . Alterna tively , you can pass one mini-ba tch\n",
      " a t a time to the \n",
      "partial_fit()\n",
      "  method, but this will require m uch more work, since\n",
      " you will need to perform m ultiple initializa tions and select the best one yourself (see\n",
      " the notebook for an exam ple).\n",
      " Although the Mini-ba tch K-M eans algorithm is m uch faster than the regular K-\n",
      " M eans algorithm, its inertia is generally sligh tly worse, especially as the n umber of\n",
      " clusters increases. Y ou can see this in \n",
      "Figure 9-6\n",
      " : the plot on the left com pares the\n",
      " inertias of Mini-ba tch K-M eans and regular K-M eans models trained on the previous\n",
      " da taset using various n umbers of clusters \n",
      "k\n",
      " . The difference between the two cur ves\n",
      " remains fairly constan t, but this difference becomes more and more significan t as \n",
      "k\n",
      " increases, since the inertia becomes smaller and smaller . H owever , in the plot on the\n",
      " righ t, you can see tha t Mini-ba tch K-M eans is m uch faster than regular K-M eans, and\n",
      "this difference increases with \n",
      "k\n",
      ".\n",
      " 246  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " F i g u r e 9-6. M i n i-b a t c h K-M e a ns vs K-M e a ns: w o r s e i n er t i a as k i n cr e as e s \n",
      "(le“)\n",
      "  b u t\n",
      " m u c h f as t er (r i gh t)\n",
      "Finding the Optimal Number of Clusters\n",
      " So far , we ha ve set the n umber of clusters \n",
      "k\n",
      "  to 5 beca use it was obvious by looking a t\n",
      " the da ta tha t this is the correct n umber of clusters. But in general, it will not be so\n",
      "easy to know how to set \n",
      "k\n",
      " , and the result migh t be quite bad if you set it to the wrong\n",
      " value. F or exam ple, as you can see in \n",
      "Figure 9-7\n",
      ", setting \n",
      "k\n",
      " to 3 or 8 results in fairly\n",
      "bad models:\n",
      " F i g u r e 9-7. B a d c h o i c e s f o r t h e n u m b er o f c l u s t er s\n",
      " Y ou migh t be thinking tha t we could just pick the model with the lowest inertia,\n",
      " righ t? U nfortuna tely , it is not tha t sim ple. The inertia for \n",
      "k\n",
      " =3 is 653.2, which is m uch\n",
      "higher than for \n",
      "k\n",
      "=5 (which was 211.6), but with \n",
      "k\n",
      "=8, the inertia is just 119.1. The\n",
      " inertia is not a good performance metric when tr ying to choose \n",
      "k\n",
      " since it keeps getƒ\n",
      "ting lower as we increase \n",
      "k\n",
      ". Indeed, the more clusters there are, the closer each\n",
      " instance will be to its closest cen troid, and therefore the lower the inertia will be. Let ‡ s\n",
      "plot the inertia as a function of \n",
      "k\n",
      " (see \n",
      "Figure 9-8\n",
      "):\n",
      " Clustering  |  247\n",
      "\n",
      " F i g u r e 9-8. S e l e c t i n g t h e n u m b er o f c l u s t er s k u s i n g t h e ﬁ e l b o w r u l e ﬂ\n",
      " As you can see, the inertia drops ver y quickly as we increase \n",
      "k\n",
      " up to 4, but then it\n",
      " decreases m uch more slowly as we keep increasing \n",
      "k\n",
      " . This cur ve has roughly the\n",
      " sha pe of an arm, and there is an — elbow– a t \n",
      "k\n",
      " =4 so if we did not know better , it would\n",
      " be a good choice: an y lower value would be drama tic, while an y higher value would\n",
      " not help m uch, and we migh t just be splitting perfectly good clusters in half for no\n",
      "good reason.\n",
      " This technique for choosing the best value for the n umber of clusters is ra ther coarse.\n",
      " A more precise a pproach (but also more com puta tionally expensive) is to use the \n",
      " s i l‡\n",
      " h o u e tt e s c o r e\n",
      ", which is the mean \n",
      " s i l h o u e tt e \n",
      "coe⁄cient\n",
      " \n",
      "over all the instances. An instanƒ\n",
      " ce ‡ s silhouette coefficien t is equal to (\n",
      "b\n",
      " − \n",
      "a\n",
      ") / max(\n",
      "a\n",
      ", \n",
      "b\n",
      ") where \n",
      "a\n",
      " is the mean distance\n",
      " to the other instances in the same cluster (it is the mean in tra-cluster distance), and \n",
      "b\n",
      " is the mean nearest-cluster distance, tha t is the mean distance to the instances of the\n",
      " next closest cluster (defined as the one tha t minimizes \n",
      "b\n",
      " , excluding the instance ‡ s own\n",
      " cluster). The silhouette coefficien t can var y between -1 and +1: a coefficien t close to\n",
      " +1 means tha t the instance is well inside its own cluster and far from other clusters,\n",
      " while a coefficien t close to 0 means tha t it is close to a cluster boundar y , and finally a\n",
      " coefficien t close to -1 means tha t the instance ma y ha ve been assigned to the wrong\n",
      " cluster . T o com pute the silhouette score, you can use Scikit-Learn ‡ s \n",
      "silhou\n",
      "ette_score()\n",
      "  function, giving it all the instances in the da taset, and the labels they\n",
      "were assigned:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.metrics\n",
      " \n",
      "import\n",
      " \n",
      "silhouette_score\n",
      ">>> \n",
      "silhouette_score\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "kmeans\n",
      ".\n",
      "labels_\n",
      ")\n",
      "0.655517642572828\n",
      " Let ‡ s com pare the silhouette scores for differen t n umbers of clusters (see \n",
      "Figure 9-9\n",
      "):\n",
      " 248  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " F i g u r e 9-9. S e l e c t i n g t h e n u m b er o f c l u s t er s k u s i n g t h e s i l h o u e tt e s c o r e\n",
      " As you can see, this visualiza tion is m uch richer than the previous one: in particular ,\n",
      " although it confirms tha t \n",
      "k\n",
      " =4 is a ver y good choice, it also underlines the fact tha t\n",
      "k\n",
      " =5 is quite good as well, and m uch better than \n",
      "k\n",
      "=6 or 7. This was not visible when\n",
      " com paring inertias.\n",
      " An even more informa tive visualiza tion is obtained when you plot ever y instance ‡ s\n",
      " silhouette coefficien t, sorted by the cluster they are assigned to and by the value of the\n",
      " coefficien t. This is called a \n",
      " s i l h o u e tt e d i a g r a m\n",
      " (see \n",
      "Figure 9-10\n",
      "):\n",
      " F i g u r e 9-10. S i l o u h e tt e a n a l ys i s: c o m p a r i n g t h e s i l h o u e tt e d i a g r a ms f o r v a r i o u s v a l u e s o f\n",
      "k\n",
      " The vertical dashed lines represen t the silhouette score for each n umber of clusters.\n",
      " When most of the instances in a cluster ha ve a lower coefficien t than this score (i.e., if\n",
      " man y of the instances stop short of the dashed line, ending to the left of it), then the\n",
      " cluster is ra ther bad since this means its instances are m uch too close to other clusƒ\n",
      " Clustering  |  249\n",
      "\n",
      " ters. W e can see tha t when \n",
      "k\n",
      "=3 and when \n",
      "k\n",
      "=6, we get bad clusters. But when \n",
      "k\n",
      "=4 or\n",
      "k\n",
      "=5, the clusters look pretty good − most instances extend beyond the dashed line, to\n",
      " the righ t and closer to 1.0. When \n",
      "k\n",
      " =4, the cluster a t index 1 (the third from the top),\n",
      " is ra ther big, while when \n",
      "k\n",
      " =5, all clusters ha ve similar sizes, so even though the overƒ\n",
      "all silhouette score from \n",
      "k\n",
      " =4 is sligh tly grea ter than for \n",
      "k\n",
      "=5, it seems like a good idea\n",
      "to use \n",
      "k\n",
      "=5 to get clusters of similar sizes.\n",
      "Limits of K-Means\n",
      " Despite its man y merits, most notably being fast and scalable, K-M eans is not perfect.\n",
      " As we sa w , it is necessar y to run the algorithm several times to a void sub-optimal solƒ\n",
      " utions, plus you need to specif y the n umber of clusters, which can be quite a hassle.\n",
      " M oreover , K-M eans does not beha ve ver y well when the clusters ha ve var ying sizes,\n",
      " differen t densities, or non-spherical sha pes. F or exam ple, \n",
      "Figure 9-11\n",
      " \n",
      "shows how K-\n",
      " M eans clusters a da taset con taining three ellipsoidal clusters of differen t dimensions,\n",
      " densities and orien ta tions:\n",
      " F i g u r e 9-11. K-M e a ns f a i l s t o c l u s t er t h e s e e l l i p s o i d a l b l o b s p r o p er l y\n",
      " As you can see, neither of these solutions are an y good. The solution on the left is\n",
      " better , but it still chops off 25% of the middle cluster and assigns it to the cluster on\n",
      " the righ t. The solution on the righ t is just terrible, even though its inertia is lower . So\n",
      " depending on the da ta, differen t clustering algorithms ma y perform better . F or examƒ\n",
      " ple, on these types of elliptical clusters, Ga ussian mixture models work grea t.\n",
      " I t is im portan t to scale the in put fea tures before you run K-M eans,\n",
      " or else the clusters ma y be ver y stretched, and K-M eans will perƒ\n",
      " form poorly . Scaling the fea tures does not guaran tee tha t all the\n",
      " clusters will be nice and spherical, but it generally im proves things.\n",
      " N ow let ‡ s look a t a few wa ys we can benefit from clustering. W e will use K-M eans, but\n",
      " feel free to experimen t with other clustering algorithms.\n",
      " 250  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "Using clustering for image segmentation\n",
      " I m a ge s e g m en t a t i o n\n",
      "  is the task of partitioning an image in to m ultiple segmen ts. In\n",
      " s em a n t i c s e g m en t a t i o n\n",
      " , all pixels tha t are part of the same object type get assigned to\n",
      " the same segmen t. F or exam ple, in a self-driving car‡ s vision system, all pixels tha t are\n",
      " part of a pedestrian ‡ s image migh t be assigned to the — pedestrian – segmen t (there\n",
      " would just be one segmen t con taining all the pedestrians). In \n",
      " i ns t a n c e s e g m en t a t i o n\n",
      ",\n",
      " all pixels tha t are part of the same individual object are assigned to the same segmen t.\n",
      " In this case there would be a differen t segmen t for each pedestrian. The sta te of the\n",
      " art in seman tic or instance segmen ta tion toda y is achieved using com plex architecƒ\n",
      " tures based on con volutional neural networks (see \n",
      " Cha pter 14\n",
      " ). H ere, we are going to\n",
      " do something m uch sim pler : \n",
      " c o l o r s e g m en t a t i o n\n",
      " . W e will sim ply assign pixels to the\n",
      " same segmen t if they ha ve a similar color . In some a pplica tions, this ma y be sufficien t,\n",
      " for exam ple if you wan t to analyze sa tellite images to measure how m uch total forest\n",
      " area there is in a region, color segmen ta tion ma y be just fine.\n",
      " First, let ‡ s load the image (see the upper left image in \n",
      "Figure 9-12\n",
      " ) using M a tplotlib ‡ s\n",
      "imread()\n",
      " function:\n",
      ">>> \n",
      "from\n",
      " \n",
      "matplotlib.image\n",
      " \n",
      "import\n",
      " \n",
      "imread\n",
      "  \n",
      "# you could also use †imageio.imread()†\n",
      ">>> \n",
      "image\n",
      " \n",
      "=\n",
      " \n",
      "imread\n",
      "(\n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "join\n",
      "(\n",
      "\"images\"\n",
      ",\n",
      "\"clustering\"\n",
      ",\n",
      "\"ladybug.png\"\n",
      "))\n",
      ">>> \n",
      "image\n",
      ".\n",
      "shape\n",
      "(533, 800, 3)\n",
      " The image is represen ted as a 3D arra y : the first dimension ‡ s size is the heigh t, the\n",
      " second is the width, and the third is the n umber of color channels, in this case red,\n",
      " green and blue (RGB). In other words, for each pixel there is a 3D vector con taining\n",
      " the in tensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\n",
      "if you use \n",
      "imageio.imread()\n",
      " ). Some images ma y ha ve less channels, such as gra yƒ\n",
      "scale images (one channel), or more channels, such as images with an additional\n",
      " a l p h a c h a n n e l\n",
      "  for transparency , or sa tellite images which often con tain channels for\n",
      " man y ligh t frequencies (e.g., infrared). The following code resha pes the arra y to get a\n",
      " long list of RGB colors, then it clusters these colors using K-M eans. F or exam ple, it\n",
      " ma y iden tif y a color cluster for all shades of green. N ext, for each color (e.g., dark\n",
      " green), it looks for the mean color of the pixel ‡ s color cluster . F or exam ple, all shades\n",
      " of green ma y be replaced with the same ligh t green color (assuming the mean color of\n",
      " the green cluster is ligh t green). Finally it resha pes this long list of colors to get the\n",
      " same sha pe as the original image. And we ‡ re done!\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "image\n",
      ".\n",
      "reshape\n",
      "(\n",
      "-\n",
      "1\n",
      ",\n",
      " \n",
      "3\n",
      ")\n",
      "kmeans\n",
      " \n",
      "=\n",
      " \n",
      "KMeans\n",
      "(\n",
      "n_clusters\n",
      "=\n",
      "8\n",
      ")\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ")\n",
      "segmented_img\n",
      " \n",
      "=\n",
      " \n",
      "kmeans\n",
      ".\n",
      "cluster_centers_\n",
      "[\n",
      "kmeans\n",
      ".\n",
      "labels_\n",
      "]\n",
      "segmented_img\n",
      " \n",
      "=\n",
      " \n",
      "segmented_img\n",
      ".\n",
      "reshape\n",
      "(\n",
      "image\n",
      ".\n",
      "shape\n",
      ")\n",
      " This outputs the image shown in the upper righ t of \n",
      "Figure 9-12\n",
      " . Y ou can experimen t\n",
      " with various n umbers of clusters, as shown in the figure. When you use less than 8\n",
      " clusters, notice tha t the ladybug ‡ s flash y red color fails to get a cluster of its own: it\n",
      " Clustering  |  251\n",
      "\n",
      " gets merged with colors from the en vironmen t. This is due to the fact tha t the ladyƒ\n",
      " bug is quite small, m uch smaller than the rest of the image, so even though its color is\n",
      " flash y , K-M eans fails to dedica te a cluster to it: as men tioned earlier , K-M eans prefers\n",
      "clusters of similar sizes.\n",
      " F i g u r e 9-12. I m a ge s e g m en t a t i o n u s i n g K-M e a ns w i t h v a r i o u s n u m b er s o f c o l o r c l u s t er s\n",
      " Tha t was not too hard, was it? N ow let ‡ s look a t another a pplica tion of clustering: preƒ\n",
      "processing.\n",
      "Using Clustering for Preprocessing\n",
      " Clustering can be an efficien t a pproach to dimensionality reduction, in particular as a\n",
      " preprocessing step before a super vised learning algorithm. F or exam ple, let ‡ s tackle\n",
      "the \n",
      " d i g i ts d a t as e t\n",
      "   which is a sim ple MNIST -like da taset con taining 1,797 gra yscale 8„8\n",
      " images represen ting digits 0 to 9. First, let ‡ s load the da taset:\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "load_digits\n",
      "X_digits\n",
      ",\n",
      " \n",
      "y_digits\n",
      " \n",
      "=\n",
      " \n",
      "load_digits\n",
      "(\n",
      "return_X_y\n",
      "=\n",
      "True\n",
      ")\n",
      " N ow , let ‡ s split it in to a training set and a test set:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "train_test_split\n",
      "X_train\n",
      ",\n",
      " \n",
      "X_test\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "y_test\n",
      " \n",
      "=\n",
      " \n",
      "train_test_split\n",
      "(\n",
      "X_digits\n",
      ",\n",
      " \n",
      "y_digits\n",
      ")\n",
      " N ext, let ‡ s fit a Logistic Regression model:\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "LogisticRegression\n",
      "log_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "(\n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      "log_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " Let ‡ s evalua te its accuracy on the test set:\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.9666666666666667\n",
      " 252  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " Oka y , tha t ‡ s our baseline: 96.7% accuracy . Let ‡ s see if we can do better by using K-\n",
      " M eans as a preprocessing step . W e will crea te a pipeline tha t will first cluster the\n",
      " training set in to 50 clusters and replace the images with their distances to these 50\n",
      " clusters, then a pply a logistic regression model.\n",
      " Although it is tem pting to define the n umber of clusters to 10,\n",
      " since there are 10 differen t digits, it is unlikely to perform well,\n",
      " beca use there are several differen t wa ys to write each digit.\n",
      "from\n",
      " \n",
      "sklearn.pipeline\n",
      " \n",
      "import\n",
      " \n",
      "Pipeline\n",
      "pipeline\n",
      " \n",
      "=\n",
      " \n",
      "Pipeline\n",
      "([\n",
      "    \n",
      "(\n",
      "\"kmeans\"\n",
      ",\n",
      " \n",
      "KMeans\n",
      "(\n",
      "n_clusters\n",
      "=\n",
      "50\n",
      ")),\n",
      "    \n",
      "(\n",
      "\"log_reg\"\n",
      ",\n",
      " \n",
      "LogisticRegression\n",
      "()),\n",
      "])\n",
      "pipeline\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " N ow let ‡ s evalua te this classifica tion pipeline:\n",
      ">>> \n",
      "pipeline\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.9822222222222222\n",
      " H ow about tha t? W e almost divided the error ra te by a factor of 2!\n",
      " But we chose the n umber of clusters \n",
      "k\n",
      "  com pletely arbitrarily , we can surely do better .\n",
      " Since K-M eans is just a preprocessing step in a classifica tion pipeline, finding a good\n",
      "value for \n",
      "k\n",
      " \n",
      " is m uch sim pler than earlier : there ‡ s no need to perform silhouette analysis\n",
      "or minimize the inertia, the best value of \n",
      "k\n",
      "  is sim ply the one tha t results in the best\n",
      " classifica tion performance during cross-valida tion. Let ‡ s use \n",
      "GridSearchCV\n",
      "   to find the\n",
      " optimal n umber of clusters:\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "GridSearchCV\n",
      "param_grid\n",
      " \n",
      "=\n",
      " \n",
      "dict\n",
      "(\n",
      "kmeans__n_clusters\n",
      "=\n",
      "range\n",
      "(\n",
      "2\n",
      ",\n",
      " \n",
      "100\n",
      "))\n",
      "grid_clf\n",
      " \n",
      "=\n",
      " \n",
      "GridSearchCV\n",
      "(\n",
      "pipeline\n",
      ",\n",
      " \n",
      "param_grid\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "verbose\n",
      "=\n",
      "2\n",
      ")\n",
      "grid_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      " Let ‡ s look a t best value for \n",
      "k\n",
      ", and the performance of the resulting pipeline:\n",
      ">>> \n",
      "grid_clf\n",
      ".\n",
      "best_params_\n",
      "{•kmeans__n_clusters•: 90}\n",
      ">>> \n",
      "grid_clf\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.9844444444444445\n",
      " W ith \n",
      "k\n",
      "=90 clusters, we get a small accuracy boost, reaching 98.4% accuracy on the\n",
      "test set. Cool!\n",
      " Clustering  |  253\n",
      "\n",
      "Using Clustering for Semi-Supervised Learning\n",
      " Another use case for clustering is in semi-super vised learning, when we ha ve plen ty\n",
      " of unlabeled instances and ver y few labeled instances. Let ‡ s train a logistic regression\n",
      " model on a sam ple of 50 labeled instances from the digits da taset:\n",
      "n_labeled\n",
      " \n",
      "=\n",
      " \n",
      "50\n",
      "log_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "()\n",
      "log_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      "[:\n",
      "n_labeled\n",
      "],\n",
      " \n",
      "y_train\n",
      "[:\n",
      "n_labeled\n",
      "])\n",
      " Wha t is the performance of this model on the test set?\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.8266666666666667\n",
      " The accuracy is just 82.7%: it should come as no surprise tha t this is m uch lower than\n",
      " earlier , when we trained the model on the full training set. Let ‡ s see how we can do\n",
      " better . First, let ‡ s cluster the training set in to 50 clusters, then for each cluster let ‡ s find\n",
      " the image closest to the cen troid. W e will call these images the represen ta tive images:\n",
      "k\n",
      " \n",
      "=\n",
      " \n",
      "50\n",
      "kmeans\n",
      " \n",
      "=\n",
      " \n",
      "KMeans\n",
      "(\n",
      "n_clusters\n",
      "=\n",
      "k\n",
      ")\n",
      "X_digits_dist\n",
      " \n",
      "=\n",
      " \n",
      "kmeans\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      "representative_digit_idx\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "argmin\n",
      "(\n",
      "X_digits_dist\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "0\n",
      ")\n",
      "X_representative_digits\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[\n",
      "representative_digit_idx\n",
      "]\n",
      "Figure 9-13\n",
      "  shows these 50 represen ta tive images:\n",
      " F i g u r e 9-13. \n",
      "Fi“y\n",
      "  r e p r e s en t a t i v e d i g i t i m a ge s (o n e p er c l u s t er)\n",
      " N ow let ‡ s look a t each image and man ually label it:\n",
      "y_representative_digits\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([\n",
      "4\n",
      ",\n",
      " \n",
      "8\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "6\n",
      ",\n",
      " \n",
      "8\n",
      ",\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "...\n",
      ",\n",
      " \n",
      "7\n",
      ",\n",
      " \n",
      "6\n",
      ",\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      "])\n",
      " N ow we ha ve a da taset with just 50 labeled instances, but instead of being com pletely\n",
      " random instances, each of them is a represen ta tive image of its cluster . Let ‡ s see if the\n",
      " performance is an y better :\n",
      ">>> \n",
      "log_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "()\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_representative_digits\n",
      ",\n",
      " \n",
      "y_representative_digits\n",
      ")\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.9244444444444444\n",
      " W ow! W e jum ped from 82.7% accuracy to 92.4%, although we are still only training\n",
      "the model on 50 instances. Since it is often costly and painful to label instances, espeƒ\n",
      " 254  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " cially when it has to be done man ually by experts, it is a good idea to label represen taƒ\n",
      " tive instances ra ther than just random instances.\n",
      " But perha ps we can go one step further : wha t if we propaga ted the labels to all the\n",
      "other instances in the same cluster? This is called \n",
      " l a b e l p r o p a ga t i o n\n",
      ":\n",
      "y_train_propagated\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "empty\n",
      "(\n",
      "len\n",
      "(\n",
      "X_train\n",
      "),\n",
      " \n",
      "dtype\n",
      "=\n",
      "np\n",
      ".\n",
      "int32\n",
      ")\n",
      "for\n",
      " \n",
      "i\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "k\n",
      "):\n",
      "    \n",
      "y_train_propagated\n",
      "[\n",
      "kmeans\n",
      ".\n",
      "labels_\n",
      "==\n",
      "i\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "y_representative_digits\n",
      "[\n",
      "i\n",
      "]\n",
      " N ow let ‡ s train the model again and look a t its performance:\n",
      ">>> \n",
      "log_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "()\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train_propagated\n",
      ")\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.9288888888888889\n",
      " W e got a tin y little accuracy boost. B etter than nothing, but not astounding. The\n",
      " problem is tha t we propaga ted each represen ta tive instance ‡ s label to all the instances\n",
      " in the same cluster , including the instances loca ted close to the cluster boundaries,\n",
      " which are more likely to be mislabeled. Let ‡ s see wha t ha ppens if we only propaga te\n",
      " the labels to the 20% of the instances tha t are closest to the cen troids:\n",
      "percentile_closest\n",
      " \n",
      "=\n",
      " \n",
      "20\n",
      "X_cluster_dist\n",
      " \n",
      "=\n",
      " \n",
      "X_digits_dist\n",
      "[\n",
      "np\n",
      ".\n",
      "arange\n",
      "(\n",
      "len\n",
      "(\n",
      "X_train\n",
      ")),\n",
      " \n",
      "kmeans\n",
      ".\n",
      "labels_\n",
      "]\n",
      "for\n",
      " \n",
      "i\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "k\n",
      "):\n",
      "    \n",
      "in_cluster\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "kmeans\n",
      ".\n",
      "labels_\n",
      " \n",
      "==\n",
      " \n",
      "i\n",
      ")\n",
      "    \n",
      "cluster_dist\n",
      " \n",
      "=\n",
      " \n",
      "X_cluster_dist\n",
      "[\n",
      "in_cluster\n",
      "]\n",
      "    \n",
      "cutoff_distance\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "percentile\n",
      "(\n",
      "cluster_dist\n",
      ",\n",
      " \n",
      "percentile_closest\n",
      ")\n",
      "    \n",
      "above_cutoff\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "X_cluster_dist\n",
      " \n",
      ">\n",
      " \n",
      "cutoff_distance\n",
      ")\n",
      "    \n",
      "X_cluster_dist\n",
      "[\n",
      "in_cluster\n",
      " \n",
      "&\n",
      " \n",
      "above_cutoff\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "-\n",
      "1\n",
      "partially_propagated\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "X_cluster_dist\n",
      " \n",
      "!=\n",
      " \n",
      "-\n",
      "1\n",
      ")\n",
      "X_train_partially_propagated\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[\n",
      "partially_propagated\n",
      "]\n",
      "y_train_partially_propagated\n",
      " \n",
      "=\n",
      " \n",
      "y_train_propagated\n",
      "[\n",
      "partially_propagated\n",
      "]\n",
      " N ow let ‡ s train the model again on this partially propaga ted da taset:\n",
      ">>> \n",
      "log_reg\n",
      " \n",
      "=\n",
      " \n",
      "LogisticRegression\n",
      "()\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_partially_propagated\n",
      ",\n",
      " \n",
      "y_train_partially_propagated\n",
      ")\n",
      ">>> \n",
      "log_reg\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "0.9422222222222222\n",
      " N ice! W ith just 50 labeled instances (only 5 exam ples per class on a verage!), we got\n",
      "94.2% performance, which is pretty close to the performance of logistic regression on\n",
      " the fully labeled digits da taset (which was 96.7%). This is beca use the propaga ted\n",
      " labels are actually pretty good, their accuracy is ver y close to 99%:\n",
      " Clustering  |  255\n",
      "\n",
      ">>> \n",
      "np\n",
      ".\n",
      "mean\n",
      "(\n",
      "y_train_partially_propagated\n",
      " \n",
      "==\n",
      " \n",
      "y_train\n",
      "[\n",
      "partially_propagated\n",
      "])\n",
      "0.9896907216494846\n",
      "Active Learning\n",
      " T o con tin ue im proving your model and your training set, the next step could be to do\n",
      "a few rounds of \n",
      " a c t i v e l e a r n i n g\n",
      " : this is when a h uman expert in teracts with the learnƒ\n",
      " ing algorithm, providing labels when the algorithm needs them. There are man y difƒ\n",
      " feren t stra tegies for active learning, but one of the most common ones is called\n",
      " u n c er t a i n ty s a m p l i n g\n",
      ":\n",
      "⁄\n",
      " The model is trained on the labeled instances ga thered so far , and this model is\n",
      "used to make predictions on all the unlabeled instances.\n",
      "⁄\n",
      " The instances for which the model is most uncertain (i.e., when its estima ted\n",
      " probability is lowest) m ust be labeled by the expert.\n",
      "⁄\n",
      " Then you just itera te this process again and again, un til the performance\n",
      " im provemen t stops being worth the labeling effort.\n",
      " Other stra tegies include labeling the instances tha t would result in the largest model\n",
      " change, or the largest drop in the model ‡ s valida tion error , or the instances tha t differƒ\n",
      " en t models disagree on (e.g., an SVM, a R andom F orest, and so on).\n",
      " B efore we move on to Ga ussian mixture models, let ‡ s take a look a t DBSCAN,\n",
      " another popular clustering algorithm tha t illustra tes a ver y differen t a pproach based\n",
      " on local density estima tion. This a pproach allows the algorithm to iden tif y clusters of\n",
      " arbitrar y sha pes.\n",
      "DBSCAN\n",
      " This algorithm defines clusters as con tin uous regions of high density . I t is actually\n",
      " quite sim ple:\n",
      "⁄\n",
      " F or each instance, the algorithm coun ts how man y instances are loca ted within a\n",
      " small distance µ (epsilon) from it. This region is called the instance ‡ s \n",
      "Ł-\n",
      " n ei gh b o r h o o d\n",
      ".\n",
      "⁄\n",
      " If an instance has a t least \n",
      "min_samples\n",
      " instances in its \n",
      "µ\n",
      "-neighborhood (includƒ\n",
      " ing itself ), then it is considered a \n",
      " c o r e i ns t a n c e\n",
      ". In other words, core instances are\n",
      " those tha t are loca ted in dense regions.\n",
      "⁄\n",
      " All instances in the neighborhood of a core instance belong to the same cluster .\n",
      " This ma y include other core instances, therefore a long sequence of neighboring\n",
      " core instances forms a single cluster .\n",
      " 256  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "⁄\n",
      " An y instance tha t is not a core instance and does not ha ve one in its neighborƒ\n",
      " hood is considered an anomaly .\n",
      "This algorithm works well if all the clusters are dense enough, and they are well sepaƒ\n",
      " ra ted by low-density regions. The \n",
      "DBSCAN\n",
      "  class in Scikit-Learn is as sim ple to use as\n",
      " you migh t expect. Let ‡ s test it on the moons da taset, in troduced in \n",
      " Cha pter 5\n",
      ":\n",
      "from\n",
      " \n",
      "sklearn.cluster\n",
      " \n",
      "import\n",
      " \n",
      "DBSCAN\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "make_moons\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "make_moons\n",
      "(\n",
      "n_samples\n",
      "=\n",
      "1000\n",
      ",\n",
      " \n",
      "noise\n",
      "=\n",
      "0.05\n",
      ")\n",
      "dbscan\n",
      " \n",
      "=\n",
      " \n",
      "DBSCAN\n",
      "(\n",
      "eps\n",
      "=\n",
      "0.05\n",
      ",\n",
      " \n",
      "min_samples\n",
      "=\n",
      "5\n",
      ")\n",
      "dbscan\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ")\n",
      " The labels of all the instances are now a vailable in the \n",
      "labels_\n",
      " instance variable:\n",
      ">>> \n",
      "dbscan\n",
      ".\n",
      "labels_\n",
      "array([ 0,  2, -1, -1,  1,  0,  0,  0, ...,  3,  2,  3,  3,  4,  2,  6,  3])\n",
      " N otice tha t some instances ha ve a cluster index equal to -1: this means tha t they are\n",
      " considered as anomalies by the algorithm. The indices of the core instances are a vailƒ\n",
      "able in the \n",
      "core_sample_indices_\n",
      " instance variable, and the core instances themƒ\n",
      " selves are a vailable in the \n",
      "components_\n",
      " instance variable:\n",
      ">>> \n",
      "len\n",
      "(\n",
      "dbscan\n",
      ".\n",
      "core_sample_indices_\n",
      ")\n",
      "808\n",
      ">>> \n",
      "dbscan\n",
      ".\n",
      "core_sample_indices_\n",
      "array([ 0,  4,  5,  6,  7,  8, 10, 11, ..., 992, 993, 995, 997, 998, 999])\n",
      ">>> \n",
      "dbscan\n",
      ".\n",
      "components_\n",
      "array([[-0.02137124,  0.40618608],\n",
      "       [-0.84192557,  0.53058695],\n",
      "                  ...\n",
      "       [-0.94355873,  0.3278936 ],\n",
      "       [ 0.79419406,  0.60777171]])\n",
      " This clustering is represen ted in the left plot of \n",
      "Figure 9-14\n",
      " . As you can see, it iden tiƒ\n",
      " fied quite a lot of anomalies, plus 7 differen t clusters. H ow disa ppoin ting! F ortuna tely ,\n",
      " if we widen each instance ‡ s neighborhood by increasing \n",
      "eps\n",
      " \n",
      "to 0.2, we get the clusterƒ\n",
      " ing on the righ t, which looks perfect. Let ‡ s con tin ue with this model.\n",
      " F i g u r e 9-14. D BSCAN c l u s t er i n g u s i n g tw o \n",
      "di›erent\n",
      "  n ei gh b o r h o o d r a d i u s e s\n",
      " Clustering  |  257\n",
      "\n",
      " Somewha t surprisingly , the DBSCAN class does not ha ve a \n",
      "predict()\n",
      " \n",
      "method,\n",
      "although it has a \n",
      "fit_predict()\n",
      " method. In other words, it cannot predict which\n",
      " cluster a new instance belongs to . The ra tionale for this decision is tha t several classiƒ\n",
      " fica tion algorithms could make sense here, and it is easy enough to train one, for\n",
      " exam ple a \n",
      "KNeighborsClassifier\n",
      ":\n",
      "from\n",
      " \n",
      "sklearn.neighbors\n",
      " \n",
      "import\n",
      " \n",
      "KNeighborsClassifier\n",
      "knn\n",
      " \n",
      "=\n",
      " \n",
      "KNeighborsClassifier\n",
      "(\n",
      "n_neighbors\n",
      "=\n",
      "50\n",
      ")\n",
      "knn\n",
      ".\n",
      "fit\n",
      "(\n",
      "dbscan\n",
      ".\n",
      "components_\n",
      ",\n",
      " \n",
      "dbscan\n",
      ".\n",
      "labels_\n",
      "[\n",
      "dbscan\n",
      ".\n",
      "core_sample_indices_\n",
      "])\n",
      " N ow , given a few new instances, we can predict which cluster they most likely belong\n",
      " to , and even estima te a probability for each cluster . N ote tha t we only trained them on\n",
      " the core instances, but we could also ha ve chosen to train them on all the instances,\n",
      "or all but the anomalies: this choice depends on the final task.\n",
      ">>> \n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([[\n",
      "-\n",
      "0.5\n",
      ",\n",
      " \n",
      "0\n",
      "],\n",
      " \n",
      "[\n",
      "0\n",
      ",\n",
      " \n",
      "0.5\n",
      "],\n",
      " \n",
      "[\n",
      "1\n",
      ",\n",
      " \n",
      "-\n",
      "0.1\n",
      "],\n",
      " \n",
      "[\n",
      "2\n",
      ",\n",
      " \n",
      "1\n",
      "]])\n",
      ">>> \n",
      "knn\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      "array([1, 0, 1, 0])\n",
      ">>> \n",
      "knn\n",
      ".\n",
      "predict_proba\n",
      "(\n",
      "X_new\n",
      ")\n",
      "array([[0.18, 0.82],\n",
      "       [1.  , 0.  ],\n",
      "       [0.12, 0.88],\n",
      "       [1.  , 0.  ]])\n",
      " The decision boundar y is represen ted on \n",
      "Figure 9-15\n",
      "  (the crosses represen t the 4\n",
      "instances in \n",
      "X_new\n",
      " ). N otice tha t since there is no anomaly in the KNN‡ s training set,\n",
      " the classifier alwa ys chooses a cluster , even when tha t cluster is far a wa y . H owever , it\n",
      " is fairly straigh tfor ward to in troduce a maxim um distance, in which case the two\n",
      " instances tha t are far a wa y from both clusters are classified as anomalies. T o do this,\n",
      "we can use the \n",
      "kneighbors()\n",
      " method of the \n",
      "KNeighborsClassifier\n",
      ": given a set of\n",
      "instances, it returns the distances and the indices of the \n",
      "k\n",
      " nearest neighbors in the\n",
      " training set (two ma trices, each with \n",
      "k\n",
      " columns):\n",
      ">>> \n",
      "y_dist\n",
      ",\n",
      " \n",
      "y_pred_idx\n",
      " \n",
      "=\n",
      " \n",
      "knn\n",
      ".\n",
      "kneighbors\n",
      "(\n",
      "X_new\n",
      ",\n",
      " \n",
      "n_neighbors\n",
      "=\n",
      "1\n",
      ")\n",
      ">>> \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "dbscan\n",
      ".\n",
      "labels_\n",
      "[\n",
      "dbscan\n",
      ".\n",
      "core_sample_indices_\n",
      "][\n",
      "y_pred_idx\n",
      "]\n",
      ">>> \n",
      "y_pred\n",
      "[\n",
      "y_dist\n",
      " \n",
      ">\n",
      " \n",
      "0.2\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "-\n",
      "1\n",
      ">>> \n",
      "y_pred\n",
      ".\n",
      "ravel\n",
      "()\n",
      "array([-1,  0,  1, -1])\n",
      " 258  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " F i g u r e 9-15. \n",
      "cluster_classi†cation_diagram\n",
      " In short, DBSCAN is a ver y sim ple yet powerful algorithm, ca pable of iden tif ying an y\n",
      " n umber of clusters, of an y sha pe, it is robust to outliers, and it has just two h yperƒ\n",
      "parameters (\n",
      "eps\n",
      "   and \n",
      "min_samples\n",
      " ). H owever , if the density varies significan tly across\n",
      " the clusters, it can be im possible for it to ca pture all the clusters properly . M oreover ,\n",
      " its com puta tional com plexity is roughly O(\n",
      "m\n",
      " log \n",
      "m\n",
      "), making it pretty close to linear\n",
      " with regards to the n umber of instances. H owever , Scikit-Learn ‡ s im plemen ta tion can\n",
      "require up to O(\n",
      "m\n",
      "2\n",
      " ) memor y if \n",
      "eps\n",
      " is large.\n",
      "Other Clustering Algorithms\n",
      " Scikit-Learn im plemen ts several more clustering algorithms tha t you should take a\n",
      " look a t. W e cannot cover them all in detail here, but here is a brief over view :\n",
      "⁄\n",
      " Ag gl o m er a t i v e c l u s t er i n g\n",
      " : a hierarch y of clusters is built from the bottom up .\n",
      " Think of man y tin y bubbles floa ting on wa ter and gradually a ttaching to each\n",
      " other un til there ‡ s just one big group of bubbles. Similarly , a t each itera tion\n",
      " agglomera tive clustering connects the nearest pair of clusters (starting with indiƒ\n",
      " vidual instances). If you dra w a tree with a branch for ever y pair of clusters tha t\n",
      " merged, you get a binar y tree of clusters, where the lea ves are the individual\n",
      " instances. This a pproach scales ver y well to large n umbers of instances or clusƒ\n",
      " ters, it can ca pture clusters of various sha pes, it produces a flexible and informaƒ\n",
      "tive cluster tree instead of forcing you to choose a particular cluster scale, and it\n",
      " can be used with an y pair wise distance. I t can scale nicely to large n umbers of\n",
      " instances if you provide a connectivity ma trix. This is a sparse \n",
      "m\n",
      " by \n",
      "m\n",
      " \n",
      " ma trix\n",
      " tha t indica tes which pairs of instances are neighbors (e.g., returned by\n",
      "sklearn.neighbors.kneighbors_graph()\n",
      " ). W ithout a connectivity ma trix, the\n",
      " algorithm does not scale well to large da tasets.\n",
      "⁄\n",
      " B i r c h\n",
      " : this algorithm was designed specifically for ver y large da tasets, and it can\n",
      " be faster than ba tch K-M eans, with similar results, as long as the n umber of feaƒ\n",
      " tures is not too large (<20). I t builds a tree structure during training con taining\n",
      " Clustering  |  259\n",
      "\n",
      " just enough informa tion to quickly assign each new instance to a cluster , without\n",
      " ha ving to store all the instances in the tree: this allows it to use limited memor y ,\n",
      " while handle h uge da tasets.\n",
      "⁄\n",
      "Mean-shi“\n",
      " : this algorithm starts by placing a circle cen tered on each instance,\n",
      " then for each circle it com putes the mean of all the instances loca ted within it,\n",
      " and it shifts the circle so tha t it is cen tered on the mean. N ext, it itera tes this\n",
      " mean-shift step un til all the circles stop moving (i.e., un til each of them is cenƒ\n",
      " tered on the mean of the instances it con tains). This algorithm shifts the circles\n",
      " in the direction of higher density , un til each of them has found a local density\n",
      " maxim um. Finally , all the instances whose circles ha ve settled in the same place\n",
      " (or close enough) are assigned to the same cluster . This has some of the same feaƒ\n",
      " tures as DBSCAN, in particular it can find an y n umber of clusters of an y sha pe, it\n",
      " has just one h yperparameter (the radius of the circles, called the bandwidth) and\n",
      " it relies on local density estima tion. H owever , it tends to chop clusters in to pieces\n",
      " when they ha ve in ternal density varia tions. U nfortuna tely , its com puta tional\n",
      " com plexity is O(\n",
      "m\n",
      "2\n",
      " ), so it is not suited for large da tasets.\n",
      "⁄\n",
      "A⁄nity\n",
      "  p r o p a ga t i o n\n",
      ": this algorithm uses a voting system, where instances vote for\n",
      " similar instances to be their represen ta tives, and once the algorithm con verges,\n",
      " each represen ta tive and its voters form a cluster . This algorithm can detect an y\n",
      " n umber of clusters of differen t sizes. U nfortuna tely , this algorithm has a com puƒ\n",
      " ta tional com plexity of O(\n",
      "m\n",
      "2\n",
      " ), so it is not suited for large da tasets.\n",
      "⁄\n",
      " S p e c t r a l c l u s t er i n g\n",
      " : this algorithm takes a similarity ma trix between the instances\n",
      " and crea tes a low-dimensional embedding from it (i.e., it reduces its dimensionƒ\n",
      "ality), then it uses another clustering algorithm in this low-dimensional space\n",
      " (Scikit-Learn ‡ s im plemen ta tion uses K-M eans). Spectral clustering can ca pture\n",
      " com plex cluster structures, and it can also be used to cut gra phs (e.g., to iden tif y\n",
      "clusters of friends on a social network), however it does not scale well to large\n",
      " n umber of instances, and it does not beha ve well when the clusters ha ve ver y difƒ\n",
      " feren t sizes.\n",
      " N ow let ‡ s dive in to Ga ussian mixture models, which can be used for density estimaƒ\n",
      "tion, clustering and anomaly detection.\n",
      "Gaussian Mixtures\n",
      "A \n",
      " G a u s s i a n m ixt u r e m o d e l\n",
      "  (GMM) is a probabilistic model tha t assumes tha t the\n",
      " instances were genera ted from a mixture of several Ga ussian distributions whose\n",
      " parameters are unknown. All the instances genera ted from a single Ga ussian distriƒ\n",
      " bution form a cluster tha t typically looks like an ellipsoid. Each cluster can ha ve a difƒ\n",
      " feren t ellipsoidal sha pe, size, density and orien ta tion, just like in \n",
      "Figure 9-11\n",
      ". When\n",
      " you obser ve an instance, you know it was genera ted from one of the Ga ussian distriƒ\n",
      " 260  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "7\n",
      "Phi (¾ or €) is the 21\n",
      "st\n",
      " letter of the Greek alphabet.\n",
      "8\n",
      " M ost of these nota tions are standard, but a few additional nota tions were taken from the W ikipedia article on\n",
      " pla te nota tion\n",
      ".\n",
      " butions, but you are not told which one, and you do not know wha t the parameters of\n",
      "these distributions are.\n",
      " There are several GMM varian ts: in the sim plest varian t, im plemen ted in the \n",
      "Gaus\n",
      "sianMixture\n",
      "  class, you m ust know in advance the n umber \n",
      "k\n",
      "  of Ga ussian distribuƒ\n",
      " tions. The da taset \n",
      "X\n",
      "  is assumed to ha ve been genera ted through the following\n",
      "probabilistic process:\n",
      "⁄\n",
      " F or each instance, a cluster is picked randomly among \n",
      "k\n",
      " \n",
      "clusters. The probability\n",
      "of choosing the \n",
      "j\n",
      "th\n",
      "  cluster is defined by the cluster‡ s weigh t \n",
      "’\n",
      "(\n",
      "j\n",
      ")\n",
      ".\n",
      "7\n",
      " The index of the\n",
      "cluster chosen for the \n",
      "i\n",
      "th\n",
      " instance is noted \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      ".\n",
      "⁄\n",
      "If \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      "=\n",
      "j\n",
      ", meaning the \n",
      "i\n",
      "th\n",
      " instance has been assigned to the \n",
      "j\n",
      "th\n",
      " \n",
      " cluster , the loca tion\n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      "  of this instance is sam pled randomly from the Ga ussian distribution with\n",
      "mean \n",
      "—\n",
      "(\n",
      "j\n",
      ")\n",
      "  and covariance ma trix \n",
      "†\n",
      "(\n",
      "j\n",
      ")\n",
      ". This is noted \n",
      "i\n",
      "\n",
      "Œ\n",
      "j\n",
      ",\n",
      "Š\n",
      "j\n",
      ".\n",
      " This genera tive process can be represen ted as a \n",
      " g r a p h i c a l m o d e l\n",
      " (see \n",
      "Figure 9-16\n",
      ").\n",
      " This is a gra ph which represen ts the structure of the conditional dependencies\n",
      "between random variables.\n",
      " F i g u r e 9-16. G a u s s i a n m ixt u r e m o d e l\n",
      " H ere is how to in terpret it:\n",
      "8\n",
      "⁄\n",
      " The circles represen t random variables.\n",
      "⁄\n",
      " The squares represen t fixed values (i.e., parameters of the model).\n",
      " Gaussian Mixtures  |  261\n",
      "\n",
      "⁄\n",
      "The large rectangles are called \n",
      " p l a t e s\n",
      " : they indica te tha t their con ten t is repea ted\n",
      "several times.\n",
      "⁄\n",
      " The n umber indica ted a t the bottom righ t hand side of each pla te indica tes how\n",
      " man y times its con ten t is repea ted, so there are \n",
      "m\n",
      " random variables \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      " (from \n",
      "z\n",
      "(1)\n",
      "to \n",
      "z\n",
      "(\n",
      "m\n",
      ")\n",
      ") and \n",
      "m\n",
      " random variables \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      ", and \n",
      "k\n",
      " means \n",
      "—\n",
      "(\n",
      "j\n",
      ")\n",
      " and \n",
      "k\n",
      "  covariance ma trices\n",
      "†\n",
      "(\n",
      "j\n",
      ")\n",
      " , but just one weigh t vector \n",
      "–\n",
      "  (con taining all the weigh ts \n",
      "’\n",
      "(1)\n",
      " to \n",
      "’\n",
      "(\n",
      "k\n",
      ")\n",
      ").\n",
      "⁄\n",
      "Each variable \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      "  is dra wn from the \n",
      " c a t e go r i c a l d i s t r i b u t i o n\n",
      "  with weigh ts \n",
      "–\n",
      ". Each\n",
      "variable \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " \n",
      " is dra wn from the normal distribution with the mean and covariance\n",
      " ma trix defined by its cluster \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      ".\n",
      "⁄\n",
      " The solid arrows represen t conditional dependencies. F or exam ple, the probabilƒ\n",
      "ity distribution for each random variable \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      "  depends on the weigh t vector \n",
      "–\n",
      ".\n",
      " N ote tha t when an arrow crosses a pla te boundar y , it means tha t it a pplies to all\n",
      " the repetitions of tha t pla te, so for exam ple the weigh t vector \n",
      "–\n",
      " \n",
      "conditions the\n",
      "probability distributions of all the random variables \n",
      "x\n",
      "(1)\n",
      " to \n",
      "x\n",
      "(\n",
      "m\n",
      ")\n",
      ".\n",
      "⁄\n",
      "The squiggly arrow from \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      "   to \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " \n",
      " represen ts a switch: depending on the value of\n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      ", the instance \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      "  will be sam pled from a differen t Ga ussian distribution. F or\n",
      " exam ple, if \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      "=\n",
      "j\n",
      ", then \n",
      "i\n",
      "\n",
      "Œ\n",
      "j\n",
      ",\n",
      "Š\n",
      "j\n",
      ".\n",
      "⁄\n",
      " Shaded nodes indica te tha t the value is known, so in this case only the random\n",
      "variables \n",
      "x\n",
      "(\n",
      "i\n",
      ")\n",
      " \n",
      " ha ve known values: they are called \n",
      " o b s er v e d v a r i a b l e s\n",
      ". The unknown\n",
      "random variables \n",
      "z\n",
      "(\n",
      "i\n",
      ")\n",
      " are called \n",
      " l a t en t v a r i a b l e s\n",
      ".\n",
      " So wha t can you do with such a model? W ell, given the da taset \n",
      "X\n",
      " , you typically wan t\n",
      " to start by estima ting the weigh ts \n",
      "–\n",
      " \n",
      "and all the distribution parameters \n",
      "—\n",
      "(1)\n",
      "   to \n",
      "—\n",
      "(\n",
      "k\n",
      ")\n",
      "   and\n",
      "†\n",
      "(1)\n",
      " to \n",
      "†\n",
      "(\n",
      "k\n",
      ")\n",
      " . Scikit-Learn ‡ s \n",
      "GaussianMixture\n",
      " class makes this trivial:\n",
      "from\n",
      " \n",
      "sklearn.mixture\n",
      " \n",
      "import\n",
      " \n",
      "GaussianMixture\n",
      "gm\n",
      " \n",
      "=\n",
      " \n",
      "GaussianMixture\n",
      "(\n",
      "n_components\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "n_init\n",
      "=\n",
      "10\n",
      ")\n",
      "gm\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ")\n",
      " Let ‡ s look a t the parameters tha t the algorithm estima ted:\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "weights_\n",
      "array([0.20965228, 0.4000662 , 0.39028152])\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "means_\n",
      "array([[ 3.39909717,  1.05933727],\n",
      "       [-1.40763984,  1.42710194],\n",
      "       [ 0.05135313,  0.07524095]])\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "covariances_\n",
      "array([[[ 1.14807234, -0.03270354],\n",
      "        [-0.03270354,  0.95496237]],\n",
      "       [[ 0.63478101,  0.72969804],\n",
      "        [ 0.72969804,  1.1609872 ]],\n",
      " 262  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "       [[ 0.68809572,  0.79608475],\n",
      "        [ 0.79608475,  1.21234145]]])\n",
      " Grea t, it worked fine! Indeed, the weigh ts tha t were used to genera te the da ta were\n",
      " 0.2, 0.4 and 0.4, and similarly , the means and covariance ma trices were ver y close to\n",
      "those found by the algorithm. But how? This class relies on the \n",
      " E xp e c t a t i o n-\n",
      " M axi m iz a t i o n\n",
      "  (EM) algorithm, which has man y similarities with the K-M eans algoƒ\n",
      " rithm: it also initializes the cluster parameters randomly , then it repea ts two steps\n",
      " un til con vergence, first assigning instances to clusters (this is called the \n",
      " exp e c t a t i o n\n",
      " s t e p\n",
      " ) then upda ting the clusters (this is called the \n",
      " m axi m iz a t i o n s t e p\n",
      "). Sounds familƒ\n",
      " iar? Indeed, in the con text of clustering you can think of EM as a generaliza tion of K-\n",
      " M eans which not only finds the cluster cen ters (\n",
      "—\n",
      "(1)\n",
      " to \n",
      "—\n",
      "(\n",
      "k\n",
      ")\n",
      " ), but also their size, sha pe\n",
      " and orien ta tion (\n",
      "†\n",
      "(1)\n",
      " to \n",
      "†\n",
      "(\n",
      "k\n",
      ")\n",
      " ), as well as their rela tive weigh ts (\n",
      "’\n",
      "(1)\n",
      " to \n",
      "’\n",
      "(\n",
      "k\n",
      ")\n",
      " ). U nlike K-\n",
      " M eans, EM uses soft cluster assignmen ts ra ther than hard assignmen ts: for each\n",
      " instance during the expecta tion step , the algorithm estima tes the probability tha t it\n",
      " belongs to each cluster (based on the curren t cluster parameters). Then, during the\n",
      " maximiza tion step , each cluster is upda ted using \n",
      " a l l\n",
      "  the instances in the da taset, with\n",
      " each instance weigh ted by the estima ted probability tha t it belongs to tha t cluster .\n",
      "These probabilities are called the \n",
      " r e s p o ns i b i l i t i e s\n",
      " \n",
      "of the clusters for the instances. Durƒ\n",
      " ing the maximiza tion step , each cluster‡ s upda te will mostly be im pacted by the\n",
      " instances it is most responsible for .\n",
      " U nfortuna tely , just like K-M eans, EM can end up con verging to\n",
      "poor solutions, so it needs to be run several times, keeping only the\n",
      " best solution. This is wh y we set \n",
      "n_init\n",
      "   to 10. B e careful: by defa ult\n",
      "n_init\n",
      " is only set to 1.\n",
      " Y ou can check whether or not the algorithm con verged and how man y itera tions it\n",
      "took:\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "converged_\n",
      "True\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "n_iter_\n",
      "3\n",
      " Oka y , now tha t you ha ve an estima te of the loca tion, size, sha pe, orien ta tion and relaƒ\n",
      " tive weigh t of each cluster , the model can easily assign each instance to the most likely\n",
      " cluster (hard clustering) or estima te the probability tha t it belongs to a particular\n",
      " cluster (soft clustering). F or this, just use the \n",
      "predict()\n",
      " \n",
      "method for hard clustering,\n",
      "or the \n",
      "predict_proba()\n",
      " method for soft clustering:\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "predict\n",
      "(\n",
      "X\n",
      ")\n",
      "array([2, 2, 1, ..., 0, 0, 0])\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "predict_proba\n",
      "(\n",
      "X\n",
      ")\n",
      "array([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],\n",
      "       [1.64685609e-02, 6.75361303e-04, 9.82856078e-01],\n",
      " Gaussian Mixtures  |  263\n",
      "\n",
      "       [2.01535333e-06, 9.99923053e-01, 7.49319577e-05],\n",
      "       ...,\n",
      "       [9.99999571e-01, 2.13946075e-26, 4.28788333e-07],\n",
      "       [1.00000000e+00, 1.46454409e-41, 5.12459171e-16],\n",
      "       [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])\n",
      " I t is a \n",
      " gen er a t i v e m o d e l\n",
      " , meaning you can actually sam ple new instances from it (note\n",
      " tha t they are ordered by cluster index):\n",
      ">>> \n",
      "X_new\n",
      ",\n",
      " \n",
      "y_new\n",
      " \n",
      "=\n",
      " \n",
      "gm\n",
      ".\n",
      "sample\n",
      "(\n",
      "6\n",
      ")\n",
      ">>> \n",
      "X_new\n",
      "array([[ 2.95400315,  2.63680992],\n",
      "       [-1.16654575,  1.62792705],\n",
      "       [-1.39477712, -1.48511338],\n",
      "       [ 0.27221525,  0.690366  ],\n",
      "       [ 0.54095936,  0.48591934],\n",
      "       [ 0.38064009, -0.56240465]])\n",
      ">>> \n",
      "y_new\n",
      "array([0, 1, 2, 2, 2, 2])\n",
      " I t is also possible to estima te the density of the model a t an y given loca tion. This is\n",
      "achieved using the \n",
      "score_samples()\n",
      " method: for each instance it is given, this\n",
      " method estima tes the log of the \n",
      " p r o b a b i l i ty d ens i ty f u n c t i o n\n",
      "  (PDF) a t tha t loca tion.\n",
      " The grea ter the score, the higher the density :\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "score_samples\n",
      "(\n",
      "X\n",
      ")\n",
      "array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,\n",
      "       -4.39802535, -3.80743859])\n",
      " If you com pute the exponen tial of these scores, you get the value of the PDF a t the\n",
      " loca tion of the given instances. These are \n",
      " n o t\n",
      " probabilities, but probability \n",
      " d ens i t i e s\n",
      ":\n",
      " they can take on an y positive value, not just between 0 and 1. T o estima te the probaƒ\n",
      " bility tha t an instance will fall within a particular region, you would ha ve to in tegra te\n",
      " the PDF over tha t region (if you do so over the en tire space of possible instance locaƒ\n",
      "tions, the result will be 1).\n",
      "Figure 9-17\n",
      " shows the cluster means, the decision boundaries (dashed lines), and the\n",
      " density con tours of this model:\n",
      " 264  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " F i g u r e 9-17. Cl u s t er m e a ns, d e ci s i o n b o u n d a r i e s a n d d ens i ty c o n t o u r s o f a t r a i n e d G a u s‡\n",
      " s i a n m ixt u r e m o d e l\n",
      " N ice! The algorithm clearly found an excellen t solution. Of course, we made its task\n",
      " easy by actually genera ting the da ta using a set of 2D Ga ussian distributions (unfortuƒ\n",
      " na tely , real life da ta is not alwa ys so Ga ussian and low-dimensional), and we also ga ve\n",
      " the algorithm the correct n umber of clusters. When there are man y dimensions, or\n",
      " man y clusters, or few instances, EM can struggle to con verge to the optimal solution.\n",
      " Y ou migh t need to reduce the difficulty of the task by limiting the n umber of parameƒ\n",
      " ters tha t the algorithm has to learn: one wa y to do this is to limit the range of sha pes\n",
      " and orien ta tions tha t the clusters can ha ve. This can be achieved by im posing conƒ\n",
      " strain ts on the covariance ma trices. T o do this, just set the \n",
      "covariance_type\n",
      " \n",
      " h yperƒ\n",
      "parameter to one of the following values:\n",
      "⁄\n",
      "\"spherical\"\n",
      " : all clusters m ust be spherical, but they can ha ve differen t diameters\n",
      " (i.e., differen t variances).\n",
      "⁄\n",
      "\"diag\"\n",
      " : clusters can take on an y ellipsoidal sha pe of an y size, but the ellipsoid ‡ s\n",
      " axes m ust be parallel to the coordina te axes (i.e., the covariance ma trices m ust be\n",
      "diagonal).\n",
      "⁄\n",
      "\"tied\"\n",
      " : all clusters m ust ha ve the same ellipsoidal sha pe, size and orien ta tion\n",
      " (i.e., all clusters share the same covariance ma trix).\n",
      " By defa ult, \n",
      "covariance_type\n",
      " is equal to \n",
      "\"full\"\n",
      " , which means tha t each cluster can\n",
      " take on an y sha pe, size and orien ta tion (it has its own unconstrained covariance\n",
      " ma trix). \n",
      "Figure 9-18\n",
      " plots the solutions found by the EM algorithm when \n",
      "cova\n",
      "riance_type\n",
      " is set to \n",
      "\"tied\"\n",
      " or \"\n",
      "spherical\n",
      " — .\n",
      " Gaussian Mixtures  |  265\n",
      "\n",
      " F i g u r e 9-18. c o v a r i a n c e_ty p e_d i a g r a m\n",
      " The com puta tional com plexity of training a \n",
      "GaussianMixture\n",
      " model depends on the n umber of instances \n",
      "m\n",
      " , the n umber of\n",
      "dimensions \n",
      "n\n",
      " , the n umber of clusters \n",
      "k\n",
      " , and the constrain ts on the\n",
      " covariance ma trices. If \n",
      "covariance_type\n",
      " is \n",
      "\"spherical\n",
      " \n",
      "or \n",
      "\"diag\"\n",
      ",\n",
      "it is O(\n",
      " k m n\n",
      " ), assuming the da ta has a clustering structure. If \n",
      "cova\n",
      "riance_type\n",
      "   is \n",
      "\"tied\"\n",
      "   or \n",
      "\"full\"\n",
      ", it is O(\n",
      " k m n\n",
      "2\n",
      "   + \n",
      " k n\n",
      "3\n",
      "), so it will not\n",
      " scale to large n umbers of fea tures.\n",
      " Ga ussian mixture models can also be used for anomaly detection. Let ‡ s see how .\n",
      "Anomaly Detection using Gaussian Mixtures\n",
      " An o m a l y d e t e c t i o n\n",
      " (also called \n",
      " o u t l i er d e t e c t i o n\n",
      " ) is the task of detecting instances tha t\n",
      " devia te strongly from the norm. These instances are of course called \n",
      " a n o m a l i e s\n",
      " \n",
      "or\n",
      " o u t l i er s\n",
      ", while the normal instances are called \n",
      " i n l i er s\n",
      " . Anomaly detection is ver y useƒ\n",
      " ful in a wide variety of a pplica tions, for exam ple in fra ud detection, or for detecting\n",
      " defective products in man ufacturing, or to remove outliers from a da taset before\n",
      " training another model, which can significan tly im prove the performance of the\n",
      "resulting model.\n",
      " U sing a Ga ussian mixture model for anomaly detection is quite sim ple: an y instance\n",
      " loca ted in a low-density region can be considered an anomaly . Y ou m ust define wha t\n",
      " density threshold you wan t to use. F or exam ple, in a man ufacturing com pan y tha t\n",
      " tries to detect defective products, the ra tio of defective products is usually well-\n",
      " known. Sa y it is equal to 4%, then you can set the density threshold to be the value\n",
      " tha t results in ha ving 4% of the instances loca ted in areas below tha t threshold denƒ\n",
      " sity . If you notice tha t you get too man y false positives (i.e., perfectly good products\n",
      " tha t are flagged as defective), you can lower the threshold. Con versely , if you ha ve too\n",
      " man y false nega tives (i.e., defective products tha t the system does not flag as defecƒ\n",
      "tive), you can increase the threshold. This is the usual precision/recall tradeoff (see\n",
      " Cha pter 3\n",
      " ). H ere is how you would iden tif y the outliers using the 4th percen tile lowƒ\n",
      " 266  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " est density as the threshold (i.e., a pproxima tely 4% of the instances will be flagged as\n",
      "anomalies):\n",
      "densities\n",
      " \n",
      "=\n",
      " \n",
      "gm\n",
      ".\n",
      "score_samples\n",
      "(\n",
      "X\n",
      ")\n",
      "density_threshold\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "percentile\n",
      "(\n",
      "densities\n",
      ",\n",
      " \n",
      "4\n",
      ")\n",
      "anomalies\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "[\n",
      "densities\n",
      " \n",
      "<\n",
      " \n",
      "density_threshold\n",
      "]\n",
      " These anomalies are represen ted as stars on \n",
      "Figure 9-19\n",
      ":\n",
      " F i g u r e 9-19. An o m a l y d e t e c t i o n u s i n g a G a u s s i a n m ixt u r e m o d e l\n",
      " A closely rela ted task is \n",
      " n o v e l ty d e t e c t i o n\n",
      " : it differs from anomaly detection in tha t the\n",
      " algorithm is assumed to be trained on a — clean – da taset, uncon tamina ted by outliers,\n",
      " whereas anomaly detection does not make this assum ption. Indeed, outlier detection\n",
      " is often precisely used to clean up a da taset.\n",
      " Ga ussian mixture models tr y to fit all the da ta, including the outliƒ\n",
      " ers, so if you ha ve too man y of them, this will bias the model ‡ s view\n",
      " of — normality–: some outliers ma y wrongly be considered as norƒ\n",
      " mal. If this ha ppens, you can tr y to fit the model once, use it to\n",
      "detect and remove the most extreme outliers, then fit the model\n",
      " again on the cleaned up da taset. Another a pproach is to use robust\n",
      " covariance estima tion methods (see the \n",
      "EllipticEnvelope\n",
      " class).\n",
      " J ust like K-M eans, the \n",
      "GaussianMixture\n",
      "  algorithm requires you to specif y the n umƒ\n",
      "ber of clusters. So how can you find it?\n",
      "Selecting the Number of Clusters\n",
      " W ith K-M eans, you could use the inertia or the silhouette score to select the a pproƒ\n",
      " pria te n umber of clusters, but with Ga ussian mixtures, it is not possible to use these\n",
      " metrics beca use they are not reliable when the clusters are not spherical or ha ve difƒ\n",
      " feren t sizes. Instead, you can tr y to find the model tha t minimizes a \n",
      " t h e o r e t i c a l i n f o r‡\n",
      " Gaussian Mixtures  |  267\n",
      "\n",
      " m a t i o n cr i t er i o n\n",
      " such as the \n",
      " B a y e s i a n i n f o r m a t i o n cr i t er i o n\n",
      " (BIC) or the \n",
      " Ak a i k e\n",
      " i n f o r m a t i o n cr i t er i o n\n",
      " (AIC), defined in \n",
      " Equa tion 9-1\n",
      ".\n",
      " Eq u a t i o n 9-1. B a y e s i a n i n f o r m a t i o n cr i t er i o n (BI C) a n d Ak a i k e i n f o r m a t i o n\n",
      " cr i t er i o n (AI C)\n",
      " BI C\n",
      " = log\n",
      "m\n",
      "p\n",
      " ” 2 log\n",
      "L\n",
      " AI C\n",
      " = 2\n",
      "p\n",
      " ” 2 log\n",
      "L\n",
      "⁄\n",
      "m\n",
      "  is the n umber of instances, as alwa ys.\n",
      "⁄\n",
      "p\n",
      "  is the n umber of parameters learned by the model.\n",
      "⁄\n",
      "L\n",
      " is the maximized value of the \n",
      " l i k e l i h o o d f u n c t i o n\n",
      " of the model.\n",
      " B oth the BIC and the AIC penalize models tha t ha ve more parameters to learn (e.g.,\n",
      " more clusters), and reward models tha t fit the da ta well. They often end up selecting\n",
      " the same model, but when they differ , the model selected by the BIC tends to be simƒ\n",
      " pler (fewer parameters) than the one selected by the AIC, but it does not fit the da ta\n",
      " quite as well (this is especially true for larger da tasets).\n",
      "Likelihood function\n",
      " The terms — probability– and —likelihood – are often used in terchangeably in the\n",
      " English language, but they ha ve ver y differen t meanings in sta tistics: given a sta tistical\n",
      "model with some parameters \n",
      "•\n",
      " , the word — probability– is used to describe how pla usiƒ\n",
      "ble a future outcome \n",
      "x\n",
      " is (knowing the parameter values \n",
      "•\n",
      "), while the word —likeliƒ\n",
      " hood – is used to describe how pla usible a particular set of parameter values \n",
      "•\n",
      " \n",
      "are,\n",
      "after the outcome \n",
      "x\n",
      " is known.\n",
      " Consider a one-dimensional mixture model of two Ga ussian distributions cen tered a t\n",
      " -4 and +1. F or sim plicity , this toy model has a single parameter \n",
      "–\n",
      "  tha t con trols the\n",
      " standard devia tions of both distributions. The top left con tour plot in \n",
      "Figure 9-20\n",
      " shows the en tire model \n",
      "f\n",
      "(\n",
      "x\n",
      "; \n",
      "–\n",
      ") as a function of both \n",
      "x\n",
      "   and \n",
      "–\n",
      " . T o estima te the probabilƒ\n",
      "ity distribution of a future outcome \n",
      "x\n",
      ", you need to set the model parameter \n",
      "–\n",
      " . F or\n",
      " exam ple, if you set it to \n",
      "–\n",
      " =1.3 (the horizon tal line), you get the probability density\n",
      "function \n",
      "f\n",
      "(\n",
      "x\n",
      "; \n",
      "–\n",
      " =1.3) shown in the lower left plot. Sa y you wan t to estima te the probaƒ\n",
      " bility tha t \n",
      "x\n",
      " \n",
      " will fall between -2 and +2, you m ust calcula te the in tegral of the PDF on\n",
      " this range (i.e., the surface of the shaded region). On the other hand, if you ha ve\n",
      " obser ved a single instance \n",
      "x\n",
      "=2.5 (the vertical line in the upper left plot), you get the\n",
      "likelihood function noted \n",
      "(\n",
      "–\n",
      "|\n",
      "x\n",
      "=2.5)=f(\n",
      "x\n",
      "=2.5; \n",
      "–\n",
      " ) represen ted in the upper righ t plot.\n",
      "In short, the PDF is a function of \n",
      "x\n",
      " (with \n",
      "–\n",
      " fixed) while the likelihood function is a\n",
      "function of \n",
      "–\n",
      "   (with \n",
      "x\n",
      " \n",
      " fixed). I t is im portan t to understand tha t the likelihood function\n",
      "is \n",
      " n o t\n",
      "  a probability distribution: if you in tegra te a probability distribution over all\n",
      " 268  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "possible values of \n",
      "x\n",
      " , you alwa ys get 1, but if you in tegra te the likelihood function over\n",
      "all possible values of \n",
      "–\n",
      " , the result can be an y positive value.\n",
      " F i g u r e 9-20. A m o d e l ‹ s p a r a m e t r i c f u n c t i o n (t o p \n",
      "le“),\n",
      "  a n d s o m e d er i v e d f u n c t i o ns: a P D F\n",
      " (l o w er \n",
      "le“),\n",
      "  a l i k e l i h o o d f u n c t i o n (t o p r i gh t) a n d a l og l i k e l i h o o d f u n c t i o n (l o w er r i gh t)\n",
      " Given a da taset \n",
      "X\n",
      " , a common task is to tr y to estima te the most likely values for the\n",
      " model parameters. T o do this, you m ust find the values tha t maximize the likelihood\n",
      "function, given \n",
      "X\n",
      " . In this exam ple, if you ha ve obser ved a single instance \n",
      "x\n",
      "=2.5, the\n",
      " m axi m u m l i k e l i h o o d e s t i m a t e\n",
      " \n",
      "(MLE) of \n",
      "–\n",
      "   is \n",
      "–\n",
      "=1.5. If a prior probability distribution \n",
      "g\n",
      "over \n",
      "–\n",
      "  exists, it is possible to take it in to accoun t by maximizing \n",
      "(\n",
      "–\n",
      "|\n",
      "x\n",
      ")g(\n",
      "–\n",
      " ) ra ther\n",
      "than just maximizing \n",
      "(\n",
      "–\n",
      "|\n",
      "x\n",
      " ). This is called maxim um a-posteriori (MAP) estima tion.\n",
      "Since MAP constrains the parameter values, you can think of it as a regularized verƒ\n",
      "sion of MLE.\n",
      " N otice tha t it is equivalen t to maximize the likelihood function or to maximize its\n",
      " logarithm (represen ted in the lower righ t hand side of \n",
      "Figure 9-20\n",
      "): indeed, the logaƒ\n",
      "rithm is a strictly increasing function, so if \n",
      "–\n",
      " maximizes the log likelihood, it also\n",
      " maximizes the likelihood. I t turns out tha t it is generally easier to maximize the log\n",
      " likelihood. F or exam ple, if you obser ved several independen t instances \n",
      "x\n",
      "(1)\n",
      "   to \n",
      "x\n",
      "(\n",
      "m\n",
      ")\n",
      ", you\n",
      "would need to find the value of \n",
      "–\n",
      "  tha t maximizes the product of the individual likeliƒ\n",
      " hood functions. But it is equivalen t, and m uch sim pler , to maximize the sum (not the\n",
      "product) of the log likelihood functions, thanks to the magic of the logarithm which\n",
      " con verts products in to sums: log(\n",
      " a b\n",
      ")=log(\n",
      "a\n",
      ")+log(\n",
      "b\n",
      ").\n",
      " Once you ha ve estima ted \n",
      "–\n",
      ", the value of \n",
      "–\n",
      "  tha t maximizes the likelihood function,\n",
      " then you are ready to com pute \n",
      "L\n",
      "=\n",
      "–\n",
      ",\n",
      ". This is the value which is used to comƒ\n",
      "pute the AIC and BIC: you can think of it as a measure of how well the model fits the\n",
      " da ta.\n",
      " T o com pute the BIC and AIC, just call the \n",
      "bic()\n",
      " or \n",
      "aic()\n",
      " methods:\n",
      " Gaussian Mixtures  |  269\n",
      "\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "bic\n",
      "(\n",
      "X\n",
      ")\n",
      "8189.74345832983\n",
      ">>> \n",
      "gm\n",
      ".\n",
      "aic\n",
      "(\n",
      "X\n",
      ")\n",
      "8102.518178214792\n",
      "Figure 9-21\n",
      "  shows the BIC for differen t n umbers of clusters \n",
      "k\n",
      ". As you can see, both\n",
      "the BIC and the AIC are lowest when \n",
      "k\n",
      " =3, so it is most likely the best choice. N ote\n",
      " tha t we could also search for the best value for the \n",
      "covariance_type\n",
      "   h yperparameter .\n",
      " F or exam ple, if it is \n",
      "\"spherical\"\n",
      " \n",
      " ra ther than \n",
      "\"full\"\n",
      " , then the model has m uch fewer\n",
      " parameters to learn, but it does not fit the da ta as well.\n",
      " F i g u r e 9-21. AI C a n d BI C f o r \n",
      "di›erent\n",
      "  n u m b er s o f c l u s t er s k\n",
      "Bayesian Gaussian Mixture Models\n",
      " R a ther than man ually searching for the optimal n umber of clusters, it is possible to\n",
      "use instead the \n",
      "BayesianGaussianMixture\n",
      "  class which is ca pable of giving weigh ts\n",
      " equal (or close) to zero to unnecessar y clusters. J ust set the n umber of clusters \n",
      "n_com\n",
      "ponents\n",
      "  to a value tha t you ha ve good reason to believe is grea ter than the optimal\n",
      " n umber of clusters (this assumes some minimal knowledge about the problem a t\n",
      " hand), and the algorithm will elimina te the unnecessar y clusters a utoma tically . F or\n",
      " exam ple, let ‡ s set the n umber of clusters to 10 and see wha t ha ppens:\n",
      ">>> \n",
      "from\n",
      " \n",
      "sklearn.mixture\n",
      " \n",
      "import\n",
      " \n",
      "BayesianGaussianMixture\n",
      ">>> \n",
      "bgm\n",
      " \n",
      "=\n",
      " \n",
      "BayesianGaussianMixture\n",
      "(\n",
      "n_components\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "n_init\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "random_state\n",
      "=\n",
      "42\n",
      ")\n",
      ">>> \n",
      "bgm\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ")\n",
      ">>> \n",
      "np\n",
      ".\n",
      "round\n",
      "(\n",
      "bgm\n",
      ".\n",
      "weights_\n",
      ",\n",
      " \n",
      "2\n",
      ")\n",
      "array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\n",
      " P erfect: the algorithm a utoma tically detected tha t only 3 clusters are needed, and the\n",
      " resulting clusters are almost iden tical to the ones in \n",
      "Figure 9-17\n",
      ".\n",
      " In this model, the cluster parameters (including the weigh ts, means and covariance\n",
      " ma trices) are not trea ted as fixed model parameters an ymore, but as la ten t random\n",
      " variables, like the cluster assignmen ts (see \n",
      "Figure 9-22\n",
      "). So \n",
      "z\n",
      " now includes both the\n",
      " cluster parameters and the cluster assignmen ts.\n",
      " 270  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " F i g u r e 9-22. B a y e s i a n G a u s s i a n m ixt u r e m o d e l\n",
      " Prior knowledge about the la ten t variables \n",
      "z\n",
      " \n",
      "can be encoded in a probability distribuƒ\n",
      "tion \n",
      "p\n",
      "(\n",
      "z\n",
      ") called the \n",
      " p r i o r\n",
      " . F or exam ple, we ma y ha ve a prior belief tha t the clusters are\n",
      " likely to be few (low concen tra tion), or con versely , tha t they are more likely to be\n",
      " plen tiful (high concen tra tion). This can be adjusted using the \n",
      "weight_concentra\n",
      "tion_prior\n",
      " \n",
      " h yperparameter . Setting it to 0.01 or 1000 gives ver y differen t clusterings\n",
      "(see \n",
      "Figure 9-23\n",
      " ). H owever , the more da ta we ha ve, the less the priors ma tter . In fact,\n",
      " to plot diagrams with such large differences, you m ust use ver y strong priors and litƒ\n",
      " tle da ta.\n",
      " F i g u r e 9-23. U s i n g \n",
      "di›erent\n",
      "  c o n c en t r a t i o n p r i o r s\n",
      " The fact tha t you see only 3 regions in the righ t plot although there\n",
      " are 4 cen troids is not a bug: the weigh t of the top-righ t cluster is\n",
      " m uch larger than the weigh t of the lower -righ t cluster , so the probƒ\n",
      " ability tha t an y given poin t in this region belongs to the top-righ t\n",
      " cluster is grea ter than the probability tha t it belongs to the lower -\n",
      " righ t cluster , even near the lower -righ t cluster .\n",
      " Gaussian Mixtures  |  271\n",
      "\n",
      " Ba yes ‡ theorem (\n",
      " Equa tion 9-2\n",
      " ) tells us how to upda te the probability distribution over\n",
      " the la ten t variables after we obser ve some da ta \n",
      "X\n",
      " . I t com putes the \n",
      " p o s t er i o r\n",
      " \n",
      "distribuƒ\n",
      "tion \n",
      "p\n",
      "(\n",
      "z\n",
      "|\n",
      "X\n",
      "), which is the conditional probability of \n",
      "z\n",
      " given \n",
      "X\n",
      ".\n",
      " Eq u a t i o n 9-2. B a y e s ‹ t h e o r em\n",
      "p\n",
      "z\n",
      "X\n",
      " = Posterior =\n",
      "Likelihood„Prior\n",
      "Evidence\n",
      "=\n",
      "p\n",
      "X\n",
      "z\n",
      "p\n",
      "z\n",
      "p\n",
      "X\n",
      " U nfortuna tely , in a Ga ussian mixture model (and man y other problems), the denomiƒ\n",
      " na tor \n",
      "p\n",
      "(\n",
      "x\n",
      " ) is in tractable, as it requires in tegra ting over all the possible values of \n",
      "z\n",
      "(\n",
      " Equa tion 9-3\n",
      " ). This means considering all possible combina tions of cluster parameƒ\n",
      " ters and cluster assignmen ts.\n",
      " Eq u a t i o n 9-3. \n",
      "•e\n",
      "  e v i d en c e p(\n",
      "X\n",
      " ) i s \n",
      "o“en\n",
      "  i n t r a c t a b l e\n",
      "p\n",
      "X\n",
      "=\n",
      "À\n",
      "p\n",
      "X\n",
      "z\n",
      "p\n",
      "z\n",
      "d\n",
      "z\n",
      " This is one of the cen tral problems in Ba yesian sta tistics, and there are several\n",
      " a pproaches to solving it. One of them is \n",
      " v a r i a t i o n a l i n f er en c e\n",
      ", which picks a family of\n",
      "distributions \n",
      "q\n",
      "(\n",
      "z\n",
      "; \n",
      "ƒ\n",
      ") with its own \n",
      " v a r i a t i o n a l p a r a m e t er s\n",
      " \n",
      "ƒ\n",
      " \n",
      "(lambda), then it optimizes\n",
      "these parameters to make \n",
      "q\n",
      "(\n",
      "z\n",
      " ) a good a pproxima tion of \n",
      "p\n",
      "(\n",
      "z\n",
      "|\n",
      "X\n",
      "). This is achieved by\n",
      "finding the value of \n",
      "ƒ\n",
      "  tha t minimizes the KL divergence from \n",
      "q\n",
      "(\n",
      "z\n",
      ") to \n",
      "p\n",
      "(\n",
      "z\n",
      "|\n",
      "X\n",
      "), noted\n",
      "D\n",
      "KL\n",
      "(\n",
      "q\n",
      "p\n",
      " ). The KL divergence equa tion is shown in (see \n",
      " Equa tion 9-4\n",
      "), and it can be\n",
      "rewritten as the log of the evidence (log \n",
      "p\n",
      "(\n",
      "X\n",
      " )) min us the \n",
      " e v i d en c e l o w er b o u n d\n",
      " (ELB O). Since the log of the evidence does not depend on \n",
      "q\n",
      " , it is a constan t term, so\n",
      " minimizing the KL divergence just requires maximizing the ELB O .\n",
      " Eq u a t i o n 9-4. KL d i v er gen c e f r o m q(\n",
      "z\n",
      " ) t o p(\n",
      "z\n",
      "|\n",
      "X\n",
      ")\n",
      "D\n",
      " K L\n",
      "q\n",
      "p\n",
      "=\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      "p\n",
      "z\n",
      "X\n",
      "=\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      " ” log\n",
      "p\n",
      "z\n",
      "X\n",
      "=\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      " ” log\n",
      "p\n",
      "z\n",
      ",\n",
      "X\n",
      "p\n",
      "X\n",
      "=\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      " ” log\n",
      "p\n",
      "z\n",
      ",\n",
      "X\n",
      " + log\n",
      "p\n",
      "X\n",
      "=\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      "”\n",
      "q\n",
      "log\n",
      "p\n",
      "z\n",
      ",\n",
      "X\n",
      "+\n",
      "q\n",
      "log\n",
      "p\n",
      "X\n",
      "=\n",
      "q\n",
      "log\n",
      "p\n",
      "X\n",
      "”\n",
      "q\n",
      "log\n",
      "p\n",
      "z\n",
      ",\n",
      "X\n",
      "”\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      " = log\n",
      "p\n",
      "X\n",
      " ” ELBO\n",
      " where ELBO =\n",
      "q\n",
      "log\n",
      "p\n",
      "z\n",
      ",\n",
      "X\n",
      "”\n",
      "q\n",
      "log\n",
      "q\n",
      "z\n",
      " 272  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      " In practice, there are differen t techniques to maximize the ELB O . In \n",
      " m e a n \n",
      "†eld\n",
      "  v a r i a‡\n",
      " t i o n a l i n f er en c e\n",
      " , it is necessar y to pick the family of distributions \n",
      "q\n",
      "(\n",
      "z\n",
      "; \n",
      "ƒ\n",
      ") and the prior\n",
      "p\n",
      "(\n",
      "z\n",
      " ) ver y carefully to ensure tha t the equa tion for the ELB O sim plifies to a form tha t\n",
      " can actually be com puted. U nfortuna tely , there is no general wa y to do this, it\n",
      " depends on the task and requires some ma thema tical skills. F or exam ple, the distribuƒ\n",
      " tions and lower bound equa tions used in Scikit-Learn ‡ s \n",
      "BayesianGaussianMixture\n",
      " class are presen ted in the \n",
      " documen ta tion\n",
      " . From these equa tions it is possible to derive\n",
      " upda te equa tions for the cluster parameters and assignmen t variables: these are then\n",
      " used ver y m uch like in the Expecta tion-M aximiza tion algorithm. In fact, the com puƒ\n",
      " ta tional com plexity of the \n",
      "BayesianGaussianMixture\n",
      "  class is similar to tha t of the\n",
      "GaussianMixture\n",
      "  class (but generally significan tly slower). A sim pler a pproach to\n",
      " maximizing the ELB O is called \n",
      " b l a c k b o x s t o c h as t i c v a r i a t i o n a l i n f er en c e\n",
      "  (BBSVI): a t\n",
      " each itera tion, a few sam ples are dra wn from \n",
      "q\n",
      " \n",
      " and they are used to estima te the graƒ\n",
      " dien ts of the ELB O with regards to the varia tional parameters \n",
      "ƒ\n",
      ", which are then used\n",
      " in a gradien t ascen t step . This a pproach makes it possible to use Ba yesian inference\n",
      " with an y kind of model (provided it is differen tiable), even deep neural networks: this\n",
      " is called Ba yesian deep learning.\n",
      " If you wan t to dive deeper in to Ba yesian sta tistics, check out the\n",
      " B a y e s i a n D a t a An a l ys i s\n",
      " \n",
      "book\n",
      "  by Andrew Gelman, J ohn Carlin, H al\n",
      " Stern, Da vid Dunson, Aki V eh tari, and Donald R ubin.\n",
      " Ga ussian mixture models work grea t on clusters with ellipsoidal sha pes, but if you tr y\n",
      " to fit a da taset with differen t sha pes, you ma y ha ve bad surprises. F or exam ple, let ‡ s\n",
      " see wha t ha ppens if we use a Ba yesian Ga ussian mixture model to cluster the moons\n",
      " da taset (see \n",
      "Figure 9-24\n",
      "):\n",
      " F i g u r e 9-24. m o o ns_vs_bg m_d i a g r a m\n",
      " Oops, the algorithm despera tely searched for ellipsoids, so it found 8 differen t clusƒ\n",
      " ters instead of 2. The density estima tion is not too bad, so this model could perha ps\n",
      " be used for anomaly detection, but it failed to iden tif y the two moons. Let ‡ s now look\n",
      " a t a few clustering algorithms ca pable of dealing with arbitrarily sha ped clusters.\n",
      " Gaussian Mixtures  |  273\n",
      "\n",
      "Other Anomaly Detection and Novelty Detection Algorithms\n",
      " Scikit-Learn also im plemen ts a few algorithms dedica ted to anomaly detection or\n",
      "novelty detection:\n",
      "⁄\n",
      " F as t-M CD\n",
      "  (minim um covariance determinan t), im plemen ted by the \n",
      "EllipticEn\n",
      "velope\n",
      " class: this algorithm is useful for outlier detection, in particular to\n",
      " clean up a da taset. I t assumes tha t the normal instances (inliers) are genera ted\n",
      " from a single Ga ussian distribution (not a mixture), but it also assumes tha t the\n",
      " da taset is con tamina ted with outliers tha t were not genera ted from this Ga ussian\n",
      " distribution. When it estima tes the parameters of the Ga ussian distribution (i.e.,\n",
      " the sha pe of the elliptic en velope around the inliers), it is careful to ignore the\n",
      " instances tha t are most likely outliers. This gives a better estima tion of the elliptic\n",
      " en velope, and th us makes it better a t iden tif ying the outliers.\n",
      "⁄\n",
      " I s o l a t i o n f o r e s t\n",
      " : this is an efficien t algorithm for outlier detection, especially in\n",
      " high-dimensional da tasets. The algorithm builds a R andom F orest in which each\n",
      " Decision T ree is grown randomly : a t each node, it picks a fea ture randomly , then\n",
      "it picks a random threshold value (between the min and max value) to split the\n",
      " da taset in two . The da taset gradually gets chopped in to pieces this wa y , un til all\n",
      " instances end up isola ted from the other instances. An anomaly is usually far\n",
      " from other instances, so on a verage (across all the Decision T rees) it tends to get\n",
      " isola ted in less steps than normal instances.\n",
      "⁄\n",
      " L o c a l o u t l i er f a c t o r\n",
      "  (LOF): this algorithm is also good for outlier detection. I t\n",
      " com pares the density of instances around a given instance to the density around\n",
      " its neighbors. An anomaly is often more isola ted than its \n",
      "k\n",
      " nearest neighbors.\n",
      "⁄\n",
      " O n e-c l as s SVM\n",
      " : this algorithm is better suited for novelty detection. Recall tha t a\n",
      " kernelized SVM classifier separa tes two classes by first (im plicitly) ma pping all\n",
      " the instances to a high-dimensional space, then separa ting the two classes using a\n",
      "linear SVM classifier within this high-dimensional space (see \n",
      " Cha pter 5\n",
      "). Since\n",
      " we just ha ve one class of instances, the one-class SVM algorithm instead tries to\n",
      " separa te the instances in high-dimensional space from the origin. In the original\n",
      " space, this will correspond to finding a small region tha t encom passes all the\n",
      " instances. If a new instance does not fall within this region, it is an anomaly .\n",
      " There are a few h yperparameters to tweak: the usual ones for a kernelized SVM,\n",
      " plus a margin h yperparameter tha t corresponds to the probability of a new\n",
      " instance being mistakenly considered as novel, when it is in fact normal. I t works\n",
      " grea t, especially with high-dimensional da tasets, but just like all SVMs, it does\n",
      " not scale to large da tasets.\n",
      " 274  |  Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "PART II\n",
      "Neural Networks and Deep Learning\n",
      "\n",
      "\n",
      "1\n",
      " Y ou can get the best of both worlds by being open to biological inspira tions without being afraid to crea te\n",
      "biologically unrealistic models, as long as they work well.\n",
      "CHAPTER 10\n",
      "Introduction to \n",
      "Arti•cial\n",
      " Neural Networks\n",
      "with Keras\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 10 in the final\n",
      "release of the book.\n",
      " Birds inspired us to fly , burdock plan ts inspired velcro , and coun tless more in venƒ\n",
      " tions were inspired by na ture. I t seems only logical, then, to look a t the brain ‡ s archiƒ\n",
      " tecture for inspira tion on how to build an in telligen t machine. This is the key idea\n",
      " tha t sparked \n",
      "arti†cial\n",
      "  n eu r a l n e tw o r ks\n",
      "  (ANN s). H owever , although planes were\n",
      " inspired by birds, they don ‡ t ha ve to fla p their wings. Similarly , ANN s ha ve gradually\n",
      " become quite differen t from their biological cousins. Some researchers even argue\n",
      " tha t we should drop the biological analog y altogether (e.g., by sa ying — units – ra ther\n",
      " than — neurons –), lest we restrict our crea tivity to biologically pla usible systems.\n",
      "1\n",
      " ANN s are a t the ver y core of Deep Learning. They are versa tile, powerful, and scalaƒ\n",
      " ble, making them ideal to tackle large and highly com plex M achine Learning tasks,\n",
      " such as classif ying billions of images (e.g., Google Images), powering speech recogniƒ\n",
      " tion ser vices (e.g., A pple ‡ s Siri), recommending the best videos to wa tch to h undreds\n",
      " of millions of users ever y da y (e.g., Y ouT ube), or learning to bea t the world cham pion\n",
      " a t the game of \n",
      " G o\n",
      "  by pla ying millions of games against \n",
      " itself (DeepMind ‡ s Alphaƒ\n",
      "Zero).\n",
      "277\n",
      "\n",
      "2\n",
      " — A Logical Calculus of I deas Immanen t in N er vous A ctivity , – W . M cCulloch and W . Pitts (1943).\n",
      " In the first part of this cha pter , we will in troduce artificial neural networks, starting\n",
      " with a quick tour of the ver y first ANN architectures, leading up to \n",
      " M u l t i-L a y er P er‡\n",
      " c e p t r o ns\n",
      "   (MLP s) \n",
      " which are hea vily used toda y (other architectures will be explored in\n",
      " the next cha pters). In the second part, we will look a t how to im plemen t neural netƒ\n",
      " works using the popular K eras API. This is a bea utifully designed and sim ple high-\n",
      " level API for building, training, evalua ting and running neural networks. But don ‡ t be\n",
      " fooled by its sim plicity : it is expressive and flexible enough to let you build a wide\n",
      " variety of neural network architectures. In fact, it will probably be sufficien t for most\n",
      " of your use cases. M oreover , should you ever need extra flexibility , you can alwa ys\n",
      " write custom K eras com ponen ts using its lower -level API, as we will see in \n",
      " Cha pƒ\n",
      "ter 12\n",
      ".\n",
      " But first, let ‡ s go back in time to see how artificial neural networks came to be!\n",
      "From Biological to \n",
      "Arti•cial\n",
      " Neurons\n",
      " Surprisingly , ANN s ha ve been around for quite a while: they were first in troduced\n",
      " back in 1943 by the neuroph ysiologist W arren M cCulloch and the ma thema tician\n",
      " W alter Pitts. In their \n",
      " landmark pa per\n",
      ",\n",
      "2\n",
      "  — A Logical Calculus of I deas Immanen t in\n",
      " N er vous A ctivity , – M cCulloch and Pitts presen ted a sim plified com puta tional model\n",
      " of how biological neurons migh t work together in animal brains to perform com plex\n",
      " com puta tions using \n",
      " p r o p o s i t i o n a l l og i c\n",
      ". This was the first artificial neural network\n",
      " architecture. Since then man y other architectures ha ve been in ven ted, as we will see.\n",
      " The early successes of ANN s un til the 1960s led to the widespread belief tha t we\n",
      " would soon be con versing with truly in telligen t machines. When it became clear tha t\n",
      " this promise would go unfulfilled (a t least for quite a while), funding flew elsewhere\n",
      " and ANN s en tered a long win ter . In the early 1980s there was a revival of in terest in \n",
      " c o n n e c t i o n i s m\n",
      " \n",
      " (the study of neural networks), as new architectures were in ven ted and\n",
      " better training techniques were developed. But progress was slow , and by the 1990s\n",
      " other powerful M achine Learning techniques were in ven ted, such as Support V ector\n",
      " M achines (see \n",
      " Cha pter 5\n",
      "). These techniques seemed to offer better results and stronƒ\n",
      " ger theoretical founda tions than ANN s, so once again the study of neural networks\n",
      " en tered a long win ter .\n",
      " Finally , we are now witnessing yet another wa ve of in terest in ANN s. W ill this wa ve\n",
      " die out like the previous ones did? W ell, there are a few good reasons to believe tha t\n",
      " this wa ve is differen t and tha t it will ha ve a m uch more profound im pact on our lives:\n",
      " 278  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "⁄\n",
      " There is now a h uge quan tity of da ta a vailable to train neural networks, and\n",
      " ANN s frequen tly outperform other ML techniques on ver y large and com plex\n",
      "problems.\n",
      "⁄\n",
      " The tremendous increase in com puting power since the 1990s now makes it posƒ\n",
      " sible to train large neural networks in a reasonable amoun t of time. This is in\n",
      " part due to M oore ‡ s La w , but also thanks to the gaming industr y , which has proƒ\n",
      "duced powerful GPU cards by the millions.\n",
      "⁄\n",
      " The training algorithms ha ve been im proved. T o be fair they are only sligh tly difƒ\n",
      " feren t from the ones used in the 1990s, but these rela tively small tweaks ha ve a\n",
      " h uge positive im pact.\n",
      "⁄\n",
      " Some theoretical limita tions of ANN s ha ve turned out to be benign in practice.\n",
      " F or exam ple, man y people though t tha t ANN training algorithms were doomed\n",
      " beca use they were likely to get stuck in local optima, but it turns out tha t this is\n",
      " ra ther rare in practice (or when it is the case, they are usually fairly close to the\n",
      " global optim um).\n",
      "⁄\n",
      " ANN s seem to ha ve en tered a virtuous circle of funding and progress. Amazing\n",
      " products based on ANN s regularly make the headline news, which pulls more\n",
      " and more a tten tion and funding toward them, resulting in more and more proƒ\n",
      "gress, and even more amazing products.\n",
      "Biological Neurons\n",
      " B efore we discuss artificial neurons, let ‡ s take a quick look a t a biological neuron (repƒ\n",
      " resen ted in \n",
      "Figure 10-1\n",
      " ). I t is an un usual-looking cell mostly found in animal cerebral\n",
      " cortexes (e.g., your brain), com posed of a \n",
      " c e l l b o d y\n",
      "  con taining the n ucleus and most\n",
      " of the cell ‡ s com plex com ponen ts, and man y branching extensions called \n",
      " d en d r i t e s\n",
      ",\n",
      " plus one ver y long extension called the \n",
      " ax o n\n",
      " . The axon ‡ s length ma y be just a few\n",
      " times longer than the cell body , or up to tens of thousands of times longer . N ear its\n",
      " extremity the axon splits off in to man y branches called \n",
      " t e l o d en d r i a\n",
      " , and a t the tip of\n",
      " these branches are min uscule structures called \n",
      " s y n a p t i c t er m i n a l s\n",
      " \n",
      " (or sim ply \n",
      " s y n a p‡\n",
      " s e s\n",
      "), which are connected to the dendrites (or directly to the cell body) of other neuƒ\n",
      " rons. Biological neurons receive short electrical im pulses called \n",
      " s i g n a l s\n",
      " \n",
      "from other\n",
      " neurons via these syna pses. When a neuron receives a sufficien t n umber of signals\n",
      "from other neurons within a few milliseconds, it fires its own signals.\n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  279\n",
      "\n",
      "3\n",
      " Image by Bruce Bla us (\n",
      " Crea tive Commons 3.0\n",
      "). Reproduced from \n",
      " h ttp s://en.w i k i p e d i a.o r g/w i k i/N eu r o n\n",
      ".\n",
      "4\n",
      " In the con text of M achine Learning, the phrase — neural networks – generally refers to ANN s, not BNN s.\n",
      "5\n",
      " Dra wing of a cortical lamina tion by S. R amon y Ca jal (public domain). Reproduced from \n",
      " h ttp s://en.w i k i p e\n",
      " d i a.o r g/w i k i/C er e b r a l_c o r t ex\n",
      ".\n",
      " F i g u r e 10-1. B i o l og i c a l n eu r o n\n",
      "3\n",
      " Th us, individual biological neurons seem to beha ve in a ra ther sim ple wa y , but they\n",
      "are organized in a vast network of billions of neurons, each neuron typically connecƒ\n",
      " ted to thousands of other neurons. Highly com plex com puta tions can be performed\n",
      " by a vast network of fairly sim ple neurons, m uch like a com plex an thill can emerge\n",
      " from the combined efforts of sim ple an ts. The architecture of biological neural netƒ\n",
      "works (BNN)\n",
      "4\n",
      "  is still the subject of active research, but some parts of the brain ha ve\n",
      " been ma pped, and it seems tha t neurons are often organized in consecutive la yers, as \n",
      "shown in \n",
      "Figure 10-2\n",
      ".\n",
      " F i g u r e 10-2. M u l t i p l e l a y er s i n a b i o l og i c a l n eu r a l n e tw o r k (h u m a n c o r t ex)\n",
      "5\n",
      " 280  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "Logical Computations with Neurons\n",
      " W arren M cCulloch and W alter Pitts proposed a ver y sim ple model of the biological\n",
      " neuron, which la ter became known as an \n",
      "arti†cial\n",
      "  n eu r o n\n",
      " : it has one or more binar y\n",
      " (on/off ) in puts and one binar y output. The artificial neuron sim ply activa tes its outƒ\n",
      " put when more than a certain n umber of its in puts are active. M cCulloch and Pitts\n",
      " showed tha t even with such a sim plified model it is possible to build a network of\n",
      " artificial neurons tha t com putes an y logical proposition you wan t. F or exam ple, let ‡ s\n",
      " build a few ANN s tha t perform various logical com puta tions (see \n",
      "Figure 10-3\n",
      "),\n",
      " assuming tha t a neuron is activa ted when a t least two of its in puts are active.\n",
      " F i g u r e 10-3. ANN s p er f o r m i n g s i m p l e l og i c a l c o m p u t a t i o ns\n",
      "⁄\n",
      " The first network on the left is sim ply the iden tity function: if neuron A is activaƒ\n",
      " ted, then neuron C gets activa ted as well (since it receives two in put signals from\n",
      "neuron A), but if neuron A is off, then neuron C is off as well.\n",
      "⁄\n",
      " The second network performs a logical AND: neuron C is activa ted only when\n",
      " both neurons A and B are activa ted (a single in put signal is not enough to actiƒ\n",
      " va te neuron C).\n",
      "⁄\n",
      " The third network performs a logical OR: neuron C gets activa ted if either neuƒ\n",
      " ron A or neuron B is activa ted (or both).\n",
      "⁄\n",
      " Finally , if we suppose tha t an in put connection can inhibit the neuron ‡ s activity\n",
      " (which is the case with biological neurons), then the fourth network com putes a\n",
      " sligh tly more com plex logical proposition: neuron C is activa ted only if neuron A\n",
      "is active and if neuron B is off. If neuron A is active all the time, then you get a\n",
      " logical NOT : neuron C is active when neuron B is off, and vice versa.\n",
      " Y ou can easily imagine how these networks can be combined to com pute com plex\n",
      " logical expressions (see the exercises a t the end of the cha pter).\n",
      "The Perceptron\n",
      "The \n",
      " P er c e p t r o n\n",
      "  is one of the sim plest ANN architectures, in ven ted in 1957 by Frank\n",
      " Rosenbla tt. I t is based on a sligh tly differen t artificial neuron (see \n",
      "Figure 10-4\n",
      ") called \n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  281\n",
      "\n",
      "6\n",
      "The name \n",
      " P er c e p t r o n\n",
      "  is sometimes used to mean a tin y network with a single TL U .\n",
      "a \n",
      " t h r e s h o l d l og i c u n i t\n",
      "  (TL U), or sometimes a \n",
      " l i n e a r t h r e s h o l d u n i t\n",
      "  (L TU): the in puts\n",
      " and output are now n umbers (instead of binar y on/off values) and each in put conƒ\n",
      " nection is associa ted with a weigh t. The TL U com putes a weigh ted sum of its in puts\n",
      "(\n",
      "z\n",
      " = \n",
      "w\n",
      "1\n",
      " \n",
      "x\n",
      "1\n",
      " + \n",
      "w\n",
      "2\n",
      " \n",
      "x\n",
      "2\n",
      " + \n",
      " + \n",
      "w\n",
      "n\n",
      " \n",
      "x\n",
      "n\n",
      " \n",
      "= \n",
      "x\n",
      "T\n",
      " \n",
      "w\n",
      " ), then a pplies a \n",
      " s t e p f u n c t i o n\n",
      "  to tha t sum and\n",
      "outputs the result: \n",
      "h\n",
      "w\n",
      "(\n",
      "x\n",
      ") = step(\n",
      "z\n",
      "), where \n",
      "z\n",
      " = \n",
      "x\n",
      "T\n",
      " \n",
      "w\n",
      ".\n",
      " F i g u r e 10-4. \n",
      "•reshold\n",
      "  l og i c u n i t\n",
      " The most common step function used in P erceptrons is the \n",
      " H e a v i s i d e s t e p f u n c t i o n\n",
      "(see \n",
      " Equa tion 10-1\n",
      "). Sometimes the sign function is used instead.\n",
      " Eq u a t i o n 10-1. C o m m o n s t e p f u n c t i o ns u s e d i n P er c e p t r o ns\n",
      "heaviside\n",
      "z\n",
      "=\n",
      " 0 if\n",
      "z\n",
      " < 0\n",
      " 1 if\n",
      "z\n",
      " Ž 0\n",
      "sgn\n",
      "z\n",
      "=\n",
      " ” 1 if\n",
      "z\n",
      " < 0\n",
      " 0 if\n",
      "z\n",
      " = 0\n",
      " + 1 if\n",
      "z\n",
      " > 0\n",
      " A single TL U can be used for sim ple linear binar y classifica tion. I t com putes a linear\n",
      " combina tion of the in puts and if the result exceeds a threshold, it outputs the positive\n",
      " class or else outputs the nega tive class (just like a Logistic Regression classifier or a\n",
      " linear SVM). F or exam ple, you could use a single TL U to classif y iris flowers based on\n",
      " the petal length and width (also adding an extra bias fea ture \n",
      "x\n",
      "0\n",
      " \n",
      "= 1, just like we did in\n",
      " previous cha pters). T raining a TL U in this case means finding the righ t values for \n",
      "w\n",
      "0\n",
      ",\n",
      "w\n",
      "1\n",
      ", and \n",
      "w\n",
      "2\n",
      " (the training algorithm is discussed shortly).\n",
      " A P erceptron is sim ply com posed of a single la yer of TL U s,\n",
      "6\n",
      "   with each TL U connected\n",
      " to all the in puts. When all the neurons in a la yer are connected to ever y neuron in the\n",
      " previous la yer (i.e., its in put neurons), it is called a \n",
      " f u l l y c o n n e c t e d l a y er\n",
      " or a \n",
      " d ens e\n",
      " l a y er\n",
      " . T o represen t the fact tha t each in put is sen t to ever y TL U , it is common to dra w\n",
      "special passthrough neurons called \n",
      " i n p u t n eu r o ns\n",
      " : they just output wha tever in put\n",
      " they are fed. All the in put neurons form the \n",
      " i n p u t l a y er\n",
      " . M oreover , an extra bias feaƒ\n",
      " 282  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "ture is generally added (\n",
      "x\n",
      "0\n",
      "  = 1): it is typically represen ted using a special type of neuƒ\n",
      "ron called a \n",
      " b i as n eu r o n\n",
      " , which just outputs 1 all the time. A P erceptron with two\n",
      " in puts and three outputs is represen ted in \n",
      "Figure 10-5\n",
      " . This P erceptron can classif y\n",
      " instances sim ultaneously in to three differen t binar y classes, which makes it a m ultiƒ\n",
      " output classifier .\n",
      " F i g u r e 10-5. P er c e p t r o n d i a g r a m\n",
      " Thanks to the magic of linear algebra, it is possible to efficien tly com pute the outputs\n",
      " of a la yer of artificial neurons for several instances a t once, by using \n",
      " Equa tion 10-2\n",
      ":\n",
      " Eq u a t i o n 10-2. C o m p u t i n g t h e o u tp u ts o f a f u l l y c o n n e c t e d l a y er\n",
      "h\n",
      "W\n",
      ",\n",
      "b\n",
      "X\n",
      "=\n",
      "’\n",
      "XW\n",
      "+\n",
      "b\n",
      "⁄\n",
      " As alwa ys, \n",
      "X\n",
      "  represen ts the ma trix of in put fea tures. I t has one row per instance,\n",
      " one column per fea ture.\n",
      "⁄\n",
      " The weigh t ma trix \n",
      "W\n",
      "  con tains all the connection weigh ts except for the ones\n",
      " from the bias neuron. I t has one row per in put neuron and one column per artifiƒ\n",
      " cial neuron in the la yer .\n",
      "⁄\n",
      "The bias vector \n",
      "b\n",
      "  con tains all the connection weigh ts between the bias neuron\n",
      " and the artificial neurons. I t has one bias term per artificial neuron.\n",
      "⁄\n",
      "The function \n",
      "’\n",
      " is called the \n",
      " a c t i v a t i o n f u n c t i o n\n",
      ": when the artificial neurons are\n",
      " TL U s, it is a step function (but we will discuss other activa tion functions shortly).\n",
      " So how is a P erceptron trained? The P erceptron training algorithm proposed by\n",
      " Frank Rosenbla tt was largely inspired by \n",
      " H e b b ‹ s r u l e\n",
      ". In his book \n",
      "•e\n",
      "  O r ga n iz a t i o n o f\n",
      " B e h a v i o r\n",
      " , published in 1949, Donald H ebb suggested tha t when a biological neuron\n",
      "often triggers another neuron, the connection between these two neurons grows\n",
      " stronger . This idea was la tch y phrase:\n",
      " — Cells tha t fire together , wire together . – This rule la ter became known as H ebb ‡ s rule \n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  283\n",
      "\n",
      "7\n",
      " N ote tha t this solution is generally not unique: in general when the da ta are linearly separable, there is an\n",
      " infinity of h yperplanes tha t can separa te them.\n",
      "(or \n",
      " H e b b i a n l e a r n i n g\n",
      " ); tha t is, the connection weigh t between two neurons is\n",
      " increased whenever they ha ve the same output. P erceptrons are trained using a varƒ\n",
      " ian t of this rule tha t takes in to accoun t the error made by the network; it reinforces\n",
      " connections tha t help reduce the error . M ore specifically , the P erceptron is fed one\n",
      " training instance a t a time, and for each instance it makes its predictions. F or ever y\n",
      " output neuron tha t produced a wrong prediction, it reinforces the connection\n",
      " weigh ts from the in puts tha t would ha ve con tributed to the correct prediction. The\n",
      "rule is shown in \n",
      " Equa tion 10-3\n",
      ".\n",
      " Eq u a t i o n 10-3. P er c e p t r o n l e a r n i n g r u l e (w ei gh t u p d a t e)\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      " next step\n",
      "=\n",
      "w\n",
      "i\n",
      ",\n",
      "j\n",
      "+\n",
      "−\n",
      "y\n",
      "j\n",
      "”\n",
      "y\n",
      "j\n",
      "x\n",
      "i\n",
      "⁄\n",
      "w\n",
      "i\n",
      ", \n",
      "j\n",
      "  is the connection weigh t between the \n",
      "i\n",
      "th\n",
      "  in put neuron and the \n",
      "j\n",
      "th\n",
      " output neuƒ\n",
      "ron.\n",
      "⁄\n",
      "x\n",
      "i\n",
      " is the \n",
      "i\n",
      "th\n",
      "  in put value of the curren t training instance.\n",
      "⁄\n",
      "y\n",
      "j\n",
      " is the output of the \n",
      "j\n",
      "th\n",
      "  output neuron for the curren t training instance.\n",
      "⁄\n",
      "y\n",
      "j\n",
      " is the target output of the \n",
      "j\n",
      "th\n",
      "  output neuron for the curren t training instance.\n",
      "⁄\n",
      "−\n",
      "  is the learning ra te.\n",
      " The decision boundar y of each output neuron is linear , so P erceptrons are inca pable\n",
      " of learning com plex pa tterns (just like Logistic Regression classifiers). H owever , if the\n",
      " training instances are linearly separable, Rosenbla tt demonstra ted tha t this algorithm\n",
      " would con verge to a solution.\n",
      "7\n",
      " This is called the \n",
      " P er c e p t r o n c o n v er gen c e t h e o r em\n",
      ".\n",
      "Scikit-Learn provides a \n",
      "Perceptron\n",
      " \n",
      " class tha t im plemen ts a single TL U network. I t\n",
      " can be used pretty m uch as you would expect›for exam ple, on the iris da taset (in troƒ\n",
      "duced in \n",
      " Cha pter 4\n",
      "):\n",
      "import\n",
      " \n",
      "numpy\n",
      " \n",
      "as\n",
      " \n",
      "np\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "load_iris\n",
      "from\n",
      " \n",
      "sklearn.linear_model\n",
      " \n",
      "import\n",
      " \n",
      "Perceptron\n",
      "iris\n",
      " \n",
      "=\n",
      " \n",
      "load_iris\n",
      "()\n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "iris\n",
      ".\n",
      "data\n",
      "[:,\n",
      " \n",
      "(\n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      ")]\n",
      "  \n",
      "# petal length, petal width\n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "(\n",
      "iris\n",
      ".\n",
      "target\n",
      " \n",
      "==\n",
      " \n",
      "0\n",
      ")\n",
      ".\n",
      "astype\n",
      "(\n",
      "np\n",
      ".\n",
      "int\n",
      ")\n",
      "  \n",
      "# Iris Setosa?\n",
      "per_clf\n",
      " \n",
      "=\n",
      " \n",
      "Perceptron\n",
      "()\n",
      "per_clf\n",
      ".\n",
      "fit\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ")\n",
      " 284  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "per_clf\n",
      ".\n",
      "predict\n",
      "([[\n",
      "2\n",
      ",\n",
      " \n",
      "0.5\n",
      "]])\n",
      " Y ou ma y ha ve noticed the fact tha t the P erceptron learning algorithm strongly resemƒ\n",
      "bles \n",
      " Stochastic Gradien t Descen t. In fact, Scikit-Learn ‡ s \n",
      "Perceptron\n",
      "   class is equivalen t\n",
      "to using an \n",
      "SGDClassifier\n",
      " \n",
      " with the following h yperparameters: \n",
      "loss=\"perceptron\"\n",
      ",\n",
      "learning_rate=\"constant\"\n",
      ", \n",
      "eta0=1\n",
      " \n",
      " (the learning ra te), and \n",
      "penalty=None\n",
      "   (no reguƒ\n",
      " lariza tion).\n",
      " N ote  tha t con trar y to Logistic Regression classifiers, P erceptrons do not output a class\n",
      " probability ; ra ther , they just make predictions based on a hard threshold. This is one\n",
      " of the good reasons to prefer Logistic Regression over P erceptrons.\n",
      " In their 1969 monogra ph titled \n",
      " P er c e p t r o ns\n",
      " , M ar vin Minsky and Seymour P a pert\n",
      " highligh ted a n umber of serious weaknesses of P erceptrons, in particular the fact tha t\n",
      " they are inca pable of solving some trivial problems (e.g., the \n",
      " E x c l u s i v e O R\n",
      " \n",
      " (X OR)\n",
      " classifica tion problem; see the left side of \n",
      "Figure 10-6\n",
      " ). Of course this is true of an y\n",
      " other linear classifica tion model as well (such as Logistic Regression classifiers), but\n",
      " researchers had expected m uch more from P erceptrons, and their disa ppoin tmen t\n",
      " was grea t, and man y researchers dropped neural networks altogether in fa vor of\n",
      " higher -level problems such as logic, problem solving, and search.\n",
      " H owever , it turns out tha t some of the limita tions of P erceptrons can be elimina ted by\n",
      " stacking m ultiple P erceptrons. The resulting ANN is called a \n",
      " M u l t i-L a y er P er c e p t r o n\n",
      " (MLP). In particular , an MLP can solve the X OR problem, as you can verif y by comƒ\n",
      " puting the output of the MLP represen ted on the righ t of \n",
      "Figure 10-6\n",
      " : with in puts (0,\n",
      " 0) or (1, 1) the network outputs 0, and with in puts (0, 1) or (1, 0) it outputs 1. All\n",
      " connections ha ve a weigh t equal to 1, except the four connections where the weigh t is\n",
      " shown. T r y verif ying tha t this network indeed solves the X OR problem!\n",
      " F i g u r e 10-6. X O R \n",
      "classi†cation\n",
      "  p r o b l em a n d a n MLP t h a t s o l v e s i t\n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  285\n",
      "\n",
      "8\n",
      " In the 1990s, an ANN with more than two hidden la yers was considered deep . N owada ys, it is common to see\n",
      " ANN s with dozens of la yers, or even h undreds, so the definition of — deep – is quite fuzzy .\n",
      "9\n",
      " —Learning In ternal Represen ta tions by Error Propaga tion, – D . R umelhart, G. Hin ton, R. W illiams (1986).\n",
      "Multi-Layer Perceptron and Backpropagation\n",
      " An MLP is com posed of one (passthrough) \n",
      " i n p u t l a y er\n",
      " , one or more la yers of TL U s,\n",
      "called \n",
      " h i d d en l a y er s\n",
      " , and one final la yer of TL U s called the \n",
      " o u tp u t l a y er\n",
      " \n",
      "(see\n",
      "Figure 10-7\n",
      " ). The la yers close to the in put la yer are usually called the lower la yers,\n",
      " and the ones close to the outputs are usually called the upper la yers. E ver y la yer\n",
      " except the output la yer includes a bias neuron and is fully connected to the next la yer .\n",
      " F i g u r e 10-7. M u l t i-L a y er P er c e p t r o n\n",
      " The signal flows only in one direction (from the in puts to the outƒ\n",
      " puts), so this architecture is an exam ple \n",
      "of a \n",
      " f e e d f o r w a r d n eu r a l n e t‡\n",
      " w o r k\n",
      " (FNN).\n",
      " When an ANN con tains a deep stack of hidden la yers\n",
      "8\n",
      ", it is called a \n",
      " d e e p n eu r a l n e t‡\n",
      " w o r k\n",
      "  (DNN). The field of Deep Learning studies DNN s, and more generally models\n",
      " con taining deep stacks of com puta tions. H owever , man y people talk about Deep\n",
      " Learning whenever neural networks are in volved (even shallow ones).\n",
      " F or man y years researchers struggled to find a wa y to train MLP s, without success.\n",
      " But in 1986, Da vid R umelhart, Geoffrey Hin ton and Ronald W illiams published a\n",
      " groundbreaking pa per\n",
      "9\n",
      "  in troducing the \n",
      " b a c k p r o p a ga t i o n\n",
      " training algorithm, which is\n",
      " still used toda y . In short, it is sim ply Gradien t Descen t (in troduced in \n",
      " Cha pter 4\n",
      ")\n",
      " 286  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "10\n",
      " This technique was actually independen tly in ven ted several times by various researchers in differen t fields,\n",
      " starting with P . W erbos in 1974.\n",
      " using an efficien t technique for com puting the gradien ts a utoma tically\n",
      "10\n",
      ": in just two\n",
      " passes through the network (one for ward, one backward), the backpropaga tion algoƒ\n",
      " rithm is able to com pute the gradien t of the network ‡ s error with regards to ever y sinƒ\n",
      " gle model parameter . In other words, it can find out how each connection weigh t and\n",
      " each bias term should be tweaked in order to reduce the error . Once it has these graƒ\n",
      " dien ts, it just performs a regular Gradien t Descen t step , and the whole process is\n",
      " repea ted un til the network con verges to the solution.\n",
      " A utoma tically com puting gradien ts is called \n",
      " a u t o m a t i c \n",
      "di›erentia‡\n",
      "tion\n",
      ", or \n",
      "autodi›\n",
      " . There are various a utodiff techniques, with differƒ\n",
      " en t pros and cons. The one used by backpropaga tion is \n",
      "called\n",
      " r e v er s e-m o d e \n",
      "autodi›\n",
      " . I t is fast and precise, and is well suited when\n",
      " the function to differen tia te has man y variables (e.g., connection\n",
      " weigh ts) and few outputs (e.g., one loss). If you wan t to learn more\n",
      " about a utodiff, check out \n",
      "???\n",
      ".\n",
      " Let ‡ s run through this algorithm in a bit more detail:\n",
      "⁄\n",
      " I t handles one mini-ba tch a t a time (for exam ple con taining 32 instances each),\n",
      " and it goes through the full training set m ultiple times. Each pass is called an\n",
      " e p o c h\n",
      " , as we sa w in \n",
      " Cha pter 4\n",
      ".\n",
      "⁄\n",
      " Each mini-ba tch is passed to the network ‡ s in put la yer , which just sends it to the\n",
      " first hidden la yer . The algorithm then com putes the output of all the neurons in\n",
      " this la yer (for ever y instance in the mini-ba tch). The result is passed on to the\n",
      " next la yer , its output is com puted and passed to the next la yer , and so on un til we\n",
      " get the output of the last la yer , the output la yer . This is the \n",
      " f o r w a r d p as s\n",
      ": it is\n",
      " exactly like making predictions, except all in termedia te results are preser ved\n",
      "since they are needed for the backward pass.\n",
      "⁄\n",
      " N ext, the algorithm measures the network ‡ s output error (i.e., it uses a loss funcƒ\n",
      " tion tha t com pares the desired output and the actual output of the network, and\n",
      "returns some measure of the error).\n",
      "⁄\n",
      " Then it com putes how m uch each output connection con tributed to the error .\n",
      " This is done analytically by sim ply a pplying\n",
      "   the \n",
      " c h a i n r u l e\n",
      "   (perha ps the most funƒ\n",
      " damen tal rule in calculus), which makes this step fast and precise.\n",
      "⁄\n",
      " The algorithm then measures how m uch of these error con tributions came from\n",
      " each connection in the la yer below , again using the chain rule›and so on un til\n",
      " the algorithm reaches the in put la yer . As we explained earlier , this reverse pass\n",
      " efficien tly measures the error gradien t across all the connection weigh ts in the\n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  287\n",
      "\n",
      " network by propaga ting the error gradien t backward through the network (hence\n",
      "the name of the algorithm).\n",
      "⁄\n",
      " Finally , the algorithm performs a Gradien t Descen t step to tweak all the connecƒ\n",
      " tion weigh ts in the network, using the error gradien ts it just com puted.\n",
      " This algorithm is so im portan t, it ‡ s worth summarizing it again: for each training\n",
      " instance the backpropaga tion algorithm first makes a prediction (for ward pass),\n",
      " measures the error , then goes through each la yer in reverse to measure the error conƒ\n",
      " tribution from each connection (reverse pass), and finally sligh tly tweaks the connecƒ\n",
      " tion weigh ts to reduce the error (Gradien t Descen t step).\n",
      " I t is im portan t to initialize all the hidden la yers ‡ connection weigh ts\n",
      " randomly , or else training will fail. F or exam ple, if you initialize all\n",
      " weigh ts and biases to zero , then all neurons in a given la yer will be\n",
      " perfectly iden tical, and th us backpropaga tion will affect them in\n",
      " exactly the same wa y , so they will remain iden tical. In other words,\n",
      " despite ha ving h undreds of neurons per la yer , your model will act\n",
      " as if it had only one neuron per la yer : it won ‡ t be too smart. If\n",
      " instead you randomly initialize the weigh ts, you \n",
      " b r e a k t h e s y m m e‡\n",
      " t r y\n",
      "  and allow backpropaga tion to train a diverse team of neurons.\n",
      " In order for this algorithm to work properly , the a uthors made a key change to the\n",
      " MLP‡ s architecture: they replaced the step function with the logistic function, \n",
      "„\n",
      "(\n",
      "z\n",
      ") =\n",
      "1 / (1 + exp(−\n",
      "z\n",
      " )). This was essen tial beca use the step function con tains only fla t segƒ\n",
      " men ts, so there is no gradien t to work with (Gradien t Descen t cannot move on a fla t\n",
      " surface), while the logistic function has a well-defined nonzero deriva tive ever yƒ\n",
      " where, allowing Gradien t Descen t to make some progress a t ever y step . In fact, the\n",
      " backpropaga tion algorithm works well with man y other \n",
      " a c t i v a t i o n f u n c t i o ns\n",
      ", not just\n",
      " the logistic function. T wo other popular activa tion functions are:\n",
      "•e\n",
      "  h y p er b o l i c t a n gen t f u n c t i o n t a n h(z) = 2„(2z) ” 1\n",
      " J ust like the logistic function it is S-sha ped, con tin uous, and differen tiable, but its\n",
      "output value ranges from −1 to 1 (instead of 0 to 1 in the case of the logistic funcƒ\n",
      " tion), which tends to make each la yer‡ s output more or less cen tered around 0 a t\n",
      " the beginning of training. This often helps speed up con vergence.\n",
      "•e\n",
      " \n",
      "Recti†ed\n",
      "  L i n e a r U n i t f u n c t i o n: R eL U(z) = m ax(0, z)\n",
      " I t is con tin uous but unfortuna tely not differen tiable a t \n",
      "z\n",
      " = 0 (the slope changes\n",
      " abruptly , which can make Gradien t Descen t bounce around), and its deriva tive is\n",
      "0 for \n",
      "z\n",
      " \n",
      " < 0. H owever , in practice it works ver y well and has the advan tage of being\n",
      " 288  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "11\n",
      " Biological neurons seem to im plemen t a roughly sigmoid (S-sha ped) activa tion function, so researchers stuck\n",
      " to sigmoid functions for a ver y long time. But it turns out tha t ReL U generally works better in ANN s. This is\n",
      " one of the cases where the biological analog y was misleading.\n",
      " fast to com pute\n",
      "11\n",
      " . M ost im portan tly , the fact tha t it does not ha ve a maxim um\n",
      " output value also helps reduce some issues during Gradien t Descen t (we will\n",
      "come back to this in \n",
      " Cha pter 11\n",
      ").\n",
      " These popular activa tion functions and their deriva tives are represen ted in\n",
      "Figure 10-8\n",
      " . But wait! Wh y do we need activa tion functions in the first place? W ell, if\n",
      " you chain several linear transforma tions, all you get is a linear transforma tion. F or\n",
      " exam ple, sa y f(\n",
      "x\n",
      ") = 2 \n",
      "x\n",
      "   + 3 and g(\n",
      "x\n",
      ") = 5 \n",
      "x\n",
      " \n",
      "- 1, then chaining these two linear functions\n",
      "gives you another linear function: f(g(\n",
      "x\n",
      ")) = 2(5 \n",
      "x\n",
      " - 1) + 3 = 10 \n",
      "x\n",
      " \n",
      " + 1. So if you don ‡ t\n",
      " ha ve some non-linearity between la yers, then even a deep stack of la yers is equivalen t\n",
      " to a single la yer : you cannot solve ver y com plex problems with tha t.\n",
      " F i g u r e 10-8. Ac t i v a t i o n f u n c t i o ns a n d t h ei r d er i v a t i v e s\n",
      " Oka y! So now you know where neural nets came from, wha t their architecture is and\n",
      " how to com pute their outputs, and you also learned about the backpropaga tion algoƒ\n",
      " rithm. But wha t exactly can you do with them?\n",
      "Regression MLPs\n",
      " First, MLP s can be used for regression tasks. If you wan t to predict a single value (e.g.,\n",
      " the price of a house given man y of its fea tures), then you just need a single output\n",
      " neuron: its output is the predicted value. F or m ultivaria te regression (i.e., to predict\n",
      " m ultiple values a t once), you need one output neuron per output dimension. F or\n",
      " exam ple, to loca te the cen ter of an object on an image, you need to predict 2D coordiƒ\n",
      " na tes, so you need two output neurons. If you also wan t to place a bounding box\n",
      " around the object, then you need two more n umbers: the width and the heigh t of the\n",
      "object. So you end up with 4 output neurons.\n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  289\n",
      "\n",
      " In general, when building an MLP for regression, you do not wan t to use an y activaƒ\n",
      " tion function for the output neurons, so they are free to output an y range of values.\n",
      " H owever , if you wan t to guaran tee tha t the output will alwa ys be positive, then you\n",
      " can use the ReL U activa tion function, or the \n",
      "so“plus\n",
      " \n",
      " activa tion function in the output\n",
      " la yer . Finally , if you wan t to guaran tee tha t the predictions will fall within a given\n",
      " range of values, then you can use the logistic function or the h yperbolic tangen t, and\n",
      " scale the labels to the a ppropria te range: 0 to 1 for the logistic function, or −1 to 1 for\n",
      " the h yperbolic tangen t.\n",
      " The loss function to use during training is typically the mean squared error , but if you\n",
      " ha ve a lot of outliers in the training set, you ma y prefer to use the mean absolute\n",
      " error instead. Alterna tively , you can use the H uber loss, which is a combina tion of\n",
      "both.\n",
      " The H uber loss is quadra tic when the error is smaller than a thresƒ\n",
      "hold É (typically 1), but linear when the error is larger than \n",
      "É\n",
      ". This\n",
      " makes it less sensitive to outliers than the mean squared error , and\n",
      " it is often more precise and con verges faster than the mean absoƒ\n",
      " lute error .\n",
      " T able 10-1\n",
      "  summarizes the typical architecture of a regression MLP .\n",
      " T a b l e 10-1. T y p i c a l R e g r e s s i o n MLP Ar c h i t e c t u r e\n",
      "Hyperparameter\n",
      " Typical   Value\n",
      " #   input   neurons\n",
      " One   per   input   feature   (e.g.,   28   x   28   =   784   for   MNIST)\n",
      " #   hidden   layers\n",
      " Depends   on   the   problem.   Typically   1   to   5.\n",
      " #   neurons   per   hidden   layer\n",
      " Depends   on   the   problem.   Typically   10   to   100.\n",
      " #   output   neurons\n",
      " 1   per   prediction   dimension\n",
      " Hidden   activation\n",
      " ReLU   (or   SELU,   see  \n",
      " Chapter   11\n",
      ")\n",
      " Output   activation\n",
      " None   or   ReLU/Softplus   (if   positive   outputs)   or   Logistic/Tanh   (if   bounded   outputs)\n",
      " Loss   function\n",
      " MSE   or   MAE/Huber   (if   outliers)\n",
      "Classi•cation\n",
      " MLPs\n",
      " MLP s can also be used for classifica tion tasks. F or a binar y classifica tion problem,\n",
      " you just need a single output neuron using the logistic activa tion function: the output\n",
      " will be a n umber between 0 and 1, which you can in terpret as the estima ted probabilƒ\n",
      " ity of the positive class. Obviously , the estima ted probability of the nega tive class is\n",
      " equal to one min us tha t n umber .\n",
      " MLP s can also easily handle m ultilabel binar y classifica tion tasks (see \n",
      " Cha pter 3\n",
      " ). F or\n",
      " exam ple, you could ha ve an email classifica tion system tha t predicts whether each\n",
      " incoming email is ham or spam, and sim ultaneously predicts whether it is an urgen t\n",
      " 290  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " or non-urgen t email. In this case, you would need two output neurons, both using\n",
      " the logistic activa tion function: the first would output the probability tha t the email is\n",
      " spam and the second would output the probability tha t it is urgen t. M ore generally ,\n",
      " you would dedica te one output neuron for each positive class. N ote tha t the output\n",
      " probabilities do not necessarily add up to one. This lets the model output an y combiƒ\n",
      " na tion of labels: you can ha ve non-urgen t ham, urgen t ham, non-urgen t spam, and\n",
      " perha ps even urgen t spam (although tha t would probably be an error).\n",
      "If each instance can belong only to a single class, out of 3 or more possible classes\n",
      " (e.g., classes 0 through 9 for digit image classifica tion), then you need to ha ve one\n",
      "output neuron per class, and you should use the \n",
      "so“max\n",
      "  activa tion function for the\n",
      " whole output la yer (see \n",
      "Figure 10-9\n",
      " ). The softmax function (in troduced in \n",
      " Cha pter 4\n",
      ")\n",
      " will ensure tha t all the estima ted probabilities are between 0 and 1 and tha t they add\n",
      " up to one (which is required if the classes are exclusive). This is called m ulticlass clasƒ\n",
      " sifica tion.\n",
      " F i g u r e 10-9. A m o d er n MLP (i n c l u d i n g R eL U a n d \n",
      "so“max)\n",
      "  f o r \n",
      "classi†cation\n",
      "Regarding the loss function, since we are predicting probability distributions, the\n",
      " cross-en tropy (also called the log loss, see \n",
      " Cha pter 4\n",
      ") is generally a good choice.\n",
      " T able 10-2\n",
      "  summarizes the typical architecture of a classifica tion MLP .\n",
      " T a b l e 10-2. T y p i c a l \n",
      "Classi†cation\n",
      "  MLP Ar c h i t e c t u r e\n",
      "Hyperparameter\n",
      " Binary  \n",
      "classi•cation\n",
      " Multilabel   binary  \n",
      "classi•cation\n",
      " Multiclass  \n",
      "classi•cation\n",
      " Input   and   hidden   layers\n",
      " Same   as   regression\n",
      " Same   as   regression\n",
      " Same   as   regression\n",
      " #   output   neurons\n",
      "1\n",
      " 1   per   label\n",
      " 1   per   class\n",
      " Output   layer   activation\n",
      "Logistic\n",
      "Logistic\n",
      "Softmax\n",
      "From Biological to \n",
      "Arti•cial\n",
      "  Neurons  |  291\n",
      "\n",
      "12\n",
      " Project ONEIROS (Open-ended N euro-Electronic In telligen t Robot Opera ting System).\n",
      "Hyperparameter\n",
      " Binary  \n",
      "classi•cation\n",
      " Multilabel   binary  \n",
      "classi•cation\n",
      " Multiclass  \n",
      "classi•cation\n",
      " Loss   function\n",
      "Cross-Entropy\n",
      "Cross-Entropy\n",
      "Cross-Entropy\n",
      " B efore we go on, I recommend you go through exercise 1, a t the\n",
      " end of this cha pter . Y ou will pla y with various neural network\n",
      "architectures and visualize their outputs using the \n",
      " T ens o rF l o w P l a y‡\n",
      " g r o u n d\n",
      " . This will be ver y useful to better understand MLP s, for\n",
      " exam ple the effects of all the h yperparameters (n umber of la yers\n",
      " and neurons, activa tion functions, and more).\n",
      " N ow you ha ve all the concepts you need to start im plemen ting MLP s with K eras!\n",
      "Implementing MLPs with Keras\n",
      " K eras is a high-level Deep Learning API tha t allows you to easily build, train, evalua te\n",
      " and execute all sorts of neural networks. I ts documen ta tion (or specifica tion) is a vailƒ\n",
      " able a t \n",
      " h ttp s://k er as.i o\n",
      " . The reference im plemen ta tion is sim ply called K eras as well, so\n",
      " to a void an y confusion we will call it keras-team (since it is a vailable a t \n",
      " h ttp s://\n",
      " g i t h u b .c o m/k er as-t e a m/k er as\n",
      " ). I t was developed by Fran‹ois Chollet as part of a\n",
      "research project\n",
      "12\n",
      "  and released as an open source project in M arch 2015. I t quickly\n",
      " gained popularity owing to its ease-of-use, flexibility and bea utiful design. T o perƒ\n",
      " form the hea vy com puta tions required by neural networks, keras-team relies on a\n",
      " com puta tion backend. A t the presen t, you can choose from three popular open\n",
      " source deep learning libraries: T ensorFlow , Microsoft Cognitive T oolkit (CNTK) or\n",
      " Theano .\n",
      " M oreover , since la te 2016, other im plemen ta tions ha ve been released. Y ou can now\n",
      " run K eras on A pache MXN et, A pple ‡ s Core ML, J a vascript or T ypescript (to run K eras\n",
      "code in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\n",
      " just N vidia). M oreover , T ensorFlow itself now comes bundled with its own K eras\n",
      " im plemen ta tion called tf.keras. I t only supports T ensorFlow as the backend, but it has\n",
      " the advan tage of offering some ver y useful extra fea tures (see \n",
      "Figure 10-10\n",
      "): for\n",
      " exam ple, it supports T ensorFlow‡ s Da ta API which makes it quite easy to load and\n",
      " preprocess da ta efficien tly . F or this reason, we will use tf.keras in this book. H owever ,\n",
      " in this cha pter we will not use an y of the T ensorFlow-specific fea tures, so the code\n",
      " should run fine on other K eras im plemen ta tions as well (a t least in Python), with only\n",
      " minor modifica tions, such as changing the im ports.\n",
      " 292  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " F i g u r e 10-10. T w o K er as i m p l em en t a t i o ns: k er as-t e a m \n",
      "(le“)\n",
      "  a n d t f .k er as (r i gh t)\n",
      " As tf.keras is bundled with T ensorFlow , let ‡ s install T ensorFlow!\n",
      "Installing TensorFlow 2\n",
      " Assuming you installed J upyter and Scikit-Learn by following the installa tion instrucƒ\n",
      "tions in \n",
      " Cha pter 2\n",
      " , you can sim ply use pip to install T ensorFlow . If you crea ted an\n",
      " isola ted en vironmen t using virtualen v , you first need to activa te it:\n",
      "$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\n",
      "$ source env/bin/activate  # on Linux or MacOSX\n",
      "$ .\\env\\Scripts\\activate   # on Windows\n",
      " N ext, install T ensorFlow 2 (if you are not using a virtualen v , you will need adminisƒ\n",
      " tra tor righ ts, or to add the \n",
      "--user\n",
      " option):\n",
      "$ python3 -m pip install --upgrade tensorflow\n",
      " F or GPU support, you need to install \n",
      "tensorflow-gpu\n",
      " instead of\n",
      "tensorflow\n",
      ", and there are other libraries to install. See \n",
      "https://\n",
      "tensor…ow.org/install/gpu\n",
      " for more details.\n",
      " T o test your installa tion, open a Python shell or a J upyter notebook, then im port T enƒ\n",
      " sorFlow and tf.keras, and prin t their versions:\n",
      ">>> \n",
      "import\n",
      " \n",
      "tensorflow\n",
      " \n",
      "as\n",
      " \n",
      "tf\n",
      ">>> \n",
      "from\n",
      " \n",
      "tensorflow\n",
      " \n",
      "import\n",
      " \n",
      "keras\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "__version__\n",
      "•2.0.0•\n",
      ">>> \n",
      "keras\n",
      ".\n",
      "__version__\n",
      "•2.2.4-tf•\n",
      " Implementing MLPs with Keras  |  293\n",
      "\n",
      " The second version is the version of the K eras API im plemen ted by tf.keras. N ote tha t\n",
      "it ends with \n",
      "-tf\n",
      " , highligh ting the fact tha t tf.keras im plemen ts the K eras API, plus\n",
      " some extra T ensorFlow-specific fea tures.\n",
      " N ow let ‡ s use tf.keras! Let ‡ s start by building a sim ple image classifier .\n",
      "Building an Image \n",
      "Classi•er\n",
      " Using the Sequential API\n",
      " First, we need to load a da taset. W e will tackle \n",
      " F as h i o n MNIS T\n",
      ", which is a drop-in\n",
      " replacemen t of MNIST (in troduced in \n",
      " Cha pter 3\n",
      " ). I t has the exact same forma t as\n",
      " MNIST (70,000 gra yscale images of 28„28 pixels each, with 10 classes), but the\n",
      " images represen t fashion items ra ther than handwritten digits, so each class is more\n",
      " diverse and the problem turns out to be significan tly more challenging than MNIST .\n",
      " F or exam ple, a sim ple linear model reaches about 92% accuracy on MNIST , but only\n",
      " about 83% on F ashion MNIST .\n",
      "Using Keras to Load the Dataset\n",
      " K eras provides some utility functions to fetch and load common da tasets, including\n",
      " MNIST , F ashion MNIST , the original California housing da taset, and more. Let ‡ s load\n",
      " F ashion MNIST :\n",
      "fashion_mnist\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "datasets\n",
      ".\n",
      "fashion_mnist\n",
      "(\n",
      "X_train_full\n",
      ",\n",
      " \n",
      "y_train_full\n",
      "),\n",
      " \n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      " \n",
      "=\n",
      " \n",
      "fashion_mnist\n",
      ".\n",
      "load_data\n",
      "()\n",
      " When loading MNIST or F ashion MNIST using K eras ra ther than Scikit-Learn, one\n",
      " im portan t difference is tha t ever y image is represen ted as a 28„28 arra y ra ther than a\n",
      " 1D arra y of size 784. M oreover , the pixel in tensities are represen ted as in tegers (from\n",
      " 0 to 255) ra ther than floa ts (from 0.0 to 255.0). H ere is the sha pe and da ta type of the\n",
      "training set:\n",
      ">>> \n",
      "X_train_full\n",
      ".\n",
      "shape\n",
      "(60000, 28, 28)\n",
      ">>> \n",
      "X_train_full\n",
      ".\n",
      "dtype\n",
      "dtype(•uint8•)\n",
      " N ote tha t the da taset is already split in to a training set and a test set, but there is no\n",
      " valida tion set, so let ‡ s crea te one. M oreover , since we are going to train the neural netƒ\n",
      " work using Gradien t Descen t, we m ust scale the in put fea tures. F or sim plicity , we just\n",
      " scale the pixel in tensities down to the 0-1 range by dividing them by 255.0 (this also\n",
      " con verts them to floa ts):\n",
      "X_valid\n",
      ",\n",
      " \n",
      "X_train\n",
      " \n",
      "=\n",
      " \n",
      "X_train_full\n",
      "[:\n",
      "5000\n",
      "]\n",
      " \n",
      "/\n",
      " \n",
      "255.0\n",
      ",\n",
      " \n",
      "X_train_full\n",
      "[\n",
      "5000\n",
      ":]\n",
      " \n",
      "/\n",
      " \n",
      "255.0\n",
      "y_valid\n",
      ",\n",
      " \n",
      "y_train\n",
      " \n",
      "=\n",
      " \n",
      "y_train_full\n",
      "[:\n",
      "5000\n",
      "],\n",
      " \n",
      "y_train_full\n",
      "[\n",
      "5000\n",
      ":]\n",
      " W ith MNIST , when the label is equal to 5, it means tha t the image represen ts the\n",
      " handwritten digit 5. Easy . H owever , for F ashion MNIST , we need the list of class\n",
      " names to know wha t we are dealing with:\n",
      " 294  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "class_names\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "\"T-shirt/top\"\n",
      ",\n",
      " \n",
      "\"Trouser\"\n",
      ",\n",
      " \n",
      "\"Pullover\"\n",
      ",\n",
      " \n",
      "\"Dress\"\n",
      ",\n",
      " \n",
      "\"Coat\"\n",
      ",\n",
      "               \n",
      "\"Sandal\"\n",
      ",\n",
      " \n",
      "\"Shirt\"\n",
      ",\n",
      " \n",
      "\"Sneaker\"\n",
      ",\n",
      " \n",
      "\"Bag\"\n",
      ",\n",
      " \n",
      "\"Ankle boot\"\n",
      "]\n",
      " F or exam ple, the first image in the training set represen ts a coa t:\n",
      ">>> \n",
      "class_names\n",
      "[\n",
      "y_train\n",
      "[\n",
      "0\n",
      "]]\n",
      "•Coat•\n",
      "Figure 10-11\n",
      "  shows a few sam ples from the F ashion MNIST da taset:\n",
      " F i g u r e 10-11. S a m p l e s f r o m F as h i o n MNIS T\n",
      "Creating the Model Using the Sequential API\n",
      " N ow let ‡ s build the neural network! H ere is a classifica tion MLP with two hidden la yƒ\n",
      "ers:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "()\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(\n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      "]))\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "300\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      "))\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      "))\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      "))\n",
      " Let ‡ s go through this code line by line:\n",
      "⁄\n",
      " The first line crea tes a \n",
      "Sequential\n",
      "  model. This is the sim plest kind of K eras\n",
      " model, for neural networks tha t are just com posed of a single stack of la yers, conƒ\n",
      " nected sequen tially . This is called the sequen tial API.\n",
      "⁄\n",
      " N ext, we build the first la yer and add it to the model. I t is a \n",
      "Flatten\n",
      "  la yer whose\n",
      " role is sim ply to con vert each in put image in to a 1D arra y : if it receives in put da ta\n",
      "X\n",
      " , it com putes \n",
      "X.reshape(-1, 1)\n",
      " . This la yer does not ha ve an y parameters, it is\n",
      " just there to do some sim ple preprocessing. Since it is the first la yer in the model,\n",
      " you should specif y the \n",
      "input_shape\n",
      " : this does not include the ba tch size, only the\n",
      " sha pe of the instances. Alterna tively , you could add a \n",
      "keras.layers.InputLayer\n",
      " as the first la yer , setting \n",
      "shape=[28,28]\n",
      ".\n",
      "⁄\n",
      " N ext we add a \n",
      "Dense\n",
      "  hidden la yer with 300 neurons. I t will use the ReL U activaƒ\n",
      "tion function. Each \n",
      "Dense\n",
      "   la yer manages its own weigh t ma trix, con taining all the\n",
      " connection weigh ts between the neurons and their in puts. I t also manages a vecƒ\n",
      " Implementing MLPs with Keras  |  295\n",
      "\n",
      " tor of bias terms (one per neuron). When it receives some in put da ta, it com putes\n",
      " Equa tion 10-2\n",
      ".\n",
      "⁄\n",
      " N ext we add a second \n",
      "Dense\n",
      " \n",
      " hidden la yer with 100 neurons, also using the ReL U\n",
      " activa tion function.\n",
      "⁄\n",
      " Finally , we add a \n",
      "Dense\n",
      "  output la yer with 10 neurons (one per class), using the\n",
      " softmax activa tion function (beca use the classes are exclusive).\n",
      " Specif ying \n",
      "activation=\"relu\"\n",
      "  is equivalen t to \n",
      "activa\n",
      "tion=keras.activations.relu\n",
      " . Other activa tion functions are\n",
      " a vailable in the \n",
      "keras.activations\n",
      "  package, we will use man y of\n",
      "them in this book. See \n",
      " h ttp s://k er as.i o/a c t i v a t i o ns/\n",
      " for the full list.\n",
      " Instead of adding the la yers one by one as we just did, you can pass a list of la yers\n",
      " when crea ting the \n",
      "Sequential\n",
      " model:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(\n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      "]),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "300\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ")\n",
      "])\n",
      "Using Code Examples From keras.io\n",
      " Code exam ples documen ted on keras.io will work fine with tf.keras, but you need to\n",
      " change the im ports. F or exam ple, consider this keras.io code:\n",
      "from\n",
      " \n",
      "keras.layers\n",
      " \n",
      "import\n",
      " \n",
      "Dense\n",
      "output_layer\n",
      " \n",
      "=\n",
      " \n",
      "Dense\n",
      "(\n",
      "10\n",
      ")\n",
      " Y ou m ust change the im ports like this:\n",
      "from\n",
      " \n",
      "tensorflow.keras.layers\n",
      " \n",
      "import\n",
      " \n",
      "Dense\n",
      "output_layer\n",
      " \n",
      "=\n",
      " \n",
      "Dense\n",
      "(\n",
      "10\n",
      ")\n",
      " Or sim ply use full pa ths, if you prefer :\n",
      "from\n",
      " \n",
      "tensorflow\n",
      " \n",
      "import\n",
      " \n",
      "keras\n",
      "output_layer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ")\n",
      " This is more verbose, but I use this a pproach in this book so you can easily see which\n",
      " packages to use, and to a void confusion between standard classes and custom classes.\n",
      " In production code, I use the previous a pproach, as do most people.\n",
      " 296  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "13\n",
      " Y ou can also genera te an image of your model using \n",
      "keras.utils.plot_model()\n",
      ".\n",
      " The model ‡ s \n",
      "summary()\n",
      " \n",
      " method displa ys all the model ‡ s la yers\n",
      "13\n",
      " , including each la yer‡ s\n",
      " name (which is a utoma tically genera ted unless you set it when crea ting the la yer), its\n",
      " output sha pe (\n",
      "None\n",
      " \n",
      " means the ba tch size can be an ything), and its n umber of parameƒ\n",
      " ters. The summar y ends with the total n umber of parameters, including trainable and\n",
      " non-trainable parameters. H ere we only ha ve trainable parameters (we will see examƒ\n",
      "ples of non-trainable parameters in \n",
      " Cha pter 11\n",
      "):\n",
      ">>> \n",
      "model\n",
      ".\n",
      "summary\n",
      "()\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0\n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500\n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100\n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010\n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      " N ote tha t \n",
      "Dense\n",
      "  la yers often ha ve a \n",
      " l o t\n",
      "  of parameters. F or exam ple, the first hidden\n",
      " la yer has 784 „ 300 connection weigh ts, plus 300 bias terms, which adds up to\n",
      "235,500 parameters! This gives the model quite a lot of flexibility to fit the training\n",
      " da ta, but it also means tha t the model runs the risk of overfitting, especially when you\n",
      " do not ha ve a lot of training da ta. W e will come back to this la ter .\n",
      " Y ou can easily get a model ‡ s list of la yers, to fetch a la yer by its index, or you can fetch\n",
      "it by name:\n",
      ">>> \n",
      "model\n",
      ".\n",
      "layers\n",
      "[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x1324149b0>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\n",
      ">>> \n",
      "model\n",
      ".\n",
      "layers\n",
      "[\n",
      "1\n",
      "]\n",
      ".\n",
      "name\n",
      "•dense_3•\n",
      ">>> \n",
      "model\n",
      ".\n",
      "get_layer\n",
      "(\n",
      "•dense_3•\n",
      ")\n",
      ".\n",
      "name\n",
      "•dense_3•\n",
      " All the parameters of a la yer can be accessed using its \n",
      "get_weights()\n",
      " \n",
      "and\n",
      "set_weights()\n",
      "  method. F or a \n",
      "Dense\n",
      "  la yer , this includes both the connection weigh ts\n",
      "and the bias terms:\n",
      " Implementing MLPs with Keras  |  297\n",
      "\n",
      ">>> \n",
      "weights\n",
      ",\n",
      " \n",
      "biases\n",
      " \n",
      "=\n",
      " \n",
      "hidden1\n",
      ".\n",
      "get_weights\n",
      "()\n",
      ">>> \n",
      "weights\n",
      "array([[ 0.03854964, -0.04054524,  0.00599282, ...,  0.02566582,\n",
      "         0.01032123,  0.06914985],\n",
      "       ...,\n",
      "       [ 0.02632413, -0.05105981, -0.00332005, ...,  0.04175945,\n",
      "         0.0443138 , -0.05558084]], dtype=float32)\n",
      ">>> \n",
      "weights\n",
      ".\n",
      "shape\n",
      "(784, 300)\n",
      ">>> \n",
      "biases\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\n",
      ">>> \n",
      "biases\n",
      ".\n",
      "shape\n",
      "(300,)\n",
      " N otice tha t the \n",
      "Dense\n",
      "  la yer initialized the connection weigh ts randomly (which is\n",
      " needed to break symmetr y , as we discussed earlier), and the biases were just initialƒ\n",
      " ized to zeros, which is fine. If you ever wan t to use a differen t initializa tion method,\n",
      "you can set \n",
      "kernel_initializer\n",
      " \n",
      "(\n",
      " k er n e l\n",
      "  is another name for the ma trix of connecƒ\n",
      " tion weigh ts) or \n",
      "bias_initializer\n",
      "  when crea ting the la yer . W e will discuss initializƒ\n",
      "ers further in \n",
      " Cha pter 11\n",
      " , but if you wan t the full list, see \n",
      " h ttp s://k er as.i o/i n i t i a l iz er s/\n",
      ".\n",
      " The sha pe of the weigh t ma trix depends on the n umber of in puts.\n",
      " This is wh y it is recommended to specif y the \n",
      "input_shape\n",
      " \n",
      "when\n",
      " crea ting the first la yer in a \n",
      "Sequential\n",
      "  model. H owever , if you do\n",
      " not specif y the in put sha pe, it ‡ s oka y : K eras will sim ply wait un til it\n",
      " knows the in put sha pe before it actually builds the model. This will\n",
      " ha ppen either when you feed it actual da ta (e.g., during training),\n",
      "or when you call its \n",
      "build()\n",
      "  method. U n til the model is really\n",
      " built, the la yers will not ha ve an y weigh ts, and you will not be able\n",
      " to do certain things (such as prin t the model summar y or sa ve the\n",
      " model), so if you know the in put sha pe when crea ting the model, it\n",
      " is best to specif y it.\n",
      "Compiling the Model\n",
      " After a model is crea ted, you m ust call its \n",
      "compile()\n",
      " \n",
      " method to specif y the loss funcƒ\n",
      " tion and the optimizer to use. Optionally , you can also specif y a list of extra metrics to\n",
      " com pute during training and evalua tion:\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"sparse_categorical_crossentropy\"\n",
      ",\n",
      "              \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ",\n",
      "              \n",
      "metrics\n",
      "=\n",
      "[\n",
      "\"accuracy\"\n",
      "])\n",
      " 298  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " U sing \n",
      "loss=\"sparse_categorical_crossentropy\"\n",
      " \n",
      " is equivalen t to\n",
      "loss=keras.losses.sparse_categorical_crossentropy\n",
      ". Simiƒ\n",
      " larly , \n",
      "optimizer=\"sgd\"\n",
      "  is equivalen t to \n",
      "optimizer=keras.optimiz\n",
      "ers.SGD()\n",
      " and \n",
      "metrics=[\"accuracy\"]\n",
      "  is equivalen t to\n",
      "metrics=[keras.metrics.sparse_categorical_accuracy]\n",
      "   (when\n",
      " using this loss). W e will use man y other losses, optimizers and metƒ\n",
      "rics in this book, but for the full lists see \n",
      " h ttp s://k er as.i o/l o s s e s/\n",
      ",\n",
      " h ttp s://k er as.i o/o p t i m iz er s/\n",
      " and \n",
      " h ttp s://k er as.i o/m e t r i cs/\n",
      ".\n",
      " This requires some explana tion. First, we use the \n",
      "\"sparse_categorical_crossen\n",
      "tropy\"\n",
      "  loss beca use we ha ve sparse labels (i.e., for each instance there is just a target\n",
      "class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had\n",
      "one target probability per class for each instance (such as one-hot vectors, e.g. \n",
      "[0.,\n",
      "0., 0., 1., 0., 0., 0., 0., 0., 0.]\n",
      "  to represen t class 3), then we would need\n",
      "to use the \n",
      "\"categorical_crossentropy\"\n",
      " \n",
      " loss instead. If we were doing binar y classiƒ\n",
      " fica tion (with one or more binar y labels), then we would use the \n",
      "\"sigmoid\"\n",
      " \n",
      "(i.e.,\n",
      " logistic) activa tion function in the output la yer instead of the \n",
      "\"softmax\"\n",
      " \n",
      " activa tion\n",
      "function, and we would use the \n",
      "\"binary_crossentropy\"\n",
      " loss.\n",
      " If you wan t to con vert sparse labels (i.e., class indices) to one-hot\n",
      "vector labels, you can use the \n",
      "keras.utils.to_categorical()\n",
      " function. T o go the other wa y round, you can just use the \n",
      "np.arg\n",
      "max()\n",
      " function with \n",
      "axis=1\n",
      ".\n",
      " Secondly , regarding the optimizer , \n",
      "\"sgd\"\n",
      "  sim ply means tha t we will train the model\n",
      " using sim ple Stochastic Gradien t Descen t. In other words, K eras will perform the\n",
      " backpropaga tion algorithm described earlier (i.e., reverse-mode a utodiff + Gradien t\n",
      " Descen t). W e will discuss more efficien t optimizers in \n",
      " Cha pter 11\n",
      "  (they im prove the\n",
      " Gradien t Descen t part, not the a utodiff ).\n",
      " Finally , since this is a classifier , it ‡ s useful to measure its \n",
      "\"accuracy\"\n",
      " during training\n",
      " and evalua tion.\n",
      "Training and Evaluating the Model\n",
      " N ow the model is ready to be trained. F or this we sim ply need to call its \n",
      "fit()\n",
      " method. W e pass it the in put fea tures (\n",
      "X_train\n",
      ") and the target classes (\n",
      "y_train\n",
      "), as\n",
      " well as the n umber of epochs to train (or else it would defa ult to just 1, which would\n",
      " definitely not be enough to con verge to a good solution). W e also pass a valida tion set\n",
      " (this is optional): K eras will measure the loss and the extra metrics on this set a t the\n",
      " end of each epoch, which is ver y useful to see how well the model really performs: if\n",
      " the performance on the training set is m uch better than on the valida tion set, your\n",
      " Implementing MLPs with Keras  |  299\n",
      "\n",
      " model is probably overfitting the training set (or there is a bug, such as a da ta misƒ\n",
      " ma tch between the training set and the valida tion set):\n",
      ">>> \n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "30\n",
      ",\n",
      "... \n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "))\n",
      "...\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==========] - 3s 55us/sample - loss: 1.4948     - acc: 0.5757\n",
      "                                          - val_loss: 1.0042 - val_acc: 0.7166\n",
      "Epoch 2/30\n",
      "55000/55000 [==========] - 3s 55us/sample - loss: 0.8690     - acc: 0.7318\n",
      "                                          - val_loss: 0.7549 - val_acc: 0.7616\n",
      "[...]\n",
      "Epoch 50/50\n",
      "55000/55000 [==========] - 4s 72us/sample - loss: 0.3607     - acc: 0.8752\n",
      "                                          - val_loss: 0.3706 - val_acc: 0.8728\n",
      " And tha t ‡ s it! The neural network is trained. A t each epoch during training, K eras disƒ\n",
      " pla ys the n umber of instances processed so far (along with a progress bar), the mean\n",
      " training time per sam ple, the loss and accuracy (or an y other extra metrics you asked\n",
      " for), both on the training set and the valida tion set. Y ou can see tha t the training loss\n",
      " wen t down, which is a good sign, and the valida tion accuracy reached 87.28% after 50\n",
      " epochs, not too far from the training accuracy , so there does not seem to be m uch\n",
      "overfitting going on.\n",
      " Instead of passing a valida tion set using the \n",
      "validation_data\n",
      " argumen t, you could instead set \n",
      "validation_split\n",
      "  to the ra tio of\n",
      " the training set tha t you wan t K eras to use for valida tion (e.g., 0.1).\n",
      " If the training set was ver y skewed, with some classes being overrepresen ted and othƒ\n",
      " ers underrepresen ted, it would be useful to set the \n",
      "class_weight\n",
      "  argumen t when\n",
      "calling the \n",
      "fit()\n",
      "  method, giving a larger weigh t to underrepresen ted classes, and a\n",
      " lower weigh t to overrepresen ted classes. These weigh ts would be used by K eras when\n",
      " com puting the loss. If you need per -instance weigh ts instead, you can set the \n",
      "sam\n",
      "ple_weight\n",
      "  argumen t (it supersedes \n",
      "class_weight\n",
      "). This could be useful for examƒ\n",
      "ple if some instances were labeled by experts while others were labeled using a\n",
      " crowdsourcing pla tform: you migh t wan t to give more weigh t to the former . Y ou can\n",
      " also provide sam ple weigh ts (but not class weigh ts) for the valida tion set by adding\n",
      "them as a third item in the \n",
      "validation_data\n",
      " tuple.\n",
      "The \n",
      "fit()\n",
      " method returns a \n",
      "History\n",
      "  object con taining the training parameters (\n",
      "his\n",
      "tory.params\n",
      " ), the list of epochs it wen t through (\n",
      "history.epoch\n",
      " ), and most im porƒ\n",
      " tan tly a dictionar y (\n",
      "history.history\n",
      " ) con taining the loss and extra metrics it\n",
      " measured a t the end of each epoch on the training set and on the valida tion set (if\n",
      " 300  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " an y). If you crea te a P andas Da taFrame using this dictionar y and call its \n",
      "plot()\n",
      " method, you get the learning cur ves shown in \n",
      "Figure 10-12\n",
      ":\n",
      "import\n",
      " \n",
      "pandas\n",
      " \n",
      "as\n",
      " \n",
      "pd\n",
      "pd\n",
      ".\n",
      "DataFrame\n",
      "(\n",
      "history\n",
      ".\n",
      "history\n",
      ")\n",
      ".\n",
      "plot\n",
      "(\n",
      "figsize\n",
      "=\n",
      "(\n",
      "8\n",
      ",\n",
      " \n",
      "5\n",
      "))\n",
      "plt\n",
      ".\n",
      "grid\n",
      "(\n",
      "True\n",
      ")\n",
      "plt\n",
      ".\n",
      "gca\n",
      "()\n",
      ".\n",
      "set_ylim\n",
      "(\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ")\n",
      " \n",
      "# set the vertical range to [0-1]\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " F i g u r e 10-12. L e a r n i n g C u r v e s\n",
      " Y ou can see tha t both the training and valida tion accuracy steadily increase during\n",
      " training, while the training and valida tion loss decrease. Good! M oreover , the validaƒ\n",
      " tion cur ves are quite close to the training cur ves, which means tha t there is not too\n",
      " m uch overfitting. In this particular case, the model performed better on the validaƒ\n",
      " tion set than on the training set a t the beginning of training: this sometimes ha ppens\n",
      " by chance (especially when the valida tion set is fairly small). H owever , the training set\n",
      " performance ends up bea ting the valida tion performance, as is generally the case\n",
      " when you train for long enough. Y ou can tell tha t the model has not quite con verged\n",
      " yet, as the valida tion loss is still going down, so you should probably con tin ue trainƒ\n",
      " ing. I t ‡ s as sim ple as calling the \n",
      "fit()\n",
      "  method again, since K eras just con tin ues trainƒ\n",
      " ing where it left off (you should be able to reach close to 89% valida tion accuracy).\n",
      " If you are not sa tisfied with the performance of your model, you should go back and\n",
      " tune the model ‡ s h yperparameters, for exam ple the n umber of la yers, the n umber of\n",
      " neurons per la yer , the types of activa tion functions we use for each hidden la yer , the\n",
      " Implementing MLPs with Keras  |  301\n",
      "\n",
      " n umber of training epochs, the ba tch size (it can be set in the \n",
      "fit()\n",
      "   method using the\n",
      "batch_size\n",
      "  argumen t, which defa ults to 32). W e will get back to h yperparameter\n",
      " tuning a t the end of this cha pter . Once you are sa tisfied with your model ‡ s valida tion\n",
      " accuracy , you should evalua te it on the test set to estima te the generaliza tion error\n",
      " before you deploy the model to production. Y ou can easily do this using the \n",
      "evalu\n",
      "ate()\n",
      "  method (it also supports several other argumen ts, such as \n",
      "batch_size\n",
      " \n",
      "or \n",
      "sam\n",
      "ple_weight\n",
      " , please check the documen ta tion for more details):\n",
      ">>> \n",
      "model\n",
      ".\n",
      "evaluate\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "8832/10000 [==========================] - ETA: 0s - loss: 0.4074 - acc: 0.8540\n",
      "[0.40738476498126985, 0.854]\n",
      " As we sa w in \n",
      " Cha pter 2\n",
      " , it is common to get sligh tly lower performance on the test set\n",
      " than on the valida tion set, beca use the h yperparameters are tuned on the valida tion\n",
      " set, not the test set (however , in this exam ple, we did not do an y h yperparameter tunƒ\n",
      " ing, so the lower accuracy is just bad luck). Remember to resist the tem pta tion to\n",
      " tweak the h yperparameters on the test set, or else your estima te of the generaliza tion\n",
      "error will be too optimistic.\n",
      "Using the Model to Make Predictions\n",
      " N ext, we can use the model ‡ s \n",
      "predict()\n",
      " method to make predictions on new instanƒ\n",
      " ces. Since we don ‡ t ha ve actual new instances, we will just use the first 3 instances of\n",
      "the test set:\n",
      ">>> \n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "X_test\n",
      "[:\n",
      "3\n",
      "]\n",
      ">>> \n",
      "y_proba\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      ">>> \n",
      "y_proba\n",
      ".\n",
      "round\n",
      "(\n",
      "2\n",
      ")\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.12, 0.  , 0.79],\n",
      "       [0.  , 0.  , 0.94, 0.  , 0.02, 0.  , 0.04, 0.  , 0.  , 0.  ],\n",
      "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
      "      dtype=float32)\n",
      " As you can see, for each instance the model estima tes one probability per class, from\n",
      " class 0 to class 9. F or exam ple, for the first image it estima tes tha t the probability of\n",
      "class 9 (ankle boot) is 79%, the probability of class 7 (sneaker) is 12%, the probability\n",
      "of class 5 (sandal) is 9%, and the other classes are negligible. In other words, it\n",
      " —believes – it ‡ s footwear , probably ankle boots, but it ‡ s not en tirely sure, it migh t be\n",
      "sneakers or sandals instead. If you only care about the class with the highest estimaƒ\n",
      " ted probability (even if tha t probability is quite low) then you can use the \n",
      "pre\n",
      "dict_classes()\n",
      " method instead:\n",
      ">>> \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict_classes\n",
      "(\n",
      "X_new\n",
      ")\n",
      ">>> \n",
      "y_pred\n",
      "array([9, 2, 1])\n",
      ">>> \n",
      "np\n",
      ".\n",
      "array\n",
      "(\n",
      "class_names\n",
      ")[\n",
      "y_pred\n",
      "]\n",
      "array([•Ankle boot•, •Pullover•, •Trouser•], dtype=•<U11•)\n",
      " And the classifier actually classified all three images correctly :\n",
      " 302  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      ">>> \n",
      "y_new\n",
      " \n",
      "=\n",
      " \n",
      "y_test\n",
      "[:\n",
      "3\n",
      "]\n",
      ">>> \n",
      "y_new\n",
      "array([9, 2, 1])\n",
      " N ow you know how to build, train, evalua te and use a classifica tion MLP using the\n",
      " Sequen tial API. But wha t about regression?\n",
      "Building a Regression MLP Using the Sequential API\n",
      " Let ‡ s switch to the California housing problem and tackle it using a regression neural\n",
      " network. F or sim plicity , we will use Scikit-Learn ‡ s \n",
      "fetch_california_housing()\n",
      " function to load the da ta: this da taset is sim pler than the one we used in \n",
      " Cha pter 2\n",
      ",\n",
      " since it con tains only n umerical fea tures (there is no \n",
      "ocean_proximity\n",
      "  fea ture), and\n",
      " there is no missing value. After loading the da ta, we split it in to a training set, a valiƒ\n",
      " da tion set and a test set, and we scale all the fea tures:\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "fetch_california_housing\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "train_test_split\n",
      "from\n",
      " \n",
      "sklearn.preprocessing\n",
      " \n",
      "import\n",
      " \n",
      "StandardScaler\n",
      "housing\n",
      " \n",
      "=\n",
      " \n",
      "fetch_california_housing\n",
      "()\n",
      "X_train_full\n",
      ",\n",
      " \n",
      "X_test\n",
      ",\n",
      " \n",
      "y_train_full\n",
      ",\n",
      " \n",
      "y_test\n",
      " \n",
      "=\n",
      " \n",
      "train_test_split\n",
      "(\n",
      "    \n",
      "housing\n",
      ".\n",
      "data\n",
      ",\n",
      " \n",
      "housing\n",
      ".\n",
      "target\n",
      ")\n",
      "X_train\n",
      ",\n",
      " \n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "y_valid\n",
      " \n",
      "=\n",
      " \n",
      "train_test_split\n",
      "(\n",
      "    \n",
      "X_train_full\n",
      ",\n",
      " \n",
      "y_train_full\n",
      ")\n",
      "scaler\n",
      " \n",
      "=\n",
      " \n",
      "StandardScaler\n",
      "()\n",
      "X_train_scaled\n",
      " \n",
      "=\n",
      " \n",
      "scaler\n",
      ".\n",
      "fit_transform\n",
      "(\n",
      "X_train\n",
      ")\n",
      "X_valid_scaled\n",
      " \n",
      "=\n",
      " \n",
      "scaler\n",
      ".\n",
      "transform\n",
      "(\n",
      "X_valid\n",
      ")\n",
      "X_test_scaled\n",
      " \n",
      "=\n",
      " \n",
      "scaler\n",
      ".\n",
      "transform\n",
      "(\n",
      "X_test\n",
      ")\n",
      " Building, training, evalua ting and using a regression MLP using the Sequen tial API to\n",
      " make predictions is quite similar to wha t we did for classifica tion. The main differƒ\n",
      " ences are the fact tha t the output la yer has a single neuron (since we only wan t to\n",
      " predict a single value) and uses no activa tion function, and the loss function is the\n",
      " mean squared error . Since the da taset is quite noisy , we just use a single hidden la yer\n",
      " with fewer neurons than before, to a void overfitting:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ",\n",
      " \n",
      "input_shape\n",
      "=\n",
      "X_train\n",
      ".\n",
      "shape\n",
      "[\n",
      "1\n",
      ":]),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")\n",
      "])\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"mean_squared_error\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "20\n",
      ",\n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "))\n",
      "mse_test\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "evaluate\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "X_new\n",
      " \n",
      "=\n",
      " \n",
      "X_test\n",
      "[:\n",
      "3\n",
      "]\n",
      " \n",
      "# pretend these are new instances\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      " Implementing MLPs with Keras  |  303\n",
      "\n",
      "14\n",
      " —W ide & Deep Learning for Recommender Systems, – H eng-T ze Cheng et al. (2016).\n",
      " As you can see, the Sequen tial API is quite easy to use. H owever , although sequen tial\n",
      "models are extremely common, it is sometimes useful to build neural networks with\n",
      " more com plex topologies, or with m ultiple in puts or outputs. F or this purpose, K eras\n",
      " offers the F unctional API.\n",
      "Building Complex Models Using the Functional API\n",
      " One exam ple of a non-sequen tial neural network is a \n",
      " W i d e & D e e p\n",
      " \n",
      "neural network.\n",
      " This neural network architecture was in troduced in a \n",
      " 2016 pa per\n",
      "   by H eng-T ze Cheng\n",
      "et al.\n",
      "14\n",
      " . I t connects all or part of the in puts directly to the output la yer , as shown in\n",
      "Figure 10-13\n",
      ". This architecture makes it possible for the neural network to learn both\n",
      " deep pa tterns (using the deep pa th) and sim ple rules (through the short pa th). In\n",
      " con trast, a regular MLP forces all the da ta to flow through the full stack of la yers, th us\n",
      " sim ple pa tterns in the da ta ma y end up being distorted by this sequence of transforƒ\n",
      " ma tions.\n",
      " 304  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " F i g u r e 10-13. W i d e a n d D e e p N eu r a l N e tw o r k\n",
      " Let ‡ s build such a neural network to tackle the California housing problem:\n",
      "input\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Input\n",
      "(\n",
      "shape\n",
      "=\n",
      "X_train\n",
      ".\n",
      "shape\n",
      "[\n",
      "1\n",
      ":])\n",
      "hidden1\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ")(\n",
      "input\n",
      ")\n",
      "hidden2\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ")(\n",
      "hidden1\n",
      ")\n",
      "concat\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Concatenate\n",
      "()[\n",
      "input\n",
      ",\n",
      " \n",
      "hidden2\n",
      "])\n",
      "output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")(\n",
      "concat\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "(\n",
      "inputs\n",
      "=\n",
      "[\n",
      "input\n",
      "],\n",
      " \n",
      "outputs\n",
      "=\n",
      "[\n",
      "output\n",
      "])\n",
      " Let ‡ s go through each line of this code:\n",
      "⁄\n",
      " First, we need to crea te an \n",
      "Input\n",
      "  object. This is needed beca use we ma y ha ve\n",
      " m ultiple in puts, as we will see la ter .\n",
      "⁄\n",
      " N ext, we crea te a \n",
      "Dense\n",
      "  la yer with 30 neurons and using the ReL U activa tion\n",
      " function. As soon as it is crea ted, notice tha t we call it like a function, passing it\n",
      " the in put. This is wh y this is called the F unctional API. N ote tha t we are just tellƒ\n",
      " Implementing MLPs with Keras  |  305\n",
      "\n",
      " ing K eras how it should connect the la yers together , no actual da ta is being proƒ\n",
      "cessed yet.\n",
      "⁄\n",
      " W e then crea te a second hidden la yer , and again we use it as a function. N ote\n",
      " however tha t we pass it the output of the first hidden la yer .\n",
      "⁄\n",
      " N ext, we crea te a \n",
      "Concatenate()\n",
      " \n",
      " la yer , and once again we immedia tely use it like\n",
      " a function, to conca tena te the in put and the output of the second hidden la yer\n",
      " (you ma y prefer the \n",
      "keras.layers.concatenate()\n",
      " \n",
      " function, which crea tes a \n",
      "Con\n",
      "catenate\n",
      "  la yer and immedia tely calls it with the given in puts).\n",
      "⁄\n",
      " Then we crea te the output la yer , with a single neuron and no activa tion function,\n",
      " and we call it like a function, passing it the result of the conca tena tion.\n",
      "⁄\n",
      " Lastly , we crea te a K eras \n",
      "Model\n",
      " , specif ying which in puts and outputs to use.\n",
      " Once you ha ve built the K eras model, ever ything is exactly like earlier , so no need to\n",
      " repea t it here: you m ust com pile the model, train it, evalua te it and use it to make\n",
      "predictions.\n",
      " But wha t if you wan t to send a subset of the fea tures through the wide pa th, and a\n",
      " differen t subset (possibly overla pping) through the deep pa th (see \n",
      "Figure 10-14\n",
      ")? In\n",
      " this case, one solution is to use m ultiple in puts. F or exam ple, suppose we wan t to\n",
      " send 5 fea tures through the deep pa th (fea tures 0 to 4), and 6 fea tures through the\n",
      " wide pa th (fea tures 2 to 7):\n",
      "input_A\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Input\n",
      "(\n",
      "shape\n",
      "=\n",
      "[\n",
      "5\n",
      "])\n",
      "input_B\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Input\n",
      "(\n",
      "shape\n",
      "=\n",
      "[\n",
      "6\n",
      "])\n",
      "hidden1\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ")(\n",
      "input_B\n",
      ")\n",
      "hidden2\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ")(\n",
      "hidden1\n",
      ")\n",
      "concat\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "concatenate\n",
      "([\n",
      "input_A\n",
      ",\n",
      " \n",
      "hidden2\n",
      "])\n",
      "output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")(\n",
      "concat\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "(\n",
      "inputs\n",
      "=\n",
      "[\n",
      "input_A\n",
      ",\n",
      " \n",
      "input_B\n",
      "],\n",
      " \n",
      "outputs\n",
      "=\n",
      "[\n",
      "output\n",
      "])\n",
      " 306  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " F i g u r e 10-14. H a n d l i n g M u l t i p l e I n p u ts\n",
      " The code is self-explana tor y . N ote tha t we specified \n",
      "inputs=[input_A, input_B]\n",
      " when crea ting the model. N ow we can com pile the model as usual, but when we call\n",
      "the \n",
      "fit()\n",
      "  method, instead of passing a single in put ma trix \n",
      "X_train\n",
      " , we m ust pass a\n",
      " pair of ma trices \n",
      "(X_train_A, X_train_B)\n",
      " : one per in put. The same is true for\n",
      "X_valid\n",
      ", and also for \n",
      "X_test\n",
      " and \n",
      "X_new\n",
      " when you call \n",
      "evaluate()\n",
      " or \n",
      "predict()\n",
      ":\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"mse\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ")\n",
      "X_train_A\n",
      ",\n",
      " \n",
      "X_train_B\n",
      " \n",
      "=\n",
      " \n",
      "X_train\n",
      "[:,\n",
      " \n",
      ":\n",
      "5\n",
      "],\n",
      " \n",
      "X_train\n",
      "[:,\n",
      " \n",
      "2\n",
      ":]\n",
      "X_valid_A\n",
      ",\n",
      " \n",
      "X_valid_B\n",
      " \n",
      "=\n",
      " \n",
      "X_valid\n",
      "[:,\n",
      " \n",
      ":\n",
      "5\n",
      "],\n",
      " \n",
      "X_valid\n",
      "[:,\n",
      " \n",
      "2\n",
      ":]\n",
      "X_test_A\n",
      ",\n",
      " \n",
      "X_test_B\n",
      " \n",
      "=\n",
      " \n",
      "X_test\n",
      "[:,\n",
      " \n",
      ":\n",
      "5\n",
      "],\n",
      " \n",
      "X_test\n",
      "[:,\n",
      " \n",
      "2\n",
      ":]\n",
      "X_new_A\n",
      ",\n",
      " \n",
      "X_new_B\n",
      " \n",
      "=\n",
      " \n",
      "X_test_A\n",
      "[:\n",
      "3\n",
      "],\n",
      " \n",
      "X_test_B\n",
      "[:\n",
      "3\n",
      "]\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "((\n",
      "X_train_A\n",
      ",\n",
      " \n",
      "X_train_B\n",
      "),\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "20\n",
      ",\n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "((\n",
      "X_valid_A\n",
      ",\n",
      " \n",
      "X_valid_B\n",
      "),\n",
      " \n",
      "y_valid\n",
      "))\n",
      "mse_test\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "evaluate\n",
      "((\n",
      "X_test_A\n",
      ",\n",
      " \n",
      "X_test_B\n",
      "),\n",
      " \n",
      "y_test\n",
      ")\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "((\n",
      "X_new_A\n",
      ",\n",
      " \n",
      "X_new_B\n",
      "))\n",
      " Implementing MLPs with Keras  |  307\n",
      "\n",
      " There are also man y use cases in which you ma y wan t to ha ve m ultiple outputs:\n",
      "⁄\n",
      " The task ma y demand it, for exam ple you ma y wan t to loca te and classif y the\n",
      " main object in a picture. This is both a regression task (finding the coordina tes of\n",
      " the object ‡ s cen ter , as well as its width and heigh t) and a classifica tion task.\n",
      "⁄\n",
      " Similarly , you ma y ha ve m ultiple independen t tasks to perform based on the\n",
      " same da ta. Sure, you could train one neural network per task, but in man y cases\n",
      "you will get better results on all tasks by training a single neural network with\n",
      " one output per task. This is beca use the neural network can learn fea tures in the\n",
      " da ta tha t are useful across tasks.\n",
      "⁄\n",
      " Another use case is as a regulariza tion technique (i.e., a training constrain t whose\n",
      " objective is to reduce overfitting and th us im prove the model ‡ s ability to generalƒ\n",
      " ize). F or exam ple, you ma y wan t to add some a uxiliar y outputs in a neural netƒ\n",
      "work architecture (see \n",
      "Figure 10-15\n",
      " ) to ensure tha t the underlying part of the\n",
      "network learns something useful on its own, without relying on the rest of the\n",
      "network.\n",
      " F i g u r e 10-15. H a n d l i n g M u l t i p l e O u tp u ts ” Auxi l i a r y O u tp u t f o r R e g u l a r iz a t i o n\n",
      " A dding extra outputs is quite easy : just connect them to the a ppropria te la yers and\n",
      " add them to your model ‡ s list of outputs. F or exam ple, the following code builds the\n",
      " network represen ted in \n",
      "Figure 10-15\n",
      ":\n",
      " 308  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# Same as above, up to the main output layer\n",
      "output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")(\n",
      "concat\n",
      ")\n",
      "aux_output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")(\n",
      "hidden2\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "(\n",
      "inputs\n",
      "=\n",
      "[\n",
      "input_A\n",
      ",\n",
      " \n",
      "input_B\n",
      "],\n",
      "                           \n",
      "outputs\n",
      "=\n",
      "[\n",
      "output\n",
      ",\n",
      " \n",
      "aux_output\n",
      "])\n",
      " Each output will need its own loss function, so when we com pile the model we\n",
      " should pass a list of losses (if we pass a single loss, K eras will assume tha t the same\n",
      " loss m ust be used for all outputs). By defa ult, K eras will com pute all these losses and\n",
      " sim ply add them up to get the final loss used for training. H owever , we care m uch\n",
      " more about the main output than about the a uxiliar y output (as it is just used for regƒ\n",
      " ulariza tion), so we wan t to give the main output ‡ s loss a m uch grea ter weigh t. F ortuƒ\n",
      " na tely , it is possible to set all the loss weigh ts when com piling the model:\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "[\n",
      "\"mse\"\n",
      ",\n",
      " \n",
      "\"mse\"\n",
      "],\n",
      " \n",
      "loss_weights\n",
      "=\n",
      "[\n",
      "0.9\n",
      ",\n",
      " \n",
      "0.1\n",
      "],\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ")\n",
      " N ow when we train the model, we need to provide some labels for each output. In\n",
      " this exam ple, the main output and the a uxiliar y output should tr y to predict the same\n",
      "thing, so they should use the same labels. So instead of passing \n",
      "y_train\n",
      ", we just need\n",
      "to pass \n",
      "(y_train, y_train)\n",
      " (and the same goes for \n",
      "y_valid\n",
      " and \n",
      "y_test\n",
      "):\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "    \n",
      "[\n",
      "X_train_A\n",
      ",\n",
      " \n",
      "X_train_B\n",
      "],\n",
      " \n",
      "[\n",
      "y_train\n",
      ",\n",
      " \n",
      "y_train\n",
      "],\n",
      " \n",
      "epochs\n",
      "=\n",
      "20\n",
      ",\n",
      "    \n",
      "validation_data\n",
      "=\n",
      "([\n",
      "X_valid_A\n",
      ",\n",
      " \n",
      "X_valid_B\n",
      "],\n",
      " \n",
      "[\n",
      "y_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "]))\n",
      " When we evalua te the model, K eras will return the total loss, as well as all the individƒ\n",
      "ual losses:\n",
      "total_loss\n",
      ",\n",
      " \n",
      "main_loss\n",
      ",\n",
      " \n",
      "aux_loss\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "evaluate\n",
      "(\n",
      "    \n",
      "[\n",
      "X_test_A\n",
      ",\n",
      " \n",
      "X_test_B\n",
      "],\n",
      " \n",
      "[\n",
      "y_test\n",
      ",\n",
      " \n",
      "y_test\n",
      "])\n",
      " Similarly , the \n",
      "predict()\n",
      " method will return predictions for each output:\n",
      "y_pred_main\n",
      ",\n",
      " \n",
      "y_pred_aux\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "([\n",
      "X_new_A\n",
      ",\n",
      " \n",
      "X_new_B\n",
      "])\n",
      " As you can see, you can build an y sort of architecture you wan t quite easily with the\n",
      " F unctional API. Let ‡ s look a t one last wa y you can build K eras models.\n",
      "Building Dynamic Models Using the Subclassing API\n",
      " B oth the Sequen tial API and the F unctional API are declara tive: you start by declarƒ\n",
      " ing which la yers you wan t to use and how they should be connected, and only then\n",
      " can you start feeding the model some da ta for training or inference. This has man y\n",
      " advan tages: the model can easily be sa ved, cloned, shared, its structure can be disƒ\n",
      " pla yed and analyzed, the framework can infer sha pes and check types, so errors can\n",
      " be ca ugh t early (i.e., before an y da ta ever goes through the model). I t ‡ s also fairly easy\n",
      " to debug, since the whole model is just a sta tic gra ph of la yers. But the flip side is just\n",
      " tha t: it ‡ s sta tic. Some models in volve loops, var ying sha pes, conditional branching,\n",
      " and other dynamic beha viors. F or such cases, or sim ply if you prefer a more im peraƒ\n",
      "tive programming style, the Subclassing API is for you.\n",
      " Implementing MLPs with Keras  |  309\n",
      "\n",
      "15\n",
      " K eras models ha ve an \n",
      "output\n",
      "  a ttribute, so we cannot use tha t name for the main output la yer , which is wh y\n",
      "we renamed it to \n",
      "main_output\n",
      ".\n",
      " Sim ply subclass the \n",
      "Model\n",
      " \n",
      " class, crea te the la yers you need in the constructor , and use\n",
      " them to perform the com puta tions you wan t in the \n",
      "call()\n",
      "   method. F or exam ple, creƒ\n",
      " a ting an instance of the following \n",
      "WideAndDeepModel\n",
      " \n",
      " class gives us an equivalen t\n",
      " model to the one we just built with the F unctional API. Y ou can then com pile it, evalƒ\n",
      " ua te it and use it to make predictions, exactly like we just did.\n",
      "class\n",
      " \n",
      "WideAndDeepModel\n",
      "(\n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "units\n",
      "=\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      " \n",
      "# handles standard args (e.g., name)\n",
      "        \n",
      "self\n",
      ".\n",
      "hidden1\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "units\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "activation\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "hidden2\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "units\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "activation\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "main_output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "aux_output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      "):\n",
      "        \n",
      "input_A\n",
      ",\n",
      " \n",
      "input_B\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "        \n",
      "hidden1\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "hidden1\n",
      "(\n",
      "input_B\n",
      ")\n",
      "        \n",
      "hidden2\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "hidden2\n",
      "(\n",
      "hidden1\n",
      ")\n",
      "        \n",
      "concat\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "concatenate\n",
      "([\n",
      "input_A\n",
      ",\n",
      " \n",
      "hidden2\n",
      "])\n",
      "        \n",
      "main_output\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "main_output\n",
      "(\n",
      "concat\n",
      ")\n",
      "        \n",
      "aux_output\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "aux_output\n",
      "(\n",
      "hidden2\n",
      ")\n",
      "        \n",
      "return\n",
      " \n",
      "main_output\n",
      ",\n",
      " \n",
      "aux_output\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "WideAndDeepModel\n",
      "()\n",
      " This exam ple looks ver y m uch like the F unctional API, except we do not need to creƒ\n",
      " a te the in puts, we just use the \n",
      "input\n",
      "   argumen t to the \n",
      "call()\n",
      "   method, and we separa te\n",
      " the crea tion of the la yers\n",
      "15\n",
      " in the constructor from their usage in the \n",
      "call()\n",
      " \n",
      "method.\n",
      " H owever , the big difference is tha t you can do pretty m uch an ything you wan t in the\n",
      "call()\n",
      " method: \n",
      "for\n",
      " loops, \n",
      "if\n",
      " \n",
      " sta temen ts, low-level T ensorFlow opera tions, your\n",
      " imagina tion is the limit (see \n",
      " Cha pter 12\n",
      " )! This makes it a grea t API for researchers\n",
      " experimen ting with new ideas.\n",
      " H owever , this extra flexibility comes a t a cost: your model ‡ s architecture is hidden\n",
      "within the \n",
      "call()\n",
      " \n",
      " method, so K eras cannot easily inspect it, it cannot sa ve or clone it,\n",
      "and when you call the \n",
      "summary()\n",
      "  method, you only get a list of la yers, without an y\n",
      " informa tion on how they are connected to each other . M oreover , K eras cannot check\n",
      " types and sha pes ahead of time, and it is easier to make mistakes. So unless you really\n",
      " need tha t extra flexibility , you should probably stick to the Sequen tial API or the\n",
      " F unctional API.\n",
      " 310  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " K eras models can be used just like regular la yers, so you can easily\n",
      " com pose them to build com plex architectures.\n",
      " N ow tha t you know how to build and train neural nets using K eras, you will wan t to\n",
      " sa ve them!\n",
      "Saving and Restoring a Model\n",
      " Sa ving a trained K eras model is as sim ple as it gets:\n",
      "model\n",
      ".\n",
      "save\n",
      "(\n",
      "\"my_keras_model.h5\"\n",
      ")\n",
      " K eras will sa ve both the model ‡ s architecture (including ever y la yer‡ s h yperparameƒ\n",
      " ters) and the value of all the model parameters for ever y la yer (e.g., connection\n",
      " weigh ts and biases), using the HDF5 forma t. I t also sa ves the optimizer (including its\n",
      " h yperparameters and an y sta te it ma y ha ve).\n",
      " Y ou will typically ha ve a script tha t trains a model and sa ves it, and one or more\n",
      " scripts (or web ser vices) tha t load the model and use it to make predictions. Loading\n",
      " the model is just as easy :\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "load_model\n",
      "(\n",
      "\"my_keras_model.h5\"\n",
      ")\n",
      " This will work when using the Sequen tial API or the F unctional\n",
      " API, but unfortuna tely not when using M odel subclassing. H owƒ\n",
      " ever , you can use \n",
      "save_weights()\n",
      " and \n",
      "load_weights()\n",
      "  to a t least\n",
      " sa ve and restore the model parameters (but you will need to sa ve\n",
      " and restore ever ything else yourself ).\n",
      " But wha t if training lasts several hours? This is quite common, especially when trainƒ\n",
      " ing on large da tasets. In this case, you should not only sa ve your model a t the end of\n",
      " training, but also sa ve checkpoin ts a t regular in ter vals during training. But how can\n",
      "you tell the \n",
      "fit()\n",
      "  method to sa ve checkpoin ts? The answer is: using callbacks.\n",
      "Using Callbacks\n",
      "The \n",
      "fit()\n",
      "   method accepts a \n",
      "callbacks\n",
      " \n",
      " argumen t tha t lets you specif y a list of objects\n",
      " tha t K eras will call during training a t the start and end of training, a t the start and end\n",
      " of each epoch and even before and after processing each ba tch. F or exam ple, the \n",
      "Mod\n",
      "elCheckpoint\n",
      "  callback sa ves checkpoin ts of your model a t regular in ter vals during\n",
      " training, by defa ult a t the end of each epoch:\n",
      " Implementing MLPs with Keras  |  311\n",
      "\n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# build and compile the model\n",
      "checkpoint_cb\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "ModelCheckpoint\n",
      "(\n",
      "\"my_keras_model.h5\"\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "checkpoint_cb\n",
      "])\n",
      " M oreover , if you use a valida tion set during training, you can set\n",
      "save_best_only=True\n",
      "  when crea ting the \n",
      "ModelCheckpoint\n",
      ". In this case, it will only\n",
      " sa ve your model when its performance on the valida tion set is the best so far . This\n",
      " wa y , you do not need to worr y about training for too long and overfitting the training\n",
      " set: sim ply restore the last model sa ved after training, and this will be the best model\n",
      " on the valida tion set. This is a sim ple wa y to im plemen t early stopping (in troduced in\n",
      " Cha pter 4\n",
      "):\n",
      "checkpoint_cb\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "ModelCheckpoint\n",
      "(\n",
      "\"my_keras_model.h5\"\n",
      ",\n",
      "                                                \n",
      "save_best_only\n",
      "=\n",
      "True\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "10\n",
      ",\n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "),\n",
      "                    \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "checkpoint_cb\n",
      "])\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "load_model\n",
      "(\n",
      "\"my_keras_model.h5\"\n",
      ")\n",
      " \n",
      "# rollback to best model\n",
      " Another wa y to im plemen t early stopping is to sim ply use the \n",
      "EarlyStopping\n",
      " \n",
      "callƒ\n",
      " back. I t will in terrupt training when it measures no progress on the valida tion set for\n",
      " a n umber of epochs (defined by the \n",
      "patience\n",
      "  argumen t), and it will optionally roll\n",
      " back to the best model. Y ou can combine both callbacks to both sa ve checkpoin ts of\n",
      " your model (in case your com puter crashes), and actually in terrupt training early\n",
      " when there is no more progress (to a void wasting time and resources):\n",
      "early_stopping_cb\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "EarlyStopping\n",
      "(\n",
      "patience\n",
      "=\n",
      "10\n",
      ",\n",
      "                                                  \n",
      "restore_best_weights\n",
      "=\n",
      "True\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "100\n",
      ",\n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "),\n",
      "                    \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "checkpoint_cb\n",
      ",\n",
      " \n",
      "early_stopping_cb\n",
      "])\n",
      " The n umber of epochs can be set to a large value since training will stop a utoma tiƒ\n",
      " cally when there is no more progress. M oreover , there is no need to restore the best\n",
      " model sa ved in this case since the \n",
      "EarlyStopping\n",
      " callback will keep track of the best\n",
      " weigh ts and restore them for us a t the end of training.\n",
      " There are man y other callbacks a vailable in the \n",
      "keras.callbacks\n",
      "package. See \n",
      " h ttp s://k er as.i o/c a l l b a c ks/\n",
      ".\n",
      " If you need extra con trol, you can easily write your own custom callbacks. F or examƒ\n",
      " ple, the following custom callback will displa y the ra tio between the valida tion loss\n",
      "and the training loss during training (e.g., to detect overfitting):\n",
      " 312  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "class\n",
      " \n",
      "PrintValTrainRatioCallback\n",
      "(\n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "Callback\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "on_epoch_end\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "epoch\n",
      ",\n",
      " \n",
      "logs\n",
      "):\n",
      "        \n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "val/train: {:.2f}\"\n",
      ".\n",
      "format\n",
      "(\n",
      "logs\n",
      "[\n",
      "\"val_loss\"\n",
      "]\n",
      " \n",
      "/\n",
      " \n",
      "logs\n",
      "[\n",
      "\"loss\"\n",
      "]))\n",
      " As you migh t expect, you can im plemen t \n",
      "on_train_begin()\n",
      ", \n",
      "on_train_end()\n",
      ",\n",
      "on_epoch_begin()\n",
      ", \n",
      "on_epoch_begin()\n",
      ", \n",
      "on_batch_end()\n",
      " and \n",
      "on_batch_end()\n",
      ".\n",
      " M oreover , callbacks can also be used during evalua tion and predictions, should you\n",
      " ever need them (e.g., for debugging). In this case, you should im plemen t\n",
      "on_test_begin()\n",
      ", \n",
      "on_test_end()\n",
      ", \n",
      "on_test_batch_begin()\n",
      ", or\n",
      "on_test_batch_end()\n",
      " (called by \n",
      "evaluate()\n",
      "), or \n",
      "on_predict_begin()\n",
      ", \n",
      "on_pre\n",
      "dict_end()\n",
      ", \n",
      "on_predict_batch_begin()\n",
      ", or \n",
      "on_predict_batch_end()\n",
      " \n",
      "(called by\n",
      "predict()\n",
      ").\n",
      " N ow let ‡ s take a look a t one more tool you should definitely ha ve in your toolbox\n",
      " when using tf.keras: T ensorB oard.\n",
      "Visualization Using TensorBoard\n",
      " T ensorB oard is a grea t in teractive visualiza tion tool tha t you can use to view the\n",
      " learning cur ves during training, com pare learning cur ves between m ultiple runs, visƒ\n",
      " ualize the com puta tion gra ph, analyze training sta tistics, view images genera ted by\n",
      " your model, visualize com plex m ultidimensional da ta projected down to 3D and\n",
      " a utoma tically clustered for you, and more! This tool is installed a utoma tically when\n",
      " you install T ensorFlow , so you already ha ve it!\n",
      " T o use it, you m ust modif y your program so tha t it outputs the da ta you wan t to visuƒ\n",
      " alize to special binar y log files called \n",
      " e v en t \n",
      "†les\n",
      " . Each binar y da ta record is called a\n",
      " s u m m a r y\n",
      " . The T ensorB oard ser ver will monitor the log director y , and it will a utoma tƒ\n",
      " ically pick up the changes and upda te the visualiza tions: this allows you to visualize\n",
      " live da ta (with a short dela y), such as the learning cur ves during training. In general,\n",
      " you wan t to poin t the T ensorB oard ser ver to a root log director y , and configure your\n",
      " program so tha t it writes to a differen t subdirector y ever y time it runs. This wa y , the\n",
      " same T ensorB oard ser ver instance will allow you to visualize and com pare da ta from\n",
      " m ultiple runs of your program, without getting ever ything mixed up .\n",
      " So let ‡ s start by defining the root log director y we will use for our T ensorB oard logs,\n",
      " plus a small function tha t will genera te a subdirector y pa th based on the curren t da te\n",
      " and time, so tha t it is differen t a t ever y run. Y ou ma y wan t to include extra informaƒ\n",
      " tion in the log director y name, such as h yperparameter values tha t you are testing, to\n",
      " make it easier to know wha t you are looking a t in T ensorB oard:\n",
      "root_logdir\n",
      " \n",
      "=\n",
      " \n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "join\n",
      "(\n",
      "os\n",
      ".\n",
      "curdir\n",
      ",\n",
      " \n",
      "\"my_logs\"\n",
      ")\n",
      "def\n",
      " \n",
      "get_run_logdir\n",
      "():\n",
      "    \n",
      "import\n",
      " \n",
      "time\n",
      "    \n",
      "run_id\n",
      " \n",
      "=\n",
      " \n",
      "time\n",
      ".\n",
      "strftime\n",
      "(\n",
      "\"run_\n",
      "%Y\n",
      "_\n",
      "%m\n",
      "_\n",
      "%d\n",
      "-\n",
      "%H\n",
      "_\n",
      "%M\n",
      "_\n",
      "%S\n",
      "\"\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "os\n",
      ".\n",
      "path\n",
      ".\n",
      "join\n",
      "(\n",
      "root_logdir\n",
      ",\n",
      " \n",
      "run_id\n",
      ")\n",
      " Implementing MLPs with Keras  |  313\n",
      "\n",
      "run_logdir\n",
      " \n",
      "=\n",
      " \n",
      "get_run_logdir\n",
      "()\n",
      " \n",
      "# e.g., •./my_logs/run_2019_01_16-11_28_43•\n",
      " N ext, the good news is tha t K eras provides a nice \n",
      "TensorBoard\n",
      " callback:\n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# Build and compile your model\n",
      "tensorboard_cb\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "TensorBoard\n",
      "(\n",
      "run_logdir\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "30\n",
      ",\n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "),\n",
      "                    \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "tensorboard_cb\n",
      "])\n",
      " And tha t ‡ s all there is to it! I t could hardly be easier to use. If you run this code, the\n",
      "TensorBoard\n",
      "  callback will take care of crea ting the log director y for you (along with\n",
      " its paren t directories if needed), and during training it will crea te even t files and write\n",
      " summaries to them. After running the program a second time (perha ps changing\n",
      " some h yperparameter value), you will end up with a director y structure similar to\n",
      "this one:\n",
      "my_logs\n",
      "†‡‡ run_2019_01_16-16_51_02\n",
      "…   —‡‡ events.out.tfevents.1547628669.mycomputer.local.v2\n",
      "—‡‡ run_2019_01_16-16_56_50\n",
      "    —‡‡ events.out.tfevents.1547629020.mycomputer.local.v2\n",
      " N ext you need to start the T ensorB oard ser ver . If you installed T ensorFlow within a\n",
      " virtualen v , you should activa te it. N ext, run the following command a t the root of the\n",
      " project (or from an ywhere else as long as you poin t to the a ppropria te log director y).\n",
      "If your shell cannot find the \n",
      "tensorboard\n",
      "  script, then you m ust upda te your \n",
      "PATH\n",
      " en vironmen t variable so tha t it con tains the director y in which the script was\n",
      " installed (alterna tively , you can just replace \n",
      "tensorboard\n",
      " \n",
      "with \n",
      "python3 -m tensor\n",
      "board.main\n",
      ").\n",
      "$ \n",
      "tensorboard --logdir\n",
      "=\n",
      "./my_logs --port\n",
      "=\n",
      "6006\n",
      "TensorBoard 2.0.0 at http://mycomputer.local:6006 \n",
      "(\n",
      "Press CTRL+C to quit\n",
      ")\n",
      " Finally , open up a web browser to \n",
      " h ttp://l o c a l h o s t:6006\n",
      " . Y ou should see T ensorB oard ‡ s\n",
      " web in terface. Click on the SCALARS tab to view the learning cur ves (see\n",
      "Figure 10-16\n",
      " ). N otice tha t the training loss wen t down nicely during both runs, but\n",
      " the second run wen t down m uch faster . Indeed, we used a larger learning ra te by setƒ\n",
      "ting \n",
      "optimizer=keras.optimizers.SGD(lr=0.05)\n",
      " instead of \n",
      "optimizer=\"sgd\"\n",
      ",\n",
      " which defa ults to a learning ra te of 0.001.\n",
      " 314  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " F i g u r e 10-16. V i s u a l izi n g L e a r n i n g C u r v e s w i t h T ens o rB o a r d\n",
      " U nfortuna tely , a t the time of writing, no other da ta is exported by the \n",
      "TensorBoard\n",
      " callback, but this issue will probably be fixed by the time you read these lines. In T enƒ\n",
      " sorFlow 1, this callback exported the com puta tion gra ph and man y useful sta tistics:\n",
      "type \n",
      "help(keras.callbacks.TensorBoard)\n",
      " to see all the options.\n",
      " Let ‡ s summarize wha t you learned so far in this cha pter : we sa w where neural nets\n",
      " came from, wha t an MLP is and how you can use it for classifica tion and regression,\n",
      " how to build MLP s using tf.keras ‡ s Sequen tial API, or more com plex architectures\n",
      " using the F unctional API or \n",
      "Model\n",
      " \n",
      " Subclassing, you learned how to sa ve and restore a\n",
      " model, use callbacks for checkpoin ting, early stopping, and more, and finally how to\n",
      " use T ensorB oard for visualiza tion. Y ou can already go ahead and use neural networks\n",
      " to tackle man y problems! H owever , you ma y wonder how to choose the n umber of\n",
      " hidden la yers, the n umber of neurons in the network, and all the other h yperparameƒ\n",
      " ters. Let ‡ s look a t this now .\n",
      "Fine-Tuning Neural Network Hyperparameters\n",
      "The \n",
      " flexibility of neural networks is also one of their main dra wbacks: there are man y\n",
      " h yperparameters to tweak. N ot only can you use an y imaginable network architecƒ\n",
      " ture, but even in a sim ple MLP you can change the n umber of la yers, the n umber of\n",
      " neurons per la yer , the type of activa tion function to use in each la yer , the weigh t initiƒ\n",
      " Fine-Tuning Neural Network Hyperparameters  |  315\n",
      "\n",
      " aliza tion logic, and m uch more. H ow do you know wha t combina tion of h yperparaƒ\n",
      "meters is the best for your task?\n",
      " One option is to sim ply tr y man y combina tions of h yperparameters and see which\n",
      " one works best on the valida tion set (or using K-fold cross-valida tion). F or this, one\n",
      " a pproach is sim ply use \n",
      "GridSearchCV\n",
      " or \n",
      "RandomizedSearchCV\n",
      "  to explore the h yperƒ\n",
      "parameter space, as we did in \n",
      " Cha pter 2\n",
      " . F or this, we need to wra p our K eras models\n",
      " in objects tha t mimic regular Scikit-Learn regressors. The first step is to crea te a funcƒ\n",
      " tion tha t will build and com pile a K eras model, given a set of h yperparameters:\n",
      "def\n",
      " \n",
      "build_model\n",
      "(\n",
      "n_hidden\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "n_neurons\n",
      "=\n",
      "30\n",
      ",\n",
      " \n",
      "learning_rate\n",
      "=\n",
      "3e-3\n",
      ",\n",
      " \n",
      "input_shape\n",
      "=\n",
      "[\n",
      "8\n",
      "]):\n",
      "    \n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "()\n",
      "    \n",
      "options\n",
      " \n",
      "=\n",
      " \n",
      "{\n",
      "\"input_shape\"\n",
      ":\n",
      " \n",
      "input_shape\n",
      "}\n",
      "    \n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "n_hidden\n",
      "):\n",
      "        \n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "n_neurons\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ",\n",
      " \n",
      "**\n",
      "options\n",
      "))\n",
      "        \n",
      "options\n",
      " \n",
      "=\n",
      " \n",
      "{}\n",
      "    \n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "**\n",
      "options\n",
      "))\n",
      "    \n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "learning_rate\n",
      ")\n",
      "    \n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"mse\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "optimizer\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "model\n",
      " This function crea tes a sim ple \n",
      "Sequential\n",
      "  model for univaria te regression (only one\n",
      " output neuron), with the given in put sha pe and the given n umber of hidden la yers\n",
      " and neurons, and it com piles it using an \n",
      "SGD\n",
      " optimizer configured with the given\n",
      " learning ra te. The \n",
      "options\n",
      "  dict is used to ensure tha t the first la yer is properly given\n",
      " the in put sha pe (note tha t if \n",
      "n_hidden=0\n",
      " , the first la yer will be the output la yer). I t is\n",
      " good practice to provide reasonable defa ults to as man y h yperparameters as you can,\n",
      "as Scikit-Learn does.\n",
      " N ext, let ‡ s crea te a \n",
      "KerasRegressor\n",
      " based on this \n",
      "build_model()\n",
      " function:\n",
      "keras_reg\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "wrappers\n",
      ".\n",
      "scikit_learn\n",
      ".\n",
      "KerasRegressor\n",
      "(\n",
      "build_model\n",
      ")\n",
      "The \n",
      "KerasRegressor\n",
      "  object is a thin wra pper around the K eras model built using\n",
      "build_model()\n",
      " . Since we did not specif y an y h yperparameter when crea ting it, it will\n",
      " just use the defa ult h yperparameters we defined in \n",
      "build_model()\n",
      " . N ow we can use\n",
      " this object like a regular Scikit-Learn regressor : we can train it using its \n",
      "fit()\n",
      " method, then evalua te it using its \n",
      "score()\n",
      " method, and use it to make predictions\n",
      "using its \n",
      "predict()\n",
      "  method. N ote tha t an y extra parameter you pass to the \n",
      "fit()\n",
      " method will sim ply get passed to the underlying K eras model. Also note tha t the\n",
      " score will be the opposite of the MSE beca use Scikit-Learn wan ts scores, not losses\n",
      "(i.e., higher should be better).\n",
      "keras_reg\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "100\n",
      ",\n",
      "              \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "),\n",
      "              \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "EarlyStopping\n",
      "(\n",
      "patience\n",
      "=\n",
      "10\n",
      ")])\n",
      "mse_test\n",
      " \n",
      "=\n",
      " \n",
      "keras_reg\n",
      ".\n",
      "score\n",
      "(\n",
      "X_test\n",
      ",\n",
      " \n",
      "y_test\n",
      ")\n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "keras_reg\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_new\n",
      ")\n",
      " 316  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " H owever , we do not actually wan t to train and evalua te a single model like this, we\n",
      " wan t to train h undreds of varian ts and see which one performs best on the valida tion\n",
      " set. Since there are man y h yperparameters, it is preferable to use a randomized search\n",
      " ra ther than grid search (as we discussed in \n",
      " Cha pter 2\n",
      " ). Let ‡ s tr y to explore the n umber\n",
      " of hidden la yers, the n umber of neurons and the learning ra te:\n",
      "from\n",
      " \n",
      "scipy.stats\n",
      " \n",
      "import\n",
      " \n",
      "reciprocal\n",
      "from\n",
      " \n",
      "sklearn.model_selection\n",
      " \n",
      "import\n",
      " \n",
      "RandomizedSearchCV\n",
      "param_distribs\n",
      " \n",
      "=\n",
      " \n",
      "{\n",
      "    \n",
      "\"n_hidden\"\n",
      ":\n",
      " \n",
      "[\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "3\n",
      "],\n",
      "    \n",
      "\"n_neurons\"\n",
      ":\n",
      " \n",
      "np\n",
      ".\n",
      "arange\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "100\n",
      "),\n",
      "    \n",
      "\"learning_rate\"\n",
      ":\n",
      " \n",
      "reciprocal\n",
      "(\n",
      "3e-4\n",
      ",\n",
      " \n",
      "3e-2\n",
      "),\n",
      "}\n",
      "rnd_search_cv\n",
      " \n",
      "=\n",
      " \n",
      "RandomizedSearchCV\n",
      "(\n",
      "keras_reg\n",
      ",\n",
      " \n",
      "param_distribs\n",
      ",\n",
      " \n",
      "n_iter\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "cv\n",
      "=\n",
      "3\n",
      ")\n",
      "rnd_search_cv\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "100\n",
      ",\n",
      "                  \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid\n",
      ",\n",
      " \n",
      "y_valid\n",
      "),\n",
      "                  \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "EarlyStopping\n",
      "(\n",
      "patience\n",
      "=\n",
      "10\n",
      ")])\n",
      " As you can see, this is iden tical to wha t we did in \n",
      " Cha pter 2\n",
      " , with the exception tha t\n",
      "we pass extra parameters to the \n",
      "fit()\n",
      "  method: they sim ply get rela yed to the underƒ\n",
      " lying K eras models. N ote tha t \n",
      "RandomizedSearchCV\n",
      " \n",
      " uses K-fold cross-valida tion, so it\n",
      "does not use \n",
      "X_valid\n",
      " and \n",
      "y_valid\n",
      ". These are just used for early stopping.\n",
      " The explora tion ma y last man y hours depending on the hardware, the size of the\n",
      " da taset, the com plexity of the model and the value of \n",
      "n_iter\n",
      "   and \n",
      "cv\n",
      " . When it is over ,\n",
      " you can access the best parameters found, the best score, and the trained K eras model\n",
      "like this:\n",
      ">>> \n",
      "rnd_search_cv\n",
      ".\n",
      "best_params_\n",
      "{•learning_rate•: 0.0033625641252688094, •n_hidden•: 2, •n_neurons•: 42}\n",
      ">>> \n",
      "rnd_search_cv\n",
      ".\n",
      "best_score_\n",
      "-0.3189529188278931\n",
      ">>> \n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "rnd_search_cv\n",
      ".\n",
      "best_estimator_\n",
      ".\n",
      "model\n",
      " Y ou can now sa ve this model, evalua te it on the test set, and if you are sa tisfied with\n",
      " its performance, deploy it to production. U sing randomized search is not too hard,\n",
      " and it works well for man y fairly sim ple problems. H owever , when training is slow\n",
      " (e.g., for more com plex problems with larger da tasets), this a pproach will only\n",
      " explore a tin y portion of the h yperparameter space. Y ou can partially allevia te this\n",
      " problem by assisting the search process man ually : first run a quick random search\n",
      " using wide ranges of h yperparameter values, then run another search using smaller\n",
      " ranges of values cen tered on the best ones found during the first run, and so on. This\n",
      " will hopefully zoom in to a good set of h yperparameters. H owever , this is ver y time\n",
      "consuming, and probably not the best use of your time.\n",
      " F ortuna tely , there are man y techniques to explore a search space m uch more effiƒ\n",
      " cien tly than randomly . Their core idea is sim ple: when a region of the space turns out\n",
      " Fine-Tuning Neural Network Hyperparameters  |  317\n",
      "\n",
      "16\n",
      " —P opula tion Based T raining of N eural N etworks, – M ax J aderberg et al. (2017).\n",
      " to be good, it should be explored more. This takes care of the — zooming – process for\n",
      " you and leads to m uch better solutions in m uch less time. H ere are a few Python\n",
      " libraries you can use to optimize h yperparameters:\n",
      "⁄\n",
      " H yperopt\n",
      " : a popular Python librar y for optimizing over all sorts of com plex\n",
      " search spaces (including real values such as the learning ra te, or discrete values\n",
      " such as the n umber of la yers).\n",
      "⁄\n",
      " H yperas\n",
      ", \n",
      "kopt\n",
      " or \n",
      " T alos\n",
      " : optimizing h yperparameters for K eras model (the first\n",
      " two are based on H yperopt).\n",
      "⁄\n",
      "Scikit-Optimize\n",
      "  (skopt): a general-purpose optimiza tion librar y . The \n",
      "Bayes\n",
      "SearchCV\n",
      " \n",
      " class performs Ba yesian optimiza tion using an in terface similar to \n",
      "Grid\n",
      "SearchCV\n",
      ".\n",
      "⁄\n",
      " Spearmin t\n",
      " : a Ba yesian optimiza tion librar y .\n",
      "⁄\n",
      " Sklearn-Dea p\n",
      " : a h yperparameter optimiza tion librar y based on evolutionar y\n",
      "algorithms, also with a \n",
      "GridSearchCV\n",
      " -like in terface.\n",
      "⁄\n",
      " And man y more!\n",
      " M oreover , man y com panies offer ser vices for h yperparameter optimiza tion. F or\n",
      " exam ple Google Cloud ML Engine has a \n",
      " h yperparameter tuning ser vice\n",
      ". Other comƒ\n",
      " panies provide API s for h yperparameter optimiza tion, such as \n",
      "Arimo\n",
      ", \n",
      "SigOpt\n",
      ", \n",
      "Oscar\n",
      " and man y more.\n",
      " H yperparameter tuning is still an active area of research. E volutionar y algorithms are\n",
      " making a comeback la tely . F or exam ple, check out DeepMind ‡ s excellen t \n",
      " 2017 pa per\n",
      "16\n",
      ",\n",
      " where they join tly optimize a popula tion of models and their h yperparameters. Gooƒ\n",
      " gle also used an evolutionar y a pproach, not just to search for h yperparameters, but\n",
      "also to look for the best neural network architecture for the problem. They call this\n",
      " Au t oML\n",
      " , and it is already a vailable as a \n",
      " cloud ser vice\n",
      " . P erha ps the da ys of building\n",
      " neural networks man ually will soon be over? Check out Google ‡ s \n",
      "post\n",
      "   on this topic. In\n",
      " fact, evolutionar y algorithms ha ve also been used successfully to train individual neuƒ\n",
      " ral networks, replacing the ubiquitous Gradien t Descen t! See this \n",
      "2017 post\n",
      " \n",
      "by Uber\n",
      " where they in troduce their \n",
      " D e e p N eu r o e v o l u t i o n\n",
      " technique.\n",
      " Despite all this exciting progress, and all these tools and ser vices, it still helps to ha ve\n",
      " an idea of wha t values are reasonable for each h yperparameter , so you can build a\n",
      " quick prototype, and restrict the search space. H ere are a few guidelines for choosing\n",
      " the n umber of hidden la yers and neurons in an MLP , and selecting good values for\n",
      " some of the main h yperparameters.\n",
      " 318  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "Number of Hidden Layers\n",
      " F or man y problems, you can just begin with a single hidden la yer and you will get\n",
      " reasonable results. I t has actually been shown tha t an MLP with just one hidden la yer\n",
      " can model even the most com plex functions provided it has enough neurons. F or a\n",
      " long time, these facts con vinced researchers tha t there was no need to in vestiga te an y\n",
      " deeper neural networks. But they overlooked the fact tha t deep networks ha ve a m uch\n",
      "higher \n",
      " p a r a m e t er \n",
      "e⁄ciency\n",
      "  than shallow ones: they can model com plex functions\n",
      " using exponen tially fewer neurons than shallow nets, allowing them to reach m uch\n",
      " better performance with the same amoun t of training da ta.\n",
      " T o understand wh y , suppose you are asked to dra w a forest using some dra wing softƒ\n",
      " ware, but you are forbidden to use copy/paste. Y ou would ha ve to dra w each tree\n",
      " individually , branch per branch, leaf per leaf. If you could instead dra w one leaf,\n",
      " copy/paste it to dra w a branch, then copy/paste tha t branch to crea te a tree, and\n",
      "finally copy/paste this tree to make a forest, you would be finished in no time. Real-\n",
      " world da ta is often structured in such a hierarchical wa y and Deep N eural N etworks\n",
      " a utoma tically take advan tage of this fact: lower hidden la yers model low-level strucƒ\n",
      " tures (e.g., line segmen ts of various sha pes and orien ta tions), in termedia te hidden\n",
      " la yers combine these low-level structures to model in termedia te-level structures (e.g.,\n",
      " squares, circles), and the highest hidden la yers and the output la yer combine these\n",
      " in termedia te structures to model high-level structures (e.g., faces).\n",
      " N ot only does this hierarchical architecture help DNN s con verge faster to a good solƒ\n",
      " ution, it also im proves their ability to generalize to new da tasets. F or exam ple, if you\n",
      " ha ve already trained a model to recognize faces in pictures, and you now wan t to\n",
      "train a new neural network to recognize hairstyles, then you can kickstart training by\n",
      " reusing the lower la yers of the first network. Instead of randomly initializing the\n",
      " weigh ts and biases of the first few la yers of the new neural network, you can initialize\n",
      " them to the value of the weigh ts and biases of the lower la yers of the first network.\n",
      " This wa y the network will not ha ve to learn from scra tch all the low-level structures\n",
      " tha t occur in most pictures; it will only ha ve to learn the higher -level structures (e.g.,\n",
      "hairstyles). This is called \n",
      " t r a ns f er l e a r n i n g\n",
      ".\n",
      " In summar y , for man y problems you can start with just one or two hidden la yers and\n",
      "it will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\n",
      " da taset using just one hidden la yer with a few h undred neurons, and above 98% accuƒ\n",
      " racy using two hidden la yers with the same total amoun t of neurons, in roughly the\n",
      " same amoun t of training time). F or more com plex problems, you can gradually ram p\n",
      " up the n umber of hidden la yers, un til you start overfitting the training set. V er y comƒ\n",
      " plex tasks, such as large image classifica tion or speech recognition, typically require\n",
      " networks with dozens of la yers (or even h undreds, but not fully connected ones, as\n",
      "we will see in \n",
      " Cha pter 14\n",
      " ), and they need a h uge amoun t of training da ta. H owever ,\n",
      " you will rarely ha ve to train such networks from scra tch: it is m uch more common to\n",
      " Fine-Tuning Neural Network Hyperparameters  |  319\n",
      "\n",
      "17\n",
      " By V incen t V anhoucke in his \n",
      "Deep Learning class\n",
      "  on U dacity .com.\n",
      " reuse parts of a pretrained sta te-of-the-art network tha t performs a similar task.\n",
      " T raining will be a lot faster and require m uch less da ta (we will discuss this in \n",
      " Cha pƒ\n",
      "ter 11\n",
      ").\n",
      "Number of Neurons per Hidden Layer\n",
      "Obviously \n",
      " the n umber of neurons in the in put and output la yers is determined by the\n",
      " type of in put and output your task requires. F or exam ple, the MNIST task requires 28\n",
      " x 28 = 784 in put neurons and 10 output neurons.\n",
      " As for the hidden la yers, it used to be a common practice to size them to form a pyraƒ\n",
      " mid, with fewer and fewer neurons a t each la yer ›the ra tionale being tha t man y low-\n",
      " level fea tures can coalesce in to far fewer high-level fea tures. F or exam ple, a typical\n",
      " neural network for MNIST ma y ha ve three hidden la yers, the first with 300 neurons,\n",
      " the second with 200, and the third with 100. H owever , this practice has been largely\n",
      " abandoned now , as it seems tha t sim ply using the same n umber of neurons in all hidƒ\n",
      " den la yers performs just as well in most cases, or even better , and there is just one\n",
      " h yperparameter to tune instead of one per la yer ›for exam ple, all hidden la yers could\n",
      " sim ply ha ve 150 neurons. H owever , depending on the da taset, it can sometimes help\n",
      " to make the first hidden la yer bigger than the others.\n",
      " J ust like for the n umber of la yers, you can tr y increasing the n umber of neurons gradƒ\n",
      " ually un til the network starts overfitting. In general you will get more bang for the\n",
      " buck by increasing the n umber of la yers than the n umber of neurons per la yer .\n",
      " U nfortuna tely , as you can see, finding the perfect amoun t of neurons is still somewha t\n",
      "of a dark art.\n",
      " A sim pler a pproach is to pick a model with more la yers and neurons than you\n",
      " actually need, then use early stopping to preven t it from overfitting (and other reguƒ\n",
      " lariza tion techniques, such as \n",
      " d r o p o u t\n",
      ", as we will see in \n",
      " Cha pter 11\n",
      "). This has been\n",
      " dubbed the — stretch pan ts – a pproach:\n",
      "17\n",
      "  instead of wasting time looking for pan ts tha t\n",
      " perfectly ma tch your size, just use large stretch pan ts tha t will shrink down to the\n",
      " righ t size.\n",
      "Learning Rate, Batch Size and Other Hyperparameters\n",
      " The n umber of hidden la yers and neurons are not the only h yperparameters you can\n",
      " tweak in an MLP . H ere are some of the most im portan t ones, and some tips on how\n",
      "to set them:\n",
      "⁄\n",
      " The learning ra te is arguably the most im portan t h yperparameter . In general, the\n",
      " optimal learning ra te is about half of the maxim um learning ra te (i.e., the learnƒ\n",
      " 320  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "18\n",
      " —Practical recommenda tions for gradien t-based training of deep architectures, – Y osh ua B engio (2012).\n",
      " ing ra te above which the training algorithm diverges, as we sa w in \n",
      " Cha pter 4\n",
      "). So\n",
      " a sim ple a pproach for tuning the learning ra te is to start with a large value tha t\n",
      " makes the training algorithm diverge, then divide this value by 3 and tr y again,\n",
      " and repea t un til the training algorithm stops diverging. A t tha t poin t, you generƒ\n",
      " ally won ‡ t be too far from the optimal learning ra te. Tha t said, it is sometimes\n",
      " useful to reduce the learning ra te during training: we will discuss this in \n",
      " Cha pƒ\n",
      "ter 11\n",
      ".\n",
      "⁄\n",
      " Choosing a better optimizer than plain old Mini-ba tch Gradien t Descen t (and\n",
      " tuning its h yperparameters) is also quite im portan t. W e will discuss this in \n",
      " Cha pƒ\n",
      "ter 11\n",
      ".\n",
      "⁄\n",
      " The ba tch size can also ha ve a significan t im pact on your model ‡ s performance\n",
      " and the training time. In general the optimal ba tch size will be lower than 32 (in\n",
      " A pril 2018, Y ann Lecun even tweeted \"\n",
      " F r i en d s d o n ‹ t l e t f r i en d s u s e m i n i-b a t c h e s\n",
      " l a r ger t h a n 32\n",
      " —). A small ba tch size ensures tha t each training itera tion is ver y\n",
      " fast, and although a large ba tch size will give a more precise estima te of the gradiƒ\n",
      " en ts, in practice this does not ma tter m uch since the optimiza tion landsca pe is\n",
      " quite com plex and the direction of the true gradien ts do not poin t precisely in\n",
      " the direction of the optim um. H owever , ha ving a ba tch size grea ter than 10 helps\n",
      " take advan tage of hardware and software optimiza tions, in particular for ma trix\n",
      " m ultiplica tions, so it will speed up training. M oreover , if you use \n",
      " B a t c h N o r m a l‡\n",
      " iz a t i o n\n",
      "   (see \n",
      " Cha pter 11\n",
      " ), the ba tch size should not be too small (in general no less\n",
      "than 20).\n",
      "⁄\n",
      " W e discussed the choice of the activa tion function earlier in this cha pter : in genƒ\n",
      " eral, the ReL U activa tion function will be a good defa ult for all hidden la yers. F or\n",
      " the output la yer , it really depends on your task.\n",
      "⁄\n",
      " In most cases, the n umber of training itera tions does not actually need to be\n",
      "tweaked: just use early stopping instead.\n",
      " F or more best practices, make sure to read Y osh ua B engio ‡ s grea t \n",
      " 2012 pa per\n",
      "18\n",
      ", which\n",
      " presen ts man y practical recommenda tions for deep networks.\n",
      " This concludes this in troduction to artificial neural networks and their im plemen taƒ\n",
      " tion with K eras. In the next few cha pters, we will discuss techniques to train ver y\n",
      " deep nets, we will see how to customize your models using T ensorFlow‡ s lower -level\n",
      " API and how to load and preprocess da ta efficien tly using the Da ta API, and we will\n",
      " dive in to other popular neural network architectures: con volutional neural networks\n",
      " for image processing, recurren t neural networks for sequen tial da ta, \n",
      " a utoencoders for\n",
      " Fine-Tuning Neural Network Hyperparameters  |  321\n",
      "\n",
      "19\n",
      " A few extra ANN architectures are presen ted in \n",
      "???\n",
      ".\n",
      " represen ta tion learning, and genera tive adversarial networks to model and genera te\n",
      " da ta.\n",
      "19\n",
      "Exercises\n",
      "1.\n",
      " V isit the T ensorFlow Pla yground a t \n",
      "https://playground.tensor…ow.org/\n",
      "⁄\n",
      " La yers and pa tterns: tr y training the defa ult neural network by clicking the run\n",
      " button (top left). N otice how it quickly finds a good solution for the classificaƒ\n",
      " tion task. N otice tha t the neurons in the first hidden la yer ha ve learned sim ple\n",
      " pa tterns, while the neurons in the second hidden la yer ha ve learned to comƒ\n",
      " bine the sim ple pa tterns of the first hidden la yer in to more com plex pa tterns.\n",
      " In general, the more la yers, the more com plex the pa tterns can be.\n",
      "⁄\n",
      " A ctiva tion function: tr y replacing the T anh activa tion function with the ReL U\n",
      " activa tion function, and train the network again. N otice tha t it finds a solution\n",
      " even faster , but this time the boundaries are linear . This is due to the sha pe of\n",
      " the ReL U function.\n",
      "⁄\n",
      " Local minima: modif y the network architecture to ha ve just one hidden la yer\n",
      " with three neurons. T rain it m ultiple times (to reset the network weigh ts, click\n",
      " the reset button next to the pla y button). N otice tha t the training time varies a\n",
      " lot, and sometimes it even gets stuck in a local minim um.\n",
      "⁄\n",
      " T oo small: now remove one neuron to keep just 2. N otice tha t the neural netƒ\n",
      " work is now inca pable of finding a good solution, even if you tr y m ultiple\n",
      " times. The model has too few parameters and it systema tically underfits the\n",
      "training set.\n",
      "⁄\n",
      " Large enough: next, set the n umber of neurons to 8 and train the network sevƒ\n",
      " eral times. N otice tha t it is now consisten tly fast and never gets stuck. This\n",
      " highligh ts an im portan t finding in neural network theor y : large neural netƒ\n",
      "works almost never get stuck in local minima, and even when they do these\n",
      " local optima are almost as good as the global optim um. H owever , they can still\n",
      " get stuck on long pla tea us for a long time.\n",
      "⁄\n",
      " Deep net and vanishing gradien ts: now change the da taset to be the spiral (botƒ\n",
      " tom righ t da taset under —D A T A –). Change the network architecture to ha ve 4\n",
      " hidden la yers with 8 neurons each. N otice tha t training takes m uch longer , and\n",
      " often gets stuck on pla tea us for long periods of time. Also notice tha t the neuƒ\n",
      " rons in the highest la yers (i.e. on the righ t) tend to evolve faster than the neuƒ\n",
      " rons in the lowest la yers (i.e. on the left). This problem, called the —vanishing\n",
      " gradien ts – problem, can be allevia ted using better weigh t initializa tion and\n",
      " 322  |  Chapter 10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      " other techniques, better optimizers (such as A daGrad or A dam), or using\n",
      " Ba tch N ormaliza tion.\n",
      "⁄\n",
      " M ore: go ahead and pla y with the other parameters to get a feel of wha t they\n",
      " do . In fact, you should definitely pla y with this UI for a t least one hour , it will\n",
      " grow your in tuitions about neural networks significan tly .\n",
      "2.\n",
      " Dra w an ANN using the original artificial neurons (like the ones in \n",
      "Figure 10-3\n",
      ")\n",
      " tha t com putes \n",
      "A\n",
      " \n",
      " \n",
      "B\n",
      "   (where \n",
      "  represen ts the X OR opera tion). Hin t: \n",
      "A\n",
      " \n",
      " \n",
      "B\n",
      "   = (\n",
      "A\n",
      " Ì \n",
      "B\n",
      ") \n",
      " (Ì \n",
      "A\n",
      " \n",
      " \n",
      "B\n",
      ").\n",
      "3.\n",
      " Wh y is it generally preferable to use a Logistic Regression classifier ra ther than a\n",
      " classical P erceptron (i.e., a single la yer of threshold logic units trained using the\n",
      " P erceptron training algorithm)? H ow can you tweak a P erceptron to make it\n",
      " equivalen t to a Logistic Regression classifier?\n",
      "4.\n",
      " Wh y was the logistic activa tion function a key ingredien t in training the first\n",
      " MLP s?\n",
      "5.\n",
      " N ame three popular activa tion functions. Can you dra w them?\n",
      "6.\n",
      " Suppose you ha ve an MLP com posed of one in put la yer with 10 passthrough\n",
      " neurons, followed by one hidden la yer with 50 artificial neurons, and finally one\n",
      " output la yer with 3 artificial neurons. All artificial neurons use the ReL U activaƒ\n",
      "tion function.\n",
      "⁄\n",
      " Wha t is the sha pe of the in put ma trix \n",
      "X\n",
      "?\n",
      "⁄\n",
      " Wha t about the sha pe of the hidden la yer‡ s weigh t vector \n",
      "W\n",
      "h\n",
      " , and the sha pe of\n",
      "its bias vector \n",
      "b\n",
      "h\n",
      "?\n",
      "⁄\n",
      " Wha t is the sha pe of the output la yer‡ s weigh t vector \n",
      "W\n",
      "o\n",
      ", and its bias vector \n",
      "b\n",
      "o\n",
      "?\n",
      "⁄\n",
      " Wha t is the sha pe of the network ‡ s output ma trix \n",
      "Y\n",
      "?\n",
      "⁄\n",
      " W rite the equa tion tha t com putes the network ‡ s output ma trix \n",
      "Y\n",
      " \n",
      "as a function\n",
      "of \n",
      "X\n",
      ", \n",
      "W\n",
      "h\n",
      ", \n",
      "b\n",
      "h\n",
      ", \n",
      "W\n",
      "o\n",
      " and \n",
      "b\n",
      "o\n",
      ".\n",
      "7.\n",
      " H ow man y neurons do you need in the output la yer if you wan t to classif y email\n",
      " in to spam or ham? Wha t activa tion function should you use in the output la yer?\n",
      " If instead you wan t to tackle MNIST , how man y neurons do you need in the outƒ\n",
      " put la yer , using wha t activa tion function? Answer the same questions for getting\n",
      "your network to predict housing prices as in \n",
      " Cha pter 2\n",
      ".\n",
      "8.\n",
      " Wha t is backpropaga tion and how does it work? Wha t is the difference between\n",
      " backpropaga tion and reverse-mode a utodiff ?\n",
      "9.\n",
      " Can you list all the h yperparameters you can tweak in an MLP? If the MLP overƒ\n",
      " fits the training da ta, how could you tweak these h yperparameters to tr y to solve\n",
      "the problem?\n",
      " Exercises  |  323\n",
      "\n",
      "10.\n",
      " T rain a deep MLP on the MNIST da taset and see if you can get over 98% preciƒ\n",
      " sion. T r y adding all the bells and whistles (i.e., sa ve checkpoin ts, use early stopƒ\n",
      " ping, plot learning cur ves using T ensorB oard, and so on).\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " 324   |  Chapter  10: Introduction to \n",
      "Arti•cial\n",
      " Neural Networks with Keras\n",
      "\n",
      "CHAPTER 11\n",
      "Training Deep Neural Networks\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 11 in the final\n",
      "release of the book.\n",
      "In \n",
      " Cha pter 10\n",
      "  we in troduced artificial neural networks and trained our first deep\n",
      " neural networks. But they were ver y shallow nets, with just a few hidden la yers. Wha t\n",
      " if you need to tackle a ver y com plex problem, such as detecting h undreds of types of\n",
      " objects in high-resolution images? Y ou ma y need to train a m uch deeper DNN, perƒ\n",
      " ha ps with 10 la yers or m uch more, each con taining h undreds of neurons, connected\n",
      " by h undreds of thousands of connections. This would not be a walk in the park:\n",
      "⁄\n",
      "First, you would be faced with the tricky \n",
      " v a n i s h i n g g r a d i en ts\n",
      " \n",
      "problem (or the\n",
      " rela ted \n",
      " exp l o d i n g g r a d i en ts\n",
      " \n",
      " problem) tha t affects deep neural networks and makes\n",
      " lower la yers ver y hard to train.\n",
      "⁄\n",
      " Second, you migh t not ha ve enough training da ta for such a large network, or it\n",
      " migh t be too costly to label.\n",
      "⁄\n",
      " Third, training ma y be extremely slow .\n",
      "⁄\n",
      " F ourth, a model with millions of parameters would severely risk overfitting the\n",
      "training set, especially if there are not enough training instances, or they are too\n",
      " noisy .\n",
      " In this cha pter , we will go through each of these problems in turn and presen t techniƒ\n",
      " ques to solve them. W e will start by explaining the vanishing gradien ts problem and\n",
      " exploring some of the most popular solutions to this problem. N ext, we will look a t\n",
      " transfer learning and unsuper vised pretraining, which can help you tackle com plex\n",
      "325\n",
      "\n",
      "1\n",
      " —U nderstanding the Difficulty of T raining Deep F eedfor ward N eural N etworks, – X. Glorot, Y B engio (2010).\n",
      " tasks even when you ha ve little labeled da ta. Then we will discuss various optimizers\n",
      " tha t can speed up training large models tremendously com pared to plain \n",
      " Gradien t\n",
      " Descen t. Finally , we will go through a few popular regulariza tion techniques for large\n",
      "neural networks.\n",
      " W ith these tools, you will be able to train ver y deep nets: welcome to Deep Learning!\n",
      "Vanishing/Exploding Gradients Problems\n",
      "As we discussed in \n",
      " Cha pter 10\n",
      " , the backpropaga tion algorithm works by going from\n",
      " the output la yer to the in put la yer , propaga ting the error gradien t on the wa y . Once\n",
      " the algorithm has com puted the gradien t of the cost function with regards to each\n",
      " parameter in the network, it uses these gradien ts to upda te each parameter with a\n",
      " Gradien t Descen t step .\n",
      " U nfortuna tely , gradien ts often get smaller and smaller as the algorithm progresses\n",
      " down to the lower la yers. As a result, the Gradien t Descen t upda te lea ves the lower\n",
      " la yer connection weigh ts virtually unchanged, and training never con verges to a good\n",
      "solution. This is called the \n",
      " v a n i s h i n g g r a d i en ts\n",
      " problem. In some cases, the opposite\n",
      " can ha ppen: the gradien ts can grow bigger and bigger , so man y la yers get insanely\n",
      " large weigh t upda tes and the algorithm diverges. This is the \n",
      " exp l o d i n g g r a d i en ts\n",
      "   probƒ\n",
      "lem, \n",
      " which is mostly encoun tered in recurren t neural networks (see \n",
      "???\n",
      " ). M ore generƒ\n",
      " ally , \n",
      " deep neural networks suffer from unstable gradien ts; differen t la yers ma y learn a t\n",
      " widely differen t speeds.\n",
      " Although this unfortuna te beha vior has been em pirically obser ved for quite a while\n",
      " (it was one of the reasons wh y deep neural networks were mostly abandoned for a\n",
      " long time), it is only around 2010 tha t significan t progress was made in understandƒ\n",
      " ing it. A pa per titled \n",
      " —U nderstanding the Difficulty of T raining Deep F eedfor ward\n",
      " N eural N etworks –\n",
      " \n",
      " by Xa vier Glorot and Y osh ua B engio\n",
      "1\n",
      " \n",
      "found a few suspects, includƒ\n",
      " ing the combina tion of the popular logistic sigmoid activa tion function and the\n",
      " weigh t initializa tion technique tha t was most popular a t the time, namely  random iniƒ\n",
      " tializa tion using a normal distribution with a mean of 0 and a standard devia tion of 1.\n",
      " In short, they showed tha t with this activa tion function and this initializa tion scheme,\n",
      " the variance of the outputs of each la yer is m uch grea ter than the variance of its\n",
      " in puts. Going for ward in the network, the variance keeps increasing after each la yer\n",
      " un til the activa tion function sa tura tes a t the top la yers. This is actually made worse by\n",
      " the fact tha t the logistic function has a mean of 0.5, not 0 (the h yperbolic tangen t\n",
      " function has a mean of 0 and beha ves sligh tly better than the logistic function in deep\n",
      "networks).\n",
      " 326  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "2\n",
      " H ere ‡ s an analog y : if you set a microphone am plifier‡ s knob too close to zero , people won ‡ t hear your voice, but\n",
      " if you set it too close to the max, your voice will be sa tura ted and people won ‡ t understand wha t you are sa yƒ\n",
      " ing. N ow imagine a chain of such am plifiers: they all need to be set properly in order for your voice to come\n",
      " out loud and clear a t the end of the chain. Y our voice has to come out of each am plifier a t the same am plitude\n",
      "as it came in.\n",
      " Looking a t the logistic activa tion function (see \n",
      "Figure 11-1\n",
      " ), you can see tha t when\n",
      " in puts become large (nega tive or positive), the function sa tura tes a t 0 or 1, with a\n",
      " deriva tive extremely close to 0. Th us when backpropaga tion kicks in, it has virtually\n",
      " no gradien t to propaga te back through the network, and wha t little gradien t exists\n",
      " keeps getting diluted as backpropaga tion progresses down through the top la yers, so\n",
      " there is really nothing left for the lower la yers.\n",
      " F i g u r e 11-1. L og i s t i c a c t i v a t i o n f u n c t i o n s a t u r a t i o n\n",
      "Glorot and He Initialization\n",
      " In their pa per , Glorot and B engio propose a wa y to significan tly allevia te this probƒ\n",
      " lem. W e need the signal to flow properly in both directions: in the for ward direction\n",
      " when making predictions, and in the reverse direction when backpropaga ting gradiƒ\n",
      " en ts. W e don ‡ t wan t the signal to die out, nor do we wan t it to explode and sa tura te.\n",
      " F or the signal to flow properly , the a uthors argue tha t we need the variance of the\n",
      " outputs of each la yer to be equal to the variance of its in puts,\n",
      "2\n",
      " and we also need the\n",
      " gradien ts to ha ve equal variance before and after flowing through a la yer in the\n",
      " reverse direction (please check out the pa per if you are in terested in the ma thema tical\n",
      " details). I t is actually not possible to guaran tee both unless the la yer has an equal\n",
      " n umber of in puts and neurons (these n umbers are called the \n",
      " f a n-i n\n",
      "   and \n",
      " f a n-o u t\n",
      "   of the\n",
      " la yer), but they proposed a good com promise tha t has proven to work ver y well in\n",
      " practice: the connection weigh ts of each la yer m ust be initialized randomly as\n",
      " Vanishing/Exploding Gradients Problems  |  327\n",
      "\n",
      "3\n",
      " Such as —Delving Deep in to Rectifiers: Surpassing H uman-Level P erformance on ImageN et Classifica tion, – K.\n",
      " H e et al. (2015).\n",
      "described in \n",
      " Equa tion 11-1\n",
      ", where \n",
      " f an\n",
      "avg\n",
      "=\n",
      " f an\n",
      "in\n",
      "+\n",
      " f an\n",
      "out\n",
      " / 2\n",
      " . This initializa tion\n",
      " stra teg y is called \n",
      " X a v i er i n i t i a l iz a t i o n\n",
      "  (after the a uthor‡ s first name) or \n",
      " G l o r o t i n i t i a l i‡\n",
      " z a t i o n\n",
      " (after his last name).\n",
      " Eq u a t i o n 11-1. G l o r o t i n i t i a l iz a t i o n (w h en u s i n g t h e l og i s t i c a c t i v a t i o n f u n c t i o n)\n",
      "Normal distribution with mean 0 and varianceŁ\n",
      "„\n",
      "2\n",
      "=\n",
      "1\n",
      "fan\n",
      "avg\n",
      "Or a uniform distribution betweenŁ”\n",
      "r\n",
      " ŁandŁ +\n",
      "r\n",
      ", withŁ\n",
      "r\n",
      "=\n",
      "3\n",
      "fan\n",
      "avg\n",
      "If you just replace \n",
      " f a n\n",
      " a vg\n",
      " with \n",
      " f a n\n",
      "in\n",
      " in \n",
      " Equa tion 11-1\n",
      " , you get an initializa tion stra teg y\n",
      " tha t was actually already proposed by Y ann LeCun in the 1990s, \n",
      "called \n",
      " L eC u n i n i t i a l i‡\n",
      " z a t i o n\n",
      ", which was even recommended in the 1998 book \n",
      " N eu r a l N e tw o r ks: T r i c ks o f t h e\n",
      " T r a d e\n",
      "  by Genevieve Orr and Kla us-Robert M ﬁller (Springer). I t is equivalen t to\n",
      " Glorot initializa tion when \n",
      " f a n\n",
      "in\n",
      "   = \n",
      " f a n\n",
      "out\n",
      " . I t took over a decade for researchers to realize\n",
      " just how im portan t this trick really is. U sing Glorot initializa tion can speed up trainƒ\n",
      " ing considerably , and it is one of the tricks tha t led to the curren t success of Deep\n",
      "Learning.\n",
      "Some \n",
      " pa pers\n",
      "3\n",
      "  ha ve provided similar stra tegies for differen t activa tion functions.\n",
      " These stra tegies differ only by the scale of the variance and whether they use \n",
      " f a n\n",
      " a vg\n",
      "   or\n",
      " f a n\n",
      "in\n",
      ", as shown in \n",
      " T able 11-1\n",
      "  (for the uniform distribution, just com pute \n",
      "r\n",
      "=\n",
      "3\n",
      "„\n",
      "2\n",
      ").\n",
      " The initializa tion stra teg y for the ReL U activa tion function (and its varian ts, includƒ\n",
      " ing the EL U activa tion described shortly) is sometimes \n",
      "called \n",
      " H e i n i t i a l iz a t i o n\n",
      " \n",
      "(after\n",
      " the last name of its a uthor). The SEL U activa tion function will be explained la ter in\n",
      " this cha pter . I t should be used with LeCun initializa tion (preferably with a normal\n",
      "distribution, as we will see).\n",
      " T a b l e 11-1. I n i t i a l iz a t i o n p a r a m e t er s f o r e a c h ty p e o f a c t i v a t i o n f u n c t i o n\n",
      "Initialization\n",
      " Activation   functions\n",
      "•\n",
      " —   (Normal)\n",
      "Glorot\n",
      " None,   Tanh,   Logistic,   Softmax\n",
      " 1   /  \n",
      "fan\n",
      "avg\n",
      "He\n",
      " ReLU   &   variants\n",
      " 2   /  \n",
      "fan\n",
      "in\n",
      "LeCun\n",
      "SELU\n",
      " 1   /  \n",
      "fan\n",
      "in\n",
      " By defa ult, K eras uses Glorot initializa tion with a uniform distribution. Y ou can\n",
      " change this to H e initializa tion by setting \n",
      "kernel_initializer=\"he_uniform\"\n",
      "   or \n",
      "ker\n",
      "nel_initializer=\"he_normal\"\n",
      "  when crea ting a la yer , like this:\n",
      " 328  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "4\n",
      " U nless it is part of the first hidden la yer , a dead neuron ma y sometimes come back to life: gradien t descen t\n",
      " ma y indeed tweak neurons in the la yers below in such a wa y tha t the weigh ted sum of the dead neuron ‡ s\n",
      " in puts is positive again.\n",
      "5\n",
      " —Em pirical E valua tion of Rectified A ctiva tions in Con volution N etwork, – B . X u et al. (2015).\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ")\n",
      " If you wan t H e initializa tion with a uniform distribution, but based on \n",
      " f a n\n",
      " a vg\n",
      " \n",
      " ra ther\n",
      "than \n",
      " f a n\n",
      "in\n",
      ", you can use the \n",
      "VarianceScaling\n",
      " initializer like this:\n",
      "he_avg_init\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "initializers\n",
      ".\n",
      "VarianceScaling\n",
      "(\n",
      "scale\n",
      "=\n",
      "2.\n",
      ",\n",
      " \n",
      "mode\n",
      "=\n",
      "•fan_avg•\n",
      ",\n",
      "                                                 \n",
      "distribution\n",
      "=\n",
      "•uniform•\n",
      ")\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"sigmoid\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "he_avg_init\n",
      ")\n",
      "Nonsaturating Activation Functions\n",
      " One of the insigh ts in the 2010 pa per by Glorot and B engio was tha t the vanishing/\n",
      " exploding gradien ts problems were in part due to a poor choice of activa tion funcƒ\n",
      " tion. U n til then most people had assumed tha t if M other N a ture had chosen to use\n",
      " roughly sigmoid activa tion functions in biological neurons, they m ust be an excellen t\n",
      " choice. But it turns out tha t other activa tion functions beha ve m uch better in deep\n",
      " neural networks, in particular the ReL U activa tion function, mostly beca use it does\n",
      " not sa tura te for positive values (and also beca use it is quite fast to com pute).\n",
      " U nfortuna tely , the ReL U activa tion function is not perfect. I t suffers from a problem\n",
      "known as the \n",
      " d y i n g R eL U s\n",
      ": during training, some neurons effectively die, meaning\n",
      " they stop outputting an ything other than 0. In some cases, you ma y find tha t half of\n",
      " your network ‡ s neurons are dead, especially if you used a large learning ra te. A neuƒ\n",
      " ron dies when its weigh ts get tweaked in such a wa y tha t the weigh ted sum of its\n",
      " in puts are nega tive for all instances in the training set. When this ha ppens, it just\n",
      " keeps outputting 0s, and gradien t descen t does not affect it an ymore since the gradiƒ\n",
      " en t of the ReL U function is 0 when its in put is nega tive.\n",
      "4\n",
      " T o solve this problem, you ma y wan t to use a varian t of the ReL U function, such as\n",
      "the \n",
      " l e a k y R eL U\n",
      " . This function is defined as LeakyReL U\n",
      "‰\n",
      "(\n",
      "z\n",
      ") = max(\n",
      "‰z\n",
      ", \n",
      "z\n",
      ") (see\n",
      "Figure 11-2\n",
      " ). The h yperparameter \n",
      "‰\n",
      "  defines how m uch the function —leaks –: it is the\n",
      "slope of the function for \n",
      "z\n",
      " < 0, and is typically set to 0.01. This small slope ensures\n",
      " tha t leaky ReL U s never die; they can go in to a long coma, but they ha ve a chance to\n",
      " even tually wake up . A \n",
      " 2015 pa per\n",
      "5\n",
      "  com pared several varian ts of the ReL U activa tion\n",
      " function and one of its conclusions was tha t the leaky varian ts alwa ys outperformed\n",
      " the strict ReL U activa tion function. In fact, setting \n",
      "‰\n",
      "  = 0.2 (h uge leak) seemed to\n",
      "result in better performance than \n",
      "‰\n",
      "  = 0.01 (small leak). They also evalua ted the\n",
      " r a n d o m iz e d l e a k y R eL U\n",
      "   (RReL U), where \n",
      "‰\n",
      "   is picked randomly in a given range during\n",
      " training, and it is fixed to an a verage value during testing. I t also performed fairly well\n",
      "and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
      " Vanishing/Exploding Gradients Problems  |  329\n",
      "\n",
      "6\n",
      " —F ast and A ccura te Deep N etwork Learning by Exponen tial Linear U nits (EL U s), – D . Clevert, T . U n terthiner ,\n",
      " S. H ochreiter (2015).\n",
      " Finally , they also evalua ted the \n",
      " p a r a m e t r i c l e a k y R eL U\n",
      "   (PReL U),  where \n",
      "‰\n",
      "   is a uthorized\n",
      " to be learned during training (instead of being a h yperparameter , it becomes a\n",
      " parameter tha t can be modified by backpropaga tion like an y other parameter). This\n",
      " was reported to strongly outperform ReL U on large image da tasets, but on smaller\n",
      " da tasets it runs the risk of overfitting the training set.\n",
      " F i g u r e 11-2. L e a k y R eL U\n",
      "Last but not least, a \n",
      " 2015 pa per\n",
      " by Djork-Arn• Clevert et al.\n",
      "6\n",
      " \n",
      "proposed a new activaƒ\n",
      "tion function called the \n",
      " exp o n en t i a l l i n e a r u n i t\n",
      "   (EL U) \n",
      " tha t outperformed all the ReL U\n",
      " varian ts in their experimen ts: training time was reduced and the neural network perƒ\n",
      " formed better on the test set. I t is represen ted in \n",
      "Figure 11-3\n",
      ", and \n",
      " Equa tion 11-2\n",
      "shows its definition.\n",
      " Eq u a t i o n 11-2. EL U a c t i v a t i o n f u n c t i o n\n",
      "ELU\n",
      "‰\n",
      "z\n",
      "=\n",
      "‰\n",
      "exp\n",
      "z\n",
      " ” 1\n",
      "if\n",
      "z\n",
      " < 0\n",
      "z\n",
      "if\n",
      "z\n",
      " Ž 0\n",
      " 330  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "7\n",
      " — Self-N ormalizing N eural N etworks, \" G. Klamba uer , T . U n terthiner and A. M a yr (2017).\n",
      " F i g u r e 11-3. EL U a c t i v a t i o n f u n c t i o n\n",
      " I t looks a lot like the ReL U function, with a few ma jor differences:\n",
      "⁄\n",
      " First it takes on nega tive values when \n",
      "z\n",
      "  < 0, which allows the unit to ha ve an\n",
      " a verage output closer to 0. This helps allevia te the vanishing gradien ts problem,\n",
      " as discussed earlier . The h yperparameter \n",
      "‰\n",
      "  defines the value tha t the EL U funcƒ\n",
      " tion a pproaches when \n",
      "z\n",
      "  is a large nega tive n umber . I t is usually set to 1, but you\n",
      " can tweak it like an y other h yperparameter if you wan t.\n",
      "⁄\n",
      " Second, it has a nonzero gradien t for \n",
      "z\n",
      " \n",
      " < 0, which a voids the dead neurons probƒ\n",
      "lem.\n",
      "⁄\n",
      "Third, if \n",
      "‰\n",
      "  is equal to 1 then the function is smooth ever ywhere, including\n",
      "around \n",
      "z\n",
      " \n",
      " = 0, which helps speed up Gradien t Descen t, since it does not bounce as\n",
      " m uch left and righ t of \n",
      "z\n",
      " = 0.\n",
      " The main dra wback of the EL U activa tion function is tha t it is slower to com pute\n",
      " than the ReL U and its varian ts (due to the use of the exponen tial function), but durƒ\n",
      " ing training this is com pensa ted by the faster con vergence ra te. H owever , a t test time\n",
      " an EL U network will be slower than a ReL U network.\n",
      " M oreover , in a \n",
      " 2017 pa per\n",
      "7\n",
      "  by Gﬁn ter Klamba uer et al., called — Self-N ormalizing\n",
      " N eural N etworks – , the a uthors showed tha t if you build a neural network com posed\n",
      " exclusively of a stack of dense la yers, and if all hidden la yers use the SEL U activa tion\n",
      " function (which is just a scaled version of the EL U activa tion function, as its name\n",
      "suggests), then the network will \n",
      " s e l f-n o r m a l iz e\n",
      " : the output of each la yer will tend to\n",
      " preser ve mean 0 and standard devia tion 1 during training, which solves the vanishƒ\n",
      " ing/exploding gradien ts problem. As a result, this activa tion function often outperƒ\n",
      " Vanishing/Exploding Gradients Problems  |  331\n",
      "\n",
      " forms other activa tion functions ver y significan tly for such neural nets (especially\n",
      " deep ones). H owever , there are a few conditions for self-normaliza tion to ha ppen:\n",
      "⁄\n",
      " The in put fea tures m ust be standardized (mean 0 and standard devia tion 1).\n",
      "⁄\n",
      " E ver y hidden la yer‡ s weigh ts m ust also be initialized using LeCun normal initialiƒ\n",
      " za tion. In K eras, this means setting \n",
      "kernel_initializer=\"lecun_normal\"\n",
      ".\n",
      "⁄\n",
      " The network ‡ s architecture m ust be sequen tial. U nfortuna tely , if you tr y to use\n",
      " SEL U in non-sequen tial architectures, such as recurren t networks (see \n",
      "???\n",
      ") or\n",
      "networks with \n",
      " s k i p c o n n e c t i o ns\n",
      "  (i.e., connections tha t skip la yers, such as in wide\n",
      " & deep nets), self-normaliza tion will not be guaran teed, so SEL U will not necesƒ\n",
      " sarily outperform other activa tion functions.\n",
      "⁄\n",
      " The pa per only guaran tees self-normaliza tion if all la yers are dense. H owever , in\n",
      " practice the SEL U activa tion function seems to work grea t with con volutional\n",
      "neural nets as well (see \n",
      " Cha pter 14\n",
      ").\n",
      " So which activa tion function should you use for the hidden la yers\n",
      " of your deep neural networks? Although your mileage will var y , in\n",
      " general SEL U > EL U > leaky ReL U (and its varian ts) > ReL U > tanh\n",
      " > logistic. If the network ‡ s architecture preven ts it from self-\n",
      " normalizing, then EL U ma y perform better than SEL U (since SEL U\n",
      " is not smooth a t \n",
      "z\n",
      "  = 0). If you care a lot about run time la tency , then\n",
      " you ma y prefer leaky ReL U . If you don ‡ t wan t to tweak yet another\n",
      " h yperparameter , you ma y just use the defa ult \n",
      "‰\n",
      " values used by\n",
      " K eras (e.g., 0.3 for the leaky ReL U). If you ha ve spare time and\n",
      " com puting power , you can use cross-valida tion to evalua te other\n",
      " activa tion functions, in particular RReL U if your network is overƒ\n",
      " fitting, or PReL U if you ha ve a h uge training set.\n",
      " T o use the leaky ReL U activa tion function, you m ust crea te a \n",
      "LeakyReLU\n",
      "   instance like\n",
      "this:\n",
      "leaky_relu\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "LeakyReLU\n",
      "(\n",
      "alpha\n",
      "=\n",
      "0.2\n",
      ")\n",
      "layer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "leaky_relu\n",
      ",\n",
      "                           \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ")\n",
      " F or PReL U , just replace \n",
      "LeakyRelu(alpha=0.2)\n",
      " with \n",
      "PReLU()\n",
      " . There is curren tly no\n",
      " official im plemen ta tion of RReL U in K eras, but you can fairly easily im plemen t your\n",
      " own (see the exercises a t the end of \n",
      " Cha pter 12\n",
      ").\n",
      " F or SEL U activa tion, just set \n",
      "activation=\"selu\"\n",
      " and \n",
      "kernel_initial\n",
      "izer=\"lecun_normal\"\n",
      "  when crea ting a la yer :\n",
      "layer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"selu\"\n",
      ",\n",
      "                           \n",
      "kernel_initializer\n",
      "=\n",
      "\"lecun_normal\"\n",
      ")\n",
      " 332  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "8\n",
      " —Ba tch N ormaliza tion: A ccelera ting Deep N etwork T raining by Reducing In ternal Covaria te Shift, – S. I offe\n",
      "and C. Szegedy (2015).\n",
      "Batch Normalization\n",
      "Although \n",
      " using H e initializa tion along with EL U (or an y varian t of ReL U) can signifiƒ\n",
      " can tly reduce the vanishing/exploding gradien ts problems a t the beginning of trainƒ\n",
      " ing, it doesn ‡ t guaran tee tha t they won ‡ t come back during training.\n",
      "In a \n",
      " 2015 pa per\n",
      ",\n",
      "8\n",
      "  Sergey I offe and Christian Szegedy proposed a technique called\n",
      " B a t c h N o r m a l iz a t i o n\n",
      "  (BN) to address the vanishing/exploding gradien ts problems.\n",
      " The technique consists of adding an opera tion in the model just before or after the\n",
      " activa tion function of each hidden la yer , sim ply zero-cen tering and normalizing each\n",
      " in put, then scaling and shifting the result using two new parameter vectors per la yer :\n",
      " one for scaling, the other for shifting. In other words, this opera tion lets the model\n",
      " learn the optimal scale and mean of each of the la yer‡ s in puts. In man y cases, if you\n",
      " add a BN la yer as the ver y first la yer of your neural network, you do not need to\n",
      "standardize your training set (e.g., using a \n",
      "StandardScaler\n",
      " ): the BN la yer will do it\n",
      " for you (well, a pproxima tely , since it only looks a t one ba tch a t a time, and it can also\n",
      " rescale and shift each in put fea ture).\n",
      " In order to zero-cen ter and normalize the in puts, the algorithm needs to estima te\n",
      " each in put ‡ s mean and standard devia tion. I t does so by evalua ting the mean and stanƒ\n",
      " dard devia tion of each in put over the curren t mini-ba tch (hence the name —Ba tch\n",
      " N ormaliza tion –). The whole opera tion is summarized in \n",
      " Equa tion 11-3\n",
      ".\n",
      " Eq u a t i o n 11-3. B a t c h N o r m a l iz a t i o n a l go r i t h m\n",
      " 1 .\n",
      "—\n",
      "B\n",
      "=\n",
      "1\n",
      "m\n",
      "B\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "B\n",
      "x\n",
      "i\n",
      " 2 .\n",
      "⁄\n",
      "B\n",
      "2\n",
      "=\n",
      "1\n",
      "m\n",
      "B\n",
      "“\n",
      "i\n",
      " = 1\n",
      "m\n",
      "B\n",
      "x\n",
      "i\n",
      "”\n",
      "—\n",
      "B\n",
      "2\n",
      " 3 .\n",
      "x\n",
      "i\n",
      "=\n",
      "x\n",
      "i\n",
      "”\n",
      "—\n",
      "B\n",
      "⁄\n",
      "B\n",
      "2\n",
      "+\n",
      " 4 .\n",
      "z\n",
      "i\n",
      "=\n",
      "‹\n",
      "x\n",
      "i\n",
      "+\n",
      "›\n",
      "⁄\n",
      "—\n",
      "B\n",
      "  is the vector of in put means, evalua ted over the whole mini-ba tch \n",
      "B\n",
      " \n",
      "(it conƒ\n",
      " tains one mean per in put).\n",
      " Vanishing/Exploding Gradients Problems  |  333\n",
      "\n",
      "⁄\n",
      "⁄\n",
      "B\n",
      "  is the vector of in put standard devia tions, also evalua ted over the whole mini-\n",
      " ba tch (it con tains one standard devia tion per in put).\n",
      "⁄\n",
      "m\n",
      "B\n",
      "  is the n umber of instances in the mini-ba tch.\n",
      "⁄\n",
      "x\n",
      "(i)\n",
      "  is the vector of zero-cen tered and normalized in puts for instance \n",
      "i\n",
      ".\n",
      "⁄\n",
      "‹\n",
      " \n",
      " is the output scale parameter vector for the la yer (it con tains one scale parameƒ\n",
      " ter per in put).\n",
      "⁄\n",
      "  represen ts elemen t-wise m ultiplica tion (each in put is m ultiplied by its correƒ\n",
      "sponding output scale parameter).\n",
      "⁄\n",
      "›\n",
      "  is the output shift (offset) parameter vector for the la yer (it con tains one offset\n",
      " parameter per in put). Each in put is offset by its corresponding shift parameter .\n",
      "⁄\n",
      "  is a tin y n umber to a void division by zero (typically 10\n",
      "−5\n",
      "). This is called \n",
      "a\n",
      " s m o o t h i n g t er m\n",
      ".\n",
      "⁄\n",
      "z\n",
      "(i)\n",
      "  is the output of the BN opera tion: it is a rescaled and shifted version of the\n",
      " in puts.\n",
      " So during training, BN just standardizes its in puts then rescales and offsets them.\n",
      " Good! Wha t about a t test time? W ell it is not tha t sim ple. Indeed, we ma y need to\n",
      " make predictions for individual instances ra ther than for ba tches of instances: in this\n",
      " case, we will ha ve no wa y to com pute each in put ‡ s mean and standard devia tion.\n",
      " M oreover , even if we do ha ve a ba tch of instances, it ma y be too small, or the instanƒ\n",
      " ces ma y not be independen t and iden tically distributed (IID), so com puting sta tistics\n",
      " over the ba tch instances would be unreliable (during training, the ba tches should not\n",
      " be too small, if possible more than 30 instances, and all instances should be IID , as we\n",
      " sa w in \n",
      " Cha pter 4\n",
      " ). One solution could be to wait un til the end of training, then run\n",
      " the whole training set through the neural network, and com pute the mean and stanƒ\n",
      " dard devia tion of each in put of the BN la yer . These — final – in put means and standard\n",
      " devia tions can then be used instead of the ba tch in put means and standard devia tions\n",
      " when making predictions. H owever , it is often preferred to estima te these final sta tisƒ\n",
      " tics during training using a moving a verage of the la yer‡ s in put means and standard\n",
      " devia tions. T o sum up , four parameter vectors are learned in each ba tch-normalized\n",
      " la yer : \n",
      "‹\n",
      " (the ouput scale vector) and \n",
      "›\n",
      " \n",
      "(the output offset vector) are learned through\n",
      " regular backpropaga tion, and \n",
      "—\n",
      "  (the final in put mean vector), and \n",
      "⁄\n",
      " \n",
      " (the final in put\n",
      " standard devia tion vector) are estima ted using an exponen tial moving a verage. N ote\n",
      " tha t \n",
      "—\n",
      " and \n",
      "⁄\n",
      "  are estima ted during training, but they are not used a t all during trainƒ\n",
      " ing, only after training (to replace the ba tch in put means and standard devia tions in\n",
      " Equa tion 11-3\n",
      ").\n",
      " The a uthors demonstra ted tha t this technique considerably im proved all the deep\n",
      " neural networks they experimen ted with, leading to a h uge im provemen t in the\n",
      " ImageN et classifica tion task (ImageN et is a large da tabase of images classified in to\n",
      " man y classes and commonly used to evalua te com puter vision systems). The vanishƒ\n",
      " 334  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " ing gradien ts problem was strongly reduced, to the poin t tha t they could use sa tura tƒ\n",
      " ing activa tion functions such as the tanh and even the logistic activa tion function.\n",
      " The networks were also m uch less sensitive to the weigh t initializa tion. They were\n",
      " able to use m uch larger learning ra tes, significan tly speeding up the learning process.\n",
      " Specifically , they note tha t — A pplied to a sta te-of-the-art image classifica tion model,\n",
      " Ba tch N ormaliza tion achieves the same accuracy with 14 times fewer training steps,\n",
      " and bea ts the original model by a significan t margin. [Ñ] U sing an ensemble of\n",
      " ba tch-normalized networks, we im prove upon the best published result on ImageN et\n",
      " classifica tion: reaching 4.9% top-5 valida tion error (and 4.8% test error), exceeding\n",
      " the accuracy of h uman ra ters. – Finally , like a gift tha t keeps on giving, Ba tch N ormalƒ\n",
      " iza tion also acts like a regularizer , reducing the need for other regulariza tion techniƒ\n",
      " ques (such as dropout, described la ter in this cha pter).\n",
      " Ba tch N ormaliza tion does, however , add some com plexity to the model (although it\n",
      " can remove the need for normalizing the in put da ta, as we discussed earlier). M oreƒ\n",
      " over , there is a run time penalty : the neural network makes slower predictions due to\n",
      " the extra com puta tions required a t each la yer . So if you need predictions to be\n",
      " ligh tning-fast, you ma y wan t to check how well plain EL U + H e initializa tion perform\n",
      " before pla ying with Ba tch N ormaliza tion.\n",
      " Y ou ma y find tha t training is ra ther slow , beca use each epoch takes\n",
      " m uch more time when you use ba tch normaliza tion. H owever , this\n",
      " is usually coun terbalanced by the fact tha t con vergence is m uch\n",
      "faster with BN, so it will take fewer epochs to reach the same perƒ\n",
      "formance. All in all, \n",
      " w a l l t i m e\n",
      " will usually be smaller (this is the\n",
      "time measured by the clock on your wall).\n",
      "Implementing Batch Normalization with Keras\n",
      " As with most things with K eras, im plemen ting Ba tch N ormaliza tion is quite sim ple.\n",
      " J ust add a \n",
      "BatchNormalization\n",
      "  la yer before or after each hidden la yer‡ s activa tion\n",
      " function, and optionally add a BN la yer as well as the first la yer in your model. F or\n",
      " exam ple, this model a pplies BN after ever y hidden la yer and as the first la yer in the\n",
      " model (after fla ttening the in put images):\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(\n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      "]),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "300\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ")\n",
      "])\n",
      " Vanishing/Exploding Gradients Problems  |  335\n",
      "\n",
      "9\n",
      " H owever , they are estima ted during training, based on the training da ta, so arguably they \n",
      " a r e\n",
      " trainable. In\n",
      " K eras, —N on-trainable – really means — un touched by backpropaga tion – .\n",
      " Tha t ‡ s all! In this tin y exam ple with just two hidden la yers, it ‡ s unlikely tha t Ba tch\n",
      " N ormaliza tion will ha ve a ver y positive im pact, but for deeper networks it can make a\n",
      "tremendous difference.\n",
      " Let ‡ s zoom in a bit. If you displa y the model summar y , you can see tha t each BN la yer\n",
      " adds 4 parameters per in put: \n",
      "‹\n",
      ", \n",
      "›\n",
      ", \n",
      "—\n",
      " and \n",
      "⁄\n",
      "  (for exam ple, the first BN la yer adds 3136\n",
      "parameters, which is 4 times 784). The last two parameters, \n",
      "—\n",
      " and \n",
      "⁄\n",
      ", are the moving\n",
      " a verages, they are not affected by backpropaga tion, so K eras calls them —N on-\n",
      " trainable –\n",
      "9\n",
      "  (if you coun t the total n umber of BN parameters, 3136 + 1200 + 400, and\n",
      " divide by two , you get 2,368, which is the total n umber of non-trainable params in\n",
      "this model).\n",
      ">>> \n",
      "model\n",
      ".\n",
      "summary\n",
      "()\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0\n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 784)               3136\n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 300)               235500\n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 300)               1200\n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               30100\n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 100)               400\n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                1010\n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      " Let ‡ s look a t the parameters of the first BN la yer . T wo are trainable (by backprop), and\n",
      "two are not:\n",
      ">>> \n",
      "[(\n",
      "var\n",
      ".\n",
      "name\n",
      ",\n",
      " \n",
      "var\n",
      ".\n",
      "trainable\n",
      ")\n",
      " \n",
      "for\n",
      " \n",
      "var\n",
      " \n",
      "in\n",
      " \n",
      "model\n",
      ".\n",
      "layers\n",
      "[\n",
      "1\n",
      "]\n",
      ".\n",
      "variables\n",
      "]\n",
      "[(•batch_normalization_v2/gamma:0•, True),\n",
      " (•batch_normalization_v2/beta:0•, True),\n",
      " (•batch_normalization_v2/moving_mean:0•, False),\n",
      " (•batch_normalization_v2/moving_variance:0•, False)]\n",
      " N ow when you crea te a BN la yer in K eras, it also crea tes two opera tions tha t will be\n",
      " called by K eras a t each itera tion during training. These opera tions will upda te the\n",
      " 336  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " moving a verages. Since we are using the T ensorFlow backend, these opera tions are\n",
      " T ensorFlow opera tions (we will discuss TF opera tions in \n",
      " Cha pter 12\n",
      ").\n",
      ">>> \n",
      "model\n",
      ".\n",
      "layers\n",
      "[\n",
      "1\n",
      "]\n",
      ".\n",
      "updates\n",
      "[<tf.Operation •cond_2/Identity• type=Identity>,\n",
      " <tf.Operation •cond_3/Identity• type=Identity>]\n",
      " The a uthors of the BN pa per argued in fa vor of adding the BN la yers before the actiƒ\n",
      " va tion functions, ra ther than after (as we just did). There is some deba te about this, as\n",
      " it seems to depend on the task. So tha t ‡ s one more thing you can experimen t with to\n",
      " see which option works best on your da taset. T o add the BN la yers before the activaƒ\n",
      " tion functions, we m ust remove the activa tion function from the hidden la yers, and\n",
      " add them as separa te la yers after the BN la yers. M oreover , since a Ba tch N ormalizaƒ\n",
      " tion la yer includes one offset parameter per in put, you can remove the bias term from\n",
      " the previous la yer (just pass \n",
      "use_bias=False\n",
      "  when crea ting it):\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(\n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      "]),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "300\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ",\n",
      " \n",
      "use_bias\n",
      "=\n",
      "False\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Activation\n",
      "(\n",
      "\"elu\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ",\n",
      " \n",
      "use_bias\n",
      "=\n",
      "False\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Activation\n",
      "(\n",
      "\"elu\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ")\n",
      "])\n",
      "The \n",
      "BatchNormalization\n",
      "  class has quite a few h yperparameters you can tweak. The\n",
      " defa ults will usually be fine, but you ma y occasionally need to tweak the \n",
      "momentum\n",
      ".\n",
      " This h yperparameter is used when upda ting the exponen tial moving a verages: given a\n",
      "new value \n",
      "v\n",
      "  (i.e., a new vector of in put means or standard devia tions com puted over\n",
      " the curren t ba tch), the running a verage \n",
      "  is upda ted using the following equa tion:\n",
      "v\n",
      "v\n",
      " „ momentum +\n",
      "v\n",
      "„\n",
      " 1 ” momentum\n",
      " A good momen tum value is typically close to 1›for exam ple, 0.9, 0.99, or 0.999 (you\n",
      " wan t more 9s for larger da tasets and smaller mini-ba tches).\n",
      " Another im portan t h yperparameter is \n",
      "axis\n",
      ": it determines which axis should be norƒ\n",
      " malized. I t defa ults to −1, meaning tha t by defa ult it will normalize the last axis (using\n",
      " the means and standard devia tions com puted across the \n",
      " o t h er\n",
      " \n",
      " axes). F or exam ple,\n",
      " when the in put ba tch is 2D (i.e., the ba tch sha pe is [ba tch size, fea tures]), this means\n",
      " tha t each in put fea ture will be normalized based on the mean and standard devia tion\n",
      " com puted across all the instances in the ba tch. F or exam ple, the first BN la yer in the\n",
      " previous code exam ple will independen tly normalize (and rescale and shift) each of\n",
      " the 784 in put fea tures. H owever , if we move the first BN la yer before the \n",
      "Flatten\n",
      " Vanishing/Exploding Gradients Problems  |  337\n",
      "\n",
      "10\n",
      " —Fixup Initializa tion: Residual Learning W ithout N ormaliza tion, – H ong yi Zhang, Y ann N. Da uphin, T eng yu\n",
      " M a (2019).\n",
      "11\n",
      " — On the difficulty of training recurren t neural networks, – R. P ascan u et al. (2013).\n",
      " la yer , then the in put ba tches will be 3D , with sha pe [ba tch size, heigh t, width], thereƒ\n",
      " fore the BN la yer will com pute 28 means and 28 standard devia tions (one per column\n",
      " of pixels, com puted across all instances in the ba tch, and all rows in the column), and\n",
      "it will normalize all pixels in a given column using the same mean and standard deviƒ\n",
      " a tion. There will also be just 28 scale parameters and 28 shift parameters. If instead\n",
      " you still wan t to trea t each of the 784 pixels independen tly , then you should set\n",
      "axis=[1, 2]\n",
      ".\n",
      " N otice tha t the BN la yer does not perform the same com puta tion during training and\n",
      " after training: it uses ba tch sta tistics during training, and the — final – sta tistics after\n",
      " training (i.e., the final value of the moving a verages). Let ‡ s take a peek a t the source\n",
      "code of this class to see how this is handled:\n",
      "class\n",
      " \n",
      "BatchNormalization\n",
      "(\n",
      "Layer\n",
      "):\n",
      "    \n",
      "[\n",
      "...\n",
      "]\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      ",\n",
      " \n",
      "training\n",
      "=\n",
      "None\n",
      "):\n",
      "        \n",
      "if\n",
      " \n",
      "training\n",
      " \n",
      "is\n",
      " \n",
      "None\n",
      ":\n",
      "            \n",
      "training\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "backend\n",
      ".\n",
      "learning_phase\n",
      "()\n",
      "        \n",
      "[\n",
      "...\n",
      "]\n",
      "The \n",
      "call()\n",
      "  method is the one tha t actually performs the com puta tions, and as you\n",
      "can see it has an extra \n",
      "training\n",
      "  argumen t: if it is \n",
      "None\n",
      " it falls back to \n",
      "keras.back\n",
      "end.learning_phase()\n",
      ", which returns \n",
      "1\n",
      " during training (the \n",
      "fit()\n",
      " method ensures\n",
      " tha t). Other wise, it returns \n",
      "0\n",
      " . If you ever need to write a custom la yer , and it needs to\n",
      " beha ve differen tly during training and testing, sim ply use the same pa ttern (we will\n",
      " discuss custom la yers in \n",
      " Cha pter 12\n",
      ").\n",
      " Ba tch N ormaliza tion has become one of the most used la yers in deep neural netƒ\n",
      " works, to the poin t tha t it is often omitted in the diagrams, as it is assumed tha t BN is\n",
      " added after ever y la yer . H owever , a ver y recen t \n",
      " pa per\n",
      "10\n",
      "  by H ong yi Zhang et al. ma y\n",
      " well change this: the a uthors show tha t by using a novel fixed-upda te (fixup) weigh t\n",
      " initializa tion technique, they manage to train a ver y deep neural network (10,000 la yƒ\n",
      " ers!) without BN, achieving sta te-of-the-art performance on com plex image classifiƒ\n",
      " ca tion tasks.\n",
      "Gradient Clipping\n",
      " Another popular technique to lessen the exploding gradien ts problem is to sim ply\n",
      " clip the gradien ts during backpropaga tion so tha t they never exceed some threshold.\n",
      "This is called \n",
      " G r a d i en t Cl i p p i n g\n",
      ".\n",
      "11\n",
      " \n",
      " This technique is most often used in recurren t neuƒ\n",
      " 338  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " ral networks, as Ba tch N ormaliza tion is tricky to use in RNN s, as we will see in \n",
      "???\n",
      ".\n",
      " F or other types of networks, BN is usually sufficien t.\n",
      " In K eras, im plemen ting Gradien t Clipping is just a ma tter of setting the \n",
      "clipvalue\n",
      "   or\n",
      "clipnorm\n",
      "  argumen t when crea ting an optimizer . F or exam ple:\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "clipvalue\n",
      "=\n",
      "1.0\n",
      ")\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"mse\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "optimizer\n",
      ")\n",
      " This will clip ever y com ponen t of the gradien t vector to a value between −1.0 and 1.0.\n",
      " This means tha t all the partial deriva tives of the loss (with regards to each and ever y\n",
      " trainable parameter) will be clipped between −1.0 and 1.0. The threshold is a h yperƒ\n",
      " parameter you can tune. N ote tha t it ma y change the orien ta tion of the gradien t vecƒ\n",
      " tor : for exam ple, if the original gradien t vector is [0.9, 100.0], it poin ts mostly in the\n",
      "direction of the second axis, but once you clip it by value, you get [0.9, 1.0], which\n",
      " poin ts roughly in the diagonal between the two axes. In practice however , this\n",
      " a pproach works well. If you wan t to ensure tha t Gradien t Clipping does not change\n",
      " the direction of the gradien t vector , you should clip by norm by setting \n",
      "clipnorm\n",
      "instead of \n",
      "clipvalue\n",
      " . This will clip the whole gradien t if its …\n",
      "2\n",
      "  norm is grea ter than\n",
      " the threshold you picked. F or exam ple, if you set \n",
      "clipnorm=1.0\n",
      ", then the vector [0.9,\n",
      " 100.0] will be clipped to [0.00899964, 0.9999595], preser ving its orien ta tion, but\n",
      " almost elimina ting the first com ponen t. If you obser ve tha t the gradien ts explode\n",
      " during training (you can track the size of the gradien ts using T ensorB oard), you ma y\n",
      " wan t to tr y both clipping by value and clipping by norm, with differen t threshold,\n",
      " and see which option performs best on the valida tion set.\n",
      "Reusing Pretrained Layers\n",
      " I t is generally not a good idea to train a ver y large DNN from scra tch: instead, you\n",
      " should alwa ys tr y to find an existing neural network tha t accom plishes a similar task\n",
      " to the one you are tr ying to tackle (we will discuss how to find them in \n",
      " Cha pter 14\n",
      "),\n",
      " then just reuse the lower la yers of this network: this is called \n",
      " t r a ns f er l e a r n i n g\n",
      " . I t will\n",
      " not only speed up training considerably , but will also require m uch less training da ta.\n",
      " F or exam ple, suppose tha t you ha ve access to a DNN tha t was trained to classif y picƒ\n",
      " tures in to 100 differen t ca tegories, including animals, plan ts, vehicles, and ever yda y\n",
      " objects. Y ou now wan t to train a DNN to classif y specific types of vehicles. These\n",
      " tasks are ver y similar , even partly overla pping, so you should tr y to reuse parts of the\n",
      "first network (see \n",
      "Figure 11-4\n",
      ").\n",
      " Reusing Pretrained Layers  |  339\n",
      "\n",
      " F i g u r e 11-4. R eu s i n g p r e t r a i n e d l a y er s\n",
      " If the in put pictures of your new task don ‡ t ha ve the same size as\n",
      " the ones used in the original task, you will usually ha ve to add a\n",
      "preprocessing step to resize them to the size expected by the origiƒ\n",
      " nal model. M ore generally , transfer learning will work best when\n",
      " the in puts ha ve similar low-level fea tures.\n",
      " The output la yer of the original model should usually be replaced since it is most\n",
      " likely not useful a t all for the new task, and it ma y not even ha ve the righ t n umber of\n",
      "outputs for the new task.\n",
      " Similarly , the upper hidden la yers of the original model are less likely to be as useful\n",
      " as the lower la yers, since the high-level fea tures tha t are most useful for the new task\n",
      " ma y differ significan tly from the ones tha t were most useful for the original task. Y ou\n",
      " wan t to find the righ t n umber of la yers to reuse.\n",
      " The more similar the tasks are, the more la yers you wan t to reuse\n",
      " (starting with the lower la yers). F or ver y similar tasks, you can tr y\n",
      " keeping all the hidden la yers and just replace the output la yer .\n",
      " T r y freezing all the reused la yers first (i.e., make their weigh ts non-trainable, so gradiƒ\n",
      " en t descen t won ‡ t modif y them), then train your model and see how it performs.\n",
      " Then tr y unfreezing one or two of the top hidden la yers to let backpropaga tion tweak\n",
      " them and see if performance im proves. The more training da ta you ha ve, the more\n",
      " 340  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " la yers you can unfreeze. I t is also useful to reduce the learning ra te when you unfreeze\n",
      " reused la yers: this will a void wrecking their fine-tuned weigh ts.\n",
      " If you still cannot get good performance, and you ha ve little training da ta, tr y dropƒ\n",
      " ping the top hidden la yer(s) and freeze all remaining hidden la yers again. Y ou can\n",
      " itera te un til you find the righ t n umber of la yers to reuse. If you ha ve plen ty of trainƒ\n",
      " ing da ta, you ma y tr y replacing the top hidden la yers instead of dropping them, and\n",
      " even add more hidden la yers.\n",
      "Transfer Learning With Keras\n",
      " Let ‡ s look a t an exam ple. Suppose the fashion MNIST da taset only con tained 8 classes,\n",
      " for exam ple all classes except for sandals and shirts. Someone built and trained a\n",
      " K eras model on tha t set and got reasonably good performance (>90% accuracy). Let ‡ s\n",
      " call this model A. Y ou now wan t to tackle a differen t task: you ha ve images of sandals\n",
      " and shirts, and you wan t to train a binar y classifier (positive=shirts, nega tive=sanƒ\n",
      " dals). H owever , your da taset is quite small, you only ha ve 200 labeled images. When\n",
      " you train a new model for this task (let ‡ s call it model B), with the same architecture\n",
      " as model A, it performs reasonably well (97.2% accuracy), but since it ‡ s a m uch easier\n",
      "task (there are just 2 classes), you were hoping for more. While drinking your mornƒ\n",
      " ing coffee, you realize tha t your task is quite similar to task A, so perha ps transfer\n",
      " learning can help? Let ‡ s find out!\n",
      " First, you need to load model A, and crea te a new model based on the model A ‡ s la yƒ\n",
      " ers. Let ‡ s reuse all la yers except for the output la yer :\n",
      "model_A\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "load_model\n",
      "(\n",
      "\"my_model_A.h5\"\n",
      ")\n",
      "model_B_on_A\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "(\n",
      "model_A\n",
      ".\n",
      "layers\n",
      "[:\n",
      "-\n",
      "1\n",
      "])\n",
      "model_B_on_A\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"sigmoid\"\n",
      "))\n",
      " N ote tha t \n",
      "model_A\n",
      " and \n",
      "model_B_on_A\n",
      "  now share some la yers. When you train\n",
      "model_B_on_A\n",
      ", it will also affect \n",
      "model_A\n",
      " . If you wan t to a void tha t, you need to clone\n",
      "model_A\n",
      " \n",
      " before you reuse its la yers. T o do this, you m ust clone model A ‡ s architecture,\n",
      " then copy its weigh ts (since \n",
      "clone_model()\n",
      "  does not clone the weigh ts):\n",
      "model_A_clone\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "clone_model\n",
      "(\n",
      "model_A\n",
      ")\n",
      "model_A_clone\n",
      ".\n",
      "set_weights\n",
      "(\n",
      "model_A\n",
      ".\n",
      "get_weights\n",
      "())\n",
      " N ow we could just train \n",
      "model_B_on_A\n",
      "  for task B , but since the new output la yer was\n",
      " initialized randomly , it will make large errors, a t least during the first few epochs, so\n",
      " there will be large error gradien ts tha t ma y wreck the reused weigh ts. T o a void this,\n",
      " one a pproach is to freeze the reused la yers during the first few epochs, giving the new\n",
      " la yer some time to learn reasonable weigh ts. T o do this, sim ply set ever y la yer‡ s \n",
      "train\n",
      "able\n",
      "  a ttribute to \n",
      "False\n",
      "  and com pile the model:\n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "model_B_on_A\n",
      ".\n",
      "layers\n",
      "[:\n",
      "-\n",
      "1\n",
      "]:\n",
      "    \n",
      "layer\n",
      ".\n",
      "trainable\n",
      " \n",
      "=\n",
      " \n",
      "False\n",
      " Reusing Pretrained Layers  |  341\n",
      "\n",
      "model_B_on_A\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"binary_crossentropy\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ",\n",
      "                     \n",
      "metrics\n",
      "=\n",
      "[\n",
      "\"accuracy\"\n",
      "])\n",
      " Y ou m ust alwa ys com pile your model after you freeze or unfreeze\n",
      " la yers.\n",
      " N ext, we can train the model for a few epochs, then unfreeze the reused la yers (which\n",
      " requires com piling the model again) and con tin ue training to fine-tune the reused\n",
      " la yers for task B . After unfreezing the reused la yers, it is usually a good idea to reduce\n",
      " the learning ra te, once again to a void damaging the reused weigh ts:\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model_B_on_A\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_B\n",
      ",\n",
      " \n",
      "y_train_B\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "4\n",
      ",\n",
      "                           \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid_B\n",
      ",\n",
      " \n",
      "y_valid_B\n",
      "))\n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "model_B_on_A\n",
      ".\n",
      "layers\n",
      "[:\n",
      "-\n",
      "1\n",
      "]:\n",
      "    \n",
      "layer\n",
      ".\n",
      "trainable\n",
      " \n",
      "=\n",
      " \n",
      "True\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "lr\n",
      "=\n",
      "1e-4\n",
      ")\n",
      " \n",
      "# the default lr is 1e-3\n",
      "model_B_on_A\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"binary_crossentropy\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "optimizer\n",
      ",\n",
      "                     \n",
      "metrics\n",
      "=\n",
      "[\n",
      "\"accuracy\"\n",
      "])\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model_B_on_A\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_B\n",
      ",\n",
      " \n",
      "y_train_B\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "16\n",
      ",\n",
      "                           \n",
      "validation_data\n",
      "=\n",
      "(\n",
      "X_valid_B\n",
      ",\n",
      " \n",
      "y_valid_B\n",
      "))\n",
      " So , wha t ‡ s the final verdict? W ell this model ‡ s test accuracy is 99.25%, which means\n",
      " tha t transfer learning reduced the error ra te from 2.8% down to almost 0.7%! Tha t ‡ s a\n",
      "factor of 4!\n",
      ">>> \n",
      "model_B_on_A\n",
      ".\n",
      "evaluate\n",
      "(\n",
      "X_test_B\n",
      ",\n",
      " \n",
      "y_test_B\n",
      ")\n",
      "[0.06887910133600235, 0.9925]\n",
      " Are you con vinced? W ell you shouldn ‡ t be: I chea ted! :) I tried man y configura tions\n",
      " un til I found one tha t demonstra ted a strong im provemen t. If you tr y to change the\n",
      " classes or the random seed, you will see tha t the im provemen t generally drops, or\n",
      " even vanishes or reverses. Wha t I did is called — torturing the da ta un til it confesses – .\n",
      " When a pa per just looks too positive, you should be suspicious: perha ps the flash y\n",
      " new technique does not help m uch (in fact, it ma y even degrade performance), but\n",
      " the a uthors tried man y varian ts and reported only the best results (which ma y be due\n",
      " to shear luck), without men tioning how man y failures they encoun tered on the wa y .\n",
      " M ost of the time, this is not malicious a t all, but it is part of the reason wh y so man y\n",
      "results in Science can never be reproduced.\n",
      " So wh y did I chea t? W ell it turns out tha t transfer learning does not work ver y well\n",
      " with small dense networks: it works best with deep con volutional neural networks, so\n",
      "we will revisit transfer learning in \n",
      " Cha pter 14\n",
      ", using the same techniques (and this\n",
      " time there will be no chea ting, I promise!).\n",
      " 342  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "Unsupervised Pretraining\n",
      " Suppose you wan t to tackle a com plex task for which you don ‡ t ha ve m uch labeled\n",
      " training da ta, but unfortuna tely you cannot find a model trained on a similar task.\n",
      " Don ‡ t lose all hope! First, you should of course tr y to ga ther more labeled training\n",
      " da ta, but if this is too hard or too expensive, you ma y still be able to perform \n",
      " u ns u p er‡\n",
      " v i s e d p r e t r a i n i n g\n",
      " (see \n",
      "Figure 11-5\n",
      " ). I t is often ra ther chea p to ga ther unlabeled trainƒ\n",
      " ing exam ples, but quite expensive to label them. If you can ga ther plen ty of unlabeled\n",
      " training da ta, you can tr y to train the la yers one by one, starting with the lowest la yer\n",
      " and then going up , using an unsuper vised fea ture detector algorithm such \n",
      "as \n",
      " R e s t r i c‡\n",
      " t e d B o l tzm a n n M a c h i n e s\n",
      " (RBMs; see \n",
      "???\n",
      " ) or a utoencoders (see \n",
      "???\n",
      " ). Each la yer is\n",
      " trained on the output of the previously trained la yers (all la yers except the one being\n",
      " trained are frozen). Once all la yers ha ve been trained this wa y , you can add the output\n",
      " la yer for your task, and fine-tune the final network using super vised learning (i.e.,\n",
      " with the labeled training exam ples). A t this poin t, you can unfreeze all the pretrained\n",
      " la yers, or just some of the upper ones.\n",
      " F i g u r e 11-5. U ns u p er v i s e d p r e t r a i n i n g\n",
      " This is a ra ther long and tedious process, but it often works well; in fact, it is this\n",
      " technique tha t Geoffrey Hin ton and his team used in 2006 and which led to the\n",
      " revival of neural networks and the success of Deep Learning. U n til 2010, unsuperƒ\n",
      "vised pretraining (typically using RBMs) was the norm for deep nets, and it was only\n",
      " after the vanishing gradien ts problem was allevia ted tha t it became m uch more comƒ\n",
      " Reusing Pretrained Layers  |  343\n",
      "\n",
      " mon to train DNN s purely using super vised learning. H owever , unsuper vised preƒ\n",
      " training (toda y typically using a utoencoders ra ther than RBMs) is still a good option\n",
      " when you ha ve a com plex task to solve, no similar model you can reuse, and little\n",
      " labeled training da ta but plen ty of unlabeled training da ta.\n",
      "Pretraining on an Auxiliary Task\n",
      " If you do not ha ve m uch labeled training da ta, one last option is to train a first neural\n",
      " network on an a uxiliar y task for which you can easily obtain or genera te labeled\n",
      " training da ta, then reuse the lower la yers of tha t network for your actual task. The\n",
      " first neural network ‡ s lower la yers will learn fea ture detectors tha t will likely be reusaƒ\n",
      "ble by the second neural network.\n",
      " F or exam ple, if you wan t to build a system to recognize faces, you ma y only ha ve a\n",
      " few pictures of each individual›clearly not enough to train a good classifier . Ga therƒ\n",
      " ing h undreds of pictures of each person would not be practical. H owever , you could\n",
      " ga ther a lot of pictures of random people on the web and train a first neural network\n",
      " to detect whether or not two differen t pictures fea ture the same person. Such a netƒ\n",
      " work would learn good fea ture detectors for faces, so reusing its lower la yers would\n",
      " allow you to train a good face classifier using little training da ta.\n",
      " F or \n",
      " n a t u r a l l a n g u a ge p r o c e s s i n g\n",
      "  (NLP) a pplica tions, you can easily download millions\n",
      " of text documen ts and a utoma tically genera te labeled da ta from it. F or exam ple, you\n",
      " could randomly mask out some words and train a model to predict wha t the missing\n",
      " words are (e.g., it should predict tha t the missing word in the sen tence —Wha t ___\n",
      " you sa ying? – is probably — are – or —were –). If you can train a model to reach good perƒ\n",
      "formance on this task, then it will already know quite a lot about language, and you\n",
      " can certainly reuse it for your actual task, and fine-tune it on your labeled da ta (we\n",
      "will discuss more pretraining tasks in \n",
      "???\n",
      ").\n",
      " S e l f-s u p er v i s e d l e a r n i n g\n",
      "  is when you a utoma tically genera te the\n",
      " labels from the da ta itself, then you train a model on the resulting\n",
      " —labeled – da taset using super vised learning techniques. Since this\n",
      " a pproach requires no h uman labeling wha tsoever , it is best classiƒ\n",
      " fied as a form of unsuper vised learning.\n",
      "Faster Optimizers\n",
      " T raining a ver y large deep neural network can be painfully slow . So far we ha ve seen\n",
      " four wa ys to speed up training (and reach a better solution): a pplying a good initialiƒ\n",
      " za tion stra teg y for the connection weigh ts, using a good activa tion function, using\n",
      " Ba tch N ormaliza tion, and reusing parts of a pretrained network (possibly built on an\n",
      " a uxiliar y task or using unsuper vised learning). Another h uge speed boost comes from\n",
      " using a faster optimizer than the regular Gradien t Descen t optimizer . In this section\n",
      " 344  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "12\n",
      " — Some methods of speeding up the con vergence of itera tion methods, – B . P olyak (1964).\n",
      " we will presen t the most popular ones: M omen tum optimiza tion, N esterov A ccelerƒ\n",
      " a ted Gradien t, A daGrad, RMSProp , and finally A dam and N adam optimiza tion.\n",
      "Momentum Optimization\n",
      " Imagine a bowling ball rolling down a gen tle slope on a smooth surface: it will start\n",
      " out slowly , but it will quickly pick up momen tum un til it even tually reaches terminal\n",
      " velocity (if there is some friction or air resistance). This is the ver y sim ple idea behind\n",
      " M o m en t u m o p t i m iz a t i o n\n",
      ", \n",
      " proposed by B oris P olyak in 1964\n",
      ".\n",
      "12\n",
      " \n",
      " In con trast, regular\n",
      " Gradien t Descen t will sim ply take small regular steps down the slope, so it will take\n",
      " m uch more time to reach the bottom.\n",
      " Recall tha t Gradien t Descen t sim ply upda tes the weigh ts \n",
      "•\n",
      " by directly subtracting the\n",
      " gradien t of the cost function \n",
      "J\n",
      "(\n",
      "•\n",
      " ) with regards to the weigh ts (\n",
      "•\n",
      "J\n",
      "(\n",
      "•\n",
      " )) m ultiplied by\n",
      " the learning ra te \n",
      "−\n",
      " . The equa tion is: \n",
      "•\n",
      " Ò \n",
      "•\n",
      " − \n",
      "−\n",
      "•\n",
      "J\n",
      "(\n",
      "•\n",
      " ). I t does not care about wha t the\n",
      " earlier gradien ts were. If the local gradien t is tin y , it goes ver y slowly .\n",
      " M omen tum optimiza tion cares a grea t deal about wha t previous gradien ts were: a t\n",
      " each itera tion, it subtracts the local gradien t from the \n",
      " m o m en t u m v e c t o r\n",
      " \n",
      "m\n",
      " \n",
      " (m ultiƒ\n",
      " plied by the learning ra te \n",
      "−\n",
      " ), and it upda tes the weigh ts by sim ply adding this\n",
      " momen tum vector (see \n",
      " Equa tion 11-4\n",
      " ). In other words, the gradien t is used for accelƒ\n",
      " era tion, not for speed. T o sim ula te some sort of friction mechanism and preven t the\n",
      " momen tum from growing too large, the algorithm in troduces a new h yperparameter\n",
      "Ÿ\n",
      " , sim ply called the \n",
      " m o m en t u m\n",
      " , which m ust be set between 0 (high friction) and 1\n",
      " (no friction). A typical momen tum value is 0.9.\n",
      " Eq u a t i o n 11-4. M o m en t u m a l go r i t h m\n",
      " 1 .\n",
      "m\n",
      "Ÿ\n",
      "m\n",
      "”\n",
      "−\n",
      "•\n",
      "J\n",
      "•\n",
      " 2 .\n",
      "•\n",
      "•\n",
      "+\n",
      "m\n",
      " Y ou can easily verif y tha t if the gradien t remains constan t, the terminal velocity (i.e.,\n",
      " the maxim um size of the weigh t upda tes) is equal to tha t gradien t m ultiplied by the\n",
      " learning ra te \n",
      "−\n",
      "   m ultiplied by \n",
      "1\n",
      " 1 ”\n",
      "Ÿ\n",
      "   (ignoring the sign). F or exam ple, if \n",
      "Ÿ\n",
      "   = 0.9, then the\n",
      " terminal velocity is equal to 10 times the gradien t times the learning ra te, so M omenƒ\n",
      " tum optimiza tion ends up going 10 times faster than Gradien t Descen t! This allows\n",
      " M omen tum optimiza tion to esca pe from pla tea us m uch faster than Gradien t Descen t.\n",
      " In particular , we sa w in \n",
      " Cha pter 4\n",
      " \n",
      " tha t when the in puts ha ve ver y differen t scales the \n",
      " cost function will look like an elonga ted bowl (see \n",
      "Figure 4-7\n",
      " ). Gradien t Descen t goes\n",
      " down the steep slope quite fast, but then it takes a ver y long time to go down the valƒ\n",
      " Faster Optimizers  |  345\n",
      "\n",
      "13\n",
      " — A M ethod for U nconstrained Con vex Minimiza tion Problem with the R a te of Con vergence O(1/k\n",
      "2\n",
      " ), – Y urii\n",
      " N esterov (1983).\n",
      " ley . In con trast, M omen tum optimiza tion will roll down the valley faster and faster\n",
      " un til it reaches the bottom (the optim um). In deep neural networks tha t don ‡ t use\n",
      " Ba tch N ormaliza tion, the upper la yers will often end up ha ving in puts with ver y difƒ\n",
      " feren t scales, so using M omen tum optimiza tion helps a lot. I t can also help roll past\n",
      "local optima.\n",
      " Due to the momen tum, the optimizer ma y overshoot a bit, then\n",
      " come back, overshoot again, and oscilla te like this man y times\n",
      " before stabilizing a t the minim um. This is one of the reasons wh y it\n",
      " is good to ha ve a bit of friction in the system: it gets rid of these\n",
      " oscilla tions and th us speeds up con vergence.\n",
      " Im plemen ting M omen tum optimiza tion in K eras is a no-brainer : just use the \n",
      "SGD\n",
      "optimizer and set its \n",
      "momentum\n",
      "  h yperparameter , then lie back and profit!\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.001\n",
      ",\n",
      " \n",
      "momentum\n",
      "=\n",
      "0.9\n",
      ")\n",
      " The one dra wback of M omen tum optimiza tion is tha t it adds yet another h yperparaƒ\n",
      " meter to tune. H owever , the momen tum value of 0.9 usually works well in practice\n",
      " and almost alwa ys goes faster than regular Gradien t Descen t.\n",
      "Nesterov Accelerated Gradient\n",
      "One \n",
      " small varian t to M omen tum optimiza tion, proposed by \n",
      " Y urii N esterov in 1983\n",
      ",\n",
      "13\n",
      " is almost alwa ys faster than vanilla M omen tum optimiza tion. The idea of \n",
      " N e s t er o v\n",
      " M o m en t u m o p t i m iz a t i o n\n",
      ", or \n",
      " N e s t er o v Ac c e l er a t e d G r a d i en t\n",
      "  (N A G), is to measure the\n",
      " gradien t of the cost function not a t the local position but sligh tly ahead in the direcƒ\n",
      " tion of the momen tum (see \n",
      " Equa tion 11-5\n",
      "). The only difference from vanilla\n",
      " M omen tum optimiza tion is tha t the gradien t is measured a t \n",
      "•\n",
      " + \n",
      "Ÿ\n",
      "m\n",
      "  ra ther than a t \n",
      "•\n",
      ".\n",
      " Eq u a t i o n 11-5. N e s t er o v Ac c e l er a t e d G r a d i en t a l go r i t h m\n",
      " 1 .\n",
      "m\n",
      "Ÿ\n",
      "m\n",
      "”\n",
      "−\n",
      "•\n",
      "J\n",
      "•\n",
      "+\n",
      "Ÿ\n",
      "m\n",
      " 2 .\n",
      "•\n",
      "•\n",
      "+\n",
      "m\n",
      " This small tweak works beca use in general the momen tum vector will be poin ting in\n",
      " the righ t direction (i.e., toward the optim um), so it will be sligh tly more accura te to\n",
      " use the gradien t measured a bit farther in tha t direction ra ther than using the gradiƒ\n",
      " en t a t the original position, as you can see in \n",
      "Figure 11-6\n",
      " (where \n",
      "1\n",
      "  represen ts the\n",
      " gradien t of the cost function measured a t the starting poin t \n",
      "•\n",
      ", and \n",
      "2\n",
      "  represen ts the\n",
      " 346  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "14\n",
      " — A da ptive Subgradien t M ethods for Online Learning and Stochastic Optimiza tion, – J . Duchi et al. (2011).\n",
      " gradien t a t the poin t loca ted a t \n",
      "•\n",
      "   + \n",
      "Ÿ\n",
      "m\n",
      " ). As you can see, the N esterov upda te ends up\n",
      " sligh tly closer to the optim um. After a while, these small im provemen ts add up and\n",
      " N A G ends up being significan tly faster than regular M omen tum optimiza tion. M oreƒ\n",
      " over , note tha t when the momen tum pushes the weigh ts across a valley , \n",
      "1\n",
      "   con tin ues\n",
      " to push further across the valley , while \n",
      "2\n",
      " pushes back toward the bottom of the valƒ\n",
      " ley . This helps reduce oscilla tions and th us con verges faster .\n",
      " N A G will almost alwa ys speed up training com pared to regular M omen tum optimiƒ\n",
      " za tion. T o use it, sim ply set \n",
      "nesterov=True\n",
      "  when crea ting the \n",
      "SGD\n",
      "  optimizer :\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.001\n",
      ",\n",
      " \n",
      "momentum\n",
      "=\n",
      "0.9\n",
      ",\n",
      " \n",
      "nesterov\n",
      "=\n",
      "True\n",
      ")\n",
      " F i g u r e 11-6. R e g u l a r v er s u s N e s t er o v M o m en t u m o p t i m iz a t i o n\n",
      "AdaGrad\n",
      "Consider \n",
      " the elonga ted bowl problem again: Gradien t Descen t starts by quickly going\n",
      " down the steepest slope, then slowly goes down the bottom of the valley . I t would be\n",
      " nice if the algorithm could detect this early on and correct its direction to poin t a bit\n",
      " more toward the global optim um.\n",
      "The \n",
      " Ad aG r a d\n",
      " \n",
      "algorithm\n",
      "14\n",
      "  achieves this by scaling down the gradien t vector along the\n",
      "steepest dimensions (see \n",
      " Equa tion 11-6\n",
      "):\n",
      " Eq u a t i o n 11-6. Ad aG r a d a l go r i t h m\n",
      " 1 .\n",
      "s\n",
      "s\n",
      "+\n",
      "•\n",
      "J\n",
      "•\n",
      "\n",
      "•\n",
      "J\n",
      "•\n",
      " 2 .\n",
      "•\n",
      "•\n",
      "”\n",
      "−\n",
      "•\n",
      "J\n",
      "•\n",
      "s\n",
      "+\n",
      " Faster Optimizers  |  347\n",
      "\n",
      " The first step accum ula tes the square of the gradien ts in to the vector \n",
      "s\n",
      " \n",
      " (recall tha t the\n",
      "  symbol represen ts the elemen t-wise m ultiplica tion). This vectorized form is equivƒ\n",
      " alen t to com puting \n",
      "s\n",
      "i\n",
      "   Ò \n",
      "s\n",
      "i\n",
      "   + ( Œ \n",
      "J\n",
      "(\n",
      "•\n",
      ") / \n",
      "Œ \n",
      "–\n",
      "i\n",
      ")\n",
      "2\n",
      " \n",
      " for each elemen t \n",
      "s\n",
      "i\n",
      " \n",
      "of the vector \n",
      "s\n",
      "; in other\n",
      "words, each \n",
      "s\n",
      "i\n",
      "  accum ula tes the squares of the partial deriva tive of the cost function\n",
      "with regards to parameter \n",
      "–\n",
      "i\n",
      ". If the cost function is steep along the i\n",
      "th\n",
      "   dimension, then\n",
      "s\n",
      "i\n",
      "  will get larger and larger a t each itera tion.\n",
      " The second step is almost iden tical to Gradien t Descen t, but with one big difference:\n",
      " the gradien t vector is scaled down by a factor of \n",
      "+\n",
      " (the \n",
      "  symbol represen ts the\n",
      " elemen t-wise division, and \n",
      "  is a smoothing term to a void division by zero , typically\n",
      "set to 10\n",
      "−10\n",
      " ). This vectorized form is equivalen t to com puting\n",
      "–\n",
      "i\n",
      "–\n",
      "i\n",
      "”\n",
      "−\n",
      "Œ\n",
      "J\n",
      "•\n",
      " / Œ\n",
      "–\n",
      "i\n",
      "/\n",
      "s\n",
      "i\n",
      "+\n",
      " for all parameters \n",
      "–\n",
      "i\n",
      "  (sim ultaneously).\n",
      " In short, this algorithm deca ys the learning ra te, but it does so faster for steep dimenƒ\n",
      " sions than for dimensions with gen tler slopes. This is called an \n",
      " a d a p t i v e l e a r n i n g r a t e\n",
      ". \n",
      " I t helps poin t the resulting upda tes more directly toward the global optim um (see\n",
      "Figure 11-7\n",
      " ). One additional benefit is tha t it requires m uch less tuning of the learnƒ\n",
      " ing ra te h yperparameter \n",
      "−\n",
      ".\n",
      " F i g u r e 11-7. Ad aG r a d v er s u s G r a d i en t D e s c en t\n",
      " A daGrad often performs well for sim ple quadra tic problems, but unfortuna tely it\n",
      " often stops too early when training neural networks. The learning ra te gets scaled\n",
      " down so m uch tha t the algorithm ends up stopping en tirely before reaching the\n",
      " global optim um. So even though K eras has an \n",
      "Adagrad\n",
      " \n",
      " optimizer , you should not use\n",
      " it to train deep neural networks (it ma y be efficien t for sim pler tasks such as Linear\n",
      " Regression, though). H owever , understanding A dagrad is helpful to grasp the other\n",
      " ada ptive learning ra te optimizers.\n",
      " 348  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "15\n",
      " This algorithm was crea ted by Geoffrey Hin ton and T ijmen T ieleman in 2012, and presen ted by Geoffrey\n",
      " Hin ton in his Coursera class on neural networks (slides: \n",
      " h ttp s://h o m l.i n f o/57\n",
      "; video: \n",
      " h ttp s://h o m l.i n f o/58\n",
      ").\n",
      " Am usingly , since the a uthors did not write a pa per to describe it, researchers often cite — slide 29 in lecture 6–\n",
      " in their pa pers.\n",
      "16\n",
      " — A dam: A M ethod for Stochastic Optimiza tion, – D . Kingma, J . Ba (2015).\n",
      "17\n",
      " These are estima tions of the mean and (uncen tered) variance of the gradien ts. The mean is often called the\n",
      "†rst\n",
      "  m o m en t\n",
      ", while the variance is often called the \n",
      " s e c o n d m o m en t\n",
      ", hence the name of the algorithm.\n",
      "RMSProp\n",
      " Although A daGrad slows down a bit too fast and ends up never con verging to the\n",
      " global optim um, the \n",
      " RMS P r o p\n",
      " \n",
      "algorithm\n",
      "15\n",
      "  fixes this by accum ula ting only the gradiƒ\n",
      " en ts from the most recen t itera tions (as opposed to all the gradien ts since the beginƒ\n",
      " ning of training). I t does so by using exponen tial deca y in the first step (see \n",
      " Equa tion\n",
      "11-7\n",
      ").\n",
      " Eq u a t i o n 11-7. RMS P r o p a l go r i t h m\n",
      " 1 .\n",
      "s\n",
      "Ÿ\n",
      "s\n",
      "+\n",
      " 1 ”\n",
      "Ÿ\n",
      "•\n",
      "J\n",
      "•\n",
      "\n",
      "•\n",
      "J\n",
      "•\n",
      " 2 .\n",
      "•\n",
      "•\n",
      "”\n",
      "−\n",
      "•\n",
      "J\n",
      "•\n",
      "s\n",
      "+\n",
      " The deca y ra te \n",
      "Ÿ\n",
      " \n",
      " is typically set to 0.9. Y es, it is once again a new h yperparameter , but\n",
      " this defa ult value often works well, so you ma y not need to tune it a t all.\n",
      " As you migh t expect, K eras has an \n",
      "RMSProp\n",
      "  optimizer :\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "RMSprop\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.001\n",
      ",\n",
      " \n",
      "rho\n",
      "=\n",
      "0.9\n",
      ")\n",
      " Except on ver y sim ple problems, this optimizer almost alwa ys performs m uch better\n",
      " than A daGrad. In fact, it was the preferred optimiza tion algorithm of man y researchƒ\n",
      " ers un til A dam optimiza tion came around.\n",
      "Adam and Nadam Optimization\n",
      " Ad a m\n",
      ",\n",
      "16\n",
      "   which  stands for \n",
      " a d a p t i v e m o m en t e s t i m a t i o n\n",
      " ,  combines the ideas of M omenƒ\n",
      " tum optimiza tion and RMSProp: just like M omen tum optimiza tion it keeps track of\n",
      " an exponen tially deca ying a verage of past gradien ts, and just like RMSProp it keeps\n",
      " track of an exponen tially deca ying a verage of past squared gradien ts (see \n",
      " Equa tion\n",
      "11-8\n",
      ").\n",
      "17\n",
      " Faster Optimizers  |  349\n",
      "\n",
      " Eq u a t i o n 11-8. Ad a m a l go r i t h m\n",
      " 1 .\n",
      "m\n",
      "Ÿ\n",
      "1\n",
      "m\n",
      "”\n",
      " 1 ”\n",
      "Ÿ\n",
      "1\n",
      "•\n",
      "J\n",
      "•\n",
      " 2 .\n",
      "s\n",
      "Ÿ\n",
      "2\n",
      "s\n",
      "+\n",
      " 1 ”\n",
      "Ÿ\n",
      "2\n",
      "•\n",
      "J\n",
      "•\n",
      "\n",
      "•\n",
      "J\n",
      "•\n",
      " 3 .\n",
      "m\n",
      "m\n",
      " 1 ”\n",
      "Ÿ\n",
      "1\n",
      "t\n",
      " 4 .\n",
      "s\n",
      "s\n",
      " 1 ”\n",
      "Ÿ\n",
      "2\n",
      "t\n",
      " 5 .\n",
      "•\n",
      "•\n",
      "+\n",
      "−\n",
      "m\n",
      "s\n",
      "+\n",
      "⁄\n",
      "t\n",
      "  represen ts the itera tion n umber (starting a t 1).\n",
      " If you just look a t steps 1, 2, and 5, you will notice A dam ‡ s close similarity to both\n",
      " M omen tum optimiza tion and RMSProp . The only difference is tha t step 1 com putes\n",
      " an exponen tially deca ying a verage ra ther than an exponen tially deca ying sum, but\n",
      " these are actually equivalen t except for a constan t factor (the deca ying a verage is just\n",
      "1 − \n",
      "Ÿ\n",
      "1\n",
      " \n",
      " times the deca ying sum). Steps 3 and 4 are somewha t of a technical detail: since\n",
      "m\n",
      " and \n",
      "s\n",
      "  are initialized a t 0, they will be biased toward 0 a t the beginning of training,\n",
      "so these two steps will help boost \n",
      "m\n",
      " and \n",
      "s\n",
      "  a t the beginning of training.\n",
      " The momen tum deca y h yperparameter \n",
      "Ÿ\n",
      "1\n",
      " \n",
      "is typically initialized to 0.9, while the scalƒ\n",
      " ing deca y h yperparameter \n",
      "Ÿ\n",
      "2\n",
      "  is often initialized to 0.999. As earlier , the \n",
      "smoothing\n",
      "term \n",
      " \n",
      " is usually initialized to a tin y n umber such as 10\n",
      "−7\n",
      " . These are the defa ult values\n",
      "for the \n",
      "Adam\n",
      " class (to be precise, \n",
      "epsilon\n",
      "  defa ults to \n",
      "None\n",
      " , which tells K eras to use\n",
      "keras.backend.epsilon()\n",
      " , which defa ults to 10\n",
      "−7\n",
      "; you can change it using\n",
      "keras.backend.set_epsilon()\n",
      ").\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "Adam\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.001\n",
      ",\n",
      " \n",
      "beta_1\n",
      "=\n",
      "0.9\n",
      ",\n",
      " \n",
      "beta_2\n",
      "=\n",
      "0.999\n",
      ")\n",
      " Since A dam is an ada ptive learning ra te algorithm (like A daGrad and RMSProp), it\n",
      " requires less tuning of the learning ra te h yperparameter \n",
      "−\n",
      " . Y ou can often use the\n",
      " defa ult value \n",
      "−\n",
      "  = 0.001, making A dam even easier to use than Gradien t Descen t.\n",
      " If you are starting to feel over whelmed by all these differen t techniƒ\n",
      " ques, and wondering how to choose the righ t ones for your task,\n",
      " don ‡ t worr y : some practical guidelines are provided a t the end of\n",
      " this cha pter .\n",
      " Finally , two varian ts of A dam are worth men tioning:\n",
      " 350  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "18\n",
      " —Incorpora ting N esterov M omen tum in to A dam, – T imoth y Doza t (2015).\n",
      "19\n",
      " — The M arginal V alue of A da ptive Gradien t M ethods in M achine Learning, – A. C. W ilson et al. (2017).\n",
      "⁄\n",
      " A damax, in troduced in the same pa per as A dam: notice tha t in step 2 of \n",
      " Equa tion\n",
      "11-8\n",
      " , A dam accum ula tes the squares of the gradien ts in \n",
      "s\n",
      " \n",
      " (with a grea ter weigh t\n",
      " for more recen t weigh ts). In step 5, if we ignore \n",
      " and steps 3 and 4 (which are\n",
      " technical details an ywa y), A dam just scales down the parameter upda tes by the\n",
      "square root of \n",
      "s\n",
      " . In short, A dam scales down the parameter upda tes by the …\n",
      "2\n",
      " norm of the time-deca yed gradien ts (recall tha t the …\n",
      "2\n",
      " norm is the square root of\n",
      " the sum of squares). A damax just replaces the …\n",
      "2\n",
      " \n",
      "norm with the …\n",
      "‚\n",
      " \n",
      "norm (a fancy\n",
      " wa y of sa ying the max). Specifically , it replaces step 2 in \n",
      " Equa tion 11-8\n",
      " \n",
      "with\n",
      "max\n",
      "Ÿ\n",
      "2\n",
      ",\n",
      "–\n",
      "J\n",
      "–\n",
      " , it drops step 4, and in step 5 it scales down the gradien t\n",
      " upda tes by a factor of \n",
      "s\n",
      " , which is just the max of the time-deca yed gradien ts. In\n",
      " practice, this can make A damax more stable than A dam, but this really depends\n",
      " on the da taset, and in general A dam actually performs better . So it ‡ s just one\n",
      " more optimizer you can tr y if you experience problems with A dam on some task.\n",
      "⁄\n",
      " N adam optimiza tion\n",
      "18\n",
      "  is more im portan t: it is sim ply A dam optimiza tion plus\n",
      " the N esterov trick, so it will often con verge sligh tly faster than A dam. In his\n",
      " report, T imoth y Doza t com pares man y differen t optimizers on various tasks, and\n",
      " finds tha t N adam generally outperforms A dam, but is sometimes outperformed\n",
      " by RMSProp .\n",
      " A da ptive optimiza tion methods (including RMSProp , A dam and\n",
      " N adam optimiza tion) are often grea t, con verging fast to a good solƒ\n",
      " ution. H owever , a \n",
      " 2017 pa per\n",
      "19\n",
      "  by Ashia C. W ilson et al. showed\n",
      " tha t they can lead to solutions tha t generalize poorly on some da taƒ\n",
      " sets. So when you are disa ppoin ted by your model ‡ s performance,\n",
      " tr y using plain N esterov A ccelera ted Gradien t instead: your da taset\n",
      " ma y just be allergic to ada ptive gradien ts. Also check out the la test\n",
      " research, it is moving fast (e.g., A daB ound).\n",
      " All the optimiza tion techniques discussed so far only rely on the \n",
      "†rst-order\n",
      "  p a r t i a l\n",
      " d er i v a t i v e s\n",
      " \n",
      "(\n",
      " J a c o b i a ns\n",
      " ). The optimiza tion litera ture con tains amazing algorithms\n",
      "based on the \n",
      " s e c o n d-o r d er p a r t i a l d er i v a t i v e s\n",
      " \n",
      "(the \n",
      " H e s s i a ns\n",
      ", which are the partial\n",
      " deriva tives of the J acobians). U nfortuna tely , these algorithms are ver y hard to a pply\n",
      " to deep neural networks beca use there are \n",
      "n\n",
      "2\n",
      " \n",
      " H essians per output (where \n",
      "n\n",
      " \n",
      "is the\n",
      " n umber of parameters), as opposed to just \n",
      "n\n",
      "  J acobians per output. Since DNN s typiƒ\n",
      " cally ha ve tens of thousands of parameters, the second-order optimiza tion algorithms\n",
      " Faster Optimizers  |  351\n",
      "\n",
      "20\n",
      " —Primal-Dual Subgradien t M ethods for Con vex Problems, – Y urii N esterov (2005).\n",
      "21\n",
      " — A d Click Prediction: a V iew from the T renches, – H. M cM ahan et al. (2013).\n",
      " often don ‡ t even fit in memor y , and even when they do , com puting the H essians is \n",
      " just too slow .\n",
      "Training Sparse Models\n",
      " All the optimiza tion algorithms just presen ted produce dense models, meaning tha t\n",
      " most parameters will be nonzero . If you need a blazingly fast model a t run time, or if\n",
      " you need it to take up less memor y , you ma y prefer to end up with a sparse model\n",
      "instead.\n",
      " One trivial wa y to achieve this is to train the model as usual, then get rid of the tin y\n",
      " weigh ts (set them to 0). H owever , this will typically not lead to a ver y sparse model,\n",
      " and it ma y degrade the model ‡ s performance.\n",
      " A better option is to a pply strong …\n",
      "1\n",
      "  regulariza tion during training, as it pushes the\n",
      " optimizer to zero out as man y weigh ts as it can (as discussed in \n",
      " Cha pter 4\n",
      "   about Lasso\n",
      "Regression).\n",
      " H owever , in some cases these techniques ma y remain insufficien t. One last option is\n",
      " to a pply \n",
      " D u a l Av er a g i n g\n",
      ", \n",
      "often called \n",
      " F o l l o w \n",
      "•e\n",
      "  R e g u l a r iz e d L e a d er\n",
      " \n",
      "(FTRL), a \n",
      "techniƒ\n",
      " que proposed by Y urii N esterov\n",
      ".\n",
      "20\n",
      " When used with …\n",
      "1\n",
      " \n",
      " regulariza tion, this technique\n",
      " often leads to ver y sparse models. K eras im plemen ts a varian t of FTRL called \n",
      " FTRL -\n",
      " P r o xi m a l\n",
      "21\n",
      " in the \n",
      "FTRL\n",
      "  optimizer .\n",
      "Learning Rate Scheduling\n",
      " Finding a good learning ra te can be tricky . If you set it wa y too high, training ma y\n",
      "actually diverge (as we discussed in \n",
      " Cha pter 4\n",
      " ). If you set it too low , training will\n",
      " even tually con verge to the optim um, but it will take a ver y long time. If you set it\n",
      " sligh tly too high, it will make progress ver y quickly a t first, but it will end up dancing\n",
      " around the optim um, never really settling down. If you ha ve a limited com puting\n",
      " budget, you ma y ha ve to in terrupt training before it has con verged properly , yielding\n",
      "a suboptimal solution (see \n",
      "Figure 11-8\n",
      ").\n",
      " 352  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " F i g u r e 11-8. L e a r n i n g cu r v e s f o r v a r i o u s l e a r n i n g r a t e s −\n",
      "As we discussed in \n",
      " Cha pter 10\n",
      " , one a pproach is to start with a large learning ra te, and\n",
      " divide it by 3 un til the training algorithm stops diverging. Y ou will not be too far\n",
      " from the optimal learning ra te, which will learn quickly and con verge to good soluƒ\n",
      "tion.\n",
      " H owever , you can do better than a constan t learning ra te: if you start with a high\n",
      " learning ra te and then reduce it once it stops making fast progress, you can reach a\n",
      " good solution faster than with the optimal constan t learning ra te. There are man y difƒ\n",
      " feren t stra tegies to reduce the learning ra te during training. These stra tegies are called\n",
      " l e a r n i n g s c h e d u l e s\n",
      "  (we briefly in troduced this concept in \n",
      " Cha pter 4\n",
      "), the most comƒ\n",
      "mon of which are:\n",
      " P o w er s c h e d u l i n g\n",
      " Set the learning ra te to a function of the itera tion n umber \n",
      "t\n",
      ": \n",
      "−\n",
      "(\n",
      "t\n",
      ") = \n",
      "−\n",
      "0\n",
      " / (1 + \n",
      "t\n",
      "/\n",
      "k\n",
      ")\n",
      "c\n",
      ".\n",
      " The initial learning ra te \n",
      "−\n",
      "0\n",
      ", the power \n",
      "c\n",
      " (typically set to 1) and the steps \n",
      "s\n",
      " \n",
      "are\n",
      " h yperparameters. The learning ra te drops a t each step , and after \n",
      "s\n",
      " \n",
      "steps it is down\n",
      "to \n",
      "−\n",
      "0\n",
      " / 2. After \n",
      "s\n",
      " more steps, it is down to \n",
      "−\n",
      "0\n",
      " / 3. Then down to \n",
      "−\n",
      "0\n",
      " / 4, then \n",
      "−\n",
      "0\n",
      " \n",
      "/ 5,\n",
      " and so on. As you can see, this schedule first drops quickly , then more and more\n",
      " slowly . Of course, this requires tuning \n",
      "−\n",
      "0\n",
      ", \n",
      "s\n",
      " (and possibly \n",
      "c\n",
      ").\n",
      " E xp o n en t i a l s c h e d u l i n g\n",
      " Set the learning ra te to: \n",
      "−\n",
      "(\n",
      "t\n",
      ") = \n",
      "−\n",
      "0\n",
      " \n",
      "0.1\n",
      "t/s\n",
      " . The learning ra te will gradually drop by a\n",
      " factor of 10 ever y \n",
      "s\n",
      " \n",
      " steps. While power scheduling reduces the learning ra te more\n",
      " and more slowly , exponen tial scheduling keeps slashing it by a factor of \n",
      "10\n",
      " \n",
      " ever y\n",
      "s\n",
      " steps.\n",
      " P i e c e w i s e c o ns t a n t s c h e d u l i n g\n",
      " U se a constan t learning ra te for a n umber of epochs (e.g., \n",
      "−\n",
      "0\n",
      " = 0.1 for 5 epochs),\n",
      " then a smaller learning ra te for another n umber of epochs (e.g., \n",
      "−\n",
      "1\n",
      " \n",
      "= 0.001 for 50\n",
      " epochs), and so on. Although this solution can work ver y well, it requires fidƒ\n",
      " Faster Optimizers  |  353\n",
      "\n",
      "22\n",
      " — An Em pirical Study of Learning R a tes in Deep N eural N etworks for Speech Recognition, – A. Senior et al.\n",
      "(2013).\n",
      " dling around to figure out the righ t sequence of learning ra tes, and how long to\n",
      "use each of them.\n",
      " P er f o r m a n c e s c h e d u l i n g\n",
      " M easure the valida tion error ever y \n",
      "N\n",
      " steps (just like for early stopping) and\n",
      " reduce the learning ra te by a factor of \n",
      "Ž\n",
      " when the error stops dropping.\n",
      "A \n",
      " 2013 pa per\n",
      "22\n",
      "  by Andrew Senior et al. com pared the performance of some of the\n",
      "most popular learning schedules when training deep neural networks for speech recƒ\n",
      " ognition using M omen tum optimiza tion. The a uthors concluded tha t, in this setting,\n",
      " both performance scheduling and exponen tial scheduling performed well. They\n",
      " fa vored exponen tial scheduling beca use it was easy to tune and it con verged sligh tly\n",
      " faster to the optimal solution (they also men tioned tha t it was easier to im plemen t\n",
      " than performance scheduling, but in K eras both options are easy).\n",
      " Im plemen ting power scheduling in K eras is the easiest option: just set the \n",
      "decay\n",
      " h yperparameter when crea ting an optimizer . The \n",
      "decay\n",
      "  is the in verse of \n",
      "s\n",
      "  (the n umƒ\n",
      " ber of steps it takes to divide the learning ra te by one more unit), and K eras assumes\n",
      " tha t \n",
      "c\n",
      "  is equal to 1. F or exam ple:\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.01\n",
      ",\n",
      " \n",
      "decay\n",
      "=\n",
      "1e-4\n",
      ")\n",
      " Exponen tial scheduling and piecewise scheduling are quite sim ple too . Y ou first need\n",
      " to define a function tha t takes the curren t epoch and returns the learning ra te. F or\n",
      " exam ple, let ‡ s im plemen t exponen tial scheduling:\n",
      "def\n",
      " \n",
      "exponential_decay_fn\n",
      "(\n",
      "epoch\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "0.01\n",
      " \n",
      "*\n",
      " \n",
      "0.1\n",
      "**\n",
      "(\n",
      "epoch\n",
      " \n",
      "/\n",
      " \n",
      "20\n",
      ")\n",
      " If you do not wan t to hard-code \n",
      "−\n",
      "0\n",
      " and \n",
      "s\n",
      " , you can crea te a function tha t returns a\n",
      "configured function:\n",
      "def\n",
      " \n",
      "exponential_decay\n",
      "(\n",
      "lr0\n",
      ",\n",
      " \n",
      "s\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "exponential_decay_fn\n",
      "(\n",
      "epoch\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "lr0\n",
      " \n",
      "*\n",
      " \n",
      "0.1\n",
      "**\n",
      "(\n",
      "epoch\n",
      " \n",
      "/\n",
      " \n",
      "s\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "exponential_decay_fn\n",
      "exponential_decay_fn\n",
      " \n",
      "=\n",
      " \n",
      "exponential_decay\n",
      "(\n",
      "lr0\n",
      "=\n",
      "0.01\n",
      ",\n",
      " \n",
      "s\n",
      "=\n",
      "20\n",
      ")\n",
      " N ext, just crea te a \n",
      "LearningRateScheduler\n",
      " callback, giving it the schedule function,\n",
      "and pass this callback to the \n",
      "fit()\n",
      " method:\n",
      "lr_scheduler\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "LearningRateScheduler\n",
      "(\n",
      "exponential_decay_fn\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train_scaled\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "[\n",
      "...\n",
      "],\n",
      " \n",
      "callbacks\n",
      "=\n",
      "[\n",
      "lr_scheduler\n",
      "])\n",
      " 354  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "The \n",
      "LearningRateScheduler\n",
      "  will upda te the optimizer‡ s \n",
      "learning_rate\n",
      " \n",
      " a ttribute a t\n",
      " the beginning of each epoch. U pda ting the learning ra te just once per epoch is usually\n",
      " enough, but if you wan t it to be upda ted more often, for exam ple a t ever y step , you\n",
      " need to write your own callback (see the notebook for an exam ple). This can make\n",
      " sense if there are man y steps per epoch.\n",
      " The schedule function can optionally take the curren t learning ra te as a second arguƒ\n",
      " men t. F or exam ple, the following schedule function just m ultiplies the previous\n",
      " learning ra te by 0.1\n",
      "&1/20\n",
      " , which results in the same exponen tial deca y (except the deca y\n",
      " now starts a t the beginning of epoch 0 instead of 1). This im plemen ta tion relies on\n",
      " the optimizer‡ s initial learning ra te (con trar y to the previous im plemen ta tion), so\n",
      " make sure to set it a ppropria tely .\n",
      "def\n",
      " \n",
      "exponential_decay_fn\n",
      "(\n",
      "epoch\n",
      ",\n",
      " \n",
      "lr\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "lr\n",
      " \n",
      "*\n",
      " \n",
      "0.1\n",
      "**\n",
      "(\n",
      "1\n",
      " \n",
      "/\n",
      " \n",
      "20\n",
      ")\n",
      " When you sa ve a model, the optimizer and its learning ra te get sa ved along with it.\n",
      " This means tha t with this new schedule function, you could just load a trained model\n",
      " and con tin ue training where it left off, no problem. H owever , things are not so sim ple\n",
      "if your schedule function uses the \n",
      "epoch\n",
      "  argumen t: indeed, the epoch does not get\n",
      " sa ved, and it gets reset to 0 ever y time you call the \n",
      "fit()\n",
      " \n",
      "method. This could lead to a\n",
      " ver y large learning ra te when you con tin ue training a model where it left off, which\n",
      " would likely damage your model ‡ s weigh ts. One solution is to man ually set the \n",
      "fit()\n",
      " method ‡ s \n",
      "initial_epoch\n",
      "  argumen t so the \n",
      "epoch\n",
      "  starts a t the righ t value.\n",
      " F or piecewise constan t scheduling, you can use a schedule function like the following\n",
      " one (as earlier , you can define a more general function if you wan t, see the notebook\n",
      " for an exam ple), then crea te a \n",
      "LearningRateScheduler\n",
      " callback with this function\n",
      "and pass it to the \n",
      "fit()\n",
      "  method, just like we did for exponen tial scheduling:\n",
      "def\n",
      " \n",
      "piecewise_constant_fn\n",
      "(\n",
      "epoch\n",
      "):\n",
      "    \n",
      "if\n",
      " \n",
      "epoch\n",
      " \n",
      "<\n",
      " \n",
      "5\n",
      ":\n",
      "        \n",
      "return\n",
      " \n",
      "0.01\n",
      "    \n",
      "elif\n",
      " \n",
      "epoch\n",
      " \n",
      "<\n",
      " \n",
      "15\n",
      ":\n",
      "        \n",
      "return\n",
      " \n",
      "0.005\n",
      "    \n",
      "else\n",
      ":\n",
      "        \n",
      "return\n",
      " \n",
      "0.001\n",
      " F or performance scheduling, sim ply use the \n",
      "ReduceLROnPlateau\n",
      "  callback. F or examƒ\n",
      "ple, if you pass the following callback to the \n",
      "fit()\n",
      "  method, it will m ultiply the learnƒ\n",
      " ing ra te by 0.5 whenever the best valida tion loss does not im prove for 5 consecutive\n",
      " epochs (other options are a vailable, please check the documen ta tion for more\n",
      "details):\n",
      "lr_scheduler\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "callbacks\n",
      ".\n",
      "ReduceLROnPlateau\n",
      "(\n",
      "factor\n",
      "=\n",
      "0.5\n",
      ",\n",
      " \n",
      "patience\n",
      "=\n",
      "5\n",
      ")\n",
      " Lastly , tf.keras offers an alterna tive wa y to im plemen t learning ra te scheduling: just\n",
      " define the learning ra te using one of the schedules a vailable in \n",
      "keras.optimiz\n",
      " Faster Optimizers  |  355\n",
      "\n",
      "ers.schedules\n",
      " , then pass this learning ra te to an y optimizer . This a pproach upda tes\n",
      " the learning ra te a t each step ra ther than a t each epoch. F or exam ple, here is how to\n",
      " im plemen t the same exponen tial schedule as earlier :\n",
      "s\n",
      " \n",
      "=\n",
      " \n",
      "20\n",
      " \n",
      "*\n",
      " \n",
      "len\n",
      "(\n",
      "X_train\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "32\n",
      " \n",
      "# number of steps in 20 epochs (batch size = 32)\n",
      "learning_rate\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "schedules\n",
      ".\n",
      "ExponentialDecay\n",
      "(\n",
      "0.01\n",
      ",\n",
      " \n",
      "s\n",
      ",\n",
      " \n",
      "0.1\n",
      ")\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "learning_rate\n",
      ")\n",
      " This is nice and sim ple, plus when you sa ve the model, the learning ra te and its\n",
      " schedule (including its sta te) get sa ved as well. H owever , this a pproach is not part of\n",
      " the K eras API, it is specific to tf.keras.\n",
      " T o sum up , exponen tial deca y or performance scheduling can considerably speed \n",
      "up\n",
      " con vergence, so give them a tr y!\n",
      "Avoiding \n",
      "Over•tting\n",
      " Through Regularization\n",
      " W ith four parameters I can fit an elephan t and with five I can make him wiggle his\n",
      "trunk.\n",
      " ›J ohn von N eumann, \n",
      " ci t e d b y E n r i c o F er m i i n N a t u r e 427\n",
      " W ith thousands of parameters you can fit the whole zoo . Deep neural networks \n",
      "typiƒ\n",
      " cally ha ve tens of thousands of parameters, sometimes even millions. W ith so man y\n",
      " parameters, the network has an incredible amoun t of freedom and can fit a h uge variƒ\n",
      " ety of com plex da tasets. But this grea t flexibility also means tha t it is prone to overfitƒ\n",
      " ting the training set. W e need regulariza tion.\n",
      " W e already im plemen ted one of the best regulariza tion techniques in \n",
      " Cha pter 10\n",
      ":\n",
      " early stopping. M oreover , even though Ba tch N ormaliza tion was designed to solve\n",
      " the vanishing/exploding gradien ts problems, is also acts like a pretty good regularizer .\n",
      " In this section we will presen t other popular regulariza tion techniques for neural netƒ\n",
      "works: …\n",
      "1\n",
      " and …\n",
      "2\n",
      "  regulariza tion, dropout and max-norm regulariza tion.\n",
      "–\n",
      "1\n",
      " and –\n",
      "2\n",
      " Regularization\n",
      " J ust \n",
      "like you did in \n",
      " Cha pter 4\n",
      " \n",
      " for sim ple linear models, you can use …\n",
      "1\n",
      "   and …\n",
      "2\n",
      "   regulariƒ\n",
      " za tion to constrain a neural network ‡ s connection weigh ts (but typically not its biaƒ\n",
      " ses). H ere is how to a pply …\n",
      "2\n",
      "  regulariza tion to a K eras la yer‡ s connection weigh ts,\n",
      " using a regulariza tion factor of 0.01:\n",
      "layer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      "                           \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ",\n",
      "                           \n",
      "kernel_regularizer\n",
      "=\n",
      "keras\n",
      ".\n",
      "regularizers\n",
      ".\n",
      "l2\n",
      "(\n",
      "0.01\n",
      "))\n",
      "The \n",
      "l2()\n",
      "  function returns a regularizer tha t will be called to com pute the regularizaƒ\n",
      " tion loss, a t each step during training. This regulariza tion loss is then added to the\n",
      " final loss. As you migh t expect, you can just use \n",
      "keras.regularizers.l1()\n",
      " if you\n",
      " 356  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "23\n",
      " —Im proving neural networks by preven ting co-ada pta tion of fea ture detectors, – G. Hin ton et al. (2012).\n",
      "24\n",
      " —Dropout: A Sim ple W a y to Preven t N eural N etworks from O verfitting, – N. Srivasta va et al. (2014).\n",
      " wan t …\n",
      "1\n",
      "  regulariza tion, and if you wan t both …\n",
      "1\n",
      " and …\n",
      "2\n",
      "  regulariza tion, use \n",
      "keras.regu\n",
      "larizers.l1_l2()\n",
      "  (specif ying both regulariza tion factors).\n",
      " Since you will typically wan t to a pply the same regularizer to all la yers in your netƒ\n",
      " work, as well as the same activa tion function and the same initializa tion stra teg y in all\n",
      " hidden la yers, you ma y find yourself repea ting the same argumen ts over and over .\n",
      " This makes it ugly and error -prone. T o a void this, you can tr y refactoring your code\n",
      " to use loops. Another option is to use Python ‡ s \n",
      "functools.partial()\n",
      " \n",
      "function: it lets\n",
      " you crea te a thin wra pper for an y callable, with some defa ult argumen t values. F or\n",
      " exam ple:\n",
      "from\n",
      " \n",
      "functools\n",
      " \n",
      "import\n",
      " \n",
      "partial\n",
      "RegularizedDense\n",
      " \n",
      "=\n",
      " \n",
      "partial\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      ",\n",
      "                           \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      "                           \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ",\n",
      "                           \n",
      "kernel_regularizer\n",
      "=\n",
      "keras\n",
      ".\n",
      "regularizers\n",
      ".\n",
      "l2\n",
      "(\n",
      "0.01\n",
      "))\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(\n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      "]),\n",
      "    \n",
      "RegularizedDense\n",
      "(\n",
      "300\n",
      "),\n",
      "    \n",
      "RegularizedDense\n",
      "(\n",
      "100\n",
      "),\n",
      "    \n",
      "RegularizedDense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ",\n",
      "                     \n",
      "kernel_initializer\n",
      "=\n",
      "\"glorot_uniform\"\n",
      ")\n",
      "])\n",
      "Dropout\n",
      " D r o p o u t\n",
      "  is one of the most popular regulariza tion techniques for deep neural netƒ\n",
      " works. I t was \n",
      "proposed\n",
      "23\n",
      " \n",
      " by Geoffrey Hin ton in 2012 and further detailed in a \n",
      " pa per\n",
      "24\n",
      " by N itish Srivasta va et al., and it has proven to be highly successful: even the sta te-of-\n",
      " the-art neural networks got a 1−2% accuracy boost sim ply by adding dropout. This\n",
      " ma y not sound like a lot, but when a model already has 95% accuracy , getting a 2%\n",
      " accuracy boost means dropping the error ra te by almost 40% (going from 5% error to\n",
      "roughly 3%).\n",
      " I t is a fairly sim ple algorithm: a t ever y training step , ever y neuron (including the\n",
      " in put neurons, but alwa ys excluding the output neurons) has a probability \n",
      "p\n",
      " \n",
      "of being\n",
      " tem porarily — dropped out, – meaning it will be en tirely ignored during this training\n",
      " step , but it ma y be active during the next step (see \n",
      "Figure 11-9\n",
      " ). The h yperparameter\n",
      "p\n",
      " \n",
      "is called the \n",
      " d r o p o u t r a t e\n",
      ", \n",
      " and it is typically set to 50%. After training, neurons don ‡ t\n",
      " get dropped an ymore. And tha t ‡ s all (except for a technical detail we will discuss\n",
      " momen tarily).\n",
      "Avoiding \n",
      "Over•tting\n",
      "  Through Regularization  |  357\n",
      "\n",
      " F i g u r e 11-9. D r o p o u t r e g u l a r iz a t i o n\n",
      " I t is quite surprising a t first tha t this ra ther brutal technique works a t all. W ould a\n",
      " com pan y perform better if its em ployees were told to toss a coin ever y morning to\n",
      " decide whether or not to go to work? W ell, who knows; perha ps it would! The comƒ\n",
      " pan y would obviously be forced to ada pt its organiza tion; it could not rely on an y sinƒ\n",
      " gle person to fill in the coffee machine or perform an y other critical tasks, so this\n",
      " expertise would ha ve to be spread across several people. Em ployees would ha ve to\n",
      " learn to coopera te with man y of their coworkers, not just a handful of them. The\n",
      " com pan y would become m uch more resilien t. If one person quit, it wouldn ‡ t make\n",
      " m uch of a difference. I t ‡ s unclear whether this idea would actually work for com paƒ\n",
      " nies, but it certainly does for neural networks. N eurons trained with dropout cannot\n",
      " co-ada pt with their neighboring neurons; they ha ve to be as useful as possible on\n",
      " their own. They also cannot rely excessively on just a few in put neurons; they m ust\n",
      " pa y a tten tion to each of their in put neurons. They end up being less sensitive to sligh t\n",
      " changes in the in puts. In the end you get a more robust network tha t generalizes betƒ\n",
      " ter .\n",
      " Another wa y to understand the power of dropout is to realize tha t a unique neural\n",
      " network is genera ted a t each training step . Since each neuron can be either presen t or\n",
      " absen t, there is a total of 2\n",
      "N\n",
      " possible networks (where \n",
      "N\n",
      "  is the total n umber of dropƒ\n",
      " pable neurons). This is such a h uge n umber tha t it is virtually im possible for the same\n",
      " neural network to be sam pled twice. Once you ha ve run a 10,000 training steps, you\n",
      " ha ve essen tially trained 10,000 differen t neural networks (each with just one training\n",
      " instance). These neural networks are obviously not independen t since they share\n",
      " man y of their weigh ts, but they are nevertheless all differen t. The resulting neural\n",
      " network can be seen as an a veraging ensemble of all these smaller neural networks.\n",
      " There is one small but im portan t technical detail. Suppose \n",
      "p\n",
      " = 50%, in which case\n",
      " during testing a neuron will be connected to twice as man y in put neurons as it was\n",
      " (on a verage) during training. T o com pensa te for this fact, we need to m ultiply each\n",
      " 358  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "25\n",
      " This is specific to tf.keras, so you ma y prefer to use \n",
      "keras.backend.set_learning_phase(1)\n",
      " before calling\n",
      "the \n",
      "fit()\n",
      "  method (and set it back to 0 righ t after).\n",
      " neuron ‡ s in put connection weigh ts by 0.5 after training. If we don ‡ t, each neuron will\n",
      " get a total in put signal roughly twice as large as wha t the network was trained on, and\n",
      " it is unlikely to perform well. M ore generally , we need to m ultiply each in put connecƒ\n",
      " tion weigh t by the \n",
      " k e e p p r o b a b i l i ty\n",
      " (1 − \n",
      "p\n",
      " ) after training. Alterna tively , we can divide\n",
      " each neuron ‡ s output by the keep probability during training (these alterna tives are\n",
      " not perfectly equivalen t, but they work equally well).\n",
      " T o im plemen t dropout using K eras, you can use the \n",
      "keras.layers.Dropout\n",
      " \n",
      " la yer .\n",
      " During training, it randomly drops some in puts (setting them to 0) and divides the\n",
      " remaining in puts by the keep probability . After training, it does nothing a t all, it just\n",
      " passes the in puts to the next la yer . F or exam ple, the following code a pplies dropout\n",
      " regulariza tion before ever y \n",
      "Dense\n",
      "  la yer , using a dropout ra te of 0.2:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(\n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      "]),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dropout\n",
      "(\n",
      "rate\n",
      "=\n",
      "0.2\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "300\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dropout\n",
      "(\n",
      "rate\n",
      "=\n",
      "0.2\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dropout\n",
      "(\n",
      "rate\n",
      "=\n",
      "0.2\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ")\n",
      "])\n",
      "Since dropout is only active during training, the training loss is\n",
      " penalized com pared to the valida tion loss, so com paring the two\n",
      " can be misleading. In particular , a model ma y be overfitting the\n",
      " training set and yet ha ve similar training and valida tion losses. So\n",
      " make sure to evalua te the training loss without dropout (e.g., after\n",
      " training). Alterna tively , you can call the \n",
      "fit()\n",
      " method inside a\n",
      "with keras.backend.learning_phase_scope(1)\n",
      " block: this will\n",
      " force dropout to be active during both training and valida tion.\n",
      "25\n",
      " If you obser ve tha t the model is overfitting, you can increase the dropout ra te. Conƒ\n",
      " versely , you should tr y decreasing the dropout ra te if the model underfits the training\n",
      " set. I t can also help to increase the dropout ra te for large la yers, and reduce it for\n",
      " small ones. M oreover , man y sta te-of-the-art architectures only use dropout after the\n",
      " last hidden la yer , so you ma y wan t to tr y this if full dropout is too strong.\n",
      " Dropout does tend to significan tly slow down con vergence, but it usually results in a\n",
      " m uch better model when tuned properly . So , it is generally well worth the extra time\n",
      "and effort.\n",
      "Avoiding \n",
      "Over•tting\n",
      "  Through Regularization  |  359\n",
      "\n",
      "26\n",
      " —Dropout as a Ba yesian A pproxima tion: Represen ting M odel U ncertain ty in Deep Learning, – Y . Gal and Z.\n",
      "Ghahramani (2016).\n",
      "27\n",
      " Specifically , they show tha t training a dropout network is ma thema tically equivalen t to a pproxima te Ba yesian\n",
      "inference in a specific type of probabilistic model called a \n",
      " d e e p G a u s s i a n P r o c e s s\n",
      ".\n",
      " If you wan t to regularize a self-normalizing network based on the\n",
      " SEL U activa tion function (as discussed earlier), you should use\n",
      "AlphaDropout\n",
      " : this is a varian t of dropout tha t preser ves the mean\n",
      " and standard devia tion of its in puts (it was in troduced in the same\n",
      " pa per as SEL U , as regular dropout would break self-normaliza tion).\n",
      "Monte-Carlo (MC) Dropout\n",
      "In 2016, a \n",
      " pa per\n",
      "26\n",
      " \n",
      " by Y arin Gal and Zoubin Ghahramani added more good reasons to\n",
      "use dropout:\n",
      "⁄\n",
      " First, the pa per establishes a profound connection between dropout networks\n",
      " (i.e., neural networks con taining a dropout la yer before ever y weigh t la yer) and\n",
      " a pproxima te Ba yesian inference\n",
      "27\n",
      " , giving dropout a solid ma thema tical justificaƒ\n",
      "tion.\n",
      "⁄\n",
      " Second, they in troduce a powerful technique called \n",
      " M C D r o p o u t\n",
      ", which can\n",
      " boost the performance of an y trained dropout model, without ha ving to retrain it\n",
      " or even modif y it a t all!\n",
      "⁄\n",
      " M oreover , MC Dropout also provides a m uch better measure of the model ‡ s\n",
      " uncertain ty .\n",
      "⁄\n",
      " Finally , it is also amazingly sim ple to im plemen t. If this all sounds like a — one\n",
      " weird trick – advertisemen t, then take a look a t the following code. I t is the full\n",
      " im plemen ta tion of \n",
      " M C D r o p o u t\n",
      " , boosting the dropout model we trained earlier ,\n",
      "without retraining it:\n",
      "with\n",
      " \n",
      "keras\n",
      ".\n",
      "backend\n",
      ".\n",
      "learning_phase_scope\n",
      "(\n",
      "1\n",
      "):\n",
      " \n",
      "# force training mode = dropout on\n",
      "    \n",
      "y_probas\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "stack\n",
      "([\n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test_scaled\n",
      ")\n",
      "                         \n",
      "for\n",
      " \n",
      "sample\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "100\n",
      ")])\n",
      "y_proba\n",
      " \n",
      "=\n",
      " \n",
      "y_probas\n",
      ".\n",
      "mean\n",
      "(\n",
      "axis\n",
      "=\n",
      "0\n",
      ")\n",
      " W e first force training mode on, using a \n",
      "learning_phase_scope(1)\n",
      "  con text. This\n",
      "turns dropout on within the \n",
      "with\n",
      " block. Then we make 100 predictions over the test\n",
      " set, and we stack them. Since dropout is on, all predictions will be differen t. Recall\n",
      " tha t \n",
      "predict()\n",
      " \n",
      " returns a ma trix with one row per instance, and one column per class.\n",
      " Since there are 10,000 instances in the test set, and 10 classes, this is a ma trix of sha pe\n",
      " [10000, 10]. W e stack 100 such ma trices, so \n",
      "y_probas\n",
      "   is an arra y of sha pe [100, 10000,\n",
      " 10]. Once we a verage over the first dimension (\n",
      "axis=0\n",
      "), we get \n",
      "y_proba\n",
      " , an arra y of\n",
      " sha pe [10000, 10], like we would get with a single prediction. Tha t ‡ s all! A veraging\n",
      " 360  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " over m ultiple predictions with dropout on gives us a M on te Carlo estima te tha t is\n",
      " generally more reliable than the result of a single prediction with dropout off. F or\n",
      " exam ple, let ‡ s look a t the model ‡ s prediction for the first instance in the test set, with\n",
      "dropout off:\n",
      ">>> \n",
      "np\n",
      ".\n",
      "round\n",
      "(\n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "X_test_scaled\n",
      "[:\n",
      "1\n",
      "]),\n",
      " \n",
      "2\n",
      ")\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
      "      dtype=float32)\n",
      " The model seems almost certain tha t this image belongs to class 9 (ankle boot).\n",
      " Should you trust it? I s there really so little room for doubt? Com pare this with the\n",
      " predictions made when dropout is activa ted:\n",
      ">>> \n",
      "np\n",
      ".\n",
      "round\n",
      "(\n",
      "y_probas\n",
      "[:,\n",
      " \n",
      ":\n",
      "1\n",
      "],\n",
      " \n",
      "2\n",
      ")\n",
      "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],\n",
      "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],\n",
      "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
      "       [...]\n",
      " This tells a ver y differen t stor y : a pparen tly , when we activa te dropout, the model is\n",
      " not sure an ymore. I t still seems to prefer class 9, but sometimes it hesita tes with\n",
      " classes 5 (sandal) and 7 (sneaker), which makes sense given they‡ re all footwear . Once\n",
      " we a verage over the first dimension, we get the following MC dropout predictions:\n",
      ">>> \n",
      "np\n",
      ".\n",
      "round\n",
      "(\n",
      "y_proba\n",
      "[:\n",
      "1\n",
      "],\n",
      " \n",
      "2\n",
      ")\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]],\n",
      "      dtype=float32)\n",
      "The model still thinks this image belongs to class 9, but only with a 62% confidence,\n",
      " which seems m uch more reasonable than 99%. Plus it ‡ s useful to know exactly which\n",
      " other classes it thinks are likely . And you can also take a look a t the \n",
      "standard deviaƒ\n",
      " tion of the probability estima tes\n",
      ":\n",
      ">>> \n",
      "y_std\n",
      " \n",
      "=\n",
      " \n",
      "y_probas\n",
      ".\n",
      "std\n",
      "(\n",
      "axis\n",
      "=\n",
      "0\n",
      ")\n",
      ">>> \n",
      "np\n",
      ".\n",
      "round\n",
      "(\n",
      "y_std\n",
      "[:\n",
      "1\n",
      "],\n",
      " \n",
      "2\n",
      ")\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],\n",
      "      dtype=float32)\n",
      " A pparen tly there ‡ s quite a lot of variance in the probability estima tes: if you were\n",
      "building a risk-sensitive system (e.g., a medical or financial system), you should probƒ\n",
      " ably trea t such an uncertain prediction with extreme ca ution. Y ou definitely would\n",
      " not trea t it like a 99% confiden t prediction. M oreover , the model ‡ s accuracy got a\n",
      "small boost from 86.8 to 86.9:\n",
      ">>> \n",
      "accuracy\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "sum\n",
      "(\n",
      "y_pred\n",
      " \n",
      "==\n",
      " \n",
      "y_test\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "len\n",
      "(\n",
      "y_test\n",
      ")\n",
      ">>> \n",
      "accuracy\n",
      "0.8694\n",
      "Avoiding \n",
      "Over•tting\n",
      "  Through Regularization  |  361\n",
      "\n",
      " The n umber of M on te Carlo sam ples you use (100 in this exam ple)\n",
      " is a h yperparameter you can tweak. The higher it is, the more accuƒ\n",
      " ra te the predictions and their uncertain ty estima tes will be. H owƒ\n",
      " ever , it you double it, inference time will also be doubled.\n",
      " M oreover , above a certain n umber of sam ples, you will notice little\n",
      " im provemen t. So your job is to find the righ t tradeoff between\n",
      " la tency and accuracy , depending on your a pplica tion.\n",
      " If your model con tains other la yers tha t beha ve in a special wa y during training (such\n",
      " as Ba tch N ormaliza tion la yers), then you should not force training mode like we just\n",
      "did. Instead, you should replace the \n",
      "Dropout\n",
      "  la yers with the following \n",
      "MCDropout\n",
      "class:\n",
      "class\n",
      " \n",
      "MCDropout\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dropout\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "super\n",
      "()\n",
      ".\n",
      "call\n",
      "(\n",
      "inputs\n",
      ",\n",
      " \n",
      "training\n",
      "=\n",
      "True\n",
      ")\n",
      " W e just sublass the \n",
      "Dropout\n",
      "  la yer and override the \n",
      "call()\n",
      " method to force its \n",
      "train\n",
      "ing\n",
      "  argumen t to \n",
      "True\n",
      " (see \n",
      " Cha pter 12\n",
      " ). Similarly , you could define an \n",
      "MCAlphaDrop\n",
      "out\n",
      " class by subclassing \n",
      "AlphaDropout\n",
      "  instead. If you are crea ting a model from\n",
      " scra tch, it ‡ s just a ma tter of using \n",
      "MCDropout\n",
      "  ra ther than \n",
      "Dropout\n",
      " . But if you ha ve a\n",
      " model tha t was already trained using \n",
      "Dropout\n",
      " , you need to crea te a new model, idenƒ\n",
      "tical to the existing model except replacing the \n",
      "Dropout\n",
      "  la yers with \n",
      "MCDropout\n",
      ", then\n",
      " copy the existing model ‡ s weigh ts to your new model.\n",
      " In short, MC Dropout is a fan tastic technique tha t boosts dropout models and proƒ\n",
      " vides better uncertain ty estima tes. And of course, since it is just regular dropout durƒ\n",
      " ing training, it also acts like a regularizer .\n",
      "Max-Norm Regularization\n",
      " Another regulariza tion technique tha t is quite popular for neural networks is called\n",
      " m ax-n o r m r e g u l a r iz a t i o n\n",
      " : for each neuron, it constrains the weigh ts \n",
      "w\n",
      " of the incomƒ\n",
      " ing connections such tha t \n",
      " *w* \n",
      "2\n",
      " \n",
      "ł\n",
      " _r_, where \n",
      "r\n",
      "  is the max-norm h yperparameter\n",
      "and \n",
      " ’ \n",
      "2\n",
      " is the …\n",
      "2\n",
      " norm.\n",
      " M ax-norm regulariza tion does not add a regulariza tion loss term to the overall loss\n",
      " function. Instead, it is typically im plemen ted by com puting \n",
      "w\n",
      "2\n",
      " \n",
      "after each training\n",
      "step and clipping \n",
      "w\n",
      " if needed (\n",
      "w\n",
      "w\n",
      "r\n",
      "w\n",
      "2\n",
      ").\n",
      "Reducing \n",
      "r\n",
      " \n",
      " increases the amoun t of regulariza tion and helps reduce overfitting. M ax-\n",
      " norm regulariza tion can also help allevia te the vanishing/exploding gradien ts probƒ\n",
      " lems (if you are not using Ba tch N ormaliza tion).\n",
      " 362  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      " T o im plemen t max-norm regulariza tion in K eras, just set ever y hidden la yer‡ s \n",
      "ker\n",
      "nel_constraint\n",
      "  argumen t to a \n",
      "max_norm()\n",
      "  constrain t, with the a ppropria te max\n",
      " value, for exam ple:\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "100\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ",\n",
      "                   \n",
      "kernel_constraint\n",
      "=\n",
      "keras\n",
      ".\n",
      "constraints\n",
      ".\n",
      "max_norm\n",
      "(\n",
      "1.\n",
      "))\n",
      " After each training itera tion, the model ‡ s \n",
      "fit()\n",
      " method will call the object returned\n",
      "by \n",
      "max_norm()\n",
      " , passing it the la yer‡ s weigh ts and getting clipped weigh ts in return,\n",
      " which then replace the la yer‡ s weigh ts. As we will see in \n",
      " Cha pter 12\n",
      ", you can define\n",
      " your own custom constrain t function if you ever need to , and use it as the \n",
      "ker\n",
      "nel_constraint\n",
      " . Y ou can also constrain the bias terms by setting the \n",
      "bias_con\n",
      "straint\n",
      "  argumen t.\n",
      "The \n",
      "max_norm()\n",
      " \n",
      "function has an \n",
      "axis\n",
      " \n",
      " argumen t tha t defa ults to 0. A \n",
      "Dense\n",
      "   la yer usuƒ\n",
      " ally has weigh ts of sha pe [n umber of in puts, n umber of neurons], so using \n",
      "axis=0\n",
      " means tha t the max norm constrain t will a pply independen tly to each neuron ‡ s weigh t\n",
      " vector . If you wan t to use max-norm with con volutional la yers (see \n",
      " Cha pter 14\n",
      "),\n",
      "make sure to set the \n",
      "max_norm()\n",
      " \n",
      " constrain t ‡ s \n",
      "axis\n",
      "  argumen t a ppropria tely (usually\n",
      "axis=[0, 1, 2]\n",
      ").\n",
      "Summary and Practical Guidelines\n",
      "In \n",
      " this cha pter , we ha ve covered a wide range of techniques and you ma y be wonderƒ\n",
      " ing which ones you should use. The configura tion in \n",
      " T able 11-2\n",
      " \n",
      "will work fine in\n",
      " most cases, without requiring m uch h yperparameter tuning.\n",
      " T a b l e 11-2. D ef a u l t D NN \n",
      "con†guration\n",
      "Hyperparameter\n",
      " Default   value\n",
      " Kernel   initializer:\n",
      " LeCun   initialization\n",
      " Activation   function:\n",
      "SELU\n",
      "Normalization:\n",
      " None   (self-normalization)\n",
      "Regularization:\n",
      " Early   stopping\n",
      "Optimizer:\n",
      "Nadam\n",
      " Learning   rate   schedule:\n",
      " Performance   scheduling\n",
      " Don ‡ t forget to standardize the in put fea tures! Of course, you should also tr y to reuse\n",
      " parts of a pretrained neural network if you can find one tha t solves a similar problem,\n",
      " or use unsuper vised pretraining if you ha ve a lot of unlabeled da ta, or pretraining on\n",
      " an a uxiliar y task if you ha ve a lot of labeled da ta for a similar task.\n",
      " The defa ult configura tion in \n",
      " T able 11-2\n",
      "  ma y need to be tweaked:\n",
      " Summary and Practical Guidelines  |  363\n",
      "\n",
      "⁄\n",
      "If your model self-normalizes:\n",
      "›\n",
      " If it overfits the training set, then you should add alpha dropout (and alwa ys\n",
      " use early stopping as well). Do not use other regulariza tion methods, or else\n",
      " they would break self-normaliza tion.\n",
      "⁄\n",
      " If your model cannot self-normalize (e.g., it is a recurren t net or it con tains skip\n",
      "connections):\n",
      "›\n",
      " Y ou can tr y using EL U (or another activa tion function) instead of SEL U , it\n",
      " ma y perform better . M ake sure to change the initializa tion method accordƒ\n",
      " ingly (e.g., H e init for EL U or ReL U).\n",
      "›\n",
      " If it is a deep network, you should use Ba tch N ormaliza tion after ever y hidden\n",
      " la yer . If it overfits the training set, you can also tr y using max-norm or …\n",
      "2\n",
      "   regƒ\n",
      " ulariza tion.\n",
      "⁄\n",
      "If you need a sparse model, you can use …\n",
      "1\n",
      "   regulariza tion (and optionally zero out\n",
      " the tin y weigh ts after training). If you need an even sparser model, you can tr y\n",
      " using FTRL instead of N adam optimiza tion, along with …\n",
      "1\n",
      "  regulariza tion. In an y\n",
      " case, this will break self-normaliza tion, so you will need to switch to BN if your\n",
      " model is deep .\n",
      "⁄\n",
      " If you need a low-la tency model (one tha t performs ligh tning-fast predictions),\n",
      " you ma y need to use less la yers, a void Ba tch N ormaliza tion, and possibly replace\n",
      " the SEL U activa tion function with the leaky ReL U . H a ving a sparse model will\n",
      " also help . Y ou ma y also wan t to reduce the floa t precision from 32-bits to 16-bit\n",
      "(or even 8-bits) (see \n",
      "???\n",
      ").\n",
      "⁄\n",
      " If you are building a risk-sensitive a pplica tion, or inference la tency is not ver y\n",
      " im portan t in your a pplica tion, you can use MC Dropout to boost performance\n",
      " and get more reliable probability estima tes, along with uncertain ty estima tes.\n",
      " W ith these guidelines, you are now ready to train ver y deep nets! I hope you are now\n",
      " con vinced tha t you can go a ver y long wa y using just K eras. H owever , there ma y\n",
      " come a time when you need to ha ve even more con trol, for exam ple to write a custom\n",
      " loss function or to tweak the training algorithm. F or such cases, you will need to use\n",
      " T ensorFlow‡ s lower -level API, as we will see in the next cha pter .\n",
      "Exercises\n",
      "1.\n",
      " I s it oka y to initialize all the weigh ts to the same value as long as tha t value is\n",
      " selected randomly using H e initializa tion?\n",
      "2.\n",
      " I s it oka y to initialize the bias terms to 0?\n",
      "3.\n",
      " N ame three advan tages of the SEL U activa tion function over ReL U .\n",
      " 364  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "4.\n",
      " In which cases would you wan t to use each of the following activa tion functions:\n",
      " SEL U , leaky ReL U (and its varian ts), ReL U , tanh, logistic, and softmax?\n",
      "5.\n",
      " Wha t ma y ha ppen if you set the \n",
      "momentum\n",
      "  h yperparameter too close to 1 (e.g.,\n",
      "0.99999) when using an \n",
      "SGD\n",
      " optimizer?\n",
      "6.\n",
      " N ame three wa ys you can produce a sparse model.\n",
      "7.\n",
      "Does dropout slow down training? Does it slow down inference (i.e., making\n",
      " predictions on new instances)? Wha t are about MC dropout?\n",
      "8.\n",
      "Deep Learning.\n",
      "a.\n",
      " Build a DNN with five hidden la yers of 100 neurons each, H e initializa tion,\n",
      " and the EL U activa tion function.\n",
      " b .\n",
      " U sing A dam optimiza tion and early stopping, tr y training it on MNIST but\n",
      "only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\n",
      " next exercise. Y ou will need a softmax output la yer with five neurons, and as\n",
      " alwa ys make sure to sa ve checkpoin ts a t regular in ter vals and sa ve the final\n",
      " model so you can reuse it la ter .\n",
      "c.\n",
      " T une the h yperparameters using cross-valida tion and see wha t precision you\n",
      "can achieve.\n",
      "d.\n",
      " N ow tr y adding Ba tch N ormaliza tion and com pare the learning cur ves: is it\n",
      " con verging faster than before? Does it produce a better model?\n",
      "e.\n",
      " I s the model overfitting the training set? T r y adding dropout to ever y la yer\n",
      " and tr y again. Does it help?\n",
      "9.\n",
      " T ransfer learning.\n",
      "a.\n",
      " Crea te a new DNN tha t reuses all the pretrained hidden la yers of the previous\n",
      " model, freezes them, and replaces the softmax output la yer with a new one.\n",
      " b .\n",
      " T rain this new DNN on digits 5 to 9, using only 100 images per digit, and time\n",
      " how long it takes. Despite this small n umber of exam ples, can you achieve\n",
      "high precision?\n",
      "c.\n",
      " T r y caching the frozen la yers, and train the model again: how m uch faster is it\n",
      "now?\n",
      "d.\n",
      " T r y again reusing just four hidden la yers instead of five. Can you achieve a\n",
      "higher precision?\n",
      "e.\n",
      " N ow unfreeze the top two hidden la yers and con tin ue training: can you get\n",
      "the model to perform even better?\n",
      "10.\n",
      " Pretraining on an a uxiliar y task.\n",
      "a.\n",
      " In this exercise you will build a DNN tha t com pares two MNIST digit images\n",
      " and predicts whether they represen t the same digit or not. Then you will reuse\n",
      " the lower la yers of this network to train an MNIST classifier using ver y little\n",
      " Exercises  |  365\n",
      "\n",
      " training da ta. Start by building two DNN s (let ‡ s call them DNN A and B), both\n",
      " similar to the one you built earlier but without the output la yer : each DNN\n",
      " should ha ve five hidden la yers of 100 neurons each, H e initializa tion, and EL U\n",
      " activa tion. N ext, add one more hidden la yer with 10 units on top of both\n",
      " DNN s. T o do this, you should use a \n",
      "keras.layers.Concatenate\n",
      " \n",
      " la yer to conƒ\n",
      " ca tena te the outputs of both DNN s for each instance, then feed the result to\n",
      " the hidden la yer . Finally , add an output la yer with a single neuron using the\n",
      " logistic activa tion function.\n",
      " b .\n",
      " Split the MNIST training set in two sets: split #1 should con taining 55,000\n",
      " images, and split #2 should con tain con tain 5,000 images. Crea te a function\n",
      " tha t genera tes a training ba tch where each instance is a pair of MNIST images\n",
      " picked from split #1. H alf of the training instances should be pairs of images\n",
      " tha t belong to the same class, while the other half should be images from difƒ\n",
      " feren t classes. F or each pair , the training label should be 0 if the images are\n",
      " from the same class, or 1 if they are from differen t classes.\n",
      "c.\n",
      " T rain the DNN on this training set. F or each image pair , you can sim ultaneƒ\n",
      " ously feed the first image to DNN A and the second image to DNN B . The\n",
      "whole network will gradually learn to tell whether two images belong to the\n",
      "same class or not.\n",
      "d.\n",
      " N ow crea te a new DNN by reusing and freezing the hidden la yers of DNN A\n",
      " and adding a softmax output la yer on top with 10 neurons. T rain this network\n",
      " on split #2 and see if you can achieve high performance despite ha ving only\n",
      "500 images per class.\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " 366  |  Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "CHAPTER 12\n",
      "Custom Models and Training with\n",
      "TensorFlow\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 12 in the final\n",
      "release of the book.\n",
      " So far we ha ve used only T ensorFlow‡ s high level API, tf.keras, but it already got us\n",
      " pretty far : we built various neural network architectures, including regression and\n",
      " classifica tion nets, wide & deep nets and self-normalizing nets, using all sorts of techƒ\n",
      " niques, such as Ba tch N ormaliza tion, dropout, learning ra te schedules, and more. In\n",
      " fact, 95% of the use cases you will encoun ter will not require an ything else than\n",
      " tf.keras (and tf.da ta, see \n",
      " Cha pter 13\n",
      " ). But now it ‡ s time to dive deeper in to T ensorFlow\n",
      " and take a look a t its lower -level \n",
      "Python API\n",
      ". This will be useful when you need extra\n",
      " con trol, to write custom loss functions, custom metrics, la yers, models, initializers,\n",
      " regularizers, weigh t constrain ts and more. Y ou ma y even need to fully con trol the\n",
      " training loop itself, for exam ple to a pply special transforma tions or constrain ts to the\n",
      " gradien ts (beyond just clipping them), or to use m ultiple optimizers for differen t\n",
      " parts of the network. W e will cover all these cases in this cha pter , then we will also\n",
      " look a t how you can boost your custom models and training algorithms using T enƒ\n",
      " sorFlow‡ s a utoma tic gra ph genera tion fea ture. But first, let ‡ s take a quick tour of T enƒ\n",
      " sorFlow .\n",
      "367\n",
      "\n",
      "1\n",
      " T ensorFlow also includes another Deep Learning API called the \n",
      " E s t i m a t o r s AP I\n",
      ", but it is now recommended\n",
      "to use tf.keras instead.\n",
      " T ensorFlow 2.0 was released in M arch 2019, making T ensorFlow\n",
      " m uch easier to use. The first edition of this book used TF 1, while\n",
      "this edition uses TF 2.\n",
      "A Quick Tour of TensorFlow\n",
      " As you know , \n",
      " T ens o rF l o w\n",
      "  is a powerful librar y for n umerical com puta tion, particuƒ\n",
      " larly well suited and fine-tuned for large-scale M achine Learning (but you could use\n",
      " it for an ything else tha t requires hea vy com puta tions). I t was developed by \n",
      "the Google\n",
      " Brain team and it powers man y of Google ‡ s large-scale ser vices, such as Google Cloud\n",
      " Speech, Google Photos, and Google Search. I t was open sourced in N ovember 2015,\n",
      " and it is now the most popular deep learning librar y (in terms of cita tions in pa pers,\n",
      " adoption in com panies, stars on gith ub , etc.): coun tless projects use T ensorFlow for\n",
      " all sorts of M achine Learning tasks, such as image classifica tion, na tural language\n",
      " processing (NLP), recommender systems, time series forecasting, and m uch more.\n",
      " So wha t does T ensorFlow actually offer? H ere ‡ s a summar y :\n",
      "⁄\n",
      " I ts core is ver y similar to N umPy , but with GPU support.\n",
      "⁄\n",
      " I t also supports distributed com puting (across m ultiple devices and ser vers).\n",
      "⁄\n",
      " I t includes a kind of just-in-time ( JIT) com piler tha t allows it to optimize com puƒ\n",
      " ta tions for speed and memor y usage: it works by extracting the \n",
      " c o m p u t a t i o n\n",
      " g r a p h\n",
      " \n",
      " from a Python function, then optimizing it (e.g., by pruning un used nodes)\n",
      " and finally running it efficien tly (e.g., by a utoma tically running independen t\n",
      " opera tions in parallel).\n",
      "⁄\n",
      " Com puta tion gra phs can be exported to a portable forma t, so you can train a\n",
      " T ensorFlow model in one en vironmen t (e.g., using Python on Lin ux), and run it\n",
      " in another (e.g., using J a va on an Android device).\n",
      "⁄\n",
      " I t im plemen ts a utodiff (see \n",
      " Cha pter 10\n",
      " and \n",
      "???\n",
      " ), and provides some excellen t\n",
      " optimizers, such as RMSProp , N adam and FTRL (see \n",
      " Cha pter 11\n",
      "), so you can\n",
      "easily minimize all sorts of loss functions.\n",
      "⁄\n",
      " T ensorFlow offers man y more fea tures, built on top of these core fea tures: the\n",
      " most im portan t is of course tf.keras\n",
      "1\n",
      " , but it also has da ta loading & preprocessing\n",
      " ops (tf.da ta, tf.io , etc.), image processing ops (tf.image), signal processing ops\n",
      "(tf.signal), and more (see \n",
      "Figure 12-1\n",
      " \n",
      " for an over view of T ensorFlow‡ s Python\n",
      "API).\n",
      " 368  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "2\n",
      " If you ever need to (but you probably won ‡ t), you can write your own opera tions using the C++ API.\n",
      "3\n",
      " If you are a researcher , you ma y be eligible to use these TPU s for free, see \n",
      "https://tensor…ow.org/tfrc/\n",
      " for more\n",
      "details.\n",
      " F i g u r e 12-1. T ens o rF l o w ‹ s P y t h o n AP I\n",
      " W e will cover man y of the packages and functions of the T ensorƒ\n",
      " Flow API, but it ‡ s im possible to cover them all so you should really\n",
      " take some time to browse through the API: you will find tha t it is\n",
      " quite rich and well documen ted.\n",
      " A t the lowest level, each T ensorFlow opera tion is im plemen ted using highly efficien t\n",
      "C++ code\n",
      "2\n",
      " . M an y opera tions (or \n",
      " o p s\n",
      "  for short) ha ve m ultiple im plemen ta tions, called\n",
      " k er n e l s\n",
      " : each kernel is dedica ted to a specific device type, such as CPU s, GPU s, or\n",
      " even TPU s (\n",
      " T ens o r P r o c e s s i n g U n i ts\n",
      " ). As you ma y know , GPU s can drama tically speed\n",
      " up com puta tions by splitting com puta tions in to man y smaller ch unks and running\n",
      " them in parallel across man y GPU threads. TPU s are even faster . Y ou can purchase\n",
      " your own GPU devices (for now , T ensorFlow only supports N vidia cards with CUD A\n",
      " Com pute Ca pability 3.5+), but TPU s are only a vailable on \n",
      " G o ogl e Cl o u d M a c h i n e\n",
      " L e a r n i n g E n g i n e\n",
      " (see \n",
      "???\n",
      ").\n",
      "3\n",
      " T ensorFlow‡ s architecture is shown in \n",
      "Figure 12-2\n",
      ": most of the time your code will\n",
      " use the high level API s (especially tf.keras and tf.da ta), but when you need more flexiƒ\n",
      " bility you will use the lower level Python API, handling tensors directly . N ote tha t\n",
      " API s for other languages are also a vailable. In an y case, T ensorFlow‡ s execution\n",
      " A Quick Tour of TensorFlow  |  369\n",
      "\n",
      " engine will take care of running the opera tions efficien tly , even across m ultiple deviƒ\n",
      " ces and machines if you tell it to .\n",
      " F i g u r e 12-2. T ens o rF l o w ‹ s a r c h i t e c t u r e\n",
      " T ensorFlow runs not only on W indows, Lin ux, and M acOS, but also on mobile deviƒ\n",
      "ces (using \n",
      " T ens o rF l o w L i t e\n",
      "), including both iOS and Android (see \n",
      "???\n",
      "). If you do not\n",
      " wan t to use the Python API, there are also C++, J a va, Go and Swift API s. There is\n",
      " even a J a vascript im plemen ta tion called \n",
      " T ens o rF l o w .js\n",
      "  tha t makes it possible to run\n",
      " your models directly in your browser .\n",
      " There ‡ s more to T ensorFlow than just the librar y . T ensorFlow is a t the cen ter of an\n",
      " extensive ecosystem of libraries. First, there ‡ s T ensorB oard for visualiza tion (see\n",
      " Cha pter 10\n",
      " ). N ext, there ‡ s \n",
      " T ensorFlow Extended (TFX)\n",
      ", which is a set of libraries built\n",
      " by Google to productionize T ensorFlow projects: it includes tools for da ta valida tion,\n",
      " preprocessing, model analysis and ser ving (with TF Ser ving, see \n",
      "???\n",
      "). Google also\n",
      " la unched \n",
      " T ens o rF l o w H u b\n",
      " , a wa y to easily download and reuse pretrained neural netƒ\n",
      " works. Y ou can also get man y neural network architectures, some of them pretrained,\n",
      " in T ensorFlow‡ s \n",
      "model garden\n",
      ". Check out the \n",
      " T ensorFlow Resources\n",
      ", or \n",
      "https://\n",
      "github.com/jtoy/awesome-tensor…ow\n",
      "  for more T ensorFlow-based projects. Y ou will\n",
      " find h undreds of T ensorFlow projects on GitH ub , so it is often easy to find existing\n",
      " code for wha tever you are tr ying to do .\n",
      " M ore and more ML pa pers are released along with their im plemenƒ\n",
      " ta tion, and sometimes even with pretrained models. Check out\n",
      " h ttp s://p a p er s w i t h c o d e.c o m/\n",
      " to easily find them.\n",
      " 370  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " Last but not least, T ensorFlow has a dedica ted team of passiona te and helpful develƒ\n",
      " opers, and a large comm unity con tributing to im proving it. T o ask technical quesƒ\n",
      "tions, you should use \n",
      "http://stackover…ow.com/\n",
      " \n",
      "and tag your question with \n",
      "tensor…ow\n",
      "and \n",
      " p y t h o n\n",
      " . Y ou can file bugs and fea ture requests through GitH ub . F or general disƒ\n",
      "cussions, join the \n",
      "Google group\n",
      ".\n",
      " Oka y , it ‡ s time to start coding!\n",
      "Using TensorFlow like NumPy\n",
      " T ensorFlow‡ s API revolves around \n",
      " t ens o r s\n",
      " , hence the name T ensor -Flow . A tensor is\n",
      " usually a m ultidimensional arra y (exactly like a N umPy \n",
      "ndarray\n",
      "), but it can also hold\n",
      " a scalar (a sim ple value, such as 42). These tensors will be im portan t when we crea te\n",
      " custom cost functions, custom metrics, custom la yers and more, so let ‡ s see how to\n",
      " crea te and manipula te them.\n",
      "Tensors and Operations\n",
      " Y ou can easily crea te a tensor , using \n",
      "tf.constant()\n",
      " . F or exam ple, here is a tensor\n",
      " represen ting a ma trix with two rows and three columns of floa ts:\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "constant\n",
      "([[\n",
      "1.\n",
      ",\n",
      " \n",
      "2.\n",
      ",\n",
      " \n",
      "3.\n",
      "],\n",
      " \n",
      "[\n",
      "4.\n",
      ",\n",
      " \n",
      "5.\n",
      ",\n",
      " \n",
      "6.\n",
      "]])\n",
      " \n",
      "# matrix\n",
      "<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "42\n",
      ")\n",
      " \n",
      "# scalar\n",
      "<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\n",
      " J ust like an \n",
      "ndarray\n",
      ", a \n",
      "tf.Tensor\n",
      "  has a sha pe and a da ta type (\n",
      "dtype\n",
      "):\n",
      ">>> \n",
      "t\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "constant\n",
      "([[\n",
      "1.\n",
      ",\n",
      " \n",
      "2.\n",
      ",\n",
      " \n",
      "3.\n",
      "],\n",
      " \n",
      "[\n",
      "4.\n",
      ",\n",
      " \n",
      "5.\n",
      ",\n",
      " \n",
      "6.\n",
      "]])\n",
      ">>> \n",
      "t\n",
      ".\n",
      "shape\n",
      "TensorShape([2, 3])\n",
      ">>> \n",
      "t\n",
      ".\n",
      "dtype\n",
      "tf.float32\n",
      " Indexing works m uch like in N umPy :\n",
      ">>> \n",
      "t\n",
      "[:,\n",
      " \n",
      "1\n",
      ":]\n",
      "<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[2., 3.],\n",
      "       [5., 6.]], dtype=float32)>\n",
      ">>> \n",
      "t\n",
      "[\n",
      "...\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "tf\n",
      ".\n",
      "newaxis\n",
      "]\n",
      "<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\n",
      "array([[2.],\n",
      "       [5.]], dtype=float32)>\n",
      " M ost im portan tly , all sorts of tensor opera tions are a vailable:\n",
      ">>> \n",
      "t\n",
      " \n",
      "+\n",
      " \n",
      "10\n",
      "<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\n",
      " Using TensorFlow like NumPy  |  371\n",
      "\n",
      "array([[11., 12., 13.],\n",
      "       [14., 15., 16.]], dtype=float32)>\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "square\n",
      "(\n",
      "t\n",
      ")\n",
      "<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=\n",
      "array([[ 1.,  4.,  9.],\n",
      "       [16., 25., 36.]], dtype=float32)>\n",
      ">>> \n",
      "t\n",
      " \n",
      "@\n",
      " \n",
      "tf\n",
      ".\n",
      "transpose\n",
      "(\n",
      "t\n",
      ")\n",
      "<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[14., 32.],\n",
      "       [32., 77.]], dtype=float32)>\n",
      " N ote tha t writing \n",
      "t + 10\n",
      " \n",
      " is equivalen t to calling \n",
      "tf.add(t, 10)\n",
      "   (indeed, Python calls\n",
      "the magic method \n",
      "t.__add__(10)\n",
      ", which just calls \n",
      "tf.add(t, 10)\n",
      " ). Other opera tors\n",
      "(like \n",
      "-\n",
      ", \n",
      "*\n",
      ", etc.) are also supported. The \n",
      "@\n",
      " \n",
      " opera tor was added in Python 3.5, for ma trix\n",
      " m ultiplica tion: it is equivalen t to calling the \n",
      "tf.matmul()\n",
      " function.\n",
      " Y ou will find all the basic ma th opera tions you need (e.g., \n",
      "tf.add()\n",
      ", \n",
      "tf.multiply()\n",
      ",\n",
      "tf.square()\n",
      ", \n",
      "tf.exp()\n",
      ", \n",
      "tf.sqrt()\n",
      " Ñ), and more generally most opera tions tha t you\n",
      " can find in N umPy (e.g., \n",
      "tf.reshape()\n",
      ", \n",
      "tf.squeeze()\n",
      ", \n",
      "tf.tile()\n",
      "), but sometimes\n",
      " with a differen t name (e.g., \n",
      "tf.reduce_mean()\n",
      ", \n",
      "tf.reduce_sum()\n",
      ", \n",
      "tf.reduce_max()\n",
      ",\n",
      "tf.math.log()\n",
      "  are the equivalen t of \n",
      "np.mean()\n",
      ", \n",
      "np.sum()\n",
      ", \n",
      "np.max()\n",
      " \n",
      "and \n",
      "np.log()\n",
      ").\n",
      " When the name differs, there is often a good reason for it: for exam ple, in T ensorƒ\n",
      " Flow you m ust write \n",
      "tf.transpose(t)\n",
      ", you cannot just write \n",
      "t.T\n",
      " \n",
      " like in N umPy . The\n",
      " reason is tha t it does not do exactly the same thing: in T ensorFlow , a new tensor is\n",
      " crea ted with its own copy of the transposed da ta, while in N umPy , \n",
      "t.T\n",
      " \n",
      "is just a transƒ\n",
      " posed view on the same da ta. Similarly , the \n",
      "tf.reduce_sum()\n",
      " \n",
      " opera tion is named this\n",
      " wa y beca use its GPU kernel (i.e., GPU im plemen ta tion) uses a reduce algorithm tha t\n",
      " does not guaran tee the order in which the elemen ts are added: beca use 32-bit floa ts\n",
      " ha ve limited precision, this means tha t the result ma y change ever so sligh tly ever y\n",
      " time you call this opera tion. The same is true of \n",
      "tf.reduce_mean()\n",
      " \n",
      "(but of course\n",
      "tf.reduce_max()\n",
      " is deterministic).\n",
      " 372  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "4\n",
      "A notable exception is \n",
      "tf.math.log()\n",
      " which is commonly used but there is no \n",
      "tf.log()\n",
      "  alias (as it migh t be\n",
      "confused with logging).\n",
      " M an y functions and classes ha ve aliases. F or exam ple, \n",
      "tf.add()\n",
      "and \n",
      "tf.math.add()\n",
      "  are the same function. This allows T ensorFlow\n",
      " to ha ve concise names for the most common opera tions\n",
      "4\n",
      ", while\n",
      " preser ving well organized packages.\n",
      "Keras† Low-Level API\n",
      " The K eras API actually has its own low-level API, loca ted in \n",
      "keras.backend\n",
      " . I t\n",
      "includes functions like \n",
      "square()\n",
      ", \n",
      "exp()\n",
      ", \n",
      "sqrt()\n",
      " and so on. In tf.keras, these funcƒ\n",
      " tions generally just call the corresponding T ensorFlow opera tions. If you wan t to\n",
      " write code tha t will be portable to other K eras im plemen ta tions, you should use these\n",
      " K eras functions. H owever , they only cover a subset of all functions a vailable in T enƒ\n",
      " sorFlow , so in this book we will use the T ensorFlow opera tions directly . H ere is as\n",
      " sim ple exam ple using \n",
      "keras.backend\n",
      ", which is commonly named \n",
      "K\n",
      " for short:\n",
      ">>> \n",
      "from\n",
      " \n",
      "tensorflow\n",
      " \n",
      "import\n",
      " \n",
      "keras\n",
      ">>> \n",
      "K\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "backend\n",
      ">>> \n",
      "K\n",
      ".\n",
      "square\n",
      "(\n",
      "K\n",
      ".\n",
      "transpose\n",
      "(\n",
      "t\n",
      "))\n",
      " \n",
      "+\n",
      " \n",
      "10\n",
      "<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\n",
      "array([[11., 26.],\n",
      "       [14., 35.],\n",
      "       [19., 46.]], dtype=float32)>\n",
      "Tensors and NumPy\n",
      " T ensors pla y nice with N umPy : you can crea te a tensor from a N umPy arra y , and vice\n",
      " versa, and you can even a pply T ensorFlow opera tions to N umPy arra ys and N umPy\n",
      " opera tions to tensors:\n",
      ">>> \n",
      "a\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([\n",
      "2.\n",
      ",\n",
      " \n",
      "4.\n",
      ",\n",
      " \n",
      "5.\n",
      "])\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "a\n",
      ")\n",
      "<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\n",
      ">>> \n",
      "t\n",
      ".\n",
      "numpy\n",
      "()\n",
      " \n",
      "# or np.array(t)\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "square\n",
      "(\n",
      "a\n",
      ")\n",
      "<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\n",
      ">>> \n",
      "np\n",
      ".\n",
      "square\n",
      "(\n",
      "t\n",
      ")\n",
      "array([[ 1.,  4.,  9.],\n",
      "       [16., 25., 36.]], dtype=float32)\n",
      " Using TensorFlow like NumPy  |  373\n",
      "\n",
      " N otice tha t N umPy uses 64-bit precision by defa ult, while T ensorƒ\n",
      " Flow uses 32-bit. This is beca use 32-bit precision is generally more\n",
      "than enough for neural networks, plus it runs faster and uses less\n",
      " RAM. So when you crea te a tensor from a N umPy arra y , make sure\n",
      "to set \n",
      "dtype=tf.float32\n",
      ".\n",
      "Type Conversions\n",
      " T ype con versions can significan tly h urt performance, and they can easily go unnoƒ\n",
      " ticed when they are done a utoma tically . T o a void this, T ensorFlow does not perform\n",
      " an y type con versions a utoma tically : it just raises an exception if you tr y to execute an\n",
      " opera tion on tensors with incom pa tible types. F or exam ple, you cannot add a floa t\n",
      " tensor and an in teger tensor , and you cannot even add a 32-bit floa t and a 64-bit floa t:\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "2.\n",
      ")\n",
      " \n",
      "+\n",
      " \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "40\n",
      ")\n",
      "Traceback[...]InvalidArgumentError[...]expected to be a float[...]\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "2.\n",
      ")\n",
      " \n",
      "+\n",
      " \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "40.\n",
      ",\n",
      " \n",
      "dtype\n",
      "=\n",
      "tf\n",
      ".\n",
      "float64\n",
      ")\n",
      "Traceback[...]InvalidArgumentError[...]expected to be a double[...]\n",
      " This ma y be a bit annoying a t first, but remember tha t it ‡ s for a good ca use! And of\n",
      "course you can use \n",
      "tf.cast()\n",
      "  when you really need to con vert types:\n",
      ">>> \n",
      "t2\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "40.\n",
      ",\n",
      " \n",
      "dtype\n",
      "=\n",
      "tf\n",
      ".\n",
      "float64\n",
      ")\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "2.0\n",
      ")\n",
      " \n",
      "+\n",
      " \n",
      "tf\n",
      ".\n",
      "cast\n",
      "(\n",
      "t2\n",
      ",\n",
      " \n",
      "tf\n",
      ".\n",
      "float32\n",
      ")\n",
      "<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\n",
      "Variables\n",
      " So far , we ha ve used constan t tensors: as their name suggests, you cannot modif y\n",
      " them. H owever , the weigh ts in a neural network need to be tweaked by backpropagaƒ\n",
      " tion, and other parameters ma y also need to change over time (e.g., a momen tum\n",
      " optimizer keeps track of past gradien ts). Wha t we need is a \n",
      "tf.Variable\n",
      ":\n",
      ">>> \n",
      "v\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "Variable\n",
      "([[\n",
      "1.\n",
      ",\n",
      " \n",
      "2.\n",
      ",\n",
      " \n",
      "3.\n",
      "],\n",
      " \n",
      "[\n",
      "4.\n",
      ",\n",
      " \n",
      "5.\n",
      ",\n",
      " \n",
      "6.\n",
      "]])\n",
      ">>> \n",
      "v\n",
      "<tf.Variable •Variable:0• shape=(2, 3) dtype=float32, numpy=\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      "A \n",
      "tf.Variable\n",
      "  acts m uch like a constan t tensor : you can perform the same operaƒ\n",
      " tions with it, it pla ys nicely with N umPy as well, and it is just as picky with types. But\n",
      "it can also be modified in place using the \n",
      "assign()\n",
      " method (or \n",
      "assign_add()\n",
      " \n",
      "or\n",
      "assign_sub()\n",
      "  which incremen t or decremen t the variable by the given value). Y ou\n",
      " can also modif y individual cells (or slices), using the cell ‡ s (or slice ‡ s) \n",
      "assign()\n",
      " method (direct item assignmen t will not work), or using the \n",
      "scatter_update()\n",
      " \n",
      "or\n",
      "scatter_nd_update()\n",
      " methods:\n",
      "v\n",
      ".\n",
      "assign\n",
      "(\n",
      "2\n",
      " \n",
      "*\n",
      " \n",
      "v\n",
      ")\n",
      "           \n",
      "# => [[2., 4., 6.], [8., 10., 12.]]\n",
      "v\n",
      "[\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      "]\n",
      ".\n",
      "assign\n",
      "(\n",
      "42\n",
      ")\n",
      "        \n",
      "# => [[2., 42., 6.], [8., 10., 12.]]\n",
      " 374  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "v\n",
      "[:,\n",
      " \n",
      "2\n",
      "]\n",
      ".\n",
      "assign\n",
      "([\n",
      "0.\n",
      ",\n",
      " \n",
      "1.\n",
      "])\n",
      "  \n",
      "# => [[2., 42., 0.], [8., 10., 1.]]\n",
      "v\n",
      ".\n",
      "scatter_nd_update\n",
      "(\n",
      "indices\n",
      "=\n",
      "[[\n",
      "0\n",
      ",\n",
      " \n",
      "0\n",
      "],\n",
      " \n",
      "[\n",
      "1\n",
      ",\n",
      " \n",
      "2\n",
      "]],\n",
      " \n",
      "updates\n",
      "=\n",
      "[\n",
      "100.\n",
      ",\n",
      " \n",
      "200.\n",
      "])\n",
      "                          \n",
      "# => [[100., 42., 0.], [8., 10., 200.]]\n",
      " In practice you will rarely ha ve to crea te variables man ually , since\n",
      " K eras provides an \n",
      "add_weight()\n",
      " \n",
      " method tha t will take care of it for\n",
      " you, as we will see. M oreover , model parameters will generally be\n",
      " upda ted directly by the optimizers, so you will rarely need to\n",
      " upda te variables man ually .\n",
      "Other Data Structures\n",
      " T ensorFlow supports several other da ta structures, including the following (please see\n",
      "the notebook or \n",
      "???\n",
      " for more details):\n",
      "⁄\n",
      " S p a r s e t ens o r s\n",
      " \n",
      "(\n",
      "tf.SparseTensor\n",
      " ) efficien tly represen t tensors con taining mostly\n",
      "0s. The \n",
      "tf.sparse\n",
      "  package con tains opera tions for sparse tensors.\n",
      "⁄\n",
      " T ens o r a r r a ys\n",
      " \n",
      "(\n",
      "tf.TensorArray\n",
      " ) are lists of tensors. They ha ve a fixed size by\n",
      " defa ult, but can optionally be made dynamic. All tensors they con tain m ust ha ve\n",
      " the same sha pe and da ta type.\n",
      "⁄\n",
      " R a g ge d t ens o r s\n",
      " \n",
      "(\n",
      "tf.RaggedTensor\n",
      " ) represen t sta tic lists of lists of tensors, where\n",
      " ever y tensor has the same sha pe and da ta type. The \n",
      "tf.ragged\n",
      " \n",
      " package con tains\n",
      " opera tions for ragged tensors.\n",
      "⁄\n",
      " S t r i n g t ens o r s\n",
      " \n",
      "are regular tensors of type \n",
      "tf.string\n",
      " . These actually represen t byte\n",
      " strings, not U nicode strings, so if you crea te a string tensor using a U nicode\n",
      "string (e.g., a regular Python 3 string like \n",
      "\"caf–\"ƒ\n",
      "), then it will get encoded to\n",
      " UTF -8 a utoma tically (e.g., \n",
      "b\"caf\\xc3\\xa9\"\n",
      " ). Alterna tively , you can represen t\n",
      " U nicode strings using tensors of type \n",
      "tf.int32\n",
      " , where each item represen ts a\n",
      " U nicode codepoin t (e.g., \n",
      "[99, 97, 102, 233]\n",
      "). The \n",
      "tf.strings\n",
      " \n",
      "package (with\n",
      "an \n",
      "s\n",
      " ) con tains ops for byte strings and U nicode strings (and to con vert one in to\n",
      "the other).\n",
      "⁄\n",
      " S e ts\n",
      "  are just represen ted as regular tensors (or sparse tensors) con taining one or\n",
      " more sets, and you can manipula te them using opera tions from the \n",
      "tf.sets\n",
      "package.\n",
      "⁄\n",
      " Q u eu e s\n",
      ", including First In, First Out (FIFO) queues (\n",
      "FIFOQueue\n",
      " ), queues tha t can\n",
      "prioritize some items (\n",
      "PriorityQueue\n",
      " ), queues tha t sh uffle their items (\n",
      "Random\n",
      "ShuffleQueue\n",
      " ), and queues tha t can ba tch items of differen t sha pes by padding\n",
      "(\n",
      "PaddingFIFOQueue\n",
      "). These classes are all in the \n",
      "tf.queue\n",
      " package.\n",
      " W ith tensors, opera tions, variables and various da ta structures a t your disposal, you\n",
      "are now ready to customize your models and training algorithms!\n",
      " Using TensorFlow like NumPy  |  375\n",
      "\n",
      "Customizing Models and Training Algorithms\n",
      " Let ‡ s start by crea ting a custom loss function, which is a sim ple and common use case.\n",
      "Custom Loss Functions\n",
      " Suppose you wan t to train a regression model, but your training set is a bit noisy . Of\n",
      " course, you start by tr ying to clean up your da taset by removing or fixing the outliers,\n",
      " but it turns out to be insufficien t, the da taset is still noisy . Which loss function should\n",
      " you use? The mean squared error migh t penalize large errors too m uch, so your\n",
      " model will end up being im precise. The mean absolute error would not penalize outƒ\n",
      " liers as m uch, but training migh t take a while to con verge and the trained model\n",
      " migh t not be ver y precise. This is probably a good time to use the H uber loss (in troƒ\n",
      "duced in \n",
      " Cha pter 10\n",
      " ) instead of the good old MSE. The H uber loss is not curren tly\n",
      " part of the official K eras API, but it is a vailable in tf.keras (just use an instance of the\n",
      "keras.losses.Huber\n",
      "  class). But let ‡ s pretend it ‡ s not there: im plemen ting it is easy as\n",
      " pie! J ust crea te a function tha t takes the labels and predictions as argumen ts, and use\n",
      " T ensorFlow opera tions to com pute ever y instance ‡ s loss:\n",
      "def\n",
      " \n",
      "huber_fn\n",
      "(\n",
      "y_true\n",
      ",\n",
      " \n",
      "y_pred\n",
      "):\n",
      "    \n",
      "error\n",
      " \n",
      "=\n",
      " \n",
      "y_true\n",
      " \n",
      "-\n",
      " \n",
      "y_pred\n",
      "    \n",
      "is_small_error\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "<\n",
      " \n",
      "1\n",
      "    \n",
      "squared_loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "square\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "2\n",
      "    \n",
      "linear_loss\n",
      "  \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "0.5\n",
      "    \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "where\n",
      "(\n",
      "is_small_error\n",
      ",\n",
      " \n",
      "squared_loss\n",
      ",\n",
      " \n",
      "linear_loss\n",
      ")\n",
      " F or better performance, you should use a vectorized im plemen taƒ\n",
      " tion, as in this exam ple. M oreover , if you wan t to benefit from T enƒ\n",
      " sorFlow‡ s gra ph fea tures, you should use only T ensorFlow\n",
      " opera tions.\n",
      " I t is also preferable to return a tensor con taining one loss per instance, ra ther than\n",
      " returning the mean loss. This wa y , K eras can a pply class weigh ts or sam ple weigh ts\n",
      "when requested (see \n",
      " Cha pter 10\n",
      ").\n",
      " N ext, you can just use this loss when you com pile the K eras model, then train your\n",
      "model:\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "huber_fn\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"nadam\"\n",
      ")\n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "X_train\n",
      ",\n",
      " \n",
      "y_train\n",
      ",\n",
      " \n",
      "[\n",
      "...\n",
      "])\n",
      " And tha t ‡ s it! F or each ba tch during training, K eras will call the \n",
      "huber_fn()\n",
      " \n",
      "function\n",
      " to com pute the loss, and use it to perform a Gradien t Descen t step . M oreover , it will\n",
      " keep track of the total loss since the beginning of the epoch, and it will displa y the\n",
      "mean loss.\n",
      " 376  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " But wha t ha ppens to this custom loss when we sa ve the model?\n",
      "Saving and Loading Models That Contain Custom Components\n",
      " Sa ving a model con taining a custom loss function actually works fine, as K eras just\n",
      " sa ves the name of the function. H owever , whenever you load it, you need to provide a\n",
      " dictionar y tha t ma ps the function name to the actual function. M ore generally , when\n",
      " you load a model con taining custom objects, you need to ma p the names to the\n",
      "objects:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "load_model\n",
      "(\n",
      "\"my_model_with_a_custom_loss.h5\"\n",
      ",\n",
      "                                \n",
      "custom_objects\n",
      "=\n",
      "{\n",
      "\"huber_fn\"\n",
      ":\n",
      " \n",
      "huber_fn\n",
      "})\n",
      " W ith the curren t im plemen ta tion, an y error between -1 and 1 is considered — small – .\n",
      " But wha t if we wan t a differen t threshold? One solution is to crea te a function tha t\n",
      " crea tes a configured loss function:\n",
      "def\n",
      " \n",
      "create_huber\n",
      "(\n",
      "threshold\n",
      "=\n",
      "1.0\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "huber_fn\n",
      "(\n",
      "y_true\n",
      ",\n",
      " \n",
      "y_pred\n",
      "):\n",
      "        \n",
      "error\n",
      " \n",
      "=\n",
      " \n",
      "y_true\n",
      " \n",
      "-\n",
      " \n",
      "y_pred\n",
      "        \n",
      "is_small_error\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "<\n",
      " \n",
      "threshold\n",
      "        \n",
      "squared_loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "square\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "2\n",
      "        \n",
      "linear_loss\n",
      "  \n",
      "=\n",
      " \n",
      "threshold\n",
      " \n",
      "*\n",
      " \n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "threshold\n",
      "**\n",
      "2\n",
      " \n",
      "/\n",
      " \n",
      "2\n",
      "        \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "where\n",
      "(\n",
      "is_small_error\n",
      ",\n",
      " \n",
      "squared_loss\n",
      ",\n",
      " \n",
      "linear_loss\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "huber_fn\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "create_huber\n",
      "(\n",
      "2.0\n",
      "),\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"nadam\"\n",
      ")\n",
      " U nfortuna tely , when you sa ve the model, the \n",
      "threshold\n",
      "   will not be sa ved. This means\n",
      " tha t you will ha ve to specif y the \n",
      "threshold\n",
      "  value when loading the model (note tha t\n",
      "the name to use is \n",
      "\"huber_fn\"\n",
      " , which is the name of the function we ga ve K eras, not\n",
      " the name of the function tha t crea ted it):\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "load_model\n",
      "(\n",
      "\"my_model_with_a_custom_loss_threshold_2.h5\"\n",
      ",\n",
      "                                \n",
      "custom_objects\n",
      "=\n",
      "{\n",
      "\"huber_fn\"\n",
      ":\n",
      " \n",
      "create_huber\n",
      "(\n",
      "2.0\n",
      ")})\n",
      " Y ou can solve this by crea ting a subclass of the \n",
      "keras.losses.Loss\n",
      " \n",
      " class, and im pleƒ\n",
      " men t its \n",
      "get_config()\n",
      " method:\n",
      "class\n",
      " \n",
      "HuberLoss\n",
      "(\n",
      "keras\n",
      ".\n",
      "losses\n",
      ".\n",
      "Loss\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "threshold\n",
      "=\n",
      "1.0\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "self\n",
      ".\n",
      "threshold\n",
      " \n",
      "=\n",
      " \n",
      "threshold\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "y_true\n",
      ",\n",
      " \n",
      "y_pred\n",
      "):\n",
      "        \n",
      "error\n",
      " \n",
      "=\n",
      " \n",
      "y_true\n",
      " \n",
      "-\n",
      " \n",
      "y_pred\n",
      "        \n",
      "is_small_error\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "<\n",
      " \n",
      "self\n",
      ".\n",
      "threshold\n",
      "        \n",
      "squared_loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "square\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "2\n",
      "        \n",
      "linear_loss\n",
      "  \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "threshold\n",
      " \n",
      "*\n",
      " \n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "error\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "self\n",
      ".\n",
      "threshold\n",
      "**\n",
      "2\n",
      " \n",
      "/\n",
      " \n",
      "2\n",
      "        \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "where\n",
      "(\n",
      "is_small_error\n",
      ",\n",
      " \n",
      "squared_loss\n",
      ",\n",
      " \n",
      "linear_loss\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "get_config\n",
      "(\n",
      "self\n",
      "):\n",
      "        \n",
      "base_config\n",
      " \n",
      "=\n",
      " \n",
      "super\n",
      "()\n",
      ".\n",
      "get_config\n",
      "()\n",
      "        \n",
      "return\n",
      " \n",
      "{\n",
      "**\n",
      "base_config\n",
      ",\n",
      " \n",
      "\"threshold\"\n",
      ":\n",
      " \n",
      "self\n",
      ".\n",
      "threshold\n",
      "}\n",
      " Customizing Models and Training Algorithms  |  377\n",
      "\n",
      "5\n",
      " I t would not be a good idea to use a weigh ted mean: if we did, then two instances with the same weigh t but in\n",
      " differen t ba tches would ha ve a differen t im pact on training, depending on the total weigh t of each ba tch.\n",
      " The K eras API only specifies how to use subclassing to define la yƒ\n",
      " ers, models, callbacks, and regularizers. If you build other com poƒ\n",
      " nen ts (such as losses, metrics, initializers or constrain ts) using\n",
      " subclassing, they ma y not be portable to other K eras im plemen taƒ\n",
      "tions.\n",
      " Let ‡ s walk through this code:\n",
      "⁄\n",
      "The constructor accepts \n",
      "**kwargs\n",
      "  and passes them to the paren t constructor ,\n",
      " which handles standard h yperparameters: the \n",
      "name\n",
      " \n",
      "of the loss and the \n",
      "reduction\n",
      " algorithm to use to aggrega te the individual instance losses. By defa ult, it is\n",
      "\"sum_over_batch_size\"\n",
      " , which means tha t the loss will be the sum of the\n",
      " instance losses, possibly weigh ted by the sam ple weigh ts, if an y , and then divide\n",
      " the result by the ba tch size (not by the sum of weigh ts, so this is \n",
      " n o t\n",
      "   the weigh ted\n",
      "mean).\n",
      "5\n",
      ". Other possible values are \n",
      "\"sum\"\n",
      " and \n",
      "None\n",
      ".\n",
      "⁄\n",
      "The \n",
      "call()\n",
      "  method takes the labels and predictions, com putes all the instance\n",
      "losses, and returns them.\n",
      "⁄\n",
      "The \n",
      "get_config()\n",
      "  method returns a dictionar y ma pping each h yperparameter\n",
      " name to its value. I t first calls the paren t class ‡ s \n",
      "get_config()\n",
      " method, then adds\n",
      " the new h yperparameters to this dictionar y (note tha t the con venien t \n",
      "{**x}\n",
      " \n",
      "synƒ\n",
      "tax was added in Python 3.5).\n",
      " Y ou can then use an y instance of this class when you com pile the model:\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "HuberLoss\n",
      "(\n",
      "2.\n",
      "),\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"nadam\"\n",
      ")\n",
      " When you sa ve the model, the threshold will be sa ved along with it, and when you\n",
      " load the model you just need to ma p the class name to the class itself:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "load_model\n",
      "(\n",
      "\"my_model_with_a_custom_loss_class.h5\"\n",
      ",\n",
      "                                \n",
      "custom_objects\n",
      "=\n",
      "{\n",
      "\"HuberLoss\"\n",
      ":\n",
      " \n",
      "HuberLoss\n",
      "})\n",
      " When you sa ve a model, K eras calls the loss instance ‡ s \n",
      "get_config()\n",
      " \n",
      "method and\n",
      " sa ves the config as JSON in the HDF5 file. When you load the model, it calls the\n",
      "from_config()\n",
      " class method on the \n",
      "HuberLoss\n",
      "  class: this method is im plemen ted by\n",
      "the base class (\n",
      "Loss\n",
      " ) and just crea tes an instance of the class, passing \n",
      "**config\n",
      "   to the\n",
      " constructor .\n",
      " Tha t ‡ s it for losses! I t was not too hard, was it? W ell it ‡ s just as sim ple for custom actiƒ\n",
      " va tion functions, initializers, regularizers, and constrain ts. Let ‡ s look a t these now .\n",
      " 378  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "Custom Activation Functions, Initializers, Regularizers, and\n",
      "Constraints\n",
      " M ost K eras functionalities, such as losses, regularizers, constrain ts, initializers, metƒ\n",
      " rics, activa tion functions, la yers and even full models can be customized in ver y m uch\n",
      " the same wa y . M ost of the time, you will just need to write a sim ple function, with the\n",
      " a ppropria te in puts and outputs. F or exam ple, here are exam ples of a custom activaƒ\n",
      " tion function (equivalen t to \n",
      "keras.activations.softplus\n",
      " \n",
      "or \n",
      "tf.nn.softplus\n",
      "), a\n",
      " custom Glorot initializer (equivalen t to \n",
      "keras.initializers.glorot_normal\n",
      "), a cusƒ\n",
      "tom …\n",
      "1\n",
      "  regularizer (equivalen t to \n",
      "keras.regularizers.l1(0.01)\n",
      ") and a custom conƒ\n",
      " strain t tha t ensures weigh ts are all positive (equivalen t to\n",
      "keras.constraints.nonneg()\n",
      " or \n",
      "tf.nn.relu\n",
      "):\n",
      "def\n",
      " \n",
      "my_softplus\n",
      "(\n",
      "z\n",
      "):\n",
      " \n",
      "# return value is just tf.nn.softplus(z)\n",
      "    \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "math\n",
      ".\n",
      "log\n",
      "(\n",
      "tf\n",
      ".\n",
      "exp\n",
      "(\n",
      "z\n",
      ")\n",
      " \n",
      "+\n",
      " \n",
      "1.0\n",
      ")\n",
      "def\n",
      " \n",
      "my_glorot_initializer\n",
      "(\n",
      "shape\n",
      ",\n",
      " \n",
      "dtype\n",
      "=\n",
      "tf\n",
      ".\n",
      "float32\n",
      "):\n",
      "    \n",
      "stddev\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "sqrt\n",
      "(\n",
      "2.\n",
      " \n",
      "/\n",
      " \n",
      "(\n",
      "shape\n",
      "[\n",
      "0\n",
      "]\n",
      " \n",
      "+\n",
      " \n",
      "shape\n",
      "[\n",
      "1\n",
      "]))\n",
      "    \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "random\n",
      ".\n",
      "normal\n",
      "(\n",
      "shape\n",
      ",\n",
      " \n",
      "stddev\n",
      "=\n",
      "stddev\n",
      ",\n",
      " \n",
      "dtype\n",
      "=\n",
      "dtype\n",
      ")\n",
      "def\n",
      " \n",
      "my_l1_regularizer\n",
      "(\n",
      "weights\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "reduce_sum\n",
      "(\n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "0.01\n",
      " \n",
      "*\n",
      " \n",
      "weights\n",
      "))\n",
      "def\n",
      " \n",
      "my_positive_weights\n",
      "(\n",
      "weights\n",
      "):\n",
      " \n",
      "# return value is just tf.nn.relu(weights)\n",
      "    \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "where\n",
      "(\n",
      "weights\n",
      " \n",
      "<\n",
      " \n",
      "0.\n",
      ",\n",
      " \n",
      "tf\n",
      ".\n",
      "zeros_like\n",
      "(\n",
      "weights\n",
      "),\n",
      " \n",
      "weights\n",
      ")\n",
      " As you can see, the argumen ts depend on the type of custom function. These custom\n",
      " functions can then be used normally , for exam ple:\n",
      "layer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "my_softplus\n",
      ",\n",
      "                           \n",
      "kernel_initializer\n",
      "=\n",
      "my_glorot_initializer\n",
      ",\n",
      "                           \n",
      "kernel_regularizer\n",
      "=\n",
      "my_l1_regularizer\n",
      ",\n",
      "                           \n",
      "kernel_constraint\n",
      "=\n",
      "my_positive_weights\n",
      ")\n",
      " The activa tion function will be a pplied to the output of this \n",
      "Dense\n",
      " \n",
      " la yer , and its result\n",
      " will be passed on to the next la yer . The la yer‡ s weigh ts will be initialized using the\n",
      " value returned by the initializer . A t each training step the weigh ts will be passed to the\n",
      " regulariza tion function to com pute the regulariza tion loss, which will be added to the\n",
      " main loss to get the final loss used for training. Finally , the constrain t function will be\n",
      " called after each training step , and the la yer‡ s weigh ts will be replaced by the conƒ\n",
      " strained weigh ts.\n",
      " If a function has some h yperparameters tha t need to be sa ved along with the model,\n",
      " then you will wan t to subclass the a ppropria te class, such as \n",
      "keras.regulariz\n",
      "ers.Regularizer\n",
      ", \n",
      "keras.constraints.Constraint\n",
      ", \n",
      "keras.initializers.Initial\n",
      "izer\n",
      " or \n",
      "keras.layers.Layer\n",
      "  (for an y la yer , including activa tion functions). F or\n",
      " exam ple, m uch like we did for the custom loss, here is a sim ple class for …\n",
      "1\n",
      "   regularizaƒ\n",
      " Customizing Models and Training Algorithms  |  379\n",
      "\n",
      "6\n",
      " H owever , the H uber loss is seldom used as a metric (the MAE or MSE are preferred).\n",
      " tion, tha t sa ves its \n",
      "factor\n",
      " \n",
      " h yperparameter (this time we do not need to call the paren t\n",
      "constructor or the \n",
      "get_config()\n",
      "  method, as they are not defined by the paren t class):\n",
      "class\n",
      " \n",
      "MyL1Regularizer\n",
      "(\n",
      "keras\n",
      ".\n",
      "regularizers\n",
      ".\n",
      "Regularizer\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "factor\n",
      "):\n",
      "        \n",
      "self\n",
      ".\n",
      "factor\n",
      " \n",
      "=\n",
      " \n",
      "factor\n",
      "    \n",
      "def\n",
      " \n",
      "__call__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "weights\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "reduce_sum\n",
      "(\n",
      "tf\n",
      ".\n",
      "abs\n",
      "(\n",
      "self\n",
      ".\n",
      "factor\n",
      " \n",
      "*\n",
      " \n",
      "weights\n",
      "))\n",
      "    \n",
      "def\n",
      " \n",
      "get_config\n",
      "(\n",
      "self\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "{\n",
      "\"factor\"\n",
      ":\n",
      " \n",
      "self\n",
      ".\n",
      "factor\n",
      "}\n",
      " N ote tha t you m ust im plemen t the \n",
      "call()\n",
      " \n",
      " method for losses, la yers (including activaƒ\n",
      "tion functions) and models, or the \n",
      "__call__()\n",
      " method for regularizers, initializers\n",
      " and constrain ts. F or metrics, things are a bit differen t, as we will see now .\n",
      "Custom Metrics\n",
      " Losses and metrics are conceptually not the same thing: losses are used by Gradien t\n",
      " Descen t to \n",
      " t r a i n\n",
      " \n",
      " a model, so they m ust be differen tiable (a t least where they are evaluƒ\n",
      " a ted) and their gradien ts should not be 0 ever ywhere. Plus, it ‡ s oka y if they are not\n",
      " easily in terpretable by h umans (e.g. cross-en tropy). In con trast, metrics are used to\n",
      " e v a l u a t e\n",
      "  a model, they m ust be more easily in terpretable, and they can be non-\n",
      " differen tiable or ha ve 0 gradien ts ever ywhere (e.g., accuracy).\n",
      " Tha t said, in most cases, defining a custom metric function is exactly the same as\n",
      " defining a custom loss function. In fact, we could even use the H uber loss function we\n",
      " crea ted earlier as a metric\n",
      "6\n",
      ", it would work just fine (and persistence would also work\n",
      " the same wa y , in this case only sa ving the name of the function, \n",
      "\"huber_fn\"\n",
      "):\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"mse\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"nadam\"\n",
      ",\n",
      " \n",
      "metrics\n",
      "=\n",
      "[\n",
      "create_huber\n",
      "(\n",
      "2.0\n",
      ")])\n",
      " F or each ba tch during training, K eras will com pute this metric and keep track of its\n",
      " mean since the beginning of the epoch. M ost of the time, this is exactly wha t you\n",
      " wan t. But not alwa ys! Consider a binar y classifier‡ s precision, for exam ple. As we sa w\n",
      "in \n",
      " Cha pter 3\n",
      " , precision is the n umber of true positives divided by the n umber of posiƒ\n",
      "tive predictions (including both true positives and false positives). Suppose the model\n",
      " made 5 positive predictions in the first ba tch, 4 of which were correct: tha t ‡ s 80% preƒ\n",
      " cision. Then suppose the model made 3 positive predictions in the second ba tch, but\n",
      " they were all incorrect: tha t ‡ s 0% precision for the second ba tch. If you just com pute\n",
      "the mean of these two precisions, you get 40%. But wait a second, this is \n",
      " n o t\n",
      "   the modƒ\n",
      " el ‡ s precision over these two ba tches! Indeed, there were a total of 4 true positives (4 +\n",
      " 0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. Wha t\n",
      " we need is an object tha t can keep track of the n umber of true positives and the n umƒ\n",
      " 380  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " ber of false positives, and com pute their ra tio when requested. This is precisely wha t\n",
      "the \n",
      "keras.metrics.Precision\n",
      " class does:\n",
      ">>> \n",
      "precision\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "metrics\n",
      ".\n",
      "Precision\n",
      "()\n",
      ">>> \n",
      "precision\n",
      "([\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "[\n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      "])\n",
      "<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\n",
      ">>> \n",
      "precision\n",
      "([\n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "[\n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "0\n",
      ",\n",
      " \n",
      "0\n",
      "])\n",
      "<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\n",
      " In this exam ple, we crea ted a \n",
      "Precision\n",
      " object, then we used it like a function, passƒ\n",
      " ing it the labels and predictions for the first ba tch, then for the second ba tch (note\n",
      " tha t we could also ha ve passed sam ple weigh ts). W e used the same n umber of true\n",
      " and false positives as in the exam ple we just discussed. After the first ba tch, it returns\n",
      " the precision of 80%, then after the second ba tch it returns 50% (which is the overall\n",
      " precision so far , not the second ba tch ‡ s precision). This is called a \n",
      " s t r e a m i n g m e t r i c\n",
      "   (or\n",
      " s t a t ef u l m e t r i c\n",
      " ), as it is gradually upda ted, ba tch after ba tch.\n",
      " A t an y poin t, we can call the \n",
      "result()\n",
      "  method to get the curren t value of the metric.\n",
      " W e can also look a t its variables (tracking the n umber of true and false positives)\n",
      "using the \n",
      "variables\n",
      "  a ttribute, and reset these variables using the \n",
      "reset_states()\n",
      "method:\n",
      ">>> \n",
      "p\n",
      ".\n",
      "result\n",
      "()\n",
      "<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\n",
      ">>> \n",
      "p\n",
      ".\n",
      "variables\n",
      "[<tf.Variable •true_positives:0• [...] numpy=array([4.], dtype=float32)>,\n",
      " <tf.Variable •false_positives:0• [...] numpy=array([4.], dtype=float32)>]\n",
      ">>> \n",
      "p\n",
      ".\n",
      "reset_states\n",
      "()\n",
      " \n",
      "# both variables get reset to 0.0\n",
      " If you need to crea te such a streaming metric, you can just crea te a subclass of the\n",
      "keras.metrics.Metric\n",
      "  class. H ere is a sim ple exam ple tha t keeps track of the total\n",
      " H uber loss and the n umber of instances seen so far . When asked for the result, it\n",
      " returns the ra tio , which is sim ply the mean H uber loss:\n",
      "class\n",
      " \n",
      "HuberMetric\n",
      "(\n",
      "keras\n",
      ".\n",
      "metrics\n",
      ".\n",
      "Metric\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "threshold\n",
      "=\n",
      "1.0\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      " \n",
      "# handles base args (e.g., dtype)\n",
      "        \n",
      "self\n",
      ".\n",
      "threshold\n",
      " \n",
      "=\n",
      " \n",
      "threshold\n",
      "        \n",
      "self\n",
      ".\n",
      "huber_fn\n",
      " \n",
      "=\n",
      " \n",
      "create_huber\n",
      "(\n",
      "threshold\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "total\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "add_weight\n",
      "(\n",
      "\"total\"\n",
      ",\n",
      " \n",
      "initializer\n",
      "=\n",
      "\"zeros\"\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "count\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "add_weight\n",
      "(\n",
      "\"count\"\n",
      ",\n",
      " \n",
      "initializer\n",
      "=\n",
      "\"zeros\"\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "update_state\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "y_true\n",
      ",\n",
      " \n",
      "y_pred\n",
      ",\n",
      " \n",
      "sample_weight\n",
      "=\n",
      "None\n",
      "):\n",
      "        \n",
      "metric\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "huber_fn\n",
      "(\n",
      "y_true\n",
      ",\n",
      " \n",
      "y_pred\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "total\n",
      ".\n",
      "assign_add\n",
      "(\n",
      "tf\n",
      ".\n",
      "reduce_sum\n",
      "(\n",
      "metric\n",
      "))\n",
      "        \n",
      "self\n",
      ".\n",
      "count\n",
      ".\n",
      "assign_add\n",
      "(\n",
      "tf\n",
      ".\n",
      "cast\n",
      "(\n",
      "tf\n",
      ".\n",
      "size\n",
      "(\n",
      "y_true\n",
      "),\n",
      " \n",
      "tf\n",
      ".\n",
      "float32\n",
      "))\n",
      "    \n",
      "def\n",
      " \n",
      "result\n",
      "(\n",
      "self\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "self\n",
      ".\n",
      "total\n",
      " \n",
      "/\n",
      " \n",
      "self\n",
      ".\n",
      "count\n",
      "    \n",
      "def\n",
      " \n",
      "get_config\n",
      "(\n",
      "self\n",
      "):\n",
      "        \n",
      "base_config\n",
      " \n",
      "=\n",
      " \n",
      "super\n",
      "()\n",
      ".\n",
      "get_config\n",
      "()\n",
      "        \n",
      "return\n",
      " \n",
      "{\n",
      "**\n",
      "base_config\n",
      ",\n",
      " \n",
      "\"threshold\"\n",
      ":\n",
      " \n",
      "self\n",
      ".\n",
      "threshold\n",
      "}\n",
      " Customizing Models and Training Algorithms  |  381\n",
      "\n",
      "7\n",
      " This class is for illustra tion purposes only . A sim pler and better im plemen ta tion would just subclass the\n",
      "keras.metrics.Mean\n",
      "  class, see the notebook for an exam ple.\n",
      " Let ‡ s walk through this code:\n",
      "7\n",
      ":\n",
      "⁄\n",
      "The constructor uses the \n",
      "add_weight()\n",
      "  method to crea te the variables needed to\n",
      " keep track of the metric ‡ s sta te over m ultiple ba tches, in this case the sum of all\n",
      " H uber losses (\n",
      "total\n",
      " ) and the n umber of instances seen so far (\n",
      "count\n",
      " ). Y ou could\n",
      " just crea te variables man ually if you preferred. K eras tracks an y \n",
      "tf.Variable\n",
      "   tha t\n",
      " is set as an a ttribute (and more generally , an y — trackable – object, such as la yers or\n",
      "models).\n",
      "⁄\n",
      "The \n",
      "update_state()\n",
      " \n",
      "method is called when you use an instance of this class as a\n",
      "function (as we did with the \n",
      "Precision\n",
      " \n",
      " object). I t upda tes the variables given the\n",
      " labels and predictions for one ba tch (and sam ple weigh ts, but in this case we just\n",
      "ignore them).\n",
      "⁄\n",
      "The \n",
      "result()\n",
      "  method com putes and returns the final result, in this case just the\n",
      " mean H uber metric over all instances. When you use the metric as a function, the\n",
      "update_state()\n",
      " method gets called first, then the \n",
      "result()\n",
      " method is called,\n",
      "and its output is returned.\n",
      "⁄\n",
      " W e also im plemen t the \n",
      "get_config()\n",
      " method to ensure the \n",
      "threshold\n",
      " \n",
      "gets\n",
      " sa ved along with the model.\n",
      "⁄\n",
      " The defa ult im plemen ta tion of the \n",
      "reset_states()\n",
      " method just resets all variƒ\n",
      "ables to 0.0 (but you can override it if needed).\n",
      " K eras will take care of variable persistence seamlessly , no action is\n",
      "required.\n",
      " When you define a metric using a sim ple function, K eras a utoma tically calls it for\n",
      " each ba tch, and it keeps track of the mean during each epoch, just like we did manƒ\n",
      " ually . So the only benefit of our \n",
      "HuberMetric\n",
      " \n",
      " class is tha t the \n",
      "threshold\n",
      "   will be sa ved.\n",
      " But of course, some metrics, like precision, cannot sim ply be a veraged over ba tches:\n",
      " in thoses cases, there ‡ s no other option than to im plemen t a streaming metric.\n",
      " N ow tha t we ha ve built a streaming metric, building a custom la yer will seem like a\n",
      "walk in the park!\n",
      " 382  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "Custom Layers\n",
      " Y ou ma y occasionally wan t to build an architecture tha t con tains an exotic la yer for\n",
      " which T ensorFlow does not provide a defa ult im plemen ta tion. In this case, you will\n",
      " need to crea te a custom la yer . Or sometimes you ma y sim ply wan t to build a ver y\n",
      " repetitive architecture, con taining iden tical blocks of la yers repea ted man y times, and\n",
      " it would be con venien t to trea t each block of la yers as a single la yer . F or exam ple, if\n",
      " the model is a sequence of la yers A, B , C, A, B , C, A, B , C, then you migh t wan t to\n",
      " define a custom la yer D con taining la yers A, B , C, and your model would then sim ply\n",
      " be D , D , D . Let ‡ s see how to build custom la yers.\n",
      " First, some la yers ha ve no weigh ts, such as \n",
      "keras.layers.Flatten\n",
      " \n",
      "or \n",
      "keras.lay\n",
      "ers.ReLU\n",
      " . If you wan t to crea te a custom la yer without an y weigh ts, the sim plest\n",
      " option is to write a function and wra p it in a \n",
      "keras.layers.Lambda\n",
      " \n",
      " la yer . F or examƒ\n",
      " ple, the following la yer will a pply the exponen tial function to its in puts:\n",
      "exponential_layer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Lambda\n",
      "(\n",
      "lambda\n",
      " \n",
      "x\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "exp\n",
      "(\n",
      "x\n",
      "))\n",
      " This custom la yer can then be used like an y other la yer , using the sequen tial API, the\n",
      " functional API, or the subclassing API. Y ou can also use it as an activa tion function\n",
      "(or you could just use \n",
      "activation=tf.exp\n",
      ", or \n",
      "activation=keras.activations.expo\n",
      "nential\n",
      " , or sim ply \n",
      "activation=\"exponential\"\n",
      " ). The exponen tial la yer is sometimes\n",
      " used in the output la yer of a regression model when the values to predict ha ve ver y\n",
      " differen t scales (e.g., 0.001, 10., 1000.).\n",
      " As you probably guessed by now , to build a custom sta teful la yer (i.e., a la yer with\n",
      " weigh ts), you need to crea te a subclass of the \n",
      "keras.layers.Layer\n",
      "  class. F or examƒ\n",
      " ple, the following class im plemen ts a sim plified version of the \n",
      "Dense\n",
      "  la yer :\n",
      "class\n",
      " \n",
      "MyDense\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Layer\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "units\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "None\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "units\n",
      " \n",
      "=\n",
      " \n",
      "units\n",
      "        \n",
      "self\n",
      ".\n",
      "activation\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "activations\n",
      ".\n",
      "get\n",
      "(\n",
      "activation\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "build\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "batch_input_shape\n",
      "):\n",
      "        \n",
      "self\n",
      ".\n",
      "kernel\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "add_weight\n",
      "(\n",
      "            \n",
      "name\n",
      "=\n",
      "\"kernel\"\n",
      ",\n",
      " \n",
      "shape\n",
      "=\n",
      "[\n",
      "batch_input_shape\n",
      "[\n",
      "-\n",
      "1\n",
      "],\n",
      " \n",
      "self\n",
      ".\n",
      "units\n",
      "],\n",
      "            \n",
      "initializer\n",
      "=\n",
      "\"glorot_normal\"\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "bias\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "add_weight\n",
      "(\n",
      "            \n",
      "name\n",
      "=\n",
      "\"bias\"\n",
      ",\n",
      " \n",
      "shape\n",
      "=\n",
      "[\n",
      "self\n",
      ".\n",
      "units\n",
      "],\n",
      " \n",
      "initializer\n",
      "=\n",
      "\"zeros\"\n",
      ")\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "build\n",
      "(\n",
      "batch_input_shape\n",
      ")\n",
      " \n",
      "# must be at the end\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "self\n",
      ".\n",
      "activation\n",
      "(\n",
      "X\n",
      " \n",
      "@\n",
      " \n",
      "self\n",
      ".\n",
      "kernel\n",
      " \n",
      "+\n",
      " \n",
      "self\n",
      ".\n",
      "bias\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "compute_output_shape\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "batch_input_shape\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "TensorShape\n",
      "(\n",
      "batch_input_shape\n",
      ".\n",
      "as_list\n",
      "()[:\n",
      "-\n",
      "1\n",
      "]\n",
      " \n",
      "+\n",
      " \n",
      "[\n",
      "self\n",
      ".\n",
      "units\n",
      "])\n",
      " Customizing Models and Training Algorithms  |  383\n",
      "\n",
      "8\n",
      " This function is specific to tf.keras. Y ou could use \n",
      "keras.activations.Activation\n",
      " instead.\n",
      "9\n",
      " The K eras API calls this argumen t \n",
      "input_shape\n",
      " , but since it also includes the ba tch dimension, I prefer to call\n",
      "it \n",
      "batch_input_shape\n",
      ". Same for \n",
      "compute_output_shape()\n",
      ".\n",
      "    \n",
      "def\n",
      " \n",
      "get_config\n",
      "(\n",
      "self\n",
      "):\n",
      "        \n",
      "base_config\n",
      " \n",
      "=\n",
      " \n",
      "super\n",
      "()\n",
      ".\n",
      "get_config\n",
      "()\n",
      "        \n",
      "return\n",
      " \n",
      "{\n",
      "**\n",
      "base_config\n",
      ",\n",
      " \n",
      "\"units\"\n",
      ":\n",
      " \n",
      "self\n",
      ".\n",
      "units\n",
      ",\n",
      "                \n",
      "\"activation\"\n",
      ":\n",
      " \n",
      "keras\n",
      ".\n",
      "activations\n",
      ".\n",
      "serialize\n",
      "(\n",
      "self\n",
      ".\n",
      "activation\n",
      ")}\n",
      " Let ‡ s walk through this code:\n",
      "⁄\n",
      " The constructor takes all the h yperparameters as argumen ts (in this exam ple just\n",
      "units\n",
      " and \n",
      "activation\n",
      " ), and im portan tly it also takes a \n",
      "**kwargs\n",
      "  argumen t. I t\n",
      " calls the paren t constructor , passing it the \n",
      "kwargs\n",
      ": this takes care of standard\n",
      " argumen ts such as \n",
      "input_shape\n",
      ", \n",
      "trainable\n",
      ", \n",
      "name\n",
      " , and so on. Then it sa ves the\n",
      " h yperparameters as a ttributes, con verting the \n",
      "activation\n",
      " \n",
      " argumen t to the\n",
      " a ppropria te activa tion function using the \n",
      "keras.activations.get()\n",
      "   function (it\n",
      "accepts functions, standard strings like \n",
      "\"relu\"\n",
      " or \n",
      "\"selu\"\n",
      " , or sim ply \n",
      "None\n",
      ")\n",
      "8\n",
      ".\n",
      "⁄\n",
      "The \n",
      "build()\n",
      "  method ‡ s role is to crea te the la yer‡ s variables, by calling the\n",
      "add_weight()\n",
      "  method for each weigh t. The \n",
      "build()\n",
      " method is called the first\n",
      " time the la yer is used. A t tha t poin t, K eras will know the sha pe of this la yer‡ s\n",
      " in puts, and it will pass it to the \n",
      "build()\n",
      "   method\n",
      "9\n",
      " , which is often necessar y to creƒ\n",
      " a te some of the weigh ts. F or exam ple, we need to know the n umber of neurons in\n",
      " the previous la yer in order to crea te the connection weigh ts ma trix (i.e., the \n",
      "\"ker\n",
      "nel\"\n",
      " ): this corresponds to the size of the last dimension of the in puts. A t the end\n",
      "of the \n",
      "build()\n",
      "  method (and only a t the end), you m ust call the paren t ‡ s \n",
      "build()\n",
      " method: this tells K eras tha t the la yer is built (it just sets \n",
      "self.built = True\n",
      ").\n",
      "⁄\n",
      "The \n",
      "call()\n",
      "  method actually performs the desired opera tions. In this case, we\n",
      " com pute the ma trix m ultiplica tion of the in puts \n",
      "X\n",
      "  and the la yer‡ s kernel, we add\n",
      " the bias vector , we a pply the activa tion function to the result, and this gives us the\n",
      " output of the la yer .\n",
      "⁄\n",
      "The \n",
      "compute_output_shape()\n",
      "  method sim ply returns the sha pe of this la yer‡ s\n",
      " outputs. In this case, it is the same sha pe as the in puts, except the last dimension\n",
      " is replaced with the n umber of neurons in the la yer . N ote tha t in tf.keras, sha pes\n",
      "are instances of the \n",
      "tf.TensorShape\n",
      "  class, which you can con vert to Python lists\n",
      "using \n",
      "as_list()\n",
      ".\n",
      "⁄\n",
      "The \n",
      "get_config()\n",
      "  method is just like earlier . N ote tha t we sa ve the activa tion\n",
      " function ‡ s full configura tion by calling \n",
      "keras.activations.serialize()\n",
      ".\n",
      " Y ou can now use a \n",
      "MyDense\n",
      "  la yer just like an y other la yer!\n",
      " 384  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " Y ou can generally omit the \n",
      "compute_output_shape()\n",
      " method, as\n",
      " tf.keras a utoma tically infers the output sha pe, except when the\n",
      " la yer is dynamic (as we will see shortly). In other K eras im plemenƒ\n",
      " ta tions, this method is either required or by defa ult it assumes the\n",
      " output sha pe is the same as the in put sha pe.\n",
      " T o crea te a la yer with m ultiple in puts (e.g., \n",
      "Concatenate\n",
      " ), the argumen t to the \n",
      "call()\n",
      " method should be a tuple con taining all the in puts, and similarly the argumen t to the\n",
      "compute_output_shape()\n",
      "  method should be a tuple con taining each in put ‡ s ba tch\n",
      " sha pe. T o crea te a la yer with m ultiple outputs, the \n",
      "call()\n",
      " method should return the\n",
      "list of outputs, and the \n",
      "compute_output_shape()\n",
      "  should return the list of ba tch outƒ\n",
      " put sha pes (one per output). F or exam ple, the following toy la yer takes two in puts\n",
      "and returns three outputs:\n",
      "class\n",
      " \n",
      "MyMultiLayer\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Layer\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      "):\n",
      "        \n",
      "X1\n",
      ",\n",
      " \n",
      "X2\n",
      " \n",
      "=\n",
      " \n",
      "X\n",
      "        \n",
      "return\n",
      " \n",
      "[\n",
      "X1\n",
      " \n",
      "+\n",
      " \n",
      "X2\n",
      ",\n",
      " \n",
      "X1\n",
      " \n",
      "*\n",
      " \n",
      "X2\n",
      ",\n",
      " \n",
      "X1\n",
      " \n",
      "/\n",
      " \n",
      "X2\n",
      "]\n",
      "    \n",
      "def\n",
      " \n",
      "compute_output_shape\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "batch_input_shape\n",
      "):\n",
      "        \n",
      "b1\n",
      ",\n",
      " \n",
      "b2\n",
      " \n",
      "=\n",
      " \n",
      "batch_input_shape\n",
      "        \n",
      "return\n",
      " \n",
      "[\n",
      "b1\n",
      ",\n",
      " \n",
      "b1\n",
      ",\n",
      " \n",
      "b1\n",
      "]\n",
      " \n",
      "# should probably handle broadcasting rules\n",
      " This la yer ma y now be used like an y other la yer , but of course only using the funcƒ\n",
      " tional and subclassing API s, not the sequen tial API (which only accepts la yers with\n",
      " one in put and one output).\n",
      " If your la yer needs to ha ve a differen t beha vior during training and during testing\n",
      "(e.g., if it uses \n",
      "Dropout\n",
      " or \n",
      "BatchNormalization\n",
      "  la yers), then you m ust add a \n",
      "train\n",
      "ing\n",
      "  argumen t to the \n",
      "call()\n",
      "  method and use this argumen t to decide wha t to do . F or\n",
      " exam ple, let ‡ s crea te a la yer tha t adds Ga ussian noise during training (for regularizaƒ\n",
      " tion), but does nothing during testing (K eras actually has a la yer tha t does the same\n",
      "thing: \n",
      "keras.layers.GaussianNoise\n",
      "):\n",
      "class\n",
      " \n",
      "MyGaussianNoise\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Layer\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "stddev\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "stddev\n",
      " \n",
      "=\n",
      " \n",
      "stddev\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "X\n",
      ",\n",
      " \n",
      "training\n",
      "=\n",
      "None\n",
      "):\n",
      "        \n",
      "if\n",
      " \n",
      "training\n",
      ":\n",
      "            \n",
      "noise\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "random\n",
      ".\n",
      "normal\n",
      "(\n",
      "tf\n",
      ".\n",
      "shape\n",
      "(\n",
      "X\n",
      "),\n",
      " \n",
      "stddev\n",
      "=\n",
      "self\n",
      ".\n",
      "stddev\n",
      ")\n",
      "            \n",
      "return\n",
      " \n",
      "X\n",
      " \n",
      "+\n",
      " \n",
      "noise\n",
      "        \n",
      "else\n",
      ":\n",
      "            \n",
      "return\n",
      " \n",
      "X\n",
      "    \n",
      "def\n",
      " \n",
      "compute_output_shape\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "batch_input_shape\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "batch_input_shape\n",
      " Customizing Models and Training Algorithms  |  385\n",
      "\n",
      "10\n",
      " The name — subclassing API– usually refers only to the crea tion of custom models by subclassing, although\n",
      " man y other things can be crea ted by subclassing, as we sa w in this cha pter .\n",
      " W ith tha t, you can now build an y custom la yer you need! N ow let ‡ s crea te custom\n",
      "models.\n",
      "Custom Models\n",
      " W e already looked a t custom model classes in \n",
      " Cha pter 10\n",
      " \n",
      "when we discussed the subƒ\n",
      "classing API.\n",
      "10\n",
      "  I t is actually quite straigh tfor ward, just subclass the \n",
      "keras.mod\n",
      "els.Model\n",
      "  class, crea te la yers and variables in the constructor , and im plemen t the\n",
      "call()\n",
      "  method to do wha tever you wan t the model to do . F or exam ple, suppose you\n",
      " wan t to build the model represen ted in \n",
      "Figure 12-3\n",
      ":\n",
      " F i g u r e 12-3. C u s t o m M o d e l E x a m p l e\n",
      " The in puts go through a first dense la yer , then through a \n",
      " r e s i d u a l b l o c k\n",
      "  com posed of\n",
      " two dense la yers and an addition opera tion (as we will see in \n",
      " Cha pter 14\n",
      ", a residual\n",
      " block adds its in puts to its outputs), then through this same residual block 3 more\n",
      "times, then through a second residual block, and the final result goes through a dense\n",
      " output la yer . N ote tha t this model does not make m uch sense, it ‡ s just an exam ple to\n",
      " illustra te the fact tha t you can easily build an y kind of model you wan t, even con tainƒ\n",
      " 386  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " ing loops and skip connections. T o im plemen t this model, it is best to first crea te a\n",
      "ResidualBlock\n",
      "  la yer , since we are going to crea te a couple iden tical blocks (and we\n",
      " migh t wan t to reuse it in another model):\n",
      "class\n",
      " \n",
      "ResidualBlock\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Layer\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "n_layers\n",
      ",\n",
      " \n",
      "n_neurons\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "hidden\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "n_neurons\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      "                                          \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ")\n",
      "                       \n",
      "for\n",
      " \n",
      "_\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "n_layers\n",
      ")]\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      "):\n",
      "        \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "        \n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "self\n",
      ".\n",
      "hidden\n",
      ":\n",
      "            \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "layer\n",
      "(\n",
      "Z\n",
      ")\n",
      "        \n",
      "return\n",
      " \n",
      "inputs\n",
      " \n",
      "+\n",
      " \n",
      "Z\n",
      " This la yer is a bit special since it con tains other la yers. This is handled transparen tly\n",
      " by K eras: it a utoma tically detects tha t the \n",
      "hidden\n",
      "  a ttribute con tains trackable objects\n",
      " (la yers in this case), so their variables are a utoma tically added to this la yer‡ s list of\n",
      " variables. The rest of this class is self-explana tor y . N ext, let ‡ s use the subclassing API\n",
      "to define the model itself:\n",
      "class\n",
      " \n",
      "ResidualRegressor\n",
      "(\n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "output_dim\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "hidden1\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      "                                          \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "block1\n",
      " \n",
      "=\n",
      " \n",
      "ResidualBlock\n",
      "(\n",
      "2\n",
      ",\n",
      " \n",
      "30\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "block2\n",
      " \n",
      "=\n",
      " \n",
      "ResidualBlock\n",
      "(\n",
      "2\n",
      ",\n",
      " \n",
      "30\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "out\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "output_dim\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      "):\n",
      "        \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "hidden1\n",
      "(\n",
      "inputs\n",
      ")\n",
      "        \n",
      "for\n",
      " \n",
      "_\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "1\n",
      " \n",
      "+\n",
      " \n",
      "3\n",
      "):\n",
      "            \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "block1\n",
      "(\n",
      "Z\n",
      ")\n",
      "        \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "block2\n",
      "(\n",
      "Z\n",
      ")\n",
      "        \n",
      "return\n",
      " \n",
      "self\n",
      ".\n",
      "out\n",
      "(\n",
      "Z\n",
      ")\n",
      " W e crea te the la yers in the constructor , and use them in the \n",
      "call()\n",
      " \n",
      "method. This\n",
      " model can then be used like an y other model (com pile it, fit it, evalua te it and use it to\n",
      " make predictions). If you also wan t to be able to sa ve the model using the \n",
      "save()\n",
      "method, and load it using the \n",
      "keras.models.load_model()\n",
      "  function, you m ust\n",
      " im plemen t the \n",
      "get_config()\n",
      " method (as we did earlier) in both the \n",
      "ResidualBlock\n",
      "class and the \n",
      "ResidualRegressor\n",
      "  class. Alterna tively , you can just sa ve and load the\n",
      " weigh ts using the \n",
      "save_weights()\n",
      " and \n",
      "load_weights()\n",
      " methods.\n",
      "The \n",
      "Model\n",
      " \n",
      "class is actually a subclass of the \n",
      "Layer\n",
      " \n",
      "class, so models can be defined and\n",
      " used exactly like la yers. But a model also has some extra functionalities, including of\n",
      "course its \n",
      "compile()\n",
      ", \n",
      "fit()\n",
      ", \n",
      "evaluate()\n",
      " and \n",
      "predict()\n",
      " methods (and a few varƒ\n",
      " Customizing Models and Training Algorithms  |  387\n",
      "\n",
      " ian ts, such as \n",
      "train_on_batch()\n",
      " or \n",
      "fit_generator()\n",
      "), plus the \n",
      "get_layers()\n",
      " method (which can return an y of the model ‡ s la yers by name or by index), and the\n",
      "save()\n",
      " method (and support for \n",
      "keras.models.load_model()\n",
      " and \n",
      "keras.mod\n",
      "els.clone_model()\n",
      " ). So if models provide more functionalities than la yers, wh y not\n",
      " just define ever y la yer as a model? W ell, technically you could, but it is probably\n",
      " cleaner to distinguish the in ternal com ponen ts of your model (la yers or reusable\n",
      " blocks of la yers) from the model itself. The former should subclass the \n",
      "Layer\n",
      " \n",
      "class,\n",
      " while the la tter should subclass the \n",
      "Model\n",
      " class.\n",
      " W ith tha t, you can quite na turally and concisely build almost an y model tha t you find\n",
      " in a pa per , either using the sequen tial API, the functional API, the subclassing API, or\n",
      " even a mix of these. — Almost – an y model? Y es, there are still a couple things tha t we\n",
      " need to look a t: first, how to define losses or metrics based on model in ternals, and\n",
      " second how to build a custom training loop .\n",
      "Losses and Metrics Based on Model Internals\n",
      "The custom losses and metrics we defined earlier were all based on the labels and the\n",
      " predictions (and optionally sam ple weigh ts). H owever , you will occasionally wan t to\n",
      " define losses based on other parts of your model, such as the weigh ts or activa tions of\n",
      " its hidden la yers. This ma y be useful for regulariza tion purposes, or to monitor some\n",
      " in ternal aspect of your model.\n",
      " T o define a custom loss based on model in ternals, just com pute it based on an y part\n",
      " of the model you wan t, then pass the result to the \n",
      "add_loss()\n",
      "  method. F or exam ple,\n",
      " the following custom model represen ts a standard MLP regressor with 5 hidden la yƒ\n",
      " ers, except it also im plemen ts a \n",
      " r e c o ns t r u c t i o n l o s s\n",
      " (see \n",
      "???\n",
      "): we add an extra \n",
      "Dense\n",
      " la yer on top of the last hidden la yer , and its role is to tr y to reconstruct the in puts of\n",
      " the model. Since the reconstruction m ust ha ve the same sha pe as the model ‡ s in puts,\n",
      " we need to crea te this \n",
      "Dense\n",
      "  la yer in the \n",
      "build()\n",
      "  method to ha ve access to the sha pe\n",
      " of the in puts. In the \n",
      "call()\n",
      " \n",
      " method, we com pute both the regular output of the MLP ,\n",
      " plus the output of the reconstruction la yer . W e then com pute the mean squared difƒ\n",
      " ference between the reconstructions and the in puts, and we add this value (times\n",
      " 0.05) to the model ‡ s list of losses by calling \n",
      "add_loss()\n",
      " . During training, K eras will\n",
      " add this loss to the main loss (which is wh y we scaled down the reconstruction loss,\n",
      " to ensure the main loss domina tes). As a result, the model will be forced to preser ve\n",
      " as m uch informa tion as possible through the hidden la yers, even informa tion tha t is\n",
      "not directly useful for the regression task itself. In practice, this loss sometimes\n",
      " im proves generaliza tion; it is a regulariza tion loss:\n",
      "class\n",
      " \n",
      "ReconstructingRegressor\n",
      "(\n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "output_dim\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "hidden\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"selu\"\n",
      ",\n",
      "                                          \n",
      "kernel_initializer\n",
      "=\n",
      "\"lecun_normal\"\n",
      ")\n",
      " 388  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "                       \n",
      "for\n",
      " \n",
      "_\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "5\n",
      ")]\n",
      "        \n",
      "self\n",
      ".\n",
      "out\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "output_dim\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "build\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "batch_input_shape\n",
      "):\n",
      "        \n",
      "n_inputs\n",
      " \n",
      "=\n",
      " \n",
      "batch_input_shape\n",
      "[\n",
      "-\n",
      "1\n",
      "]\n",
      "        \n",
      "self\n",
      ".\n",
      "reconstruct\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "n_inputs\n",
      ")\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "build\n",
      "(\n",
      "batch_input_shape\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      "):\n",
      "        \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "        \n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "self\n",
      ".\n",
      "hidden\n",
      ":\n",
      "            \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "layer\n",
      "(\n",
      "Z\n",
      ")\n",
      "        \n",
      "reconstruction\n",
      " \n",
      "=\n",
      " \n",
      "self\n",
      ".\n",
      "reconstruct\n",
      "(\n",
      "Z\n",
      ")\n",
      "        \n",
      "recon_loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "reduce_mean\n",
      "(\n",
      "tf\n",
      ".\n",
      "square\n",
      "(\n",
      "reconstruction\n",
      " \n",
      "-\n",
      " \n",
      "inputs\n",
      "))\n",
      "        \n",
      "self\n",
      ".\n",
      "add_loss\n",
      "(\n",
      "0.05\n",
      " \n",
      "*\n",
      " \n",
      "recon_loss\n",
      ")\n",
      "        \n",
      "return\n",
      " \n",
      "self\n",
      ".\n",
      "out\n",
      "(\n",
      "Z\n",
      ")\n",
      " Similarly , you can add a custom metric based on model in ternals by com puting it in\n",
      " an y wa y you wan t, as long a t the result is the output of a metric object. F or exam ple,\n",
      " you can crea te a \n",
      "keras.metrics.Mean()\n",
      "  object in the constructor , then call it in the\n",
      "call()\n",
      " method, passing it the \n",
      "recon_loss\n",
      ", and finally add it to the model by calling\n",
      " the model ‡ s \n",
      "add_metric()\n",
      "  method. This wa y , when you train the model, K eras will\n",
      " displa y both the mean loss over each epoch (the loss is the sum of the main loss plus\n",
      "0.05 times the reconstruction loss) and the mean reconstruction error over each\n",
      " epoch. B oth will go down during training:\n",
      "Epoch 1/5\n",
      "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
      "Epoch 2/5\n",
      "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
      "[...]\n",
      " In over 99% of the cases, ever ything we ha ve discussed so far will be sufficien t to\n",
      " im plemen t wha tever model you wan t to build, even with com plex architectures, losƒ\n",
      " ses, metrics, and so on. H owever , in some rare cases you ma y need to customize the\n",
      " training loop itself. H owever , before we get there, we need to look a t how to com pute\n",
      " gradien ts a utoma tically in T ensorFlow .\n",
      "Computing Gradients Using \n",
      "Autodi…\n",
      " T o understand how to use a utodiff (see \n",
      " Cha pter 10\n",
      " and \n",
      "???\n",
      " ) to com pute gradien ts\n",
      " a utoma tically , let ‡ s consider a sim ple toy function:\n",
      "def\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "3\n",
      " \n",
      "*\n",
      " \n",
      "w1\n",
      " \n",
      "**\n",
      " \n",
      "2\n",
      " \n",
      "+\n",
      " \n",
      "2\n",
      " \n",
      "*\n",
      " \n",
      "w1\n",
      " \n",
      "*\n",
      " \n",
      "w2\n",
      " If you know calculus, you can analytically find tha t the partial deriva tive of this funcƒ\n",
      "tion with regards to \n",
      "w1\n",
      "   is \n",
      "6 * w1\n",
      " \n",
      "+\n",
      " \n",
      "2 * w2\n",
      " . Y ou can also find tha t its partial deriva tive\n",
      "with regards to \n",
      "w2\n",
      "   is \n",
      "2 * w1\n",
      " . F or exam ple, a t the poin t \n",
      "(w1, w2)\n",
      " \n",
      "=\n",
      " \n",
      "(5, 3)\n",
      ", these parƒ\n",
      " Customizing Models and Training Algorithms  |  389\n",
      "\n",
      " tial deriva tives are equal to 36 and 10, respectively , so the gradien t vector a t this poin t\n",
      " is (36, 10). But if this were a neural network, the function would be m uch more comƒ\n",
      "plex, typically with tens of thousands of parameters, and finding the partial derivaƒ\n",
      " tives analytically by hand would be an almost im possible task. One solution could be\n",
      " to com pute an a pproxima tion of each partial deriva tive by measuring how m uch the\n",
      " function ‡ s output changes when you tweak the corresponding parameter :\n",
      ">>> \n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      " \n",
      "=\n",
      " \n",
      "5\n",
      ",\n",
      " \n",
      "3\n",
      ">>> \n",
      "eps\n",
      " \n",
      "=\n",
      " \n",
      "1e-6\n",
      ">>> \n",
      "(\n",
      "f\n",
      "(\n",
      "w1\n",
      " \n",
      "+\n",
      " \n",
      "eps\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "))\n",
      " \n",
      "/\n",
      " \n",
      "eps\n",
      "36.000003007075065\n",
      ">>> \n",
      "(\n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      " \n",
      "+\n",
      " \n",
      "eps\n",
      ")\n",
      " \n",
      "-\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "))\n",
      " \n",
      "/\n",
      " \n",
      "eps\n",
      "10.000000003174137\n",
      " Looks about righ t! This works ra ther well and it is trivial to im plemen t, but it is just\n",
      " an a pproxima tion, and im portan tly you need to call \n",
      "f()\n",
      "  a t least once per parameter\n",
      " (not twice, since we could com pute \n",
      "f(w1, w2)\n",
      "  just once). This makes this a pproach\n",
      " in tractable for large neural networks. So instead we should use a utodiff (see \n",
      " Cha pƒ\n",
      "ter 10\n",
      " and \n",
      "???\n",
      " ). T ensorFlow makes this pretty sim ple:\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "Variable\n",
      "(\n",
      "5.\n",
      "),\n",
      " \n",
      "tf\n",
      ".\n",
      "Variable\n",
      "(\n",
      "3.\n",
      ")\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "[\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "])\n",
      " W e first define two variables \n",
      "w1\n",
      " and \n",
      "w2\n",
      " , then we crea te a \n",
      "tf.GradientTape\n",
      " \n",
      " con text\n",
      " tha t will a utoma tically record ever y opera tion tha t in volves a variable, and finally we\n",
      " ask this ta pe to com pute the gradien ts of the result \n",
      "z\n",
      " with regards to both variables\n",
      "[w1, w2]\n",
      " . Let ‡ s take a look a t the gradien ts tha t T ensorFlow com puted:\n",
      ">>> \n",
      "gradients\n",
      "[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,\n",
      " <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\n",
      " P erfect! N ot only is the result accura te (the precision is only limited by the floa ting\n",
      " poin t errors), but the \n",
      "gradient()\n",
      "  method only goes through the recorded com putaƒ\n",
      " tions once (in reverse order), no ma tter how man y variables there are, so it is incrediƒ\n",
      " bly efficien t. I t ‡ s like magic!\n",
      " Only put the strict minim um inside the \n",
      "tf.GradientTape()\n",
      " \n",
      "block,\n",
      " to sa ve memor y . Alterna tively , you can pa use recording by crea ting\n",
      "a \n",
      "with tape.stop_recording()\n",
      " block inside the \n",
      "tf.Gradient\n",
      "Tape()\n",
      " block.\n",
      " The ta pe is a utoma tically erased immedia tely after you call its \n",
      "gradient()\n",
      "   method, so\n",
      " you will get an exception if you tr y to call \n",
      "gradient()\n",
      " twice:\n",
      " 390  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      "dz_dw1\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "w1\n",
      ")\n",
      " \n",
      "# => tensor 36.0\n",
      "dz_dw2\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      " \n",
      "# RuntimeError!\n",
      "If you need to call \n",
      "gradient()\n",
      "  more than once, you m ust make the ta pe persisten t,\n",
      "and delete it when you are done with it to free resources:\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "(\n",
      "persistent\n",
      "=\n",
      "True\n",
      ")\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      "dz_dw1\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "w1\n",
      ")\n",
      " \n",
      "# => tensor 36.0\n",
      "dz_dw2\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      " \n",
      "# => tensor 10.0, works fine now!\n",
      "del\n",
      " \n",
      "tape\n",
      " By defa ult, the ta pe will only track opera tions in volving variables, so if you tr y to\n",
      " com pute the gradien t of \n",
      "z\n",
      " \n",
      " with regards to an ything else than a variable, the result will\n",
      "be \n",
      "None\n",
      ":\n",
      "c1\n",
      ",\n",
      " \n",
      "c2\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "5.\n",
      "),\n",
      " \n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "3.\n",
      ")\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "c1\n",
      ",\n",
      " \n",
      "c2\n",
      ")\n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "[\n",
      "c1\n",
      ",\n",
      " \n",
      "c2\n",
      "])\n",
      " \n",
      "# returns [None, None]\n",
      " H owever , you can force the ta pe to wa tch an y tensors you like, to record ever y operaƒ\n",
      " tion tha t in volves them. Y ou can then com pute gradien ts with regards to these tenƒ\n",
      "sors, as if they were variables:\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "    \n",
      "tape\n",
      ".\n",
      "watch\n",
      "(\n",
      "c1\n",
      ")\n",
      "    \n",
      "tape\n",
      ".\n",
      "watch\n",
      "(\n",
      "c2\n",
      ")\n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "c1\n",
      ",\n",
      " \n",
      "c2\n",
      ")\n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "[\n",
      "c1\n",
      ",\n",
      " \n",
      "c2\n",
      "])\n",
      " \n",
      "# returns [tensor 36., tensor 10.]\n",
      " This can be useful in some cases, for exam ple if you wan t to im plemen t a regularizaƒ\n",
      " tion loss tha t penalizes activa tions tha t var y a lot when the in puts var y little: the loss\n",
      " will be based on the gradien t of the activa tions with regards to the in puts. Since the\n",
      " in puts are not variables, you would need to tell the ta pe to wa tch them.\n",
      " If you com pute the gradien t of a list of tensors (e.g., \n",
      "[z1, z2, z3]\n",
      ") with regards to\n",
      "some variables (e.g., \n",
      "[w1, w2]\n",
      " ), T ensorFlow actually efficien tly com putes the sum of\n",
      " the gradien ts of these tensors (i.e., \n",
      "gradient(z1, [w1, w2])\n",
      ", plus \n",
      "gradient(z2,\n",
      "[w1, w2])\n",
      ", plus \n",
      "gradient(z3, [w1, w2])\n",
      " ). Due to the wa y reverse-mode a utodiff\n",
      " works, it is not possible to com pute the individual gradien ts (\n",
      "z1\n",
      ", \n",
      "z2\n",
      " and \n",
      "z3\n",
      ") without\n",
      "actually calling \n",
      "gradient()\n",
      " \n",
      " m ultiple times (once for \n",
      "z1\n",
      ", once for \n",
      "z2\n",
      " \n",
      "and once for \n",
      "z3\n",
      "),\n",
      " which requires making the ta pe persisten t (and deleting it after wards).\n",
      " Customizing Models and Training Algorithms  |  391\n",
      "\n",
      " M oreover , it is actually possible to com pute second order partial deriva tives (the H esƒ\n",
      " sians, i.e., the partial deriva tives of the partial deriva tives)! T o do this, we need to\n",
      " record the opera tions tha t are performed when com puting the first-order partial\n",
      " deriva tives (the J acobians): this requires a second ta pe. H ere is how it works:\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "(\n",
      "persistent\n",
      "=\n",
      "True\n",
      ")\n",
      " \n",
      "as\n",
      " \n",
      "hessian_tape\n",
      ":\n",
      "    \n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "jacobian_tape\n",
      ":\n",
      "        \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      "    \n",
      "jacobians\n",
      " \n",
      "=\n",
      " \n",
      "jacobian_tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "[\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "])\n",
      "hessians\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "hessian_tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "jacobian\n",
      ",\n",
      " \n",
      "[\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "])\n",
      "            \n",
      "for\n",
      " \n",
      "jacobian\n",
      " \n",
      "in\n",
      " \n",
      "jacobians\n",
      "]\n",
      "del\n",
      " \n",
      "hessian_tape\n",
      " The inner ta pe is used to com pute the J acobians, as we did earlier . The outer ta pe is\n",
      " used to com pute the partial deriva tives of each J acobian. Since we need to call \n",
      "gradi\n",
      "ent()\n",
      "  once for each J acobian (or else we would get the sum of the partial deriva tives\n",
      " over all the J abobians, as explained earlier), we need the outer ta pe to be persisten t, so\n",
      " we delete it a t the end. The J acobians are obviously the same as earlier (36 and 5), but\n",
      " now we also ha ve the H essians:\n",
      ">>> \n",
      "hessians\n",
      " \n",
      "# dz_dw1_dw1, dz_dw1_dw2, dz_dw2_dw1, dz_dw2_dw2\n",
      "[[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>,\n",
      "  <tf.Tensor: id=830595, shape=(), dtype=float32, numpy=2.0>],\n",
      " [<tf.Tensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]]\n",
      " Let ‡ s verif y these H essians. The first two are the partial deriva tives of \n",
      "6 * w1\n",
      " \n",
      "+\n",
      " \n",
      "2 * w2\n",
      " (which is, as we sa w earlier , the partial deriva tive of \n",
      "f\n",
      " with regards to \n",
      "w1\n",
      "), with\n",
      "regards to \n",
      "w1\n",
      "   and \n",
      "w2\n",
      ". The result is correct: 6 for \n",
      "w1\n",
      " \n",
      "and 2 for \n",
      "w2\n",
      ". The next two are the\n",
      " partial deriva tives of \n",
      "2 * w1\n",
      "  (the partial deriva tive of \n",
      "f\n",
      " with regards to \n",
      "w2\n",
      "), with\n",
      "regards to \n",
      "w1\n",
      " and \n",
      "w2\n",
      ", which are 2 for \n",
      "w1\n",
      " and 0 for \n",
      "w2\n",
      " . N ote tha t T ensorFlow returns\n",
      "None\n",
      " instead of 0 since \n",
      "w2\n",
      "  does not a ppear a t all in \n",
      "2 * w1\n",
      " . T ensorFlow also returns\n",
      "None\n",
      "  when you use an opera tion whose gradien ts are not defined (e.g., \n",
      "tf.argmax()\n",
      ").\n",
      " In some rare cases you ma y wan t to stop gradien ts from backpropaga ting through\n",
      " some part of your neural network. T o do this, you m ust use the \n",
      "tf.stop_gradient()\n",
      " function: it just returns its in puts during the for ward pass (like \n",
      "tf.identity()\n",
      "), but\n",
      " it does not let gradien ts through during backpropaga tion (it acts like a constan t). F or\n",
      " exam ple:\n",
      "def\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "3\n",
      " \n",
      "*\n",
      " \n",
      "w1\n",
      " \n",
      "**\n",
      " \n",
      "2\n",
      " \n",
      "+\n",
      " \n",
      "tf\n",
      ".\n",
      "stop_gradient\n",
      "(\n",
      "2\n",
      " \n",
      "*\n",
      " \n",
      "w1\n",
      " \n",
      "*\n",
      " \n",
      "w2\n",
      ")\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "f\n",
      "(\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      ")\n",
      " \n",
      "# same result as without stop_gradient()\n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "[\n",
      "w1\n",
      ",\n",
      " \n",
      "w2\n",
      "])\n",
      " \n",
      "# => returns [tensor 30., None]\n",
      " 392  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " Finally , you ma y occasionally run in to some n umerical issues when com puting gradiƒ\n",
      " en ts. F or exam ple, if you com pute the gradien ts of the \n",
      "my_softplus()\n",
      " \n",
      "function for\n",
      " large in puts, the result will be N aN:\n",
      ">>> \n",
      "x\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "Variable\n",
      "([\n",
      "100.\n",
      "])\n",
      ">>> \n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "... \n",
      "    \n",
      "z\n",
      " \n",
      "=\n",
      " \n",
      "my_softplus\n",
      "(\n",
      "x\n",
      ")\n",
      "...\n",
      ">>> \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "z\n",
      ",\n",
      " \n",
      "[\n",
      "x\n",
      "])\n",
      "<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\n",
      " This is beca use com puting the gradien ts of this function using a utodiff leads to some\n",
      " n umerical difficulties: due to floa ting poin t precision errors, a utodiff ends up comƒ\n",
      " puting infinity divided by infinity (which returns N aN). F ortuna tely , we can analytiƒ\n",
      " cally find tha t the deriva tive of the softplus function is just 1 / (1 + 1 / exp(x)), which\n",
      " is n umerically stable. N ext, we can tell T ensorFlow to use this stable function when\n",
      " com puting the gradien ts of the \n",
      "my_softplus()\n",
      "  function, by decora ting it with\n",
      "@tf.custom_gradient\n",
      ", and making it return both its normal output and the function\n",
      " tha t com putes the deriva tives (note tha t it will receive as in put the gradien ts tha t were\n",
      " backpropaga ted so far , down to the softplus function, and according to the chain rule\n",
      " we should m ultiply them with this function ‡ s gradien ts):\n",
      "@tf.custom_gradient\n",
      "def\n",
      " \n",
      "my_better_softplus\n",
      "(\n",
      "z\n",
      "):\n",
      "    \n",
      "exp\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "exp\n",
      "(\n",
      "z\n",
      ")\n",
      "    \n",
      "def\n",
      " \n",
      "my_softplus_gradients\n",
      "(\n",
      "grad\n",
      "):\n",
      "        \n",
      "return\n",
      " \n",
      "grad\n",
      " \n",
      "/\n",
      " \n",
      "(\n",
      "1\n",
      " \n",
      "+\n",
      " \n",
      "1\n",
      " \n",
      "/\n",
      " \n",
      "exp\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "tf\n",
      ".\n",
      "math\n",
      ".\n",
      "log\n",
      "(\n",
      "exp\n",
      " \n",
      "+\n",
      " \n",
      "1\n",
      "),\n",
      " \n",
      "my_softplus_gradients\n",
      " N ow when we com pute the gradien ts of the \n",
      "my_better_softplus()\n",
      " function, we get\n",
      " the proper result, even for large in put values (however , the main output still explodes\n",
      " beca use of the exponen tial: one workaround is to use \n",
      "tf.where()\n",
      " \n",
      "to just return the\n",
      " in puts when they are large).\n",
      " Congra tula tions! Y ou can now com pute the gradien ts of an y function (provided it is\n",
      " differen tiable a t the poin t where you com pute it), you can even com pute H essians,\n",
      " block backpropaga tion when needed and even write your own gradien t functions!\n",
      "This is probably more flexibility than you will ever need, even if you build your own\n",
      " custom training loops, as we will see now .\n",
      "Custom Training Loops\n",
      "In some rare cases, the \n",
      "fit()\n",
      "  method ma y not be flexible enough for wha t you need\n",
      " to do . F or exam ple, the W ide and Deep pa per we discussed in \n",
      " Cha pter 10\n",
      " \n",
      "actually\n",
      " uses two differen t optimizers: one for the wide pa th and the other for the deep pa th.\n",
      "Since the \n",
      "fit()\n",
      "  method only uses one optimizer (the one tha t we specif y when\n",
      " Customizing Models and Training Algorithms  |  393\n",
      "\n",
      " com piling the model), im plemen ting this pa per requires writing your own custom\n",
      " loop .\n",
      " Y ou ma y also like to write your own custom training loops sim ply to feel more confiƒ\n",
      " den t tha t it does precisely wha t you in ten t it to do (perha ps you are unsure about\n",
      "some details of the \n",
      "fit()\n",
      "  method). I t can sometimes feel safer to make ever ything\n",
      " explicit. H owever , remember tha t writing a custom training loop will make your code\n",
      " longer , more error prone and harder to main tain.\n",
      " U nless you really need the extra flexibility , you should prefer using\n",
      "the \n",
      "fit()\n",
      "  method ra ther than im plemen ting your own training\n",
      " loop , especially if you work in a team.\n",
      " First, let ‡ s build a sim ple model. N o need to com pile it, since we will handle the trainƒ\n",
      " ing loop man ually :\n",
      "l2_reg\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "regularizers\n",
      ".\n",
      "l2\n",
      "(\n",
      "0.05\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "30\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"elu\"\n",
      ",\n",
      " \n",
      "kernel_initializer\n",
      "=\n",
      "\"he_normal\"\n",
      ",\n",
      "                       \n",
      "kernel_regularizer\n",
      "=\n",
      "l2_reg\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "kernel_regularizer\n",
      "=\n",
      "l2_reg\n",
      ")\n",
      "])\n",
      " N ext, let ‡ s crea te a tin y function tha t will randomly sam ple a ba tch of instances from\n",
      "the training set (in \n",
      " Cha pter 13\n",
      " \n",
      " we will discuss the Da ta API, which offers a m uch betƒ\n",
      " ter alterna tive):\n",
      "def\n",
      " \n",
      "random_batch\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "y\n",
      ",\n",
      " \n",
      "batch_size\n",
      "=\n",
      "32\n",
      "):\n",
      "    \n",
      "idx\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "random\n",
      ".\n",
      "randint\n",
      "(\n",
      "len\n",
      "(\n",
      "X\n",
      "),\n",
      " \n",
      "size\n",
      "=\n",
      "batch_size\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "X\n",
      "[\n",
      "idx\n",
      "],\n",
      " \n",
      "y\n",
      "[\n",
      "idx\n",
      "]\n",
      " Let ‡ s also define a function tha t will displa y the training sta tus, including the n umber\n",
      " of steps, the total n umber of steps, the mean loss since the start of the epoch (i.e., we\n",
      "will use the \n",
      "Mean\n",
      "  metric to com pute it), and other metrics:\n",
      "def\n",
      " \n",
      "print_status_bar\n",
      "(\n",
      "iteration\n",
      ",\n",
      " \n",
      "total\n",
      ",\n",
      " \n",
      "loss\n",
      ",\n",
      " \n",
      "metrics\n",
      "=\n",
      "None\n",
      "):\n",
      "    \n",
      "metrics\n",
      " \n",
      "=\n",
      " \n",
      "\" - \"\n",
      ".\n",
      "join\n",
      "([\n",
      "\"{}: {:.4f}\"\n",
      ".\n",
      "format\n",
      "(\n",
      "m\n",
      ".\n",
      "name\n",
      ",\n",
      " \n",
      "m\n",
      ".\n",
      "result\n",
      "())\n",
      "                         \n",
      "for\n",
      " \n",
      "m\n",
      " \n",
      "in\n",
      " \n",
      "[\n",
      "loss\n",
      "]\n",
      " \n",
      "+\n",
      " \n",
      "(\n",
      "metrics\n",
      " \n",
      "or\n",
      " \n",
      "[])])\n",
      "    \n",
      "end\n",
      " \n",
      "=\n",
      " \n",
      "\"\"\n",
      " \n",
      "if\n",
      " \n",
      "iteration\n",
      " \n",
      "<\n",
      " \n",
      "total\n",
      " \n",
      "else\n",
      " \n",
      "\"\n",
      "\\n\n",
      "\"\n",
      "    \n",
      "print\n",
      "(\n",
      "\"\n",
      "\\r\n",
      "{}/{} - \"\n",
      ".\n",
      "format\n",
      "(\n",
      "iteration\n",
      ",\n",
      " \n",
      "total\n",
      ")\n",
      " \n",
      "+\n",
      " \n",
      "metrics\n",
      ",\n",
      "          \n",
      "end\n",
      "=\n",
      "end\n",
      ")\n",
      " This code is self-explana tor y , unless you are unfamiliar with Python string forma tƒ\n",
      "ting: \n",
      "{:.4f}\n",
      "  will forma t a floa t with 4 digits after the decimal poin t. M oreover , using\n",
      "\\r\n",
      " (carriage return) along with \n",
      "end=\"\"\n",
      "  ensures tha t the sta tus bar alwa ys gets prin ted\n",
      "on the same line. In the notebook, the \n",
      "print_status_bar()\n",
      " function also includes a\n",
      " progress bar , but you could use the handy tqdm librar y instead.\n",
      " 394  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " W ith tha t, let ‡ s get down to business! First, we need to define some h yperparameters,\n",
      " choose the optimizer , the loss function and the metrics (just the MAE in this examƒ\n",
      "ple):\n",
      "n_epochs\n",
      " \n",
      "=\n",
      " \n",
      "5\n",
      "batch_size\n",
      " \n",
      "=\n",
      " \n",
      "32\n",
      "n_steps\n",
      " \n",
      "=\n",
      " \n",
      "len\n",
      "(\n",
      "X_train\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "batch_size\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "Nadam\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.01\n",
      ")\n",
      "loss_fn\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "losses\n",
      ".\n",
      "mean_squared_error\n",
      "mean_loss\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "metrics\n",
      ".\n",
      "Mean\n",
      "()\n",
      "metrics\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "keras\n",
      ".\n",
      "metrics\n",
      ".\n",
      "MeanAbsoluteError\n",
      "()]\n",
      "And now we are ready to build the custom loop!\n",
      "for\n",
      " \n",
      "epoch\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "n_epochs\n",
      " \n",
      "+\n",
      " \n",
      "1\n",
      "):\n",
      "    \n",
      "print\n",
      "(\n",
      "\"Epoch {}/{}\"\n",
      ".\n",
      "format\n",
      "(\n",
      "epoch\n",
      ",\n",
      " \n",
      "n_epochs\n",
      "))\n",
      "    \n",
      "for\n",
      " \n",
      "step\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "n_steps\n",
      " \n",
      "+\n",
      " \n",
      "1\n",
      "):\n",
      "        \n",
      "X_batch\n",
      ",\n",
      " \n",
      "y_batch\n",
      " \n",
      "=\n",
      " \n",
      "random_batch\n",
      "(\n",
      "X_train_scaled\n",
      ",\n",
      " \n",
      "y_train\n",
      ")\n",
      "        \n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      "            \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      "(\n",
      "X_batch\n",
      ",\n",
      " \n",
      "training\n",
      "=\n",
      "True\n",
      ")\n",
      "            \n",
      "main_loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "reduce_mean\n",
      "(\n",
      "loss_fn\n",
      "(\n",
      "y_batch\n",
      ",\n",
      " \n",
      "y_pred\n",
      "))\n",
      "            \n",
      "loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "add_n\n",
      "([\n",
      "main_loss\n",
      "]\n",
      " \n",
      "+\n",
      " \n",
      "model\n",
      ".\n",
      "losses\n",
      ")\n",
      "        \n",
      "gradients\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "loss\n",
      ",\n",
      " \n",
      "model\n",
      ".\n",
      "trainable_variables\n",
      ")\n",
      "        \n",
      "optimizer\n",
      ".\n",
      "apply_gradients\n",
      "(\n",
      "zip\n",
      "(\n",
      "gradients\n",
      ",\n",
      " \n",
      "model\n",
      ".\n",
      "trainable_variables\n",
      "))\n",
      "        \n",
      "mean_loss\n",
      "(\n",
      "loss\n",
      ")\n",
      "        \n",
      "for\n",
      " \n",
      "metric\n",
      " \n",
      "in\n",
      " \n",
      "metrics\n",
      ":\n",
      "            \n",
      "metric\n",
      "(\n",
      "y_batch\n",
      ",\n",
      " \n",
      "y_pred\n",
      ")\n",
      "        \n",
      "print_status_bar\n",
      "(\n",
      "step\n",
      " \n",
      "*\n",
      " \n",
      "batch_size\n",
      ",\n",
      " \n",
      "len\n",
      "(\n",
      "y_train\n",
      "),\n",
      " \n",
      "mean_loss\n",
      ",\n",
      " \n",
      "metrics\n",
      ")\n",
      "    \n",
      "print_status_bar\n",
      "(\n",
      "len\n",
      "(\n",
      "y_train\n",
      "),\n",
      " \n",
      "len\n",
      "(\n",
      "y_train\n",
      "),\n",
      " \n",
      "mean_loss\n",
      ",\n",
      " \n",
      "metrics\n",
      ")\n",
      "    \n",
      "for\n",
      " \n",
      "metric\n",
      " \n",
      "in\n",
      " \n",
      "[\n",
      "mean_loss\n",
      "]\n",
      " \n",
      "+\n",
      " \n",
      "metrics\n",
      ":\n",
      "        \n",
      "metric\n",
      ".\n",
      "reset_states\n",
      "()\n",
      " There ‡ s a lot going on in this code, so let ‡ s walk through it:\n",
      "⁄\n",
      " W e crea te two nested loops: one for the epochs, the other for the ba tches within\n",
      "an epoch.\n",
      "⁄\n",
      " Then we sam ple a random ba tch from the training set.\n",
      "⁄\n",
      "Inside the \n",
      "tf.GradientTape()\n",
      "  block, we make a prediction for one ba tch (using\n",
      " the model as a function), and we com pute the loss: it is equal to the main loss\n",
      " plus the other losses (in this model, there is one regulariza tion loss per la yer).\n",
      "Since the \n",
      "mean_squared_error()\n",
      " function returns one loss per instance, we\n",
      " com pute the mean over the ba tch using \n",
      "tf.reduce_mean()\n",
      " \n",
      " (if you wan ted to\n",
      " a pply differen t weigh ts to each instance, this is where you would do it). The reguƒ\n",
      " lariza tion losses are already reduced to a single scalar each, so we just need to\n",
      "sum them (using \n",
      "tf.add_n()\n",
      " , which sums m ultiple tensors of the same sha pe\n",
      " and da ta type).\n",
      " Customizing Models and Training Algorithms  |  395\n",
      "\n",
      "11\n",
      " The truth is we did not process ever y single instance in the training set beca use we sam pled instances ranƒ\n",
      " domly , so some were processed more than once while others were not processed a t all. In practice tha t ‡ s fine.\n",
      " M oreover , if the training set size is not a m ultiple of the ba tch size, we will miss a few instances.\n",
      "12\n",
      " Alterna tively , check out \n",
      "K.learning_phase()\n",
      ", \n",
      "K.set_learning_phase()\n",
      " and \n",
      "K.learning_phase_scope()\n",
      ".\n",
      "13\n",
      " W ith the exception of optimizers, as ver y few people ever customize these: see the notebook for an exam ple.\n",
      "⁄\n",
      " N ext, we ask the \n",
      "tape\n",
      "  to com pute the gradien t of the loss with regards to each\n",
      "trainable variable (\n",
      " n o t\n",
      "  all variables!), and we a pply them to the optimizer to perƒ\n",
      " form a Gradien t Descen t step .\n",
      "⁄\n",
      " N ext we upda te the mean loss and the metrics (over the curren t epoch), and we\n",
      " displa y the sta tus bar .\n",
      "⁄\n",
      " A t the end of each epoch, we displa y the sta tus bar again to make it look comƒ\n",
      "plete\n",
      "11\n",
      "  and to prin t a line feed, and we reset the sta tes of the mean loss and the\n",
      "metrics.\n",
      " If you set the optimizer‡ s \n",
      "clipnorm\n",
      "   or \n",
      "clipvalue\n",
      " \n",
      " h yperparameters, it will take care of\n",
      " this for you. If you wan t to a pply an y other transforma tion to the gradien ts, sim ply do\n",
      "so before calling the \n",
      "apply_gradients()\n",
      " method.\n",
      " If you add weigh t constrain ts to your model (e.g., by setting \n",
      "kernel_constraint\n",
      " \n",
      "or\n",
      "bias_constraint\n",
      "  when crea ting a la yer), you should upda te the training loop to\n",
      " a pply these constrain ts just after \n",
      "apply_gradients()\n",
      ":\n",
      "for\n",
      " \n",
      "variable\n",
      " \n",
      "in\n",
      " \n",
      "model\n",
      ".\n",
      "variables\n",
      ":\n",
      "    \n",
      "if\n",
      " \n",
      "variable\n",
      ".\n",
      "constraint\n",
      " \n",
      "is\n",
      " \n",
      "not\n",
      " \n",
      "None\n",
      ":\n",
      "        \n",
      "variable\n",
      ".\n",
      "assign\n",
      "(\n",
      "variable\n",
      ".\n",
      "constraint\n",
      "(\n",
      "variable\n",
      "))\n",
      " M ost im portan tly , this training loop does not handle la yers tha t beha ve differen tly\n",
      "during training and testing (e.g., \n",
      "BatchNormalization\n",
      " or \n",
      "Dropout\n",
      " ). T o handle these,\n",
      "you need to call the model with \n",
      "training=True\n",
      "  and make sure it propaga tes this to\n",
      " ever y la yer tha t needs it.\n",
      "12\n",
      " As you can see, there are quite a lot of things you need to get righ t, it is easy to make a\n",
      " mistake. But on the brigh t side, you get full con trol, so it ‡ s your call.\n",
      " N ow tha t you know how to customize an y part of your models\n",
      "13\n",
      " \n",
      "and training algoƒ\n",
      " rithms, let ‡ s see how you can use T ensorFlow‡ s a utoma tic gra ph genera tion fea ture: it\n",
      " can speed up your custom code considerably , and it will also make it portable to an y\n",
      " pla tform supported by T ensorFlow (see \n",
      "???\n",
      ").\n",
      "TensorFlow Functions and Graphs\n",
      " In T ensorFlow 1, gra phs were una voidable (as were the com plexities tha t came with\n",
      " them): they were a cen tral part of T ensorFlow‡ s API. In T ensorFlow 2, they are still\n",
      " 396  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      " there, but not as cen tral, and m uch (m uch!) sim pler to use. T o demonstra te this, let ‡ s\n",
      " start with a trivial function tha t just com putes the cube of its in put:\n",
      "def\n",
      " \n",
      "cube\n",
      "(\n",
      "x\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "x\n",
      " \n",
      "**\n",
      " \n",
      "3\n",
      " W e can obviously call this function with a Python value, such as an in t or a floa t, or\n",
      " we can call it with a tensor :\n",
      ">>> \n",
      "cube\n",
      "(\n",
      "2\n",
      ")\n",
      "8\n",
      ">>> \n",
      "cube\n",
      "(\n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "2.0\n",
      "))\n",
      "<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\n",
      " N ow , let ‡ s use \n",
      "tf.function()\n",
      "  to con vert this Python function to a \n",
      " T ens o rF l o w F u n c‡\n",
      " t i o n\n",
      ":\n",
      ">>> \n",
      "tf_cube\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "function\n",
      "(\n",
      "cube\n",
      ")\n",
      ">>> \n",
      "tf_cube\n",
      "<tensorflow.python.eager.def_function.Function at 0x1546fc080>\n",
      " This TF F unction can then be used exactly like the original Python function, and it\n",
      "will return the same result (but as tensors):\n",
      ">>> \n",
      "tf_cube\n",
      "(\n",
      "2\n",
      ")\n",
      "<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\n",
      ">>> \n",
      "tf_cube\n",
      "(\n",
      "tf\n",
      ".\n",
      "constant\n",
      "(\n",
      "2.0\n",
      "))\n",
      "<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\n",
      " U nder the hood, \n",
      "tf.function()\n",
      " \n",
      " analyzed the com puta tions performed by the \n",
      "cube()\n",
      " function and genera ted an equivalen t com puta tion gra ph! As you can see, it was\n",
      " ra ther painless (we will see how this works shortly). Alterna tively , we could ha ve used\n",
      "tf.function\n",
      "  as a decora tor ; this is actually more common:\n",
      "@tf.function\n",
      "def\n",
      " \n",
      "tf_cube\n",
      "(\n",
      "x\n",
      "):\n",
      "    \n",
      "return\n",
      " \n",
      "x\n",
      " \n",
      "**\n",
      " \n",
      "3\n",
      " The original Python function is still a vailable via the TF F unction ‡ s \n",
      "python_function\n",
      " a ttribute, in case you ever need it:\n",
      ">>> \n",
      "tf_cube\n",
      ".\n",
      "python_function\n",
      "(\n",
      "2\n",
      ")\n",
      "8\n",
      " T ensorFlow optimizes the com puta tion gra ph, pruning un used nodes, sim plif ying\n",
      "expressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized\n",
      " gra ph is ready , the TF F unction efficien tly executes the opera tions in the gra ph, in the\n",
      " a ppropria te order (and in parallel when it can). As a result, a TF F unction will usually\n",
      " run m uch faster than the original Python function, especially if it performs com plex\n",
      " TensorFlow Functions and Graphs  |  397\n",
      "\n",
      "14\n",
      " H owever , in this trivial exam ple, the com puta tion gra ph is so small tha t there is nothing a t all to optimize, so\n",
      "tf_cube()\n",
      "  actually runs m uch slower than \n",
      "cube()\n",
      ".\n",
      " com puta tions.\n",
      "14\n",
      "  M ost of the time you will not really need to know more than tha t:\n",
      " when you wan t to boost a Python function, just transform it in to a TF F unction.\n",
      " Tha t ‡ s all!\n",
      " M oreover , when you write a custom loss function, a custom metric, a custom la yer or\n",
      " an y other custom function, and you use it in a K eras model (as we did throughout\n",
      " this cha pter), K eras a utoma tically con verts your function in to a TF F unction, no need\n",
      "to use \n",
      "tf.function()\n",
      " . So most of the time, all this magic is 100% transparen t.\n",
      " Y ou can tell K eras \n",
      " n o t\n",
      "  to con vert your Python functions to TF\n",
      " F unctions by setting \n",
      "dynamic=True\n",
      "  when crea ting a custom la yer\n",
      " or a custom model. Alterna tively , you can set \n",
      "run_eagerly=True\n",
      " when calling the model ‡ s \n",
      "compile()\n",
      " method.\n",
      " TF F unction genera tes a new gra ph for ever y unique set of in put sha pes and da ta\n",
      " types, and it caches it for subsequen t calls. F or exam ple, if you call \n",
      "tf_cube(tf.con\n",
      "stant(10))\n",
      " , a gra ph will be genera ted for in t32 tensors of sha pe []. Then if you call\n",
      "tf_cube(tf.constant(20))\n",
      " , the same gra ph will be reused. But if you then call\n",
      "tf_cube(tf.constant([10, 20]))\n",
      " , a new gra ph will be genera ted for in t32 tensors\n",
      " of sha pe [2]. This is how TF F unctions handle polymorphism (i.e., var ying argumen t\n",
      " types and sha pes). H owever , this is only true for tensor argumen ts: if you pass n umerƒ\n",
      " ical Python values to a TF F unction, a new gra ph will be genera ted for ever y distinct\n",
      " value: for exam ple, calling \n",
      "tf_cube(10)\n",
      " and \n",
      "tf_cube(20)\n",
      "  will genera te two gra phs.\n",
      " If you call a TF F unction man y times with differen t n umerical\n",
      " Python values, then man y gra phs will be genera ted, slowing down\n",
      "your program and using up a lot of RAM. Python values should be\n",
      " reser ved for argumen ts tha t will ha ve few unique values, such as\n",
      " h yperparameters like the n umber of neurons per la yer . This allows\n",
      " T ensorFlow to better optimize each varian t of your model.\n",
      "Autograph and Tracing\n",
      " So how does T ensorFlow genera te gra phs? W ell, first it starts by analyzing the Python\n",
      " function ‡ s source code to ca pture all the con trol flow sta temen ts, such as \n",
      "for\n",
      " \n",
      "loops\n",
      "and \n",
      "while\n",
      " loops, \n",
      "if\n",
      "  sta temen ts, as well as \n",
      "break\n",
      ", \n",
      "continue\n",
      " and \n",
      "return\n",
      " \n",
      " sta temen ts.\n",
      "This first step is called \n",
      " a u t og r a p h\n",
      " . The reason T ensorFlow has to analyze the source\n",
      " code is tha t Python does not provide an y other wa y to ca pture con trol flow sta teƒ\n",
      " men ts: it offers magic methods like \n",
      "__add__()\n",
      "   or \n",
      "__mul__()\n",
      " \n",
      " to ca pture opera tors like\n",
      " 398  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "+\n",
      " and \n",
      "*\n",
      ", but there are no \n",
      "__while__()\n",
      " or \n",
      "__if__()\n",
      " magic methods. After analyzing\n",
      " the function ‡ s code, a utogra ph outputs an upgraded version of tha t function in which\n",
      " all the con trol flow sta temen ts are replaced by the a ppropria te T ensorFlow operaƒ\n",
      "tions, such as \n",
      "tf.while_loop()\n",
      " for loops and \n",
      "tf.cond()\n",
      " \n",
      "for \n",
      "if\n",
      " \n",
      " sta temen ts. F or\n",
      " exam ple, in \n",
      "Figure 12-4\n",
      " , a utogra ph analyzes the source code of the \n",
      "sum_squares()\n",
      " Python function, and it genera tes the \n",
      "tf__sum_squares()\n",
      " function. In this function,\n",
      "the \n",
      "for\n",
      " loop is replaced by the definition of the \n",
      "loop_body()\n",
      "  function (con taining\n",
      "the body of the original \n",
      "for\n",
      " \n",
      "loop), followed by a call to the \n",
      "for_stmt()\n",
      "   function. This\n",
      " call will build the a ppropria te \n",
      "tf.while_loop()\n",
      "  opera tion in the com puta tion gra ph.\n",
      " F i g u r e 12-4. H o w T ens o rF l o w gen er a t e s g r a p hs u s i n g a u t og r a p h a n d t r a ci n g\n",
      " N ext, T ensorFlow calls this — upgraded – function, but instead of passing the actual\n",
      " argumen t, it passes a \n",
      " s y m b o l i c t ens o r\n",
      " , meaning a tensor without an y actual value, only\n",
      " a name, a da ta type, and a sha pe. F or exam ple, if you call \n",
      "sum_squares(tf.con\n",
      "stant(10))\n",
      ", then the \n",
      "tf__sum_squares()\n",
      " \n",
      "function will actually be called with a symƒ\n",
      " bolic tensor of type in t32 and sha pe []. The function will run in \n",
      " g r a p h m o d e\n",
      ", meaning\n",
      " tha t each T ensorFlow opera tion will just add a node in the gra ph to represen t itself\n",
      "and its output tensor(s) (as opposed to the regular mode, called \n",
      " e a ger ex e cu t i o n\n",
      ", or\n",
      " e a ger m o d e\n",
      " ). In gra ph mode, TF opera tions do not perform an y actual com puta tions.\n",
      " This should feel familiar if you know T ensorFlow 1, as gra ph mode was the defa ult\n",
      "mode. In \n",
      "Figure 12-4\n",
      ", you can see the \n",
      "tf__sum_squares()\n",
      " \n",
      "function being called with\n",
      " a symbolic tensor as argumen t (in this case, an in t32 tensor of sha pe []), and the final\n",
      " gra ph genera ted during tracing. The ellipses represen t opera tions, and the arrows\n",
      " represen t tensors (both the genera ted function and the gra ph are sim plified).\n",
      " TensorFlow Functions and Graphs  |  399\n",
      "\n",
      " T o view the genera ted function ‡ s source code, you can call \n",
      "tf.auto\n",
      "graph.to_code(sum_squares.python_function)\n",
      ". The code is not\n",
      " mean t to be pretty , but it can sometimes help for debugging.\n",
      "TF Function Rules\n",
      " M ost of the time, con verting a Python function tha t performs T ensorFlow opera tions\n",
      " in to a TF F unction is trivial: just decora te it with \n",
      "@tf.function\n",
      " \n",
      " or let K eras take care\n",
      " of it for you. H owever , there are a few rules to respect:\n",
      "⁄\n",
      " If you call an y external librar y , including N umPy or even the standard librar y ,\n",
      " this call will run only during tracing, it will not be part of the gra ph. Indeed, a\n",
      " T ensorFlow gra ph can only include T ensorFlow constructs (tensors, opera tions,\n",
      " variables, da tasets, and so on). So make sure you use \n",
      "tf.reduce_sum()\n",
      "   instead of\n",
      "np.sum()\n",
      ", and \n",
      "tf.sort()\n",
      " instead of the built-in \n",
      "sorted()\n",
      " \n",
      "function, and so on\n",
      " (unless you really wan t the code to run only during tracing).\n",
      "›\n",
      " F or exam ple, if you define a TF function \n",
      "f(x)\n",
      "  tha t just returns \n",
      "np.ran\n",
      "dom.rand()\n",
      " , a random n umber will only be genera ted when the function is\n",
      "traced, so \n",
      "f(tf.constant(2.))\n",
      " and \n",
      "f(tf.constant(3.))\n",
      " will return the\n",
      " same random n umber , but \n",
      "f(tf.constant([2., 3.]))\n",
      " \n",
      " will return a differen t\n",
      "one. If you replace \n",
      "np.random.rand()\n",
      " with \n",
      "tf.random.uniform([])\n",
      ", then a\n",
      " new random n umber will be genera ted upon ever y call, since the opera tion\n",
      " will be part of the gra ph.\n",
      "›\n",
      " If your non-T ensorFlow code has side-effects (such as logging something or\n",
      " upda ting a Python coun ter), then you should not expect tha t side-effect to\n",
      " occur ever y time you call the TF F unction, as it will only occur when the funcƒ\n",
      "tion is traced.\n",
      "›\n",
      " Y ou can wra p arbitrar y Python code in a \n",
      "tf.py_function()\n",
      " \n",
      " opera tion, but\n",
      " this will hinder performance, as T ensorFlow will not be able to do an y gra ph\n",
      " optimiza tion on this code, and it will also reduce portability , as the gra ph will\n",
      " only run on pla tforms where Python is a vailable (and the righ t libraries\n",
      "installed).\n",
      "⁄\n",
      " Y ou can call other Python functions or TF F unctions, but they should follow the\n",
      " same rules, as T ensorFlow will also ca pture their opera tions in the com puta tion\n",
      " gra ph. N ote tha t these other functions do not need to be decora ted with\n",
      "@tf.function\n",
      ".\n",
      "⁄\n",
      " If the function crea tes a T ensorFlow variable (or an y other sta teful T ensorFlow\n",
      " object, such as a da taset or a queue), it m ust do so upon the ver y first call, and\n",
      " only then, or else you will get an exception. I t is usually preferable to crea te variƒ\n",
      " ables outside of the TF F unction (e.g., in the \n",
      "build()\n",
      "  method of a custom la yer).\n",
      " 400  |  Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "⁄\n",
      " The source code of your Python function should be a vailable to T ensorFlow . If\n",
      " the source code is una vailable (for exam ple, if you define your function in the\n",
      "Python shell, which does not give access to the source code, or if you deploy only\n",
      " the com piled Python files \n",
      "*.pyc\n",
      "  to production), then the gra ph genera tion proƒ\n",
      " cess will fail or ha ve limited functionality .\n",
      "⁄\n",
      " T ensorFlow will only ca pture \n",
      "for\n",
      " \n",
      " loops tha t itera te over a tensor or a \n",
      "Dataset\n",
      ". So\n",
      "make sure you use \n",
      "for i in tf.range(10)\n",
      "   ra ther than \n",
      "for i in range(10)\n",
      ", or\n",
      " else the loop will not be ca ptured in the gra ph. Instead, it will run during tracing.\n",
      " This ma y be wha t you wan t, if the \n",
      "for\n",
      "   loop is mean t to build the gra ph, for examƒ\n",
      " ple to crea te each la yer in a neural network.\n",
      "⁄\n",
      " And as alwa ys, for performance reasons, you should prefer a vectorized im pleƒ\n",
      " men ta tion whenever you can, ra ther than using loops.\n",
      " I t ‡ s time to sum up! In this cha pter we started with a brief over view of T ensorFlow ,\n",
      " then we looked a t T ensorFlow‡ s low-level API, including tensors, opera tions, variables\n",
      " and special da ta structures. W e then used these tools to customize almost ever y comƒ\n",
      " ponen t in tf.keras. Finally , we looked a t how TF F unctions can boost performance,\n",
      " how gra phs are genera ted using a utogra ph and tracing, and wha t rules to follow when\n",
      " you write TF F unctions (if you would like to open the black box a bit further , for\n",
      " exam ple to explore the genera ted gra phs, you will find further technical details\n",
      "in \n",
      "???\n",
      ").\n",
      " In the next cha pter , we will look a t how to efficien tly load and preprocess da ta with\n",
      " T ensorFlow .\n",
      " TensorFlow Functions and Graphs  |  401\n",
      "\n",
      "\n",
      "CHAPTER 13\n",
      "Loading and Preprocessing Data with\n",
      "TensorFlow\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 13 in the final\n",
      "release of the book.\n",
      " So far we ha ve used only da tasets tha t fit in memor y , but Deep Learning systems are\n",
      " often trained on ver y large da tasets tha t will not fit in RAM. Ingesting a large da taset\n",
      " and preprocessing it efficien tly can be tricky to im plemen t with other Deep Learning\n",
      " libraries, but T ensorFlow makes it easy thanks to the \n",
      " D a t a AP I\n",
      " : you just crea te a da taƒ\n",
      " set object, tell it where to get the da ta, then transform it in an y wa y you wan t, and\n",
      " T ensorFlow takes care of all the im plemen ta tion details, such as m ultithreading,\n",
      " queuing, ba tching, prefetching, and so on.\n",
      " Off the shelf, the Da ta API can read from text files (such as CSV files), binar y files\n",
      " with fixed-size records, and binar y files tha t use T ensorFlow‡ s TFRecord forma t,\n",
      " which supports records of var ying sizes. TFRecord is a flexible and efficien t binar y\n",
      " forma t based on Protocol Buffers (an open source binar y forma t). The Da ta API also\n",
      " has support for reading from SQL da tabases. M oreover , man y Open Source extenƒ\n",
      " sions are a vailable to read from all sorts of da ta sources, such as Google ‡ s BigQuer y\n",
      " ser vice.\n",
      " H owever , reading h uge da tasets efficien tly is not the only difficulty : the da ta also\n",
      " needs to be preprocessed. Indeed, it is not alwa ys com posed strictly of con venien t\n",
      " n umerical fields: sometimes there will be text fea tures, ca tegorical fea tures, and so on.\n",
      " T o handle this, T ensorFlow provides the \n",
      " F e a t u r e s AP I\n",
      " : it lets you easily con vert these\n",
      " fea tures to n umerical fea tures tha t can be consumed by your neural network. F or\n",
      "403\n",
      "\n",
      " exam ple, ca tegorical fea tures with a large n umber of ca tegories (such as cities, or\n",
      "words) can be encoded using \n",
      " em b e d d i n g s\n",
      " (as we will see, an embedding is a trainable\n",
      " dense vector tha t represen ts a ca tegor y).\n",
      " B oth the Da ta API and the F ea tures API work seamlessly with\n",
      "tf.keras.\n",
      " In this cha pter , we will cover the Da ta API, the TFRecord forma t and the F ea tures\n",
      " API in detail. W e will also take a quick look a t a few rela ted projects from T ensorƒ\n",
      " Flow‡ s ecosystem:\n",
      "⁄\n",
      " TF T ransform (\n",
      " t f .T r a ns f o r m\n",
      ") makes it possible to write a single preprocessing\n",
      " function tha t can be run both in ba tch mode on your full training set, before\n",
      " training (to speed it up), and then exported to a TF F unction and incorpora ted\n",
      " in to your trained model, so tha t once it is deployed in production, it can take\n",
      " care of preprocessing new instances on the fly .\n",
      "⁄\n",
      " TF Da tasets (TFDS) provides a con venien t function to download man y common\n",
      " da tasets of all kinds, including large ones like ImageN et, and it provides con veƒ\n",
      " nien t da taset objects to manipula te them using the Da ta API.\n",
      " So let ‡ s get started!\n",
      "The Data API\n",
      " The whole Da ta API revolves around the concept of a \n",
      " d a t as e t\n",
      " : as you migh t suspect,\n",
      " this represen ts a sequence of da ta items. U sually you will use da tasets tha t gradually\n",
      " read da ta from disk, but for sim plicity let ‡ s just crea te a da taset en tirely in RAM using\n",
      "tf.data.Dataset.from_tensor_slices()\n",
      ":\n",
      ">>> \n",
      "X\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "range\n",
      "(\n",
      "10\n",
      ")\n",
      "  \n",
      "# any data tensor\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "Dataset\n",
      ".\n",
      "from_tensor_slices\n",
      "(\n",
      "X\n",
      ")\n",
      ">>> \n",
      "dataset\n",
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "The \n",
      "from_tensor_slices()\n",
      "  function takes a tensor and crea tes a \n",
      "tf.data.Dataset\n",
      " whose elemen ts are all the slices of \n",
      "X\n",
      "  (along the first dimension), so this da taset conƒ\n",
      " tains 10 items: tensors 0, 1, 2, Ñ, 9. In this case we would ha ve obtained the same\n",
      " da taset if we had used \n",
      "tf.data.Dataset.range(10)\n",
      ".\n",
      " Y ou can sim ply itera te over a da taset ‡ s items like this:\n",
      ">>> \n",
      "for\n",
      " \n",
      "item\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ":\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "item\n",
      ")\n",
      " 404  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "[...]\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "Chaining Transformations\n",
      " Once you ha ve a da taset, you can a pply all sorts of transforma tions to it by calling its\n",
      " transforma tion methods. Each method returns a new da taset, so you can chain transƒ\n",
      " forma tions like this (this chain is illustra ted in \n",
      "Figure 13-1\n",
      "):\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "repeat\n",
      "(\n",
      "3\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "7\n",
      ")\n",
      ">>> \n",
      "for\n",
      " \n",
      "item\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ":\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "item\n",
      ")\n",
      "...\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
      " F i g u r e 13-1. Ch a i n i n g D a t as e t T r a ns f o r m a t i o ns\n",
      " In this exam ple, we first call the \n",
      "repeat()\n",
      "  method on the original da taset, and it\n",
      " returns a new da taset tha t will repea t the items of the original da taset 3 times. Of\n",
      " course, this will not copy the whole da ta in memor y 3 times! In fact, if you call this\n",
      " method with no argumen ts, the new da taset will repea t the source da taset forever .\n",
      "Then we call the \n",
      "batch()\n",
      "  method on this new da taset, and again this crea tes a new\n",
      " da taset. This one will group the items of the previous da taset in ba tches of 7 items.\n",
      " Finally , we itera te over the items of this final da taset. As you can see, the \n",
      "batch()\n",
      " method had to output a final ba tch of size 2 instead of 7, but you can call it with\n",
      "drop_remainder=True\n",
      "  if you wan t it to drop this final ba tch so tha t all ba tches ha ve\n",
      "the exact same size.\n",
      " The Data API  |  405\n",
      "\n",
      " The da taset methods do \n",
      " n o t\n",
      "  modif y da tasets, they crea te new ones,\n",
      " so make sure to keep a reference to these new da tasets (e.g., \n",
      "data\n",
      "set = ...\n",
      " ), or else nothing will ha ppen.\n",
      " Y ou can also a pply an y transforma tion you wan t to the items by calling the \n",
      "map()\n",
      " method. F or exam ple, this crea tes a new da taset with all items doubled:\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "map\n",
      "(\n",
      "lambda\n",
      " \n",
      "x\n",
      ":\n",
      " \n",
      "x\n",
      " \n",
      "*\n",
      " \n",
      "2\n",
      ")\n",
      " \n",
      "# Items: [0,2,4,6,8,10,12]\n",
      " This function is the one you will call to a pply an y preprocessing you wan t to your\n",
      " da ta. Sometimes, this will include com puta tions tha t can be quite in tensive, such as\n",
      " resha ping or rota ting an image, so you will usually wan t to spa wn m ultiple threads to\n",
      " speed things up: it ‡ s as sim ple as setting the \n",
      "num_parallel_calls\n",
      "  argumen t.\n",
      "While the \n",
      "map()\n",
      " \n",
      " a pplies a transforma tion to each item, the \n",
      "apply()\n",
      " \n",
      " method a pplies a\n",
      " transforma tion to the da taset as a whole. F or exam ple, the following code — unba tches –\n",
      " the da taset, by a pplying the \n",
      "unbatch()\n",
      "  function to the da taset (this function is curƒ\n",
      " ren tly experimen tal, but it will most likely move to the core API in a future release).\n",
      " Each item in the new da taset will be a single in teger tensor instead of a ba tch of 7\n",
      " in tegers:\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "apply\n",
      "(\n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "experimental\n",
      ".\n",
      "unbatch\n",
      "())\n",
      " \n",
      "# Items: 0,2,4,...\n",
      " I t is also possible to sim ply filter the da taset using the \n",
      "filter()\n",
      " method:\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "filter\n",
      "(\n",
      "lambda\n",
      " \n",
      "x\n",
      ":\n",
      " \n",
      "x\n",
      " \n",
      "<\n",
      " \n",
      "10\n",
      ")\n",
      " \n",
      "# Items: 0 2 4 6 8 0 2 4 6...\n",
      " Y ou will often wan t to look a t just a few items from a da taset. Y ou can use the \n",
      "take()\n",
      " method for tha t:\n",
      ">>> \n",
      "for\n",
      " \n",
      "item\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ".\n",
      "take\n",
      "(\n",
      "3\n",
      "):\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "item\n",
      ")\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "Shuƒing\n",
      " the Data\n",
      " As you know , Gradien t Descen t works best when the instances in the training set are\n",
      " independen t and iden tically distributed (see \n",
      " Cha pter 4\n",
      " ). A sim ple wa y to ensure this\n",
      " is to sh uffle the instances. F or this, you can just use the \n",
      "shuffle()\n",
      "  method. I t will\n",
      " crea te a new da taset tha t will start by filling up a buffer with the first items of the\n",
      " source da taset, then whenever it is asked for an item, it will pull one out randomly\n",
      " from the buffer , and replace it with a fresh one from the source da taset, un til it has\n",
      " itera ted en tirely through the source da taset. A t this poin t it con tin ues to pull out\n",
      " items randomly from the buffer un til it is em pty . Y ou m ust specif y the buffer size, and\n",
      " 406  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "1\n",
      " Imagine a sorted deck of cards on your left: suppose you just take the top 3 cards and sh uffle them, then pick\n",
      " one randomly and put it to your righ t, keeping the other 2 in your hands. T ake another card on your left,\n",
      " sh uffle the 3 cards in your hands and pick one of them randomly , and put it on your righ t. When you are\n",
      " done going through all the cards like this, you will ha ve a deck of cards on your righ t: do you think it will be\n",
      " perfectly sh uffled?\n",
      " it is im portan t to make it large enough or else sh uffling will not be ver y efficien t.\n",
      "1\n",
      " H owever , obviously do not exceed the amoun t of RAM you ha ve, and even if you\n",
      " ha ve plen ty of it, there ‡ s no need to go well beyond the da taset ‡ s size. Y ou can provide\n",
      " a random seed if you wan t the same random order ever y time you run your program.\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "Dataset\n",
      ".\n",
      "range\n",
      "(\n",
      "10\n",
      ")\n",
      ".\n",
      "repeat\n",
      "(\n",
      "3\n",
      ")\n",
      " \n",
      "# 0 to 9, three times\n",
      ">>> \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "shuffle\n",
      "(\n",
      "buffer_size\n",
      "=\n",
      "5\n",
      ",\n",
      " \n",
      "seed\n",
      "=\n",
      "42\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "7\n",
      ")\n",
      ">>> \n",
      "for\n",
      " \n",
      "item\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ":\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "item\n",
      ")\n",
      "...\n",
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n",
      "If you call \n",
      "repeat()\n",
      " \n",
      " on a sh uffled da taset, by defa ult it will genera te\n",
      " a new order a t ever y itera tion. This is generally a good idea, but if\n",
      " you prefer to reuse the same order a t each itera tion (e.g., for tests\n",
      "or debugging), you can set \n",
      "reshuffle_each_iteration=False\n",
      ".\n",
      " F or a large da taset tha t does not fit in memor y , this sim ple sh uffling-buffer a pproach\n",
      " ma y not be sufficien t, since the buffer will be small com pared to the da taset. One solƒ\n",
      " ution is to sh uffle the source da ta itself (for exam ple, on Lin ux you can sh uffle text\n",
      "files using the \n",
      "shuf\n",
      "  command). This will definitely im prove sh uffling a lot! H owever ,\n",
      " even if the source da ta is sh uffled, you will usually wan t to sh uffle it some more, or\n",
      " else the same order will be repea ted a t each epoch, and the model ma y end up being\n",
      " biased (e.g., due to some spurious pa tterns presen t by chance in the source da ta ‡ s\n",
      " order). T o sh uffle the instances some more, a common a pproach is to split the source\n",
      " da ta in to m ultiple files, then read them in a random order during training. H owever ,\n",
      " instances loca ted in the same file will still end up close to each other . T o a void this\n",
      " you can pick m ultiple files randomly , and read them sim ultaneously , in terlea ving\n",
      " their lines. Then on top of tha t you can add a sh uffling buffer using the \n",
      "shuffle()\n",
      " method. If all this sounds like a lot of work, don ‡ t worr y : the Da ta API actually makes\n",
      " all this possible in just a few lines of code. Let ‡ s see how to do this.\n",
      " The Data API  |  407\n",
      "\n",
      "Interleaving Lines From Multiple Files\n",
      " First, let ‡ s suppose tha t you loaded the California housing da taset, you sh uffled it\n",
      " (unless it was already sh uffled), you split it in to a training set, a valida tion set and a\n",
      " test set, then you split each set in to man y CSV files tha t each look like this (each row\n",
      " con tains 8 in put fea tures plus the target median house value):\n",
      "MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\n",
      "3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\n",
      "[...]\n",
      " Let ‡ s also suppose \n",
      "train_filepaths\n",
      "  con tains the list of file pa ths (and you also ha ve\n",
      "valid_filepaths\n",
      " and \n",
      "test_filepaths\n",
      "):\n",
      ">>> \n",
      "train_filepaths\n",
      "[•datasets/housing/my_train_00.csv•, •datasets/housing/my_train_01.csv•,...]\n",
      " N ow let ‡ s crea te a da taset con taining only these file pa ths:\n",
      "filepath_dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "Dataset\n",
      ".\n",
      "list_files\n",
      "(\n",
      "train_filepaths\n",
      ",\n",
      " \n",
      "seed\n",
      "=\n",
      "42\n",
      ")\n",
      " By defa ult, the \n",
      "list_files()\n",
      " \n",
      " function returns a da taset tha t sh uffles the file pa ths. In\n",
      "general this is a good thing, but you can set \n",
      "shuffle=False\n",
      "  if you do not wan t tha t,\n",
      "for some reason.\n",
      " N ext, we can call the \n",
      "interleave()\n",
      "  method to read from 5 files a t a time and in terƒ\n",
      " lea ve their lines (skipping the first line of each file, which is the header row , using the\n",
      "skip()\n",
      " method):\n",
      "n_readers\n",
      " \n",
      "=\n",
      " \n",
      "5\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "filepath_dataset\n",
      ".\n",
      "interleave\n",
      "(\n",
      "    \n",
      "lambda\n",
      " \n",
      "filepath\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TextLineDataset\n",
      "(\n",
      "filepath\n",
      ")\n",
      ".\n",
      "skip\n",
      "(\n",
      "1\n",
      "),\n",
      "    \n",
      "cycle_length\n",
      "=\n",
      "n_readers\n",
      ")\n",
      "The \n",
      "interleave()\n",
      "  method will crea te a da taset tha t will pull 5 file pa ths from the\n",
      "filepath_dataset\n",
      " , and for each one it will call the function we ga ve it (a lambda in\n",
      " this exam ple) to crea te a new da taset, in this case a \n",
      "TextLineDataset\n",
      " . I t will then\n",
      " cycle through these 5 da tasets, reading one line a t a time from each un til all da tasets\n",
      " are out of items. Then it will get the next 5 file pa ths from the \n",
      "filepath_dataset\n",
      ", and\n",
      " in terlea ve them the same wa y , and so on un til it runs out of file pa ths.\n",
      " F or in terlea ving to work best, it is preferable to ha ve files of iden tiƒ\n",
      " cal length, or else the end of the longest files will not be in terlea ved.\n",
      " 408  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      " By defa ult, \n",
      "interleave()\n",
      "  does not use parallelism, it just reads one line a t a time\n",
      " from each file, sequen tially . H owever , if you wan t it to actually read files in parallel,\n",
      "you can set the \n",
      "num_parallel_calls\n",
      "  argumen t to the n umber of threads you wan t.\n",
      " Y ou can even set it to \n",
      "tf.data.experimental.AUTOTUNE\n",
      "  to make T ensorFlow choose\n",
      " the righ t n umber of threads dynamically based on the a vailable CPU (however , this is\n",
      " an experimen tal fea ture for now). Let ‡ s look a t wha t the da taset con tains now :\n",
      ">>> \n",
      "for\n",
      " \n",
      "line\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ".\n",
      "take\n",
      "(\n",
      "5\n",
      "):\n",
      "... \n",
      "    \n",
      "print\n",
      "(\n",
      "line\n",
      ".\n",
      "numpy\n",
      "())\n",
      "...\n",
      "b•4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782•\n",
      "b•4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215•\n",
      "b•3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625•\n",
      "b•3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526•\n",
      "b•3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442•\n",
      " These are the first rows (ignoring the header row) of 5 CSV files, chosen randomly .\n",
      "Looks good! But as you can see, these are just byte strings, we need to parse them,\n",
      " and also scale the da ta.\n",
      "Preprocessing the Data\n",
      " Let ‡ s im plemen t a small function tha t will perform this preprocessing:\n",
      "X_mean\n",
      ",\n",
      " \n",
      "X_std\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# mean and scale of each feature in the training set\n",
      "n_inputs\n",
      " \n",
      "=\n",
      " \n",
      "8\n",
      "def\n",
      " \n",
      "preprocess\n",
      "(\n",
      "line\n",
      "):\n",
      "  \n",
      "defs\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "0.\n",
      "]\n",
      " \n",
      "*\n",
      " \n",
      "n_inputs\n",
      " \n",
      "+\n",
      " \n",
      "[\n",
      "tf\n",
      ".\n",
      "constant\n",
      "([],\n",
      " \n",
      "dtype\n",
      "=\n",
      "tf\n",
      ".\n",
      "float32\n",
      ")]\n",
      "  \n",
      "fields\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "decode_csv\n",
      "(\n",
      "line\n",
      ",\n",
      " \n",
      "record_defaults\n",
      "=\n",
      "defs\n",
      ")\n",
      "  \n",
      "x\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "stack\n",
      "(\n",
      "fields\n",
      "[:\n",
      "-\n",
      "1\n",
      "])\n",
      "  \n",
      "y\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "stack\n",
      "(\n",
      "fields\n",
      "[\n",
      "-\n",
      "1\n",
      ":])\n",
      "  \n",
      "return\n",
      " \n",
      "(\n",
      "x\n",
      " \n",
      "-\n",
      " \n",
      "X_mean\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "X_std\n",
      ",\n",
      " \n",
      "y\n",
      " Let ‡ s walk through this code:\n",
      "⁄\n",
      " First, we assume tha t you ha ve precom puted the mean and standard devia tion of\n",
      " each fea ture in the training set. \n",
      "X_mean\n",
      "   and \n",
      "X_std\n",
      " \n",
      " are just 1D tensors (or N umPy\n",
      " arra ys) con taining 8 floa ts, one per in put fea ture.\n",
      "⁄\n",
      "The \n",
      "preprocess()\n",
      " \n",
      " function takes one CSV line, and starts by parsing it. F or this,\n",
      "it uses the \n",
      "tf.io.decode_csv()\n",
      "  function, which takes two argumen ts: the first is\n",
      " the line to parse, and the second is an arra y con taining the defa ult value for each\n",
      " column in the CSV file. This tells T ensorFlow not only the defa ult value for each\n",
      " column, but also the n umber of columns and the type of each column. In this\n",
      " exam ple, we tell it tha t all fea ture columns are floa ts and missing values should\n",
      " defa ult to 0, but we provide an em pty arra y of type \n",
      "tf.float32\n",
      " \n",
      " as the defa ult\n",
      " value for the last column (the target): this tells T ensorFlow tha t this column conƒ\n",
      " The Data API  |  409\n",
      "\n",
      " tains floa ts, but tha t there is no defa ult value, so it will raise an exception if it\n",
      " encoun ters a missing value.\n",
      "⁄\n",
      "The \n",
      "decode_csv()\n",
      " function returns a list of scalar tensors (one per column) but\n",
      " we need to return 1D tensor arra ys. So we call \n",
      "tf.stack()\n",
      " \n",
      "on all tensors except\n",
      " for the last one (the target): this will stack these tensors in to a 1D arra y . W e then\n",
      " do the same for the target value (this makes it a 1D tensor arra y with a single\n",
      " value, ra ther than a scalar tensor).\n",
      "⁄\n",
      " Finally , we scale the in put fea tures by subtracting the fea ture means and then\n",
      " dividing by the fea ture standard devia tions, and we return a tuple con taining the\n",
      " scaled fea tures and the target.\n",
      " Let ‡ s test this preprocessing function:\n",
      ">>> \n",
      "preprocess\n",
      "(\n",
      "b\n",
      "•4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782•\n",
      ")\n",
      "(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\n",
      " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
      "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
      " <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\n",
      " W e can now a pply this preprocessing function to the da taset.\n",
      "Putting Everything Together\n",
      " T o make the code reusable, let ‡ s put together ever ything we ha ve discussed so far in to\n",
      " a small helper function: it will crea te and return a da taset tha t will efficien tly load Calƒ\n",
      " ifornia housing da ta from m ultiple CSV files, then sh uffle it, preprocess it and ba tch it\n",
      "(see \n",
      "Figure 13-2\n",
      "):\n",
      "def\n",
      " \n",
      "csv_reader_dataset\n",
      "(\n",
      "filepaths\n",
      ",\n",
      " \n",
      "repeat\n",
      "=\n",
      "None\n",
      ",\n",
      " \n",
      "n_readers\n",
      "=\n",
      "5\n",
      ",\n",
      "                       \n",
      "n_read_threads\n",
      "=\n",
      "None\n",
      ",\n",
      " \n",
      "shuffle_buffer_size\n",
      "=\n",
      "10000\n",
      ",\n",
      "                       \n",
      "n_parse_threads\n",
      "=\n",
      "5\n",
      ",\n",
      " \n",
      "batch_size\n",
      "=\n",
      "32\n",
      "):\n",
      "    \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "Dataset\n",
      ".\n",
      "list_files\n",
      "(\n",
      "filepaths\n",
      ")\n",
      ".\n",
      "repeat\n",
      "(\n",
      "repeat\n",
      ")\n",
      "    \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "interleave\n",
      "(\n",
      "        \n",
      "lambda\n",
      " \n",
      "filepath\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TextLineDataset\n",
      "(\n",
      "filepath\n",
      ")\n",
      ".\n",
      "skip\n",
      "(\n",
      "1\n",
      "),\n",
      "        \n",
      "cycle_length\n",
      "=\n",
      "n_readers\n",
      ",\n",
      " \n",
      "num_parallel_calls\n",
      "=\n",
      "n_read_threads\n",
      ")\n",
      "    \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "shuffle\n",
      "(\n",
      "shuffle_buffer_size\n",
      ")\n",
      "    \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "map\n",
      "(\n",
      "preprocess\n",
      ",\n",
      " \n",
      "num_parallel_calls\n",
      "=\n",
      "n_parse_threads\n",
      ")\n",
      "    \n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "batch\n",
      "(\n",
      "batch_size\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "dataset\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      " 410  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "2\n",
      " In general, just prefetching one ba tch is fine, but in some cases you ma y need to prefetch a few more. Alternaƒ\n",
      " tively , you can let T ensorFlow decide a utoma tically by passing \n",
      "tf.data.experimental.AUTOTUNE\n",
      " (this is an\n",
      " experimen tal fea ture for now).\n",
      " F i g u r e 13-2. L o a d i n g a n d P r e p r o c e s s i n g D a t a F r o m M u l t i p l e CSV F i l e s\n",
      " E ver ything should make sense in this code, except the ver y last line (\n",
      "prefetch(1)\n",
      "),\n",
      " which is actually quite im portan t for performance.\n",
      "Prefetching\n",
      "By calling \n",
      "prefetch(1)\n",
      "  a t the end, we are crea ting a da taset tha t will do its best to\n",
      " alwa ys be one ba tch ahead\n",
      "2\n",
      ". In other words, while our training algorithm is working\n",
      " on one ba tch, the da taset will already be working in parallel on getting the next ba tch\n",
      " ready . This can im prove performance drama tically , as is illustra ted on \n",
      "Figure 13-3\n",
      ". If\n",
      " we also ensure tha t loading and preprocessing are m ultithreaded (by setting \n",
      "num_par\n",
      "allel_calls\n",
      " when calling \n",
      "interleave()\n",
      " and \n",
      "map()\n",
      " ), we can exploit m ultiple cores\n",
      " on the CPU and hopefully make preparing one ba tch of da ta shorter than running a\n",
      " training step on the GPU: this wa y the GPU will be almost 100% utilized (except for\n",
      " the da ta transfer time from the CPU to the GPU), and training will run m uch faster .\n",
      " The Data API  |  411\n",
      "\n",
      " F i g u r e 13-3. S p e e d u p T r a i n i n g \n",
      "•anks\n",
      "  t o P r ef e t c h i n g a n d M u l t i t h r e a d i n g\n",
      "If you plan to purchase a GPU card, its processing power and its\n",
      " memor y size are of course ver y im portan t (in particular , a large\n",
      " RAM is crucial for com puter vision), but its \n",
      " m em o r y b a n d w i d t h\n",
      " \n",
      "is\n",
      " just as im portan t as the processing power to get good performance:\n",
      " this is the n umber of gigabytes of da ta it can get in or out of its\n",
      "RAM per second.\n",
      " W ith tha t, you can now build efficien t in put pipelines to load and preprocess da ta\n",
      " from m ultiple text files. W e ha ve discussed the most common da taset methods, but\n",
      " there are a few more you ma y wan t to look a t: \n",
      "concatenate()\n",
      ", \n",
      "zip()\n",
      ", \n",
      "window()\n",
      ",\n",
      "reduce()\n",
      ", \n",
      "cache()\n",
      ", \n",
      "shard()\n",
      ", \n",
      "flat_map()\n",
      "   and \n",
      "padded_batch()\n",
      ". There are also a couƒ\n",
      "ple more class methods: \n",
      "from_generator()\n",
      "   and \n",
      "from_tensors()\n",
      " , which crea te a new\n",
      " da taset from a Python genera tor or a list of tensors respectively . Please check the API\n",
      " documen ta tion for more details. Also note tha t there are experimen tal fea tures a vailƒ\n",
      "able in \n",
      "tf.data.experimental\n",
      " , man y of which will most likely make it to the core\n",
      "API in future releases (e.g., check out the \n",
      "CsvDataset\n",
      " class and the \n",
      "SqlDataset\n",
      "classes).\n",
      " 412  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "3\n",
      " Support for da tasets is specific to tf.keras, it will not work on other im plemen ta tions of the K eras API.\n",
      "4\n",
      " The n umber of steps per epoch is optional if the da taset just goes through the da ta once, but if you do not\n",
      " specif y it, the progress bar will not be displa yed during the first epoch.\n",
      "5\n",
      " N ote tha t for now the da taset m ust be crea ted within the TF F unction. This ma y be fixed by the time you read\n",
      " these lines (see T ensorFlow issue #25414).\n",
      "Using the Dataset With tf.keras\n",
      " N ow we can use the \n",
      "csv_reader_dataset()\n",
      " \n",
      " function to crea te a da taset for the trainƒ\n",
      " ing set (ensuring it repea ts the da ta forever), the valida tion set and the test set:\n",
      "train_set\n",
      " \n",
      "=\n",
      " \n",
      "csv_reader_dataset\n",
      "(\n",
      "train_filepaths\n",
      ",\n",
      " \n",
      "repeat\n",
      "=\n",
      "None\n",
      ")\n",
      "valid_set\n",
      " \n",
      "=\n",
      " \n",
      "csv_reader_dataset\n",
      "(\n",
      "valid_filepaths\n",
      ")\n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "csv_reader_dataset\n",
      "(\n",
      "test_filepaths\n",
      ")\n",
      " And now we can sim ply build and train a K eras model using these da tasets.\n",
      "3\n",
      " \n",
      "All we\n",
      "need to do is to call the \n",
      "fit()\n",
      "  method with the da tasets instead of \n",
      "X_train\n",
      " \n",
      "and\n",
      "y_train\n",
      " , and specif y the n umber of steps per epoch for each set:\n",
      "4\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "...\n",
      "])\n",
      "model\n",
      ".\n",
      "compile\n",
      "([\n",
      "...\n",
      "])\n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "train_set\n",
      ",\n",
      " \n",
      "steps_per_epoch\n",
      "=\n",
      "len\n",
      "(\n",
      "X_train\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "batch_size\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "10\n",
      ",\n",
      "          \n",
      "validation_data\n",
      "=\n",
      "valid_set\n",
      ",\n",
      "          \n",
      "validation_steps\n",
      "=\n",
      "len\n",
      "(\n",
      "X_valid\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "batch_size\n",
      ")\n",
      " Similarly , we can pass a da taset to the \n",
      "evaluate()\n",
      " and \n",
      "predict()\n",
      "   methods (and again\n",
      " specif y the n umber of steps per epoch):\n",
      "model\n",
      ".\n",
      "evaluate\n",
      "(\n",
      "test_set\n",
      ",\n",
      " \n",
      "steps\n",
      "=\n",
      "len\n",
      "(\n",
      "X_test\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "batch_size\n",
      ")\n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "new_set\n",
      ",\n",
      " \n",
      "steps\n",
      "=\n",
      "len\n",
      "(\n",
      "X_new\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "batch_size\n",
      ")\n",
      " U nlike the other sets, the \n",
      "new_set\n",
      " \n",
      " will usually not con tain labels (if it does, K eras will\n",
      " just ignore them). N ote tha t in all these cases, you can still use N umPy arra ys instead\n",
      " of da tasets if you wan t (but of course they need to ha ve been loaded and preprocessed\n",
      "first).\n",
      " If you wan t to build your own custom training loop (as in \n",
      " Cha pter 12\n",
      "), you can just\n",
      " itera te over the training set, ver y na turally :\n",
      "for\n",
      " \n",
      "X_batch\n",
      ",\n",
      " \n",
      "y_batch\n",
      " \n",
      "in\n",
      " \n",
      "train_set\n",
      ":\n",
      "    \n",
      "[\n",
      "...\n",
      "]\n",
      " \n",
      "# perform one gradient descent step\n",
      " In fact, it is even possible to crea te a tf.function (see \n",
      " Cha pter 12\n",
      " ) tha t performs the\n",
      "whole training loop!\n",
      "5\n",
      "@tf.function\n",
      "def\n",
      " \n",
      "train\n",
      "(\n",
      "model\n",
      ",\n",
      " \n",
      "optimizer\n",
      ",\n",
      " \n",
      "loss_fn\n",
      ",\n",
      " \n",
      "n_epochs\n",
      ",\n",
      " \n",
      "[\n",
      "...\n",
      "]):\n",
      "    \n",
      "train_set\n",
      " \n",
      "=\n",
      " \n",
      "csv_reader_dataset\n",
      "(\n",
      "train_filepaths\n",
      ",\n",
      " \n",
      "repeat\n",
      "=\n",
      "n_epochs\n",
      ",\n",
      " \n",
      "[\n",
      "...\n",
      "])\n",
      "    \n",
      "for\n",
      " \n",
      "X_batch\n",
      ",\n",
      " \n",
      "y_batch\n",
      " \n",
      "in\n",
      " \n",
      "train_set\n",
      ":\n",
      "        \n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "GradientTape\n",
      "()\n",
      " \n",
      "as\n",
      " \n",
      "tape\n",
      ":\n",
      " The Data API  |  413\n",
      "\n",
      "            \n",
      "y_pred\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      "(\n",
      "X_batch\n",
      ")\n",
      "            \n",
      "main_loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "reduce_mean\n",
      "(\n",
      "loss_fn\n",
      "(\n",
      "y_batch\n",
      ",\n",
      " \n",
      "y_pred\n",
      "))\n",
      "            \n",
      "loss\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "add_n\n",
      "([\n",
      "main_loss\n",
      "]\n",
      " \n",
      "+\n",
      " \n",
      "model\n",
      ".\n",
      "losses\n",
      ")\n",
      "        \n",
      "grads\n",
      " \n",
      "=\n",
      " \n",
      "tape\n",
      ".\n",
      "gradient\n",
      "(\n",
      "loss\n",
      ",\n",
      " \n",
      "model\n",
      ".\n",
      "trainable_variables\n",
      ")\n",
      "        \n",
      "optimizer\n",
      ".\n",
      "apply_gradients\n",
      "(\n",
      "zip\n",
      "(\n",
      "grads\n",
      ",\n",
      " \n",
      "model\n",
      ".\n",
      "trainable_variables\n",
      "))\n",
      " Congra tula tions, you now know how to build powerful in put pipelines using the Da ta\n",
      " API! H owever , so far we ha ve used CSV files, which are common, sim ple and con veƒ\n",
      " nien t, but they are not really efficien t, and they do not support large or com plex da ta\n",
      " structures ver y well, such as images or a udio . So let ‡ s use TFRecords instead.\n",
      " If you are ha ppy with CSV files (or wha tever other forma t you are\n",
      "using), you do not \n",
      " h a v e\n",
      "  to use TFRecords. As the sa ying goes, if it\n",
      " ain ‡ t broke, don ‡ t fix it! TFRecords are useful when the bottleneck\n",
      " during training is loading and parsing the da ta.\n",
      "The TFRecord Format\n",
      " The TFRecord forma t is T ensorFlow‡ s preferred forma t for storing large amoun ts of\n",
      " da ta and reading it efficien tly . I t is a ver y sim ple binar y forma t tha t just con tains a\n",
      " sequence of binar y records of var ying sizes (each record just has a length, a CRC\n",
      " checksum to check tha t the length was not corrupted, then the actual da ta, and finally\n",
      " a CRC checksum for the da ta). Y ou can easily crea te a TFRecord file using the\n",
      "tf.io.TFRecordWriter\n",
      " class:\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "TFRecordWriter\n",
      "(\n",
      "\"my_data.tfrecord\"\n",
      ")\n",
      " \n",
      "as\n",
      " \n",
      "f\n",
      ":\n",
      "    \n",
      "f\n",
      ".\n",
      "write\n",
      "(\n",
      "b\n",
      "\"This is the first record\"\n",
      ")\n",
      "    \n",
      "f\n",
      ".\n",
      "write\n",
      "(\n",
      "b\n",
      "\"And this is the second record\"\n",
      ")\n",
      "And you can then use a \n",
      "tf.data.TFRecordDataset\n",
      " to read one or more TFRecord\n",
      "files:\n",
      "filepaths\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "\"my_data.tfrecord\"\n",
      "]\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TFRecordDataset\n",
      "(\n",
      "filepaths\n",
      ")\n",
      "for\n",
      " \n",
      "item\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ":\n",
      "    \n",
      "print\n",
      "(\n",
      "item\n",
      ")\n",
      "This will output:\n",
      "tf.Tensor(b•This is the first record•, shape=(), dtype=string)\n",
      "tf.Tensor(b•And this is the second record•, shape=(), dtype=string)\n",
      " By defa ult, a \n",
      "TFRecordDataset\n",
      " will read files one by one, but you\n",
      " can make it read m ultiple files in parallel and in terlea ve their\n",
      "records by setting \n",
      "num_parallel_reads\n",
      " . Alterna tively , you could\n",
      "obtain the same result by using \n",
      "list_files()\n",
      " and \n",
      "interleave()\n",
      " as we did earlier to read m ultiple CSV files.\n",
      " 414  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "6\n",
      " Since protobuf objects are mean t to be serialized and transmitted, they are called \n",
      " m e s s a ge s\n",
      ".\n",
      "Compressed TFRecord Files\n",
      " I t can sometimes be useful to com press your TFRecord files, especially if they need to\n",
      " be loaded via a network connection. Y ou can crea te a com pressed TFRecord file by\n",
      "setting the \n",
      "options\n",
      "  argumen t:\n",
      "options\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "TFRecordOptions\n",
      "(\n",
      "compression_type\n",
      "=\n",
      "\"GZIP\"\n",
      ")\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "TFRecordWriter\n",
      "(\n",
      "\"my_compressed.tfrecord\"\n",
      ",\n",
      " \n",
      "options\n",
      ")\n",
      " \n",
      "as\n",
      " \n",
      "f\n",
      ":\n",
      "  \n",
      "[\n",
      "...\n",
      "]\n",
      " When reading a com pressed TFRecord file, you need to specif y the com pression type:\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TFRecordDataset\n",
      "([\n",
      "\"my_compressed.tfrecord\"\n",
      "],\n",
      "                                  \n",
      "compression_type\n",
      "=\n",
      "\"GZIP\"\n",
      ")\n",
      "A Brief Introduction to Protocol \n",
      "Bu…ers\n",
      " E ven though each record can use an y binar y forma t you wan t, TFRecord files usually\n",
      " con tain serialized Protocol Buffers (also called \n",
      " p r o t o b u f s\n",
      "). This is a portable, extensiƒ\n",
      " ble and efficien t binar y forma t developed a t Google back in 2001 and Open Sourced\n",
      "in 2008, and they are now widely used, in particular in \n",
      "gRPC\n",
      " , Google ‡ s remote proceƒ\n",
      " dure call system. Protocol Buffers are defined using a sim ple language tha t looks like\n",
      "this:\n",
      "syntax\n",
      " \n",
      "=\n",
      " \n",
      "\"proto3\"\n",
      ";\n",
      "message\n",
      " \n",
      "Person\n",
      " \n",
      "{\n",
      "  \n",
      "string\n",
      " \n",
      "name\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      "  \n",
      "int32\n",
      " \n",
      "id\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      ";\n",
      "  \n",
      "repeated\n",
      " \n",
      "string\n",
      " \n",
      "email\n",
      " \n",
      "=\n",
      " \n",
      "3\n",
      ";\n",
      "}\n",
      " This definition sa ys we are using the protobuf forma t version 3, and it specifies tha t\n",
      "each \n",
      "Person\n",
      "   object\n",
      "6\n",
      " \n",
      " ma y (optionally) ha ve a \n",
      "name\n",
      "   of type \n",
      "string\n",
      ", an \n",
      "id\n",
      "   of type \n",
      "int32\n",
      ",\n",
      "and zero or more \n",
      "email\n",
      " fields, each of type \n",
      "string\n",
      " . The n umbers 1, 2 and 3 are the\n",
      " field iden tifiers: they will be used in each record ‡ s binar y represen ta tion. Once you\n",
      " ha ve a definition in a \n",
      ".proto\n",
      " \n",
      " file, you can com pile it. This requires \n",
      "protoc\n",
      ", the protoƒ\n",
      " buf com piler , to genera te access classes in Python (or some other language). N ote tha t\n",
      " the protobuf definitions we will use ha ve already been com piled for you, and their\n",
      " Python classes are part of T ensorFlow , so you will not need to use \n",
      "protoc\n",
      ". All you\n",
      " need to know is how to use protobuf access classes in Python. T o illustra te the basics,\n",
      " let ‡ s look a t a sim ple exam ple tha t uses the access classes genera ted for the \n",
      "Person\n",
      " protobuf (the code is explained in the commen ts):\n",
      ">>> \n",
      "from\n",
      " \n",
      "person_pb2\n",
      " \n",
      "import\n",
      " \n",
      "Person\n",
      "  \n",
      "# import the generated access class\n",
      ">>> \n",
      "person\n",
      " \n",
      "=\n",
      " \n",
      "Person\n",
      "(\n",
      "name\n",
      "=\n",
      "\"Al\"\n",
      ",\n",
      " \n",
      "id\n",
      "=\n",
      "123\n",
      ",\n",
      " \n",
      "email\n",
      "=\n",
      "[\n",
      "\"a@b.com\"\n",
      "])\n",
      "  \n",
      "# create a Person\n",
      ">>> \n",
      "print\n",
      "(\n",
      "person\n",
      ")\n",
      "  \n",
      "# display the Person\n",
      " The TFRecord Format  |  415\n",
      "\n",
      "7\n",
      " This cha pter con tains the bare minim um you need to know about protobufs to use TFRecords. T o learn more\n",
      "about protobufs, please visit \n",
      " h ttp s://h o m l.i n f o/p r o t o b u f\n",
      ".\n",
      "name: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      ">>> \n",
      "person\n",
      ".\n",
      "name\n",
      "  \n",
      "# read a field\n",
      "\"Al\"\n",
      ">>> \n",
      "person\n",
      ".\n",
      "name\n",
      " \n",
      "=\n",
      " \n",
      "\"Alice\"\n",
      "  \n",
      "# modify a field\n",
      ">>> \n",
      "person\n",
      ".\n",
      "email\n",
      "[\n",
      "0\n",
      "]\n",
      "  \n",
      "# repeated fields can be accessed like arrays\n",
      "\"a@b.com\"\n",
      ">>> \n",
      "person\n",
      ".\n",
      "email\n",
      ".\n",
      "append\n",
      "(\n",
      "\"c@d.com\"\n",
      ")\n",
      "  \n",
      "# add an email address\n",
      ">>> \n",
      "s\n",
      " \n",
      "=\n",
      " \n",
      "person\n",
      ".\n",
      "SerializeToString\n",
      "()\n",
      "  \n",
      "# serialize the object to a byte string\n",
      ">>> \n",
      "s\n",
      "b•\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com•\n",
      ">>> \n",
      "person2\n",
      " \n",
      "=\n",
      " \n",
      "Person\n",
      "()\n",
      "  \n",
      "# create a new Person\n",
      ">>> \n",
      "person2\n",
      ".\n",
      "ParseFromString\n",
      "(\n",
      "s\n",
      ")\n",
      "  \n",
      "# parse the byte string (27 bytes long)\n",
      "27\n",
      ">>> \n",
      "person\n",
      " \n",
      "==\n",
      " \n",
      "person2\n",
      "  \n",
      "# now they are equal\n",
      "True\n",
      " In short, we im port the \n",
      "Person\n",
      "  class genera ted by \n",
      "protoc\n",
      " , we crea te an instance and\n",
      " we pla y with it, visualizing it, reading and writing some fields, then we serialize it\n",
      "using the \n",
      "SerializeToString()\n",
      "  method. This is the binar y da ta tha t is ready to be\n",
      " sa ved or transmitted over the network. When reading or receiving this binar y da ta,\n",
      "we can parse it using the \n",
      "ParseFromString()\n",
      " \n",
      "method, and we get a copy of the object\n",
      " tha t was serialized.\n",
      "7\n",
      " W e could sa ve the serialized \n",
      "Person\n",
      " \n",
      "object to a TFRecord file, then we could load and\n",
      " parse it: ever ything would work fine. H owever , \n",
      "SerializeToString()\n",
      "   and \n",
      "ParseFrom\n",
      "String()\n",
      " \n",
      " are not T ensorFlow opera tions (and neither are the other opera tions in this\n",
      " code), so they cannot be included in a T ensorFlow F unction (except by wra pping\n",
      "them in a \n",
      "tf.py_function()\n",
      "  opera tion, which would make the code slower and less\n",
      " portable, as we sa w in \n",
      " Cha pter 12\n",
      " ). F ortuna tely , T ensorFlow does include special proƒ\n",
      " tobuf definitions for which it provides parsing opera tions.\n",
      "TensorFlow Protobufs\n",
      "The main protobuf typically used in a TFRecord file is the \n",
      "Example\n",
      " \n",
      "protobuf, which\n",
      " represen ts one instance in a da taset. I t con tains a list of named fea tures, where each\n",
      " fea ture can either be a list of byte strings, a list of floa ts or a list of in tegers. H ere is the\n",
      "protobuf definition:\n",
      "syntax\n",
      " \n",
      "=\n",
      " \n",
      "\"proto3\"\n",
      ";\n",
      "message\n",
      " \n",
      "BytesList\n",
      " \n",
      "{\n",
      " \n",
      "repeated\n",
      " \n",
      "bytes\n",
      " \n",
      "value\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      " \n",
      "}\n",
      "message\n",
      " \n",
      "FloatList\n",
      " \n",
      "{\n",
      " \n",
      "repeated\n",
      " \n",
      "float\n",
      " \n",
      "value\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      " \n",
      "[\n",
      "packed\n",
      " \n",
      "=\n",
      " \n",
      "true\n",
      "];\n",
      " \n",
      "}\n",
      "message\n",
      " \n",
      "Int64List\n",
      " \n",
      "{\n",
      " \n",
      "repeated\n",
      " \n",
      "int64\n",
      " \n",
      "value\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      " \n",
      "[\n",
      "packed\n",
      " \n",
      "=\n",
      " \n",
      "true\n",
      "];\n",
      " \n",
      "}\n",
      " 416  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "8\n",
      " Wh y was \n",
      "Example\n",
      "  even defined since it con tains no more than a \n",
      "Features\n",
      "  object? W ell, T ensorFlow ma y one\n",
      " da y decide to add more fields to it. As long as the new \n",
      "Example\n",
      "  definition still con tains the \n",
      "features\n",
      " field,\n",
      " with the same id, it will be backward com pa tible. This extensibility is one of the grea t fea tures of protobufs.\n",
      "message\n",
      " \n",
      "Feature\n",
      " \n",
      "{\n",
      "    \n",
      "oneof\n",
      " \n",
      "kind\n",
      " \n",
      "{\n",
      "        \n",
      "BytesList\n",
      " \n",
      "bytes_list\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      "        \n",
      "FloatList\n",
      " \n",
      "float_list\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      ";\n",
      "        \n",
      "Int64List\n",
      " \n",
      "int64_list\n",
      " \n",
      "=\n",
      " \n",
      "3\n",
      ";\n",
      "    \n",
      "}\n",
      "};\n",
      "message\n",
      " \n",
      "Features\n",
      " \n",
      "{\n",
      " \n",
      "map\n",
      "<\n",
      "string\n",
      ",\n",
      " \n",
      "Feature\n",
      ">\n",
      " \n",
      "feature\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      " \n",
      "};\n",
      "message\n",
      " \n",
      "Example\n",
      " \n",
      "{\n",
      " \n",
      "Features\n",
      " \n",
      "features\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      " \n",
      "};\n",
      "The definitions of \n",
      "BytesList\n",
      ", \n",
      "FloatList\n",
      " \n",
      "and \n",
      "Int64List\n",
      "  are straigh tfor ward enough\n",
      "(\n",
      "[packed = true]\n",
      "  is used for repea ted n umerical fields, for a more efficien t encodƒ\n",
      "ing). A \n",
      "Feature\n",
      "  either con tains a \n",
      "BytesList\n",
      ", a \n",
      "FloatList\n",
      " or an \n",
      "Int64List\n",
      ". A \n",
      "Fea\n",
      "tures\n",
      "  (with an s) con tains a dictionar y tha t ma ps a fea ture name to the\n",
      " corresponding fea ture value. And finally , an \n",
      "Example\n",
      "   just con tains a \n",
      "Features\n",
      "   object.\n",
      "8\n",
      " H ere is how you could crea te a \n",
      "tf.train.Example\n",
      "  represen ting the same person as\n",
      " earlier , and write it to TFRecord file:\n",
      "from\n",
      " \n",
      "tensorflow.train\n",
      " \n",
      "import\n",
      " \n",
      "BytesList\n",
      ",\n",
      " \n",
      "FloatList\n",
      ",\n",
      " \n",
      "Int64List\n",
      "from\n",
      " \n",
      "tensorflow.train\n",
      " \n",
      "import\n",
      " \n",
      "Feature\n",
      ",\n",
      " \n",
      "Features\n",
      ",\n",
      " \n",
      "Example\n",
      "person_example\n",
      " \n",
      "=\n",
      " \n",
      "Example\n",
      "(\n",
      "    \n",
      "features\n",
      "=\n",
      "Features\n",
      "(\n",
      "        \n",
      "feature\n",
      "=\n",
      "{\n",
      "            \n",
      "\"name\"\n",
      ":\n",
      " \n",
      "Feature\n",
      "(\n",
      "bytes_list\n",
      "=\n",
      "BytesList\n",
      "(\n",
      "value\n",
      "=\n",
      "[\n",
      "b\n",
      "\"Alice\"\n",
      "])),\n",
      "            \n",
      "\"id\"\n",
      ":\n",
      " \n",
      "Feature\n",
      "(\n",
      "int64_list\n",
      "=\n",
      "Int64List\n",
      "(\n",
      "value\n",
      "=\n",
      "[\n",
      "123\n",
      "])),\n",
      "            \n",
      "\"emails\"\n",
      ":\n",
      " \n",
      "Feature\n",
      "(\n",
      "bytes_list\n",
      "=\n",
      "BytesList\n",
      "(\n",
      "value\n",
      "=\n",
      "[\n",
      "b\n",
      "\"a@b.com\"\n",
      ",\n",
      "                                                          b\n",
      "\"c@d.com\"\n",
      "]))\n",
      "        \n",
      "}))\n",
      " The code is a bit verbose and repetitive, but it ‡ s ra ther straigh tfor ward (and you could\n",
      " easily wra p it inside a small helper function). N ow tha t we ha ve an \n",
      "Example\n",
      "   protobuf,\n",
      "we can serialize it by calling its \n",
      "SerializeToString()\n",
      " method, then write the resultƒ\n",
      " ing da ta to a TFRecord file:\n",
      "with\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "TFRecordWriter\n",
      "(\n",
      "\"my_contacts.tfrecord\"\n",
      ")\n",
      " \n",
      "as\n",
      " \n",
      "f\n",
      ":\n",
      "    \n",
      "f\n",
      ".\n",
      "write\n",
      "(\n",
      "person_example\n",
      ".\n",
      "SerializeToString\n",
      "())\n",
      " N ormally you would write m uch more than just one exam ple! T ypically , you would\n",
      " crea te a con version script tha t reads from your curren t forma t (sa y , CSV files), crea tes\n",
      "an \n",
      "Example\n",
      "  protobuf for each instance, serializes them and sa ves them to several\n",
      " TFRecord files, ideally sh uffling them in the process. This requires a bit of work, so\n",
      " once again make sure it is really necessar y (perha ps your pipeline works fine with\n",
      "CSV files).\n",
      " The TFRecord Format  |  417\n",
      "\n",
      " N ow tha t we ha ve a nice TFRecord file con taining a serialized \n",
      "Example\n",
      " , let ‡ s tr y to\n",
      "load it.\n",
      "Loading and Parsing Examples\n",
      " T o load the serialized \n",
      "Example\n",
      " protobufs, we will use a \n",
      "tf.data.TFRecordDataset\n",
      "once again, and we will parse each \n",
      "Example\n",
      " \n",
      "using \n",
      "tf.io.parse_single_example()\n",
      ".\n",
      " This is a T ensorFlow opera tion so it can be included in a TF F unction. I t requires a t\n",
      " least two argumen ts: a string scalar tensor con taining the serialized da ta, and a\n",
      " description of each fea ture. The description is a dictionar y tha t ma ps each fea ture\n",
      "name to either a \n",
      "tf.io.FixedLenFeature\n",
      "  descriptor indica ting the fea ture ‡ s sha pe,\n",
      " type and defa ult value, or a \n",
      "tf.io.VarLenFeature\n",
      " \n",
      " descriptor indica ting only the type\n",
      " (if the length ma y var y , such as for the \n",
      "\"emails\"\n",
      "  fea ture). F or exam ple:\n",
      "feature_description\n",
      " \n",
      "=\n",
      " \n",
      "{\n",
      "    \n",
      "\"name\"\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "FixedLenFeature\n",
      "([],\n",
      " \n",
      "tf\n",
      ".\n",
      "string\n",
      ",\n",
      " \n",
      "default_value\n",
      "=\n",
      "\"\"\n",
      "),\n",
      "    \n",
      "\"id\"\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "FixedLenFeature\n",
      "([],\n",
      " \n",
      "tf\n",
      ".\n",
      "int64\n",
      ",\n",
      " \n",
      "default_value\n",
      "=\n",
      "0\n",
      "),\n",
      "    \n",
      "\"emails\"\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "VarLenFeature\n",
      "(\n",
      "tf\n",
      ".\n",
      "string\n",
      "),\n",
      "}\n",
      "for\n",
      " \n",
      "serialized_example\n",
      " \n",
      "in\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TFRecordDataset\n",
      "([\n",
      "\"my_contacts.tfrecord\"\n",
      "]):\n",
      "    \n",
      "parsed_example\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "parse_single_example\n",
      "(\n",
      "serialized_example\n",
      ",\n",
      "                                                \n",
      "feature_description\n",
      ")\n",
      " The fixed length fea tures are parsed as regular tensors, but the variable length feaƒ\n",
      " tures are parsed as sparse tensors. Y ou can con vert a sparse tensor to a dense tensor\n",
      "using \n",
      "tf.sparse.to_dense()\n",
      " , but in this case it is sim pler to just access its values:\n",
      ">>> \n",
      "tf\n",
      ".\n",
      "sparse\n",
      ".\n",
      "to_dense\n",
      "(\n",
      "parsed_example\n",
      "[\n",
      "\"emails\"\n",
      "],\n",
      " \n",
      "default_value\n",
      "=\n",
      "b\n",
      "\"\"\n",
      ")\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b•a@b.com•, b•c@d.com•], [...])>\n",
      ">>> \n",
      "parsed_example\n",
      "[\n",
      "\"emails\"\n",
      "]\n",
      ".\n",
      "values\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b•a@b.com•, b•c@d.com•], [...])>\n",
      "A \n",
      "BytesList\n",
      "  can con tain an y binar y da ta you wan t, including an y serialized object.\n",
      " F or exam ple, you can use \n",
      "tf.io.encode_jpeg()\n",
      " to encode an image using the JPEG\n",
      " forma t, and put this binar y da ta in a \n",
      "BytesList\n",
      " . La ter , when your code reads the\n",
      "TFRecord, it will start by parsing the \n",
      "Example\n",
      ", then you will need to call\n",
      "tf.io.decode_jpeg()\n",
      "  to parse the da ta and get the original image (or you can use\n",
      "tf.io.decode_image()\n",
      " , which can decode an y BMP , GIF , JPEG or PNG image). Y ou\n",
      " can also store an y tensor you wan t in a \n",
      "BytesList\n",
      " \n",
      "by serializing the tensor using\n",
      "tf.io.serialize_tensor()\n",
      ", then putting the resulting byte string in a \n",
      "BytesList\n",
      " fea ture. La ter , when you parse the TFRecord, you can parse this da ta using\n",
      "tf.io.parse_tensor()\n",
      ".\n",
      " Instead of parsing exam ples one by one using \n",
      "tf.io.parse_single_example()\n",
      ", you\n",
      " ma y wan t to parse them ba tch by ba tch using \n",
      "tf.io.parse_example()\n",
      ":\n",
      " 418  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TFRecordDataset\n",
      "([\n",
      "\"my_contacts.tfrecord\"\n",
      "])\n",
      ".\n",
      "batch\n",
      "(\n",
      "10\n",
      ")\n",
      "for\n",
      " \n",
      "serialized_examples\n",
      " \n",
      "in\n",
      " \n",
      "dataset\n",
      ":\n",
      "    \n",
      "parsed_examples\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "parse_example\n",
      "(\n",
      "serialized_examples\n",
      ",\n",
      "                                          \n",
      "feature_description\n",
      ")\n",
      "As you can see, the \n",
      "Example\n",
      "  proto will probably be sufficien t for most use cases.\n",
      " H owever , it ma y be a bit cumbersome to use when you are dealing with lists of lists.\n",
      " F or exam ple, suppose you wan t to classif y text documen ts. Each documen t ma y be\n",
      " represen ted as a list of sen tences, where each sen tence is represen ted as a list of\n",
      " words. And perha ps each documen t also has a list of commen ts, where each comƒ\n",
      " men t is also represen ted as a list of words. M oreover , there ma y be some con textual\n",
      " da ta as well, such as the documen t ‡ s a uthor , title and publica tion da te. T ensorFlow‡ s\n",
      "SequenceExample\n",
      " protobuf is designed for such use cases.\n",
      "Handling Lists of Lists Using the \n",
      "SequenceExample\n",
      " Protobuf\n",
      " H ere is the definition of the \n",
      "SequenceExample\n",
      " protobuf:\n",
      "message\n",
      " \n",
      "FeatureList\n",
      " \n",
      "{\n",
      " \n",
      "repeated\n",
      " \n",
      "Feature\n",
      " \n",
      "feature\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      " \n",
      "};\n",
      "message\n",
      " \n",
      "FeatureLists\n",
      " \n",
      "{\n",
      " \n",
      "map\n",
      "<\n",
      "string\n",
      ",\n",
      " \n",
      "FeatureList\n",
      ">\n",
      " \n",
      "feature_list\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      " \n",
      "};\n",
      "message\n",
      " \n",
      "SequenceExample\n",
      " \n",
      "{\n",
      "    \n",
      "Features\n",
      " \n",
      "context\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      ";\n",
      "    \n",
      "FeatureLists\n",
      " \n",
      "feature_lists\n",
      " \n",
      "=\n",
      " \n",
      "2\n",
      ";\n",
      "};\n",
      "A \n",
      "SequenceExample\n",
      "  con tains a \n",
      "Features\n",
      "  object for the con textual da ta and a \n",
      "Fea\n",
      "tureLists\n",
      "  object which con tains one or more named \n",
      "FeatureList\n",
      " \n",
      "objects (e.g., a\n",
      "FeatureList\n",
      " named \n",
      "\"content\"\n",
      " and another named \n",
      "\"comments\"\n",
      "). Each \n",
      "FeatureList\n",
      " just con tains a list of \n",
      "Feature\n",
      " \n",
      " objects, each of which ma y be a list of byte strings, a list\n",
      " of 64-bit in tegers or a list of floa ts (in this exam ple, each \n",
      "Feature\n",
      " \n",
      " would represen t a\n",
      " sen tence or a commen t, perha ps in the form of a list of word iden tifiers). Building a\n",
      "SequenceExample\n",
      " , serializing it and parsing it is ver y similar to building, serializing\n",
      "and parsing an \n",
      "Example\n",
      " , but you m ust use \n",
      "tf.io.parse_single_sequence_exam\n",
      "ple()\n",
      " to parse a single \n",
      "SequenceExample\n",
      " \n",
      "or \n",
      "tf.io.parse_sequence_example()\n",
      " \n",
      "to\n",
      " parse a ba tch, and both functions return a tuple con taining the con text fea tures (as a\n",
      " dictionar y) and the fea ture lists (also as a dictionar y). If the fea ture lists con tain\n",
      " sequences of var ying sizes (as in the exam ple above), you ma y wan t to con vert them\n",
      "to ragged tensors using \n",
      "tf.RaggedTensor.from_sparse()\n",
      " (see the notebook for the\n",
      "full code):\n",
      "parsed_context\n",
      ",\n",
      " \n",
      "parsed_feature_lists\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "parse_single_sequence_example\n",
      "(\n",
      "    \n",
      "serialized_sequence_example\n",
      ",\n",
      " \n",
      "context_feature_descriptions\n",
      ",\n",
      "    \n",
      "sequence_feature_descriptions\n",
      ")\n",
      "parsed_content\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "RaggedTensor\n",
      ".\n",
      "from_sparse\n",
      "(\n",
      "parsed_feature_lists\n",
      "[\n",
      "\"content\"\n",
      "])\n",
      " N ow tha t you know how to efficien tly store, load and parse da ta, the next step is to\n",
      " prepare it so tha t it can be fed to a neural network. This means con verting all fea tures\n",
      " The TFRecord Format  |  419\n",
      "\n",
      " in to n umerical fea tures (ideally not too sparse), scaling them, and more. In particular ,\n",
      " if your da ta con tains ca tegorical fea tures or text fea tures, they need to be con verted to\n",
      " n umbers. F or this, the \n",
      " F e a t u r e s AP I\n",
      "  can help .\n",
      "The Features API\n",
      " Preprocessing your da ta can be performed in man y wa ys: it can be done ahead of\n",
      " time when preparing your da ta files, using an y tool you like. Or you can preprocess\n",
      " your da ta on the fly when loading it with the Da ta API (e.g., using the da taset ‡ s \n",
      "map()\n",
      " method, as we sa w earlier). Or you can include a preprocessing la yer directly in your\n",
      " model. Whichever solution you prefer , the F ea tures API can help you: it is a set of\n",
      " functions a vailable in the \n",
      "tf.feature_column\n",
      " package, which let you define how\n",
      " each fea ture (or group of fea tures) in your da ta should be preprocessed (therefore you\n",
      " can think of this API as the analog of Scikit-Learn ‡ s \n",
      "ColumnTransformer\n",
      "  class). W e\n",
      " will start by looking a t the differen t types of columns a vailable, and then we will look\n",
      " a t how to use them.\n",
      " Let ‡ s go back to the varian t of the California housing da taset tha t we used in \n",
      " Cha pƒ\n",
      "ter 2\n",
      " , since it includes a ca tegorical fea ture and missing da ta. H ere is a sim ple n umeriƒ\n",
      "cal column named \n",
      "\"housing_median_age\"\n",
      ":\n",
      "housing_median_age\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "numeric_column\n",
      "(\n",
      "\"housing_median_age\"\n",
      ")\n",
      " N umeric columns let you specif y a normaliza tion function using the \n",
      "normalizer_fn\n",
      " argumen t. F or exam ple, let ‡ s tweak the \n",
      "\"housing_median_age\"\n",
      " \n",
      "column to define how\n",
      " it should be scaled. N ote tha t this requires com puting ahead of time the mean and\n",
      " standard devia tion of this fea ture in the training set:\n",
      "age_mean\n",
      ",\n",
      " \n",
      "age_std\n",
      " \n",
      "=\n",
      " \n",
      "X_mean\n",
      "[\n",
      "1\n",
      "],\n",
      " \n",
      "X_std\n",
      "[\n",
      "1\n",
      "]\n",
      "  \n",
      "# The median age is column in 1\n",
      "housing_median_age\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "numeric_column\n",
      "(\n",
      "    \n",
      "\"housing_median_age\"\n",
      ",\n",
      " \n",
      "normalizer_fn\n",
      "=\n",
      "lambda\n",
      " \n",
      "x\n",
      ":\n",
      " \n",
      "(\n",
      "x\n",
      " \n",
      "-\n",
      " \n",
      "age_mean\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "age_std\n",
      ")\n",
      " In some cases, it migh t im prove performance to bucketize some n umerical fea tures,\n",
      " effectively transforming a n umerical fea ture in to a ca tegorical fea ture. F or exam ple,\n",
      " let ‡ s crea te a bucketized column based on the \n",
      "median_income\n",
      " \n",
      "column, with 5 buckets:\n",
      " less than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice tha t when\n",
      " you specif y 4 boundaries, there are actually 5 buckets):\n",
      "median_income\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "numeric_column\n",
      "(\n",
      "\"median_income\"\n",
      ")\n",
      "bucketized_income\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "bucketized_column\n",
      "(\n",
      "    \n",
      "median_income\n",
      ",\n",
      " \n",
      "boundaries\n",
      "=\n",
      "[\n",
      "1.5\n",
      ",\n",
      " \n",
      "3.\n",
      ",\n",
      " \n",
      "4.5\n",
      ",\n",
      " \n",
      "6.\n",
      "])\n",
      "If the \n",
      "median_income\n",
      " \n",
      " fea ture is equal to , sa y , 3.2, then the \n",
      "bucketized_income\n",
      "   fea ture\n",
      " will a utoma tically be equal to 2 (i.e., the index of the corresponding income bucket).\n",
      " Choosing the righ t boundaries can be somewha t of an art, but one a pproach is to just\n",
      " use percen tiles of the da ta (e.g., the 10th percen tile, the 20th percen tile, and so on). If\n",
      " a fea ture is \n",
      " m u l t i m o d a l\n",
      " , meaning it has separa te peaks in its distribution, you ma y\n",
      " 420  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      " wan t to define a bucket for each mode, placing the boundaries in between the peaks.\n",
      " Whether you use the percen tiles or the modes, you need to analyze the distribution of\n",
      " your da ta ahead of time, just like we had to measure the mean and standard devia tion\n",
      "ahead of time to normalize the \n",
      "housing_median_age\n",
      " column.\n",
      "Categorical Features\n",
      " F or ca tegorical fea tures such as \n",
      "ocean_proximity\n",
      ", there are several options. If it is\n",
      " already represen ted as a ca tegor y ID (i.e., an in teger from 0 to the max ID), then you\n",
      "can use the \n",
      "categorical_column_with_identity()\n",
      " \n",
      " function (specif ying the max\n",
      " ID). If not, and you know the list of all possible ca tegories, then you can use \n",
      "categori\n",
      "cal_column_with_vocabulary_list()\n",
      ":\n",
      "ocean_prox_vocab\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "•<1H OCEAN•\n",
      ",\n",
      " \n",
      "•INLAND•\n",
      ",\n",
      " \n",
      "•ISLAND•\n",
      ",\n",
      " \n",
      "•NEAR BAY•\n",
      ",\n",
      " \n",
      "•NEAR OCEAN•\n",
      "]\n",
      "ocean_proximity\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "categorical_column_with_vocabulary_list\n",
      "(\n",
      "    \n",
      "\"ocean_proximity\"\n",
      ",\n",
      " \n",
      "ocean_prox_vocab\n",
      ")\n",
      " If you prefer to ha ve T ensorFlow load the vocabular y from a file, you can call \n",
      "catego\n",
      "rical_column_with_vocabulary_file()\n",
      "  instead. As you migh t expect, these two\n",
      " functions will sim ply ma p each ca tegor y to its index in the vocabular y (e.g., \n",
      "NEAR\n",
      " B AY\n",
      "  will be ma pped to 3), and unknown ca tegories will be ma pped to -1.\n",
      " F or ca tegorical columns with a large vocabular y (e.g., for zipcodes, cities, words,\n",
      " products, users, etc.), it ma y not be con venien t to get the full list of possible ca teƒ\n",
      " gories, or perha ps ca tegories ma y be added or removed so frequen tly tha t using ca teƒ\n",
      " gor y indices would be too unreliable. In this case, you ma y prefer to use a\n",
      "categorical_column_with_hash_bucket()\n",
      ". If we had a \n",
      "\"city\"\n",
      "   fea ture in the da taset,\n",
      "we could encode it like this:\n",
      "city_hash\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "categorical_column_with_hash_bucket\n",
      "(\n",
      "    \n",
      "\"city\"\n",
      ",\n",
      " \n",
      "hash_bucket_size\n",
      "=\n",
      "1000\n",
      ")\n",
      " This fea ture will com pute a hash for each ca tegor y (i.e., for each city), modulo the\n",
      " n umber of hash buckets (\n",
      "hash_bucket_size\n",
      " ). Y ou m ust set the n umber of buckets\n",
      " high enough to a void getting too man y collisions (i.e., differen t ca tegories ending up\n",
      "in the same bucket), but the higher you set it, the more RAM will be used (by the\n",
      "embedding table, as we will see shortly).\n",
      "Crossed Categorical Features\n",
      " If you suspect tha t two (or more) ca tegorical fea tures are more meaningful when used\n",
      " join tly , then you can crea te a \n",
      " cr o s s e d c o l u m n\n",
      " . F or exam ple, suppose people are particƒ\n",
      " ularly fond of old houses inland and new houses near the ocean, then it migh t help to\n",
      " The Features API  |  421\n",
      "\n",
      "9\n",
      "Since the \n",
      "housing_median_age\n",
      "  fea ture was normalized, the boundaries are for normalized ages.\n",
      " crea te a bucketized column for the \n",
      "housing_median_age\n",
      " \n",
      " fea ture\n",
      "9\n",
      ", and cross it with\n",
      "the \n",
      "ocean_proximity\n",
      "  column. The crossed column will com pute a hash of ever y age\n",
      " & ocean proximity combina tion it comes across, modulo the \n",
      "hash_bucket_size\n",
      ", and\n",
      " this will give it the cross ca tegor y ID . Y ou ma y then choose to use only this crossed\n",
      "column in your model, or also include the individual columns.\n",
      "bucketized_age\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "bucketized_column\n",
      "(\n",
      "    \n",
      "housing_median_age\n",
      ",\n",
      " \n",
      "boundaries\n",
      "=\n",
      "[\n",
      "-\n",
      "1.\n",
      ",\n",
      " \n",
      "-\n",
      "0.5\n",
      ",\n",
      " \n",
      "0.\n",
      ",\n",
      " \n",
      "0.5\n",
      ",\n",
      " \n",
      "1.\n",
      "])\n",
      " \n",
      "# age was scaled\n",
      "age_and_ocean_proximity\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "crossed_column\n",
      "(\n",
      "    \n",
      "[\n",
      "bucketized_age\n",
      ",\n",
      " \n",
      "ocean_proximity\n",
      "],\n",
      " \n",
      "hash_bucket_size\n",
      "=\n",
      "100\n",
      ")\n",
      " Another common use case for crossed columns is to cross la titude and longitude in to\n",
      " a single ca tegorical fea ture: you start by bucketizing the la titude and longitude, for\n",
      " exam ple in to 20 buckets each, then you cross these bucketized fea tures in to a \n",
      "loca\n",
      "tion\n",
      "  column. This will crea te a 20„20 grid over California, and each cell in the grid\n",
      " will correspond to one ca tegor y :\n",
      "latitude\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "numeric_column\n",
      "(\n",
      "\"latitude\"\n",
      ")\n",
      "longitude\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "numeric_column\n",
      "(\n",
      "\"longitude\"\n",
      ")\n",
      "bucketized_latitude\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "bucketized_column\n",
      "(\n",
      "    \n",
      "latitude\n",
      ",\n",
      " \n",
      "boundaries\n",
      "=\n",
      "list\n",
      "(\n",
      "np\n",
      ".\n",
      "linspace\n",
      "(\n",
      "32.\n",
      ",\n",
      " \n",
      "42.\n",
      ",\n",
      " \n",
      "20\n",
      " \n",
      "-\n",
      " \n",
      "1\n",
      ")))\n",
      "bucketized_longitude\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "bucketized_column\n",
      "(\n",
      "    \n",
      "longitude\n",
      ",\n",
      " \n",
      "boundaries\n",
      "=\n",
      "list\n",
      "(\n",
      "np\n",
      ".\n",
      "linspace\n",
      "(\n",
      "-\n",
      "125.\n",
      ",\n",
      " \n",
      "-\n",
      "114.\n",
      ",\n",
      " \n",
      "20\n",
      " \n",
      "-\n",
      " \n",
      "1\n",
      ")))\n",
      "location\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "crossed_column\n",
      "(\n",
      "    \n",
      "[\n",
      "bucketized_latitude\n",
      ",\n",
      " \n",
      "bucketized_longitude\n",
      "],\n",
      " \n",
      "hash_bucket_size\n",
      "=\n",
      "1000\n",
      ")\n",
      "Encoding Categorical Features Using One-Hot Vectors\n",
      " N o ma tter which option you choose to build a ca tegorical fea ture (ca tegorical colƒ\n",
      " umns, bucketized columns or crossed columns), it m ust be encoded before you can\n",
      " feed it to a neural network. There are two options to encode a ca tegorical fea ture:\n",
      "one-hot vectors or \n",
      " em b e d d i n g s\n",
      " . F or the first option, sim ply use the \n",
      "indicator_col\n",
      "umn()\n",
      " function:\n",
      "ocean_proximity_one_hot\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "indicator_column\n",
      "(\n",
      "ocean_proximity\n",
      ")\n",
      " A one-hot vector encoding has the size of the vocabular y length, which is fine if there\n",
      " are just a few possible ca tegories, but if the vocabular y is large, you will end up with\n",
      " too man y in puts fed to your neural network: it will ha ve too man y weigh ts to learn\n",
      " and it will probably not perform ver y well. In particular , this will typically be the case\n",
      "when you use hash buckets. In this case, you should probably encode them using\n",
      " em b e d d i n g s\n",
      " instead.\n",
      " 422  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      " As a rule of th umb (but your mileage ma y var y!), if the n umber of\n",
      " ca tegories is lower than 10, then one-hot encoding is generally the\n",
      " wa y to go . If the n umber of ca tegories is grea ter than 50 (which is\n",
      "often the case when you use hash buckets), then embeddings are\n",
      " usually preferable. In between 10 and 50 ca tegories, you ma y wan t\n",
      " to experimen t with both options and see which one works best for\n",
      " your use case. Also , embeddings typically require more training\n",
      " da ta, unless you can reuse pretrained embeddings.\n",
      "Encoding Categorical Features Using Embeddings\n",
      " An embedding is a trainable dense vector tha t represen ts a ca tegor y . By defa ult,\n",
      " embeddings are initialized randomly , so for exam ple the \n",
      "\"NEAR BAY\"\n",
      "  ca tegor y could\n",
      " be represen ted initially by a random vector such as \n",
      "[0.131, 0.890]\n",
      ", while the \n",
      "\"NEAR\n",
      "OCEAN\"\n",
      "  ca tegor y ma y be represen ted by another random vector such as \n",
      "[0.631,\n",
      "0.791]\n",
      " \n",
      " (in this exam ple, we are using 2D embeddings, but the n umber of dimensions\n",
      " is a h yperparameter you can tweak). Since these embeddings are trainable, they will\n",
      " gradually im prove during training, and as they represen t fairly similar ca tegories,\n",
      " Gradien t Descen t will certainly end up pushing them closer together , while it will\n",
      " tend to move them a wa y from the \n",
      "\"INLAND\"\n",
      " \n",
      " ca tegor y‡ s embedding (see \n",
      "Figure 13-4\n",
      ").\n",
      " Indeed, the better the represen ta tion, the easier it will be for the neural network to\n",
      " make accura te predictions, so training tends to make embeddings useful represen taƒ\n",
      " tions of the ca tegories. This is called \n",
      " r e p r e s en t a t i o n l e a r n i n g\n",
      " \n",
      "(we will see other types of\n",
      " represen ta tion learning in \n",
      "???\n",
      ").\n",
      " The Features API  |  423\n",
      "\n",
      "10\n",
      " —Distributed Represen ta tions of W ords and Phrases and their Com positionality– , T . Mikolov et al. (2013).\n",
      " F i g u r e 13-4. E m b e d d i n g s W i l l G r a d u a l l y I m p r o v e D u r i n g T r a i n i n g\n",
      "Word Embeddings\n",
      " N ot only will embeddings generally be useful represen ta tions for the task a t hand, but\n",
      "quite often these same embeddings can be reused successfully for other tasks as well.\n",
      " The most common exam ple of this is \n",
      " w o r d em b e d d i n g s\n",
      "   (i.e., embeddings of individual\n",
      " words): when you are working on a na tural language processing task, you are often\n",
      "better off reusing pretrained word embeddings than training your own. The idea of\n",
      " using vectors to represen t words da tes back to the 1960s, and man y sophistica ted\n",
      " techniques ha ve been used to genera te useful vectors, including using neural netƒ\n",
      " works, but things really took off in 2013, when T omÓÔ Mikolov and other Google\n",
      "researchers published a \n",
      " pa per\n",
      "10\n",
      " \n",
      "describing how to learn word embeddings using deep\n",
      " neural networks, m uch faster than previous a ttem pts. This allowed them to learn\n",
      " embeddings on a ver y large corpus of text: they trained a deep neural network to preƒ\n",
      " dict the words near an y given word. This allowed them to obtain astounding word\n",
      " embeddings. F or exam ple, synon yms had ver y close embeddings, and seman tically\n",
      " rela ted words such as France, Spain, I taly , and so on, ended up clustered together . But\n",
      " it ‡ s not just about proximity : word embeddings were also organized along meaningful\n",
      " axes in the embedding space. H ere is a famous exam ple: if you com pute King − M an\n",
      " + W oman (adding and subtracting the embedding vectors of these words), then the\n",
      " result will be ver y close to the embedding of the word Queen (see \n",
      "Figure 13-5\n",
      "). In\n",
      " other words, the word embeddings encode the concept of gender! Similarly , you can\n",
      " com pute M adrid − Spain + France, and of course the result is close to P aris, which\n",
      " seems to show tha t the notion of ca pital city was also encoded in the embeddings.\n",
      " 424  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      " F i g u r e 13-5. W o r d E m b e d d i n g s\n",
      " Let ‡ s go back to the F ea tures API. H ere is how you could encode the \n",
      "ocean_proxim\n",
      "ity\n",
      "  ca tegories as 2D embeddings:\n",
      "ocean_proximity_embed\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "embedding_column\n",
      "(\n",
      "ocean_proximity\n",
      ",\n",
      "                                                           \n",
      "dimension\n",
      "=\n",
      "2\n",
      ")\n",
      "Each of the five \n",
      "ocean_proximity\n",
      "  ca tegories will now be represen ted as a 2D vector .\n",
      "These vectors are stored in an \n",
      " em b e d d i n g m a t r ix\n",
      "  with one row per ca tegor y , and one\n",
      " column per embedding dimension, so in this exam ple it is a 5„2 ma trix. When an\n",
      " embedding column is given a ca tegor y index as in put (sa y , 3, which corresponds to\n",
      " the ca tegor y \n",
      "\"NEAR BAY\"\n",
      " ), it just performs a lookup in the embedding ma trix and\n",
      " returns the corresponding row (sa y , \n",
      "[0.331, 0.190]\n",
      " ). U nfortuna tely , the embedding\n",
      " ma trix can be quite large, especially when you ha ve a large vocabular y : if this is the\n",
      " case, the model can only learn good represen ta tions for the ca tegories for which it has\n",
      " sufficien t training da ta. T o reduce the size of the embedding ma trix, you can of\n",
      " course tr y lowering the \n",
      "dimension\n",
      "  h yperparameter , but if you reduce this parameter\n",
      " too m uch, the represen ta tions ma y not be as good. Another option is to reduce the\n",
      " vocabular y size (e.g., if you are dealing with text, you can tr y dropping the rare words\n",
      " from the vocabular y , and replace them all with a token like \n",
      "\"<unknown>\"\n",
      "   or \n",
      "\"<UNK>\"\n",
      ").\n",
      " If you are using hash buckets, you can also tr y reducing the \n",
      "hash_bucket_size\n",
      " \n",
      "(but\n",
      " not too m uch, or else you will get collisions).\n",
      " The Features API  |  425\n",
      "\n",
      " If there are no pretrained embeddings tha t you can reuse for the\n",
      " task you are tr ying to tackle, and if you do not ha ve enough trainƒ\n",
      " ing da ta to learn them, then you can tr y to learn them on some\n",
      " a uxiliar y task for which it is easier to obtain plen ty of training da ta.\n",
      " After tha t, you can reuse the trained embeddings for your main\n",
      "task.\n",
      "Using Feature Columns for Parsing\n",
      " Let ‡ s suppose you ha ve crea ted fea ture columns for each of your in put fea tures, as well\n",
      " as for the target. Wha t can you do with them? W ell, for one you can pass them to the\n",
      "make_parse_example_spec()\n",
      "  function to genera te fea ture descriptions (so you don ‡ t\n",
      " ha ve to do it man ually , as we did earlier):\n",
      "columns\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "bucketized_age\n",
      ",\n",
      " \n",
      ".....\n",
      ",\n",
      " \n",
      "median_house_value\n",
      "]\n",
      " \n",
      "# all features + target\n",
      "feature_descriptions\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "feature_column\n",
      ".\n",
      "make_parse_example_spec\n",
      "(\n",
      "columns\n",
      ")\n",
      " Y ou don ‡ t alwa ys ha ve to crea te a separa te fea ture column for each\n",
      " and ever y fea ture. F or exam ple, instead of ha ving 2 n umerical feaƒ\n",
      " ture columns, you could choose to ha ve a single 2D column: just\n",
      "set \n",
      "shape=[2]\n",
      " when calling \n",
      "numerical_column()\n",
      ".\n",
      " Y ou can then crea te a function tha t parses serialized exam ples using these fea ture\n",
      " descriptions, and separa tes the target column from the in put fea tures:\n",
      "def\n",
      " \n",
      "parse_examples\n",
      "(\n",
      "serialized_examples\n",
      "):\n",
      "    \n",
      "examples\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "io\n",
      ".\n",
      "parse_example\n",
      "(\n",
      "serialized_examples\n",
      ",\n",
      " \n",
      "feature_descriptions\n",
      ")\n",
      "    \n",
      "targets\n",
      " \n",
      "=\n",
      " \n",
      "examples\n",
      ".\n",
      "pop\n",
      "(\n",
      "\"median_house_value\"\n",
      ")\n",
      " \n",
      "# separate the targets\n",
      "    \n",
      "return\n",
      " \n",
      "examples\n",
      ",\n",
      " \n",
      "targets\n",
      " N ext, you can crea te a \n",
      "TFRecordDataset\n",
      "  tha t will read ba tches of serialized exam ples\n",
      " (assuming the TFRecord file con tains serialized \n",
      "Example\n",
      "   protobufs with the a ppropriƒ\n",
      " a te fea tures):\n",
      "batch_size\n",
      " \n",
      "=\n",
      " \n",
      "32\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "data\n",
      ".\n",
      "TFRecordDataset\n",
      "([\n",
      "\"my_data_with_features.tfrecords\"\n",
      "])\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      ".\n",
      "repeat\n",
      "()\n",
      ".\n",
      "shuffle\n",
      "(\n",
      "10000\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "batch_size\n",
      ")\n",
      ".\n",
      "map\n",
      "(\n",
      "parse_examples\n",
      ")\n",
      "Using Feature Columns in Your Models\n",
      " F ea ture columns can also be used directly in your model, to con vert all your in put\n",
      " fea tures in to a single dense vector which the neural network can then process. F or\n",
      "this, all you need to do is add a \n",
      "keras.layers.DenseFeatures\n",
      " \n",
      " la yer as the first la yer\n",
      " in your model, passing it the list of fea ture columns (excluding the target column):\n",
      "columns_without_target\n",
      " \n",
      "=\n",
      " \n",
      "columns\n",
      "[:\n",
      "-\n",
      "1\n",
      "]\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "DenseFeatures\n",
      "(\n",
      "feature_columns\n",
      "=\n",
      "columns_without_target\n",
      "),\n",
      " 426  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "1\n",
      ")\n",
      "])\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"mse\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ",\n",
      " \n",
      "metrics\n",
      "=\n",
      "[\n",
      "\"accuracy\"\n",
      "])\n",
      "steps_per_epoch\n",
      " \n",
      "=\n",
      " \n",
      "len\n",
      "(\n",
      "X_train\n",
      ")\n",
      " \n",
      "//\n",
      " \n",
      "batch_size\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "dataset\n",
      ",\n",
      " \n",
      "steps_per_epoch\n",
      "=\n",
      "steps_per_epoch\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "5\n",
      ")\n",
      "The \n",
      "DenseFeatures\n",
      "  la yer will take care of con verting ever y in put fea ture to a dense\n",
      " represen ta tion, and it will also a pply an y extra transforma tion we specified, such as\n",
      "scaling the \n",
      "housing_median_age\n",
      "   using the \n",
      "normalizer_fn\n",
      " \n",
      " function we provided. Y ou\n",
      " can take a closer look a t wha t the \n",
      "DenseFeatures\n",
      "  la yer does by calling it directly :\n",
      ">>> \n",
      "some_columns\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "ocean_proximity_embed\n",
      ",\n",
      " \n",
      "bucketized_income\n",
      "]\n",
      ">>> \n",
      "dense_features\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "DenseFeatures\n",
      "(\n",
      "some_columns\n",
      ")\n",
      ">>> \n",
      "dense_features\n",
      "({\n",
      "... \n",
      "    \n",
      "\"ocean_proximity\"\n",
      ":\n",
      " \n",
      "[[\n",
      "\"NEAR OCEAN\"\n",
      "],\n",
      " \n",
      "[\n",
      "\"INLAND\"\n",
      "],\n",
      " \n",
      "[\n",
      "\"INLAND\"\n",
      "]],\n",
      "... \n",
      "    \n",
      "\"median_income\"\n",
      ":\n",
      " \n",
      "[[\n",
      "3.\n",
      "],\n",
      " \n",
      "[\n",
      "7.2\n",
      "],\n",
      " \n",
      "[\n",
      "1.\n",
      "]]\n",
      "... \n",
      "})\n",
      "...\n",
      "<tf.Tensor: id=559790, shape=(3, 7), dtype=float32, numpy=\n",
      "array([[ 0. , 0. , 1. , 0. , 0. ,-0.36277947 , 0.30109018],\n",
      "       [ 0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],\n",
      "       [ 1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype=float32)>\n",
      " In this exam ple, we crea te a \n",
      "DenseFeatures\n",
      "  la yer with just two columns, and we call\n",
      " it with some da ta, in the form of a dictionar y of fea tures. In this case, since the \n",
      "bucke\n",
      "tized_income\n",
      " column relies on the \n",
      "median_income\n",
      "  column, the dictionar y m ust\n",
      "include the \n",
      "\"median_income\"\n",
      "  key , and similarly since the \n",
      "ocean_proximity_embed\n",
      "column is based on the \n",
      "ocean_proximity\n",
      "  column, the dictionar y m ust include the\n",
      "\"ocean_proximity\"\n",
      "  key . Columns are handled in alphabetical order , so first we look\n",
      " a t the bucketized income column (its name is the same as the \n",
      "median_income\n",
      "   column\n",
      "name, plus \n",
      "\"_bucketized\"\n",
      " ). The incomes 3, 7.2 and 1 get ma pped respectively to ca tƒ\n",
      " egor y 2 (for incomes between 1.5 and 3), ca tegor y 0 (for incomes below 1.5), and ca tƒ\n",
      " egor y 4 (for incomes grea ter than 6). Then these ca tegor y IDs get one-hot encoded:\n",
      " ca tegor y 2 gets encoded as \n",
      "[0., 0., 1., 0., 0.]\n",
      "  and so on (note tha t bucketized\n",
      " columns get one-hot encoded by defa ult, no need to call \n",
      "indicator_column()\n",
      " ). N ow\n",
      "on to the \n",
      "ocean_proximity_embed\n",
      " column. The \n",
      "\"NEAR OCEAN\"\n",
      " and \n",
      "\"INLAND\"\n",
      " \n",
      " ca teƒ\n",
      " gories just get ma pped to their respective embeddings (which were initialized ranƒ\n",
      " domly). The resulting tensor is the conca tena tion of the one-hot vectors and the\n",
      "embeddings.\n",
      " N ow you can feed all kinds of fea tures to a neural network, including n umerical feaƒ\n",
      " tures, ca tegorical fea tures, and even text (by splitting the text in to words, then using\n",
      " word embedding)! H owever , performing all the preprocessing on the fly can slow\n",
      " down training. Let ‡ s see how this can be im proved.\n",
      " The Features API  |  427\n",
      "\n",
      "TF Transform\n",
      " If preprocessing is com puta tionally expensive, then handling it before training ra ther\n",
      " than on the fly ma y give you a significan t speedup: the da ta will be preprocessed just\n",
      "once per instance \n",
      " b ef o r e\n",
      " \n",
      " training, ra ther than once per instance and per epoch \n",
      " d u r i n g\n",
      " training. T ools like A pache B eam let you run efficien t da ta processing pipelines over\n",
      " large amoun ts of da ta, even distributed across m ultiple ser vers, so wh y not use it to\n",
      " preprocess all the training da ta? This works grea t and indeed can speed up training,\n",
      " but there is one problem: once your model is trained, suppose you wan t to deploy it\n",
      " to a mobile a pp: you will need to write some code in your a pp to take care of preproƒ\n",
      " cessing the da ta before it is fed to the model. And suppose you also wan t to deploy\n",
      " the model to T ensorFlow .js so it runs in a web browser? Once again, you will need to\n",
      " write some preprocessing code. This can become a main tenance nigh tmare: whenƒ\n",
      " ever you wan t to change the preprocessing logic, you will need to upda te your A pache\n",
      " B eam code, your mobile a pp code and your J a vascript code. I t is not only time conƒ\n",
      " suming, but also error prone: you ma y end up with subtle differences between the\n",
      " preprocessing opera tions performed before training and the ones performed in your\n",
      " a pp or in the browser . This \n",
      " t r a i n i n g/s er v i n g s k e w\n",
      " \n",
      "will lead to bugs or degraded perforƒ\n",
      "mance.\n",
      " One im provemen t would be to take the trained model (trained on da ta tha t was preƒ\n",
      " processed by your A pache B eam code), and before deploying it to your a pp or the\n",
      " browser , add an extra in put la yer to take care of preprocessing on the fly (either by\n",
      " writing a custom la yer or by using a \n",
      "DenseFeatures\n",
      "  la yer). Tha t ‡ s definitely better ,\n",
      " since now you just ha ve two versions of your preprocessing code: the A pache B eam\n",
      " code and the preprocessing la yer‡ s code.\n",
      " But wha t if you could define your preprocessing opera tions just once? This is wha t\n",
      " TF T ransform was designed for . I t is part of \n",
      " T ensorFlow Extended\n",
      " \n",
      "(TFX), an end-to-\n",
      " end pla tform for productionizing T ensorFlow models. First, to use a TFX com ponen t,\n",
      " such as TF T ransform, you m ust install it, it does not come bundled with T ensorFlow .\n",
      " Y ou define your preprocessing function just once (in Python), by using TF T ransform\n",
      " functions for scaling, bucketizing, crossing fea tures, and more. Y ou can also use an y\n",
      " T ensorFlow opera tion you need. H ere is wha t this preprocessing function migh t look\n",
      " like if we just had two fea tures:\n",
      "import\n",
      " \n",
      "tensorflow_transform\n",
      " \n",
      "as\n",
      " \n",
      "tft\n",
      "def\n",
      " \n",
      "preprocess\n",
      "(\n",
      "inputs\n",
      "):\n",
      "  \n",
      "# inputs is a batch of input features\n",
      "    \n",
      "median_age\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "[\n",
      "\"housing_median_age\"\n",
      "]\n",
      "    \n",
      "ocean_proximity\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "[\n",
      "\"ocean_proximity\"\n",
      "]\n",
      "    \n",
      "standardized_age\n",
      " \n",
      "=\n",
      " \n",
      "tft\n",
      ".\n",
      "scale_to_z_score\n",
      "(\n",
      "median_age\n",
      " \n",
      "-\n",
      " \n",
      "tft\n",
      ".\n",
      "mean\n",
      "(\n",
      "median_age\n",
      "))\n",
      "    \n",
      "ocean_proximity_id\n",
      " \n",
      "=\n",
      " \n",
      "tft\n",
      ".\n",
      "compute_and_apply_vocabulary\n",
      "(\n",
      "ocean_proximity\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "{\n",
      "        \n",
      "\"standardized_median_age\"\n",
      ":\n",
      " \n",
      "standardized_age\n",
      ",\n",
      " 428  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "11\n",
      " A t the time of writing, TFDS requires you to download a few files man ually for ImageN et (for legal reasons),\n",
      "but this will hopefully get resolved soon.\n",
      "        \n",
      "\"ocean_proximity_id\"\n",
      ":\n",
      " \n",
      "ocean_proximity_id\n",
      "    \n",
      "}\n",
      " N ext, TF T ransform lets you a pply this \n",
      "preprocess()\n",
      " function to the whole training\n",
      " set using A pache B eam (it provides an \n",
      "AnalyzeAndTransformDataset\n",
      "  class tha t you\n",
      " can use for this purpose in your A pache B eam pipeline). In the process, it will also\n",
      " com pute all the necessar y sta tistics over the whole training set: in this exam ple, the\n",
      " mean and standard devia tion of the \n",
      "housing_median_age\n",
      " \n",
      " fea ture, and the vocabular y\n",
      "for the \n",
      "ocean_proximity\n",
      "  fea ture. The com ponen ts tha t com pute these sta tistics are\n",
      "called \n",
      " a n a l yz er s\n",
      ".\n",
      " Im portan tly , TF T ransform will also genera te an equivalen t T ensorFlow F unction tha t\n",
      " you can plug in to the model you deploy . This TF F unction con tains all the necessar y\n",
      " sta tistics com puted by A pache B eam (the mean, standard devia tion, and vocabular y),\n",
      " sim ply included as constan ts.\n",
      " A t the time of this writing, TF T ransform only supports T ensorƒ\n",
      " Flow 1. M oreover , A pache B eam only has partial support for\n",
      " Python 3. Tha t said, both these limita tions will likely be fixed by\n",
      "the time your read this.\n",
      " W ith the Da ta API, TFRecords, the F ea tures API and TF T ransform, you can build\n",
      " highly scalable in put pipelines for training, and also benefit from fast and portable\n",
      " da ta preprocessing in production.\n",
      " But wha t if you just wan ted to use a standard da taset? W ell in tha t case, things are\n",
      " m uch sim pler : just use TFDS!\n",
      "The TensorFlow Datasets (TFDS) Project\n",
      "The \n",
      " T ensorFlow Da tasets\n",
      "   project makes it trivial to download common da tasets, from\n",
      " small ones like MNIST or F ashion MNIST , to h uge da tasets like ImageN et\n",
      "11\n",
      " \n",
      "(you will\n",
      " need quite a bit of disk space!). The list includes image da tasets, text da tasets (includƒ\n",
      " ing transla tion da tasets), a udio and video da tasets, and more. Y ou can visit \n",
      " h ttp s://\n",
      " h o m l.i n f o/t f d s\n",
      "  to view the full list, along with a description of each da taset.\n",
      " TFDS is not bundled with T ensorFlow , so you need to install the \n",
      "tensorflow-\n",
      "datasets\n",
      "  librar y (e.g., using pip). Then all you need to do is call the \n",
      "tfds.load()\n",
      " function, and it will download the da ta you wan t (unless it was already downloaded\n",
      " earlier), and return the da ta as a dictionar y of \n",
      "Datasets\n",
      " (typically one for training,\n",
      " The TensorFlow Datasets (TFDS) Project  |  429\n",
      "\n",
      " and one for testing, but this depends on the da taset you choose). F or exam ple, let ‡ s\n",
      " download MNIST :\n",
      "import\n",
      " \n",
      "tensorflow_datasets\n",
      " \n",
      "as\n",
      " \n",
      "tfds\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "load\n",
      "(\n",
      "name\n",
      "=\n",
      "\"mnist\"\n",
      ")\n",
      "mnist_train\n",
      ",\n",
      " \n",
      "mnist_test\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      "[\n",
      "\"train\"\n",
      "],\n",
      " \n",
      "dataset\n",
      "[\n",
      "\"test\"\n",
      "]\n",
      " Y ou can then a pply an y transforma tion you wan t (typically repea ting, ba tching and\n",
      " prefetching), and you ‡ re ready to train your model. H ere is a sim ple exam ple:\n",
      "mnist_train\n",
      " \n",
      "=\n",
      " \n",
      "mnist_train\n",
      ".\n",
      "repeat\n",
      "(\n",
      "5\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "32\n",
      ")\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      "for\n",
      " \n",
      "item\n",
      " \n",
      "in\n",
      " \n",
      "mnist_train\n",
      ":\n",
      "    \n",
      "images\n",
      " \n",
      "=\n",
      " \n",
      "item\n",
      "[\n",
      "\"image\"\n",
      "]\n",
      "    \n",
      "labels\n",
      " \n",
      "=\n",
      " \n",
      "item\n",
      "[\n",
      "\"label\"\n",
      "]\n",
      "    \n",
      "[\n",
      "...\n",
      "]\n",
      "In general, \n",
      "load()\n",
      " \n",
      " returns a sh uffled training set, so there ‡ s no need\n",
      " to sh uffle it some more.\n",
      " N ote tha t each item in the da taset is a dictionar y con taining both the fea tures and the\n",
      " labels. But K eras expects each item to be a tuple con taining 2 elemen ts (again, the feaƒ\n",
      " tures and the labels). Y ou could transform the da taset using the \n",
      "map()\n",
      " method, like\n",
      "this:\n",
      "mnist_train\n",
      " \n",
      "=\n",
      " \n",
      "mnist_train\n",
      ".\n",
      "repeat\n",
      "(\n",
      "5\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "32\n",
      ")\n",
      "mnist_train\n",
      " \n",
      "=\n",
      " \n",
      "mnist_train\n",
      ".\n",
      "map\n",
      "(\n",
      "lambda\n",
      " \n",
      "items\n",
      ":\n",
      " \n",
      "(\n",
      "items\n",
      "[\n",
      "\"image\"\n",
      "],\n",
      " \n",
      "items\n",
      "[\n",
      "\"label\"\n",
      "]))\n",
      "mnist_train\n",
      " \n",
      "=\n",
      " \n",
      "mnist_train\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      "Or you can just ask the \n",
      "load()\n",
      " function to do this for you by setting \n",
      "as_super\n",
      "vised=True\n",
      "  (obviously this works only for labeled da tasets). Y ou can also specif y the\n",
      " ba tch size if you wan t. Then the da taset can be passed directly to your tf.keras model:\n",
      "dataset\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "load\n",
      "(\n",
      "name\n",
      "=\n",
      "\"mnist\"\n",
      ",\n",
      " \n",
      "batch_size\n",
      "=\n",
      "32\n",
      ",\n",
      " \n",
      "as_supervised\n",
      "=\n",
      "True\n",
      ")\n",
      "mnist_train\n",
      " \n",
      "=\n",
      " \n",
      "dataset\n",
      "[\n",
      "\"train\"\n",
      "]\n",
      ".\n",
      "repeat\n",
      "()\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "...\n",
      "])\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"sparse_categorical_crossentropy\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "\"sgd\"\n",
      ")\n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "mnist_train\n",
      ",\n",
      " \n",
      "steps_per_epoch\n",
      "=\n",
      "60000\n",
      " \n",
      "//\n",
      " \n",
      "32\n",
      ",\n",
      " \n",
      "epochs\n",
      "=\n",
      "5\n",
      ")\n",
      " This was quite a technical cha pter , and you ma y feel tha t it is a bit far from the\n",
      " abstract bea uty of neural networks, but the fact is deep learning often in volves large\n",
      " amoun ts of da ta, and knowing how to load, parse and preprocess it efficien tly is a\n",
      " crucial skill to ha ve. In the next cha pter , we will look a t Con volutional N eural N etƒ\n",
      "works, which are among the most successful neural net architectures for image proƒ\n",
      " cessing, and man y other a pplica tions.\n",
      " 430  |  Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "CHAPTER 14\n",
      "Deep Computer Vision Using Convolutional\n",
      "Neural Networks\n",
      " W ith Early Release ebooks, you get books in their earliest form›\n",
      " the a uthor‡ s ra w and unedited con ten t as he or she writes›so you\n",
      " can take advan tage of these technologies long before the official\n",
      " release of these titles. The following will be Cha pter 14 in the final\n",
      "release of the book.\n",
      " Although IBM ‡ s Deep Blue supercom puter bea t the chess world cham pion Garr y Kasƒ\n",
      " parov back in 1996, it wasn ‡ t un til fairly recen tly tha t com puters were able to reliably\n",
      "perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\n",
      " spoken words. Wh y are these tasks so effortless to us h umans? The answer lies in the\n",
      " fact tha t perception largely takes place outside the realm of our consciousness, within\n",
      " specialized visual, a uditor y , and other sensor y modules in our brains. By the time\n",
      " sensor y informa tion reaches our consciousness, it is already adorned with high-level\n",
      " fea tures; for exam ple, when you look a t a picture of a cute puppy , you cannot choose\n",
      " n o t\n",
      "  to see the puppy , or \n",
      " n o t\n",
      "  to notice its cuteness. N or can you explain \n",
      " h o w\n",
      " you recƒ\n",
      " ognize a cute puppy ; it ‡ s just obvious to you. Th us, we cannot trust our subjective\n",
      " experience: perception is not trivial a t all, and to understand it we m ust look a t how\n",
      " the sensor y modules work.\n",
      " Con volutional neural networks (CNN s) emerged from the study of the brain ‡ s visual\n",
      " cortex, and they ha ve been used in image recognition since the 1980s. In the last few\n",
      " years, thanks to the increase in com puta tional power , the amoun t of a vailable training\n",
      " da ta, and the tricks presen ted in \n",
      " Cha pter 11\n",
      "  for training deep nets, CNN s ha ve manƒ\n",
      " aged to achieve superh uman performance on some com plex visual tasks. They power\n",
      " image search ser vices, self-driving cars, a utoma tic video classifica tion systems, and\n",
      " more. M oreover , CNN s are not restricted to visual perception: they are also successful\n",
      "431\n",
      "\n",
      "1\n",
      " — Single U nit A ctivity in Stria te Cortex of U nrestrained Ca ts, – D . H ubel and T . W iesel (1958).\n",
      "2\n",
      " —Receptive Fields of Single N eurones in the Ca t ‡ s Stria te Cortex, – D . H ubel and T . W iesel (1959).\n",
      "3\n",
      " —Receptive Fields and F unctional Architecture of M onkey Stria te Cortex, – D . H ubel and T . W iesel (1968).\n",
      " a t man y other tasks, such as \n",
      " v o i c e r e c og n i t i o n\n",
      " or \n",
      " n a t u r a l l a n g u a ge p r o c e s s i n g\n",
      " \n",
      "(NLP);\n",
      " however , we will focus on visual a pplica tions for now .\n",
      " In this cha pter we will presen t where CNN s came from, wha t their building blocks\n",
      " look like, and how to im plemen t them using T ensorFlow and K eras. Then we will disƒ\n",
      "cuss some of the best CNN architectures, and discuss other visual tasks, including\n",
      " o b je c t d e t e c t i o n\n",
      "  (classif ying m ultiple objects in an image and placing bounding boxes\n",
      "around them) and \n",
      " s em a n t i c s e g m en t a t i o n\n",
      " \n",
      " (classif ying each pixel according to the class\n",
      "of the object it belongs to).\n",
      "The Architecture of the Visual Cortex\n",
      " Da vid H. H ubel and T orsten W iesel performed a series of experimen ts on ca ts in\n",
      "1958\n",
      "1\n",
      " and \n",
      "1959\n",
      "2\n",
      " (and a \n",
      " few years la ter on monkeys\n",
      "3\n",
      " ), giving crucial insigh ts on the\n",
      " structure of the visual cortex (the a uthors received the N obel Prize in Ph ysiolog y or\n",
      " M edicine in 1981 for their work). In particular , they showed tha t man y neurons in\n",
      " the visual cortex ha ve a small \n",
      " l o c a l r e c e p t i v e \n",
      "†eld\n",
      ", meaning they react only to visual\n",
      " stim uli loca ted in a limited region of the visual field (see \n",
      "Figure 14-1\n",
      ", in which the\n",
      " local receptive fields of five neurons are represen ted by dashed circles). The receptive\n",
      " fields of differen t neurons ma y overla p , and together they tile the whole visual field.\n",
      " M oreover , the a uthors showed tha t some neurons react only to images of horizon tal\n",
      " lines, while others react only to lines with differen t orien ta tions (two neurons ma y\n",
      " ha ve the same receptive field but react to differen t line orien ta tions). They also\n",
      " noticed tha t some neurons ha ve larger receptive fields, and they react to more comƒ\n",
      " plex pa tterns tha t are combina tions of the lower -level pa tterns. These obser va tions\n",
      " led to the idea tha t the higher -level neurons are based on the outputs of neighboring\n",
      " lower -level neurons (in \n",
      "Figure 14-1\n",
      " , notice tha t each neuron is connected only to a\n",
      " few neurons from the previous la yer). This powerful architecture is able to detect all\n",
      " sorts of com plex pa tterns in an y area of the visual field.\n",
      " 432  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "4\n",
      " —N eocognitron: A Self-organizing N eural N etwork M odel for a M echanism of P a ttern Recognition U naffected\n",
      " by Shift in P osition, – K. F ukushima (1980).\n",
      "5\n",
      " — Gradien t-Based Learning A pplied to Documen t Recognition, – Y . LeCun et al. (1998).\n",
      " F i g u r e 14-1. L o c a l r e c e p t i v e \n",
      "†elds\n",
      "  i n t h e v i s u a l c o r t ex\n",
      "These studies of the visual cortex inspired the \n",
      " neocognitron, in troduced in 1980\n",
      ",\n",
      "4\n",
      " which gradually evolved in to wha t we now call \n",
      " c o n v o l u t i o n a l n eu r a l n e tw o r ks\n",
      ". An\n",
      " im portan t milestone was a \n",
      " 1998 pa per\n",
      "5\n",
      " \n",
      " by Y ann LeCun, L•on B ottou, Y osh ua B engio ,\n",
      " and P a trick H affner , which in troduced the famous \n",
      " L eN e t-5\n",
      " architecture, \n",
      "widely used\n",
      " to recognize handwritten check n umbers. This architecture has some building blocks\n",
      " tha t you already know , such as fully connected la yers and sigmoid activa tion funcƒ\n",
      " tions, but it also in troduces two new building blocks: \n",
      " c o n v o l u t i o n a l l a y er s\n",
      "   and \n",
      " p o o l i n g\n",
      " l a y er s\n",
      " . Let ‡ s look a t them now .\n",
      " Wh y not sim ply use a regular deep neural network with fully conƒ\n",
      " nected la yers for image recognition tasks? U nfortuna tely , although\n",
      "this works fine for small images (e.g., MNIST), it breaks down for\n",
      " larger images beca use of the h uge n umber of parameters it\n",
      " requires. F or exam ple, a 100 „ 100 image has 10,000 pixels, and if\n",
      " the first la yer has just 1,000 neurons (which already severely\n",
      " restricts the amoun t of informa tion transmitted to the next la yer),\n",
      " this means a total of 10 million connections. And tha t ‡ s just the first\n",
      " la yer . CNN s solve this problem using partially connected la yers and\n",
      " weigh t sharing.\n",
      " The Architecture of the Visual Cortex  |  433\n",
      "\n",
      "6\n",
      " A con volution is a ma thema tical opera tion tha t slides one function over another and measures the in tegral of\n",
      " their poin twise m ultiplica tion. I t has deep connections with the F ourier transform and the La place transform,\n",
      " and is hea vily used in signal processing. Con volutional la yers actually use cross-correla tions, which are ver y\n",
      " similar to con volutions (see \n",
      " h ttp s://h o m l.i n f o/76\n",
      " for more details).\n",
      "Convolutional Layer\n",
      " The most im portan t building block of a CNN is the \n",
      " c o n v o l u t i o n a l l a y er\n",
      ":\n",
      "6\n",
      " neurons in\n",
      " the first con volutional la yer are not connected to ever y single pixel in the in put image\n",
      " (like they were in previous cha pters), but only to pixels in their receptive fields (see\n",
      "Figure 14-2\n",
      " ). In turn, each neuron in the second con volutional la yer is connected\n",
      " only to neurons loca ted within a small rectangle in the first la yer . This architecture\n",
      " allows the network to concen tra te on small low-level fea tures in the first hidden la yer ,\n",
      " then assemble them in to larger higher -level fea tures in the next hidden la yer , and so\n",
      "on. This hierarchical structure is common in real-world images, which is one of the\n",
      " reasons wh y CNN s work so well for image recognition.\n",
      " F i g u r e 14-2. CNN l a y er s w i t h r e c t a n g u l a r l o c a l r e c e p t i v e \n",
      "†elds\n",
      " U n til now , all m ultila yer neural networks we looked a t had la yers\n",
      " com posed of a long line of neurons, and we had to fla tten in put\n",
      " images to 1D before feeding them to the neural network. N ow each\n",
      " la yer is represen ted in 2D , which makes it easier to ma tch neurons\n",
      " with their corresponding in puts.\n",
      " A neuron loca ted in row \n",
      "i\n",
      ", column \n",
      "j\n",
      " \n",
      " of a given la yer is connected to the outputs of the\n",
      " neurons in the previous la yer loca ted in rows \n",
      "i\n",
      " to \n",
      "i\n",
      " + \n",
      "f\n",
      "h\n",
      " − 1, columns \n",
      "j\n",
      " \n",
      "to \n",
      "j\n",
      " \n",
      "+ \n",
      "f\n",
      "w\n",
      " \n",
      "− 1,\n",
      "where \n",
      "f\n",
      "h\n",
      " and \n",
      "f\n",
      "w\n",
      "  are the heigh t and width of the receptive field (see \n",
      "Figure 14-3\n",
      "). In\n",
      " order for a la yer to ha ve the same heigh t and width as the previous la yer , it is comƒ\n",
      " 434  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " mon to add zeros around the in puts, as shown in the diagram. This is \n",
      "called \n",
      " z er o p a d‡\n",
      " d i n g\n",
      ".\n",
      " F i g u r e 14-3. C o n n e c t i o ns b e tw e en l a y er s a n d z er o p a d d i n g\n",
      " I t is also possible to connect a large in put la yer to a m uch smaller la yer by spacing out\n",
      "the receptive fields, as shown in \n",
      "Figure 14-4\n",
      ". The shift from one receptive field to the\n",
      " next is called  the \n",
      " s t r i d e\n",
      " . In the diagram, a 5 „ 7 in put la yer (plus zero padding) is conƒ\n",
      " nected to a 3 „ 4 la yer , using 3 „ 3 receptive fields and a stride of 2 (in this exam ple\n",
      " the stride is the same in both directions, but it does not ha ve to be so). A neuron locaƒ\n",
      "ted in row \n",
      "i\n",
      ", column \n",
      "j\n",
      " \n",
      " in the upper la yer is connected to the outputs of the neurons in\n",
      " the previous la yer loca ted in rows \n",
      "i\n",
      "   „ \n",
      "s\n",
      "h\n",
      "   to \n",
      "i\n",
      "   „ \n",
      "s\n",
      "h\n",
      "   + \n",
      "f\n",
      "h\n",
      " \n",
      "− 1, columns \n",
      "j\n",
      "   „ \n",
      "s\n",
      "w\n",
      "   to \n",
      "j\n",
      "   „ \n",
      "s\n",
      "w\n",
      "   + \n",
      "f\n",
      "w\n",
      "   −\n",
      "1, where \n",
      "s\n",
      "h\n",
      " and \n",
      "s\n",
      "w\n",
      "  are the vertical and horizon tal strides.\n",
      " Convolutional Layer  |  435\n",
      "\n",
      " F i g u r e 14-4. R e d u ci n g d i m ens i o n a l i ty u s i n g a s t r i d e o f 2\n",
      "Filters\n",
      " A neuron ‡ s weigh ts can be represen ted as a small image the size of the receptive field.\n",
      " F or exam ple, \n",
      "Figure 14-5\n",
      " \n",
      " shows two possible sets of weigh ts, called \n",
      "†lters\n",
      "   (or \n",
      " c o n v o l u‡\n",
      " t i o n k er n e l s\n",
      " ). The first one is represen ted as a black square with a vertical white line in\n",
      " the middle (it is a 7 „ 7 ma trix full of 0s except for the cen tral column, which is full of\n",
      " 1s); neurons using these weigh ts will ignore ever ything in their receptive field except\n",
      " for the cen tral vertical line (since all in puts will get m ultiplied by 0, except for the\n",
      " ones loca ted in the cen tral vertical line). The second filter is a black square with a\n",
      " horizon tal white line in the middle. Once again, neurons using these weigh ts will\n",
      " ignore ever ything in their receptive field except for the cen tral horizon tal line.\n",
      " N ow if all neurons in a la yer use the same vertical line filter (and the same bias term),\n",
      " and you feed the network the in put image shown in \n",
      "Figure 14-5\n",
      " (bottom image), the\n",
      " la yer will output the top-left image. N otice tha t the vertical white lines get enhanced\n",
      " while the rest gets blurred. Similarly , the upper -righ t image is wha t you get if all neuƒ\n",
      " rons use the same horizon tal line filter ; notice tha t the horizon tal white lines get\n",
      " enhanced while the rest is blurred out. Th us, a la yer full of neurons using the same\n",
      "filter outputs a \n",
      " f e a t u r e m a p\n",
      " , which highligh ts the areas in an image tha t activa te the\n",
      " filter the most. Of course you do not ha ve to define the filters man ually : instead, durƒ\n",
      " ing training the con volutional la yer will a utoma tically learn the most useful filters for\n",
      " its task, and the la yers above will learn to combine them in to more com plex pa tterns.\n",
      " 436  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " F i g u r e 14-5. Ap p l y i n g tw o \n",
      "di›erent\n",
      " \n",
      "†lters\n",
      "  t o ge t tw o f e a t u r e m a p s\n",
      "Stacking Multiple Feature Maps\n",
      " U p \n",
      " to now , for sim plicity , I ha ve represen ted the output of each con volutional la yer as\n",
      " a thin 2D la yer , but in reality a con volutional la yer has m ultiple filters (you decide\n",
      " how man y), and it outputs one fea ture ma p per filter , so it is more accura tely repreƒ\n",
      " sen ted in 3D (see \n",
      "Figure 14-6\n",
      " ). T o do so , it has one neuron per pixel in each fea ture\n",
      " ma p , and all neurons within a given fea ture ma p share the same parameters (i.e., the\n",
      " same weigh ts and bias term). H owever , neurons in differen t fea ture ma ps use differƒ\n",
      " en t parameters. A neuron ‡ s receptive field is the same as described earlier , but it\n",
      " extends across all the previous la yers ‡ fea ture ma ps. In short, a con volutional la yer\n",
      " sim ultaneously a pplies m ultiple trainable filters to its in puts, making it ca pable of\n",
      " detecting m ultiple fea tures an ywhere in its in puts.\n",
      " The fact tha t all neurons in a fea ture ma p share the same parameƒ\n",
      " ters drama tically reduces the n umber of parameters in the model.\n",
      " M oreover , once the CNN has learned to recognize a pa ttern in one\n",
      " loca tion, it can recognize it in an y other loca tion. In con trast, once\n",
      " a regular DNN has learned to recognize a pa ttern in one loca tion, it\n",
      " can recognize it only in tha t particular loca tion.\n",
      " M oreover , in put images are also com posed of m ultiple subla yers: one per \n",
      " c o l o r c h a n‡\n",
      " n e l\n",
      " . There are typically three: red, green, and blue (RGB). Gra yscale images ha ve just\n",
      " Convolutional Layer  |  437\n",
      "\n",
      " one channel, but some images ma y ha ve m uch more›for exam ple, sa tellite images\n",
      " tha t ca pture extra ligh t frequencies (such as infrared).\n",
      " F i g u r e 14-6. C o n v o l u t i o n l a y er s w i t h m u l t i p l e f e a t u r e m a p s, a n d i m a ge s w i t h t h r e e c o l o r\n",
      " c h a n n e l s\n",
      " Specifically , a neuron loca ted in row \n",
      "i\n",
      ", column \n",
      "j\n",
      "   of the fea ture ma p \n",
      "k\n",
      "  in a given con voƒ\n",
      " lutional la yer \n",
      "l\n",
      "  is connected to the outputs of the neurons in the previous la yer \n",
      "l\n",
      " \n",
      "− 1,\n",
      " loca ted in rows \n",
      "i\n",
      " „ \n",
      "s\n",
      "h\n",
      " to \n",
      "i\n",
      " „ \n",
      "s\n",
      "h\n",
      " + \n",
      "f\n",
      "h\n",
      " − 1 and columns \n",
      "j\n",
      " „ \n",
      "s\n",
      "w\n",
      " to \n",
      "j\n",
      " \n",
      "„ \n",
      "s\n",
      "w\n",
      " \n",
      "+ \n",
      "f\n",
      "w\n",
      " \n",
      "− 1, across all\n",
      " fea ture ma ps (in la yer \n",
      "l\n",
      " − \n",
      "1\n",
      " ). N ote tha t all neurons loca ted in the same row \n",
      "i\n",
      " \n",
      "and colƒ\n",
      "umn \n",
      "j\n",
      "  but in differen t fea ture ma ps are connected to the outputs of the exact same\n",
      " neurons in the previous la yer .\n",
      " Equa tion 14-1\n",
      " \n",
      " summarizes the preceding explana tions in one big ma thema tical equaƒ\n",
      " tion: it shows how to com pute the output of a given neuron in a con volutional la yer .\n",
      " 438  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " I t is a bit ugly due to all the differen t indices, but all it does is calcula te the weigh ted\n",
      " sum of all the in puts, plus the bias term.\n",
      " Eq u a t i o n 14-1. C o m p u t i n g t h e o u tp u t o f a n eu r o n i n a c o n v o l u t i o n a l l a y er\n",
      "z\n",
      "i\n",
      ",\n",
      "j\n",
      ",\n",
      "k\n",
      "=\n",
      "b\n",
      "k\n",
      "+\n",
      "“\n",
      "u\n",
      " = 0\n",
      "f\n",
      "h\n",
      " ” 1\n",
      "“\n",
      "v\n",
      " = 0\n",
      "f\n",
      "w\n",
      " ” 1\n",
      "“\n",
      "k\n",
      " = 0\n",
      "f\n",
      "n\n",
      " ” 1\n",
      "x\n",
      "i\n",
      ",\n",
      "j\n",
      ",\n",
      "k\n",
      ".\n",
      "w\n",
      "u\n",
      ",\n",
      "v\n",
      ",\n",
      "k\n",
      ",\n",
      "k\n",
      "with\n",
      "i\n",
      "=\n",
      "i\n",
      "„\n",
      "s\n",
      "h\n",
      "+\n",
      "u\n",
      "j\n",
      "=\n",
      "j\n",
      "„\n",
      "s\n",
      "w\n",
      "+\n",
      "v\n",
      "⁄\n",
      "z\n",
      " i, j , k\n",
      " \n",
      " is the output of the neuron loca ted in row \n",
      "i\n",
      ", column \n",
      "j\n",
      " \n",
      " in fea ture ma p \n",
      "k\n",
      "   of the\n",
      " con volutional la yer (la yer \n",
      "l\n",
      ").\n",
      "⁄\n",
      " As explained earlier , \n",
      "s\n",
      "h\n",
      " and \n",
      "s\n",
      "w\n",
      "  are the vertical and horizon tal strides, \n",
      "f\n",
      "h\n",
      " and \n",
      "f\n",
      "w\n",
      " \n",
      "are\n",
      " the heigh t and width of the receptive field, and \n",
      "f\n",
      "n\n",
      "  is the n umber of fea ture ma ps\n",
      " in the previous la yer (la yer \n",
      "l\n",
      " − 1).\n",
      "⁄\n",
      "x\n",
      "i\n",
      ", \n",
      "j\n",
      ", \n",
      "k\n",
      "  is the output of the neuron loca ted in la yer \n",
      "l\n",
      " − 1, row \n",
      "i\n",
      ", column \n",
      "j\n",
      " , fea ture\n",
      " ma p \n",
      "k\n",
      "(or channel \n",
      "k\n",
      " if the previous la yer is the in put la yer).\n",
      "⁄\n",
      "b\n",
      "k\n",
      " \n",
      " is the bias term for fea ture ma p \n",
      "k\n",
      " \n",
      " (in la yer \n",
      "l\n",
      " ). Y ou can think of it as a knob tha t\n",
      " tweaks the overall brigh tness of the fea ture ma p \n",
      "k\n",
      ".\n",
      "⁄\n",
      "w\n",
      "u\n",
      ", \n",
      "v\n",
      ", \n",
      "k\n",
      ",\n",
      "k\n",
      "   is the connection weigh t between an y neuron in fea ture ma p \n",
      "k\n",
      "   of the la yer\n",
      "l\n",
      " \n",
      " and its in put loca ted a t row \n",
      "u\n",
      ", column \n",
      "v\n",
      " \n",
      " (rela tive to the neuron ‡ s receptive field),\n",
      " and fea ture ma p \n",
      "k\n",
      ".\n",
      "TensorFlow Implementation\n",
      " In T ensorFlow , each in put image is typically represen ted as a 3D tensor of \n",
      "shape\n",
      "[height, width, channels]\n",
      " . A mini-ba tch is represen ted as a 4D tensor of \n",
      "shape\n",
      "[mini-batch size, height, width, channels]\n",
      " . The weigh ts of a con volutional\n",
      " la yer are represen ted as a 4D tensor of sha pe [\n",
      "f\n",
      "h\n",
      ", \n",
      "f\n",
      "w\n",
      ", \n",
      "f\n",
      "n\n",
      ", \n",
      "f\n",
      "n\n",
      " ]. The bias terms of a con voƒ\n",
      " lutional la yer are sim ply represen ted as a 1D tensor of sha pe [\n",
      "f\n",
      "n\n",
      "].\n",
      " Let ‡ s look a t a sim ple exam ple. The following code loads two sam ple images, using\n",
      " Scikit-Learn ‡ s \n",
      "load_sample_images()\n",
      " (which loads two color images, one of a Chiƒ\n",
      " nese tem ple, and the other of a flower). The pixel in tensities (for each color channel)\n",
      " is represen ted as a byte from 0 to 255, so we scale these fea tures sim ply by dividing by\n",
      " 255, to get floa ts ranging from 0 to 1. Then we crea te two 7 „ 7 filters (one with a\n",
      " vertical white line in the middle, and the other with a horizon tal white line in the\n",
      " middle), and we a pply them to both images using the \n",
      "tf.nn.conv2d()\n",
      " \n",
      "function,\n",
      " which is part of T ensorFlow‡ s low-level Deep Learning API. In this exam ple, we use\n",
      "zero padding (\n",
      "padding=\"SAME\"\n",
      ") and a stride of \n",
      "2\n",
      ". \n",
      " Finally , we plot one of the resulting\n",
      " fea ture ma ps (similar to the top-righ t image in \n",
      "Figure 14-5\n",
      ").\n",
      " Convolutional Layer  |  439\n",
      "\n",
      "from\n",
      " \n",
      "sklearn.datasets\n",
      " \n",
      "import\n",
      " \n",
      "load_sample_image\n",
      "# Load sample images\n",
      "china\n",
      " \n",
      "=\n",
      " \n",
      "load_sample_image\n",
      "(\n",
      "\"china.jpg\"\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "255\n",
      "flower\n",
      " \n",
      "=\n",
      " \n",
      "load_sample_image\n",
      "(\n",
      "\"flower.jpg\"\n",
      ")\n",
      " \n",
      "/\n",
      " \n",
      "255\n",
      "images\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "array\n",
      "([\n",
      "china\n",
      ",\n",
      " \n",
      "flower\n",
      "])\n",
      "batch_size\n",
      ",\n",
      " \n",
      "height\n",
      ",\n",
      " \n",
      "width\n",
      ",\n",
      " \n",
      "channels\n",
      " \n",
      "=\n",
      " \n",
      "images\n",
      ".\n",
      "shape\n",
      "# Create 2 filters\n",
      "filters\n",
      " \n",
      "=\n",
      " \n",
      "np\n",
      ".\n",
      "zeros\n",
      "(\n",
      "shape\n",
      "=\n",
      "(\n",
      "7\n",
      ",\n",
      " \n",
      "7\n",
      ",\n",
      " \n",
      "channels\n",
      ",\n",
      " \n",
      "2\n",
      "),\n",
      " \n",
      "dtype\n",
      "=\n",
      "np\n",
      ".\n",
      "float32\n",
      ")\n",
      "filters\n",
      "[:,\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      ":,\n",
      " \n",
      "0\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      "  \n",
      "# vertical line\n",
      "filters\n",
      "[\n",
      "3\n",
      ",\n",
      " \n",
      ":,\n",
      " \n",
      ":,\n",
      " \n",
      "1\n",
      "]\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      "  \n",
      "# horizontal line\n",
      "outputs\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "nn\n",
      ".\n",
      "conv2d\n",
      "(\n",
      "images\n",
      ",\n",
      " \n",
      "filters\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "padding\n",
      "=\n",
      "\"SAME\"\n",
      ")\n",
      "plt\n",
      ".\n",
      "imshow\n",
      "(\n",
      "outputs\n",
      "[\n",
      "0\n",
      ",\n",
      " \n",
      ":,\n",
      " \n",
      ":,\n",
      " \n",
      "1\n",
      "],\n",
      " \n",
      "cmap\n",
      "=\n",
      "\"gray\"\n",
      ")\n",
      " \n",
      "# plot 1st image•s 2nd feature map\n",
      "plt\n",
      ".\n",
      "show\n",
      "()\n",
      " M ost of this code is self-explana tor y , but the \n",
      "tf.nn.conv2d()\n",
      "  line deser ves a bit of\n",
      " explana tion:\n",
      "⁄\n",
      "images\n",
      "  is the in put mini-ba tch (a 4D tensor , as explained earlier).\n",
      "⁄\n",
      "filters\n",
      "  is the set of filters to a pply (also a 4D tensor , as explained earlier).\n",
      "⁄\n",
      "strides\n",
      "  is equal to 1, but it could also be a 1D arra y with 4 elemen ts, where the\n",
      " two cen tral elemen ts are the vertical and horizon tal strides (\n",
      "s\n",
      "h\n",
      " and \n",
      "s\n",
      "w\n",
      "). The first\n",
      " and last elemen ts m ust curren tly be equal to 1. They ma y one da y be used to\n",
      " specif y a ba tch stride (to skip some instances) and a channel stride (to skip some\n",
      " of the previous la yer‡ s fea ture ma ps or channels).\n",
      "⁄\n",
      "padding\n",
      "  m ust be either \n",
      "\"VALID\"\n",
      " or \n",
      "\"SAME\"\n",
      ":\n",
      "›\n",
      "If set to \n",
      "\"VALID\"\n",
      " , the con volutional la yer does \n",
      " n o t\n",
      "  use zero padding, and ma y\n",
      " ignore some rows and columns a t the bottom and righ t of the in put image,\n",
      "depending on the stride, as shown in \n",
      "Figure 14-7\n",
      " \n",
      " (for sim plicity , only the horƒ\n",
      " izon tal dimension is shown here, but of course the same logic a pplies to the\n",
      "vertical dimension).\n",
      "›\n",
      "If set to \n",
      "\"SAME\"\n",
      " , the con volutional la yer uses zero padding if necessar y . In this\n",
      " case, the n umber of output neurons is equal to the n umber of in put neurons\n",
      " divided by the stride, rounded up (in this exam ple, 13 / 5 = 2.6, rounded up to\n",
      " 3). Then zeros are added as evenly as possible around the in puts.\n",
      " 440   |  Chapter  14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " F i g u r e 14-7. P a d d i n g o p t i o nsıi n p u t w i d t h: 13, \n",
      "†lter\n",
      "  w i d t h: 6, s t r i d e: 5\n",
      " In this exam ple, we man ually defined the filters, but in a real CNN you would norƒ\n",
      "mally define filters as trainable variables, so the neural net can learn which filters\n",
      " work best, as explained earlier . Instead of man ually crea ting the variables, however ,\n",
      " you can sim ply use the \n",
      "keras.layers.Conv2D\n",
      "  la yer :\n",
      "conv\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Conv2D\n",
      "(\n",
      "filters\n",
      "=\n",
      "32\n",
      ",\n",
      " \n",
      "kernel_size\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "1\n",
      ",\n",
      "                           \n",
      "padding\n",
      "=\n",
      "\"SAME\"\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ")\n",
      " This code crea tes a \n",
      "Conv2D\n",
      "  la yer with 32 filters, each 3 „ 3, using a stride of 1 (both\n",
      " horizon tally and vertically), SAME padding, and a pplying the ReL U activa tion funcƒ\n",
      " tion to its outputs. As you can see, con volutional la yers ha ve quite a few h yperparaƒ\n",
      " meters: you m ust choose the n umber of filters, their heigh t and width, the strides, and\n",
      " the padding type. As alwa ys, you can use cross-valida tion to find the righ t h yperparaƒ\n",
      " meter values, but this is ver y time-consuming. W e will discuss common CNN archiƒ\n",
      " tectures la ter , to give you some idea of wha t h yperparameter values work best in \n",
      "practice.\n",
      "Memory Requirements\n",
      " Another problem with CNN s is tha t the con volutional la yers require a h uge amoun t\n",
      " of RAM. This is especially true during training, beca use the reverse pass of backproƒ\n",
      " paga tion requires all the in termedia te values com puted during the for ward pass.\n",
      " F or exam ple, consider a con volutional la yer with 5 „ 5 filters, outputting 200 fea ture\n",
      " ma ps of size 150 „ 100, with stride 1 and SAME padding. If the in put is a 150 „ 100\n",
      " Convolutional Layer  |  441\n",
      "\n",
      "7\n",
      " A fully connected la yer with 150 „ 100 neurons, each connected to all 150 „ 100 „ 3 in puts, would ha ve 150\n",
      "2\n",
      "„ 100\n",
      "2\n",
      " „ 3 = 675 million parameters!\n",
      "8\n",
      " In the in terna tional system of units (SI), 1 MB = 1,000 kB = 1,000 „ 1,000 bytes = 1,000 „ 1,000 „ 8 bits.\n",
      " RGB image (three channels), then the n umber of parameters is (5 „ 5 „ 3 + 1) „ 200\n",
      " = 15,200 (the +1 corresponds to the bias terms), which is fairly small com pared to a\n",
      " fully connected la yer .\n",
      "7\n",
      " \n",
      " H owever , each of the 200 fea ture ma ps con tains 150 „ 100 neuƒ\n",
      " rons, and each of these neurons needs to com pute a weigh ted sum of its 5 „ 5 „ 3 =\n",
      " 75 in puts: tha t ‡ s a total of 225 million floa t m ultiplica tions. N ot as bad as a fully conƒ\n",
      " nected la yer , but still quite com puta tionally in tensive. M oreover , if the fea ture ma ps\n",
      " are represen ted using 32-bit floa ts, then the con volutional la yer‡ s output will occupy\n",
      "200 „ 150 „ 100 „ 32 = 96 million bits (12 MB) of RAM.\n",
      "8\n",
      " \n",
      " And tha t ‡ s just for one\n",
      " instance! If a training ba tch con tains 100 instances, then this la yer will use up 1.2 GB\n",
      "of RAM!\n",
      "During inference (i.e., when making a prediction for a new instance) the RAM occuƒ\n",
      " pied by one la yer can be released as soon as the next la yer has been com puted, so you\n",
      " only need as m uch RAM as required by two consecutive la yers. But during training\n",
      " ever ything com puted during the for ward pass needs to be preser ved for the reverse\n",
      " pass, so the amoun t of RAM needed is (a t least) the total amoun t of RAM required by\n",
      " all la yers.\n",
      " If training crashes beca use of an out-of-memor y error , you can tr y\n",
      " reducing the mini-ba tch size. Alterna tively , you can tr y reducing\n",
      " dimensionality using a stride, or removing a few la yers. Or you can\n",
      " tr y using 16-bit floa ts instead of 32-bit floa ts. Or you could distribƒ\n",
      " ute the CNN across m ultiple devices.\n",
      " N ow let ‡ s look a t the second common building block of CNN s: the \n",
      " p o o l i n g l a y er\n",
      ".\n",
      "Pooling Layer\n",
      " Once you understand how con volutional la yers work, the pooling la yers are quite\n",
      " easy to grasp . Their goal is to \n",
      " s u b s a m p l e\n",
      "  (i.e., shrink) the in put image in order to\n",
      " reduce the com puta tional load, the memor y usage, and the n umber of parameters\n",
      "(thereby limiting the risk of overfitting).\n",
      " J ust like in con volutional la yers, each neuron in a pooling la yer is connected to the\n",
      " outputs of a limited n umber of neurons in the previous la yer , loca ted within a small\n",
      " rectangular receptive field. Y ou m ust define its size, the stride, and the padding type,\n",
      " just like before. H owever , a pooling neuron has no weigh ts; all it does is aggrega te the\n",
      " in puts using an aggrega tion function such as the max or mean. \n",
      "Figure 14-8\n",
      " \n",
      "shows a\n",
      " m ax p o o l i n g l a y er\n",
      " , which is the most common type of pooling la yer . In this exam ple,\n",
      " 442  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "9\n",
      " Other kernels we discussed so far had weigh ts, but pooling kernels do not: they are just sta teless sliding winƒ\n",
      "dows.\n",
      "we use a 2 „ 2 _pooling kernel_\n",
      "9\n",
      ", with a stride of 2, and no padding. Only the max\n",
      " in put value in each receptive field makes it to the next la yer , while the other in puts\n",
      " are dropped. F or exam ple, in the lower left receptive field in \n",
      "Figure 14-8\n",
      " , the in put\n",
      " values are 1, 5, 3, 2, so only the max value, 5, is propaga ted to the next la yer . B eca use\n",
      " of the stride of 2, the output image has half the heigh t and half the width of the in put\n",
      "image (rounded down since we use no padding).\n",
      " F i g u r e 14-8. M ax p o o l i n g l a y er (2 ł 2 p o o l i n g k er n e l, s t r i d e 2, n o p a d d i n g)\n",
      " A pooling la yer typically works on ever y in put channel independƒ\n",
      " en tly , so the output depth is the same as the in put depth.\n",
      " Other than reducing com puta tions, memor y usage and the n umber of parameters, a\n",
      " max pooling la yer also in troduces some level of \n",
      " i n v a r i a n c e\n",
      "  to small transla tions, as\n",
      "shown in \n",
      "Figure 14-9\n",
      " . H ere we assume tha t the brigh t pixels ha ve a lower value than\n",
      " dark pixels, and we consider 3 images (A, B , C) going through a max pooling la yer\n",
      "with a 2 „ 2 kernel and stride 2. Images B and C are the same as image A, but shifted\n",
      " by one and two pixels to the righ t. As you can see, the outputs of the max pooling\n",
      " la yer for images A and B are iden tical. This is wha t transla tion in variance means.\n",
      " H owever , for image C, the output is differen t: it is shifted by one pixel to the righ t\n",
      " (but there is still 75% in variance). By inserting a max pooling la yer ever y few la yers in\n",
      " a CNN, it is possible to get some level of transla tion in variance a t a larger scale.\n",
      " M oreover , max pooling also offers a small amoun t of rota tional in variance and a\n",
      " sligh t scale in variance. Such in variance (even if it is limited) can be useful in cases\n",
      " where the prediction should not depend on these details, such as in classifica tion\n",
      "tasks.\n",
      " Pooling Layer  |  443\n",
      "\n",
      " F i g u r e 14-9. I n v a r i a n c e t o s m a l l t r a ns l a t i o ns\n",
      " But max pooling has some downsides: firstly , it is obviously ver y destructive: even\n",
      " with a tin y 2 „ 2 kernel and a stride of 2, the output will be two times smaller in both\n",
      " directions (so its area will be four times smaller), sim ply dropping 75% of the in put\n",
      " values. And in some a pplica tions, in variance is not desirable, for exam ple for \n",
      " s em a n‡\n",
      " t i c s e g m en t a t i o n\n",
      " : this is the task of classif ying each pixel in an image depending on the\n",
      " object tha t pixel belongs to: obviously , if the in put image is transla ted by 1 pixel to the\n",
      " righ t, the output should also be transla ted by 1 pixel to the righ t. The goal in this case\n",
      "is \n",
      " e q u i v a r i a n c e\n",
      " , not in variance: a small change to the in puts should lead to a correƒ\n",
      "sponding small change in the output.\n",
      "TensorFlow Implementation\n",
      " Im plemen ting a max pooling la yer in T ensorFlow is quite easy . The following code\n",
      " crea tes a max pooling la yer using a 2 „ 2 kernel. The strides defa ult to the kernel size,\n",
      " so this la yer will use a stride of 2 (both horizon tally and vertically). By defa ult, it uses\n",
      " V ALID padding (i.e., no padding a t all):\n",
      "max_pool\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "MaxPool2D\n",
      "(\n",
      "pool_size\n",
      "=\n",
      "2\n",
      ")\n",
      " T o crea te an \n",
      " a v er a ge p o o l i n g l a y er\n",
      ", just use \n",
      "AvgPool2D\n",
      " instead of \n",
      "MaxPool2D\n",
      ". As you\n",
      " migh t expect, it works exactly like a max pooling la yer , except it com putes the mean\n",
      " ra ther than the max. A verage pooling la yers used to be ver y popular , but people\n",
      " 444  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " mostly use max pooling la yers now , as they generally perform better . This ma y seem\n",
      " surprising, since com puting the mean generally loses less informa tion than com putƒ\n",
      " ing the max. But on the other hand, max pooling preser ves only the strongest fea ture,\n",
      " getting rid of all the meaningless ones, so the next la yers get a cleaner signal to work\n",
      " with. M oreover , max pooling offers stronger transla tion in variance than a verage\n",
      "pooling.\n",
      " N ote tha t max pooling and a verage pooling can be performed along the depth dimenƒ\n",
      " sion ra ther than the spa tial dimensions, although this is not as common. This can\n",
      " allow the CNN to learn to be in varian t to various fea tures. F or exam ple, it could learn\n",
      " m ultiple filters, each detecting a differen t rota tion of the same pa ttern, such as hand-\n",
      "written digits (see \n",
      "Figure 14-10\n",
      " ), and the depth-wise max pooling la yer would ensure\n",
      " tha t the output is the same regardless of the rota tion. The CNN could similarly learn\n",
      " to be in varian t to an ything else: thickness, brigh tness, skew , color , and so on.\n",
      " F i g u r e 14-10. D e p t h-w i s e m ax p o o l i n g c a n h e l p t h e CNN l e a r n a n y i n v a r i a n c e\n",
      " Pooling Layer  |  445\n",
      "\n",
      " K eras does not include a depth-wise max pooling la yer , but T ensorFlow‡ s low-level\n",
      "Deep Learning API does: just use the \n",
      "tf.nn.max_pool()\n",
      "  function, and specif y the\n",
      "kernel size and strides as 4-tuples. The first three values of each should be 1: this indiƒ\n",
      " ca tes tha t the kernel size and stride along the ba tch, heigh t and width dimensions\n",
      " shoud be 1. The last value should be wha tever kernel size and stride you wan t along\n",
      " the depth dimension, for exam ple 3 (this m ust be a divisor of the in put depth; for\n",
      " exam ple, it will not work if the previous la yer outputs 20 fea ture ma ps, since 20 is not\n",
      " a m ultiple of 3):\n",
      "output\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "nn\n",
      ".\n",
      "max_pool\n",
      "(\n",
      "images\n",
      ",\n",
      "                        \n",
      "ksize\n",
      "=\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "3\n",
      "),\n",
      "                        \n",
      "strides\n",
      "=\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "3\n",
      "),\n",
      "                        \n",
      "padding\n",
      "=\n",
      "\"VALID\"\n",
      ")\n",
      " If you wan t to include this as a la yer in your K eras models, you can sim ply wra p it in\n",
      "a \n",
      "Lambda\n",
      "  la yer (or crea te a custom K eras la yer):\n",
      "depth_pool\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Lambda\n",
      "(\n",
      "    \n",
      "lambda\n",
      " \n",
      "X\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "nn\n",
      ".\n",
      "max_pool\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "ksize\n",
      "=\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "3\n",
      "),\n",
      " \n",
      "strides\n",
      "=\n",
      "(\n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "3\n",
      "),\n",
      "                             \n",
      "padding\n",
      "=\n",
      "\"VALID\"\n",
      "))\n",
      " One last type of pooling la yer tha t you will often see in modern architectures is the\n",
      " gl o b a l a v er a ge p o o l i n g\n",
      "  la yer . I t works ver y differen tly : all it does is com pute the mean\n",
      " of each en tire fea ture ma p (it ‡ s like an a verage pooling la yer using a pooling kernel\n",
      " with the same spa tial dimensions as the in puts). This means tha t it just outputs a sinƒ\n",
      " gle n umber per fea ture ma p and per instance. Although this is of course extremely\n",
      " destructive (most of the informa tion in the fea ture ma p is lost), it can be useful as the\n",
      " output la yer , as we will see la ter in this cha pter . T o crea te such a la yer , sim ply use the\n",
      "keras.layers.GlobalAvgPool2D\n",
      " class:\n",
      "global_avg_pool\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "GlobalAvgPool2D\n",
      "()\n",
      " I t is actually equivalen t to this sim ple \n",
      "Lamba\n",
      " \n",
      " la yer , which com putes the mean over the\n",
      " spa tial dimensions (heigh t and width):\n",
      "global_avg_pool\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Lambda\n",
      "(\n",
      "lambda\n",
      " \n",
      "X\n",
      ":\n",
      " \n",
      "tf\n",
      ".\n",
      "reduce_mean\n",
      "(\n",
      "X\n",
      ",\n",
      " \n",
      "axis\n",
      "=\n",
      "[\n",
      "1\n",
      ",\n",
      " \n",
      "2\n",
      "]))\n",
      " N ow you know all the building blocks to crea te a con volutional neural network. Let ‡ s\n",
      "see how to assemble them.\n",
      "CNN Architectures\n",
      " T ypical CNN architectures stack a few con volutional la yers (each one generally folƒ\n",
      " lowed by a ReL U la yer), then a pooling la yer , then another few con volutional la yers\n",
      " (+ReL U), then another pooling la yer , and so on. The image gets smaller and smaller\n",
      "as it progresses through the network, but it also typically gets deeper and deeper (i.e.,\n",
      " with more fea ture ma ps) thanks to the con volutional la yers (see \n",
      "Figure 14-11\n",
      " ). A t the\n",
      " top of the stack, a regular feedfor ward neural network is added, com posed of a few\n",
      " 446  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " fully connected la yers (+ReL U s), and the final la yer outputs the prediction (e.g., a\n",
      " softmax la yer tha t outputs estima ted class probabilities).\n",
      " F i g u r e 14-11. T y p i c a l CNN a r c h i t e c t u r e\n",
      " A common mistake is to use con volution kernels tha t are too large.\n",
      " F or exam ple, instead of using a con volutional la yer with a 5 „ 5\n",
      " kernel, it is generally preferable to stack two la yers with 3 „ 3 kerƒ\n",
      " nels: it will use less parameters and require less com puta tions, and\n",
      " it will usually perform better . One exception to this recommendaƒ\n",
      " tion is for the first con volutional la yer : it can typically ha ve a large\n",
      "kernel (e.g., 5 „ 5), usually with stride of 2 or more: this will reduce\n",
      " the spa tial dimension of the image without losing too m uch inforƒ\n",
      " ma tion, and since the in put image only has 3 channels in general, it\n",
      " will not be too costly .\n",
      " H ere is how you can im plemen t a sim ple CNN to tackle the fashion MNIST da taset\n",
      " (in troduced in \n",
      " Cha pter 10\n",
      "):\n",
      "from\n",
      " \n",
      "functools\n",
      " \n",
      "import\n",
      " \n",
      "partial\n",
      "DefaultConv2D\n",
      " \n",
      "=\n",
      " \n",
      "partial\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Conv2D\n",
      ",\n",
      "                        \n",
      "kernel_size\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "•relu•\n",
      ",\n",
      " \n",
      "padding\n",
      "=\n",
      "\"SAME\"\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "([\n",
      "    \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      "=\n",
      "64\n",
      ",\n",
      " \n",
      "kernel_size\n",
      "=\n",
      "7\n",
      ",\n",
      " \n",
      "input_shape\n",
      "=\n",
      "[\n",
      "28\n",
      ",\n",
      " \n",
      "28\n",
      ",\n",
      " \n",
      "1\n",
      "]),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "MaxPooling2D\n",
      "(\n",
      "pool_size\n",
      "=\n",
      "2\n",
      "),\n",
      "    \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      "=\n",
      "128\n",
      "),\n",
      "    \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      "=\n",
      "128\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "MaxPooling2D\n",
      "(\n",
      "pool_size\n",
      "=\n",
      "2\n",
      "),\n",
      "    \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      "=\n",
      "256\n",
      "),\n",
      "    \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      "=\n",
      "256\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "MaxPooling2D\n",
      "(\n",
      "pool_size\n",
      "=\n",
      "2\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "(),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "units\n",
      "=\n",
      "128\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "•relu•\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dropout\n",
      "(\n",
      "0.5\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "units\n",
      "=\n",
      "64\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "•relu•\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dropout\n",
      "(\n",
      "0.5\n",
      "),\n",
      "    \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "units\n",
      "=\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "•softmax•\n",
      "),\n",
      "])\n",
      " CNN Architectures  |  447\n",
      "\n",
      "⁄\n",
      "In this code, we start by using the \n",
      "partial()\n",
      "  function to define a thin wra pper\n",
      "around the \n",
      "Conv2D\n",
      " class, called \n",
      "DefaultConv2D\n",
      " : it sim ply a voids ha ving to repea t\n",
      " the same h yperparameter values over and over again.\n",
      "⁄\n",
      " The first la yer uses a large kernel size, but no stride beca use the in put images are\n",
      " not ver y large. I t also sets \n",
      "input_shape=[28, 28, 1]\n",
      ", which means the images\n",
      " are 28 „ 28 pixels, with a single color channel (i.e., gra yscale).\n",
      "⁄\n",
      " N ext, we ha ve a max pooling la yer , which divides each spa tial dimension by a facƒ\n",
      "tor of two (since \n",
      "pool_size=2\n",
      ").\n",
      "⁄\n",
      " Then we repea t the same structure twice: two con volutional la yers followed by a\n",
      " max pooling la yer . F or larger images, we could repea t this structure several times\n",
      " (the n umber of repetitions is a h yperparameter you can tune).\n",
      "⁄\n",
      " N ote tha t the n umber of filters grows as we climb up the CNN towards the outƒ\n",
      " put la yer (it is initially 64, then 128, then 256): it makes sense for it to grow , since\n",
      " the n umber of low level fea tures is often fairly low (e.g., small circles, horizon tal\n",
      " lines, etc.), but there are man y differen t wa ys to combine them in to higher level\n",
      " fea tures. I t is a common practice to double the n umber of filters after each poolƒ\n",
      " ing la yer : since a pooling la yer divides each spa tial dimension by a factor of 2, we\n",
      " can afford doubling the n umber of fea ture ma ps in the next la yer , without fear of\n",
      " exploding the n umber of parameters, memor y usage, or com puta tional load.\n",
      "⁄\n",
      " N ext is the fully connected network, com posed of 2 hidden dense la yers and a\n",
      " dense output la yer . N ote tha t we m ust fla tten its in puts, since a dense network\n",
      " expects a 1D arra y of fea tures for each instance. W e also add two dropout la yers,\n",
      " with a dropout ra te of 50% each, to reduce overfitting.\n",
      " This CNN reaches over 92% accuracy on the test set. I t ‡ s not the sta te of the art, but it\n",
      " is pretty good, and clearly m uch better than wha t we achieved with dense networks in\n",
      " Cha pter 10\n",
      ".\n",
      " O ver the years, varian ts of this fundamen tal architecture ha ve been developed, leadƒ\n",
      " ing to amazing advances in the field. A good measure of this progress is the error ra te\n",
      " in com petitions such as the ILSVRC \n",
      " ImageN et challenge\n",
      " . In this com petition the\n",
      " top-5 error ra te for \n",
      " image classifica tion fell from over 26% to less than 2.3% in just six\n",
      " years. The top-five error ra te is the n umber of test images for which the system ‡ s top 5\n",
      " predictions did not include the correct answer . The images are large (256 pixels high)\n",
      " and there are 1,000 classes, some of which are really subtle (tr y distinguishing 120\n",
      " dog breeds). Looking a t the evolution of the winning en tries is a good wa y to underƒ\n",
      " stand how CNN s work.\n",
      " W e will first look a t the classical LeN et-5 architecture (1998), then three of the winƒ\n",
      " ners of the ILSVRC challenge: AlexN et (2012), GoogLeN et (2014), and ResN et\n",
      "(2015).\n",
      " 448  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "10\n",
      " — Gradien t-Based Learning A pplied to Documen t Recognition – , Y . LeCun, L. B ottou, Y . B engio and P . H affner\n",
      "(1998).\n",
      "LeNet-5\n",
      "The \n",
      " LeN et-5 architecture\n",
      "10\n",
      "  is perha ps the most widely known CNN architecture. As\n",
      " men tioned earlier , it was crea ted by Y ann LeCun in 1998 and widely used for handƒ\n",
      " written digit recognition (MNIST). I t is com posed of the la yers shown in \n",
      " T able 14-1\n",
      ".\n",
      " T a b l e 14-1. L eN e t-5 a r c h i t e c t u r e\n",
      "Layer\n",
      "Type\n",
      "Maps\n",
      "Size\n",
      " Kernel   size\n",
      "Stride\n",
      "Activation\n",
      "Out\n",
      " Fully   Connected\n",
      "–\n",
      "10\n",
      "–\n",
      "–\n",
      "RBF\n",
      "F6\n",
      " Fully   Connected\n",
      "–\n",
      "84\n",
      "–\n",
      "–\n",
      "tanh\n",
      "C5\n",
      "Convolution\n",
      "120\n",
      " 1   ‡   1\n",
      " 5   ‡   5\n",
      "1\n",
      "tanh\n",
      "S4\n",
      " Avg   Pooling\n",
      "16\n",
      " 5   ‡   5\n",
      " 2   ‡   2\n",
      "2\n",
      "tanh\n",
      "C3\n",
      "Convolution\n",
      "16\n",
      " 10   ‡   10\n",
      " 5   ‡   5\n",
      "1\n",
      "tanh\n",
      "S2\n",
      " Avg   Pooling\n",
      "6\n",
      " 14   ‡   14\n",
      " 2   ‡   2\n",
      "2\n",
      "tanh\n",
      "C1\n",
      "Convolution\n",
      "6\n",
      " 28   ‡   28\n",
      " 5   ‡   5\n",
      "1\n",
      "tanh\n",
      "In\n",
      "Input\n",
      "1\n",
      " 32   ‡   32\n",
      "–\n",
      "–\n",
      "–\n",
      "There are a few extra details to be noted:\n",
      "⁄\n",
      "MNIST images are 28 „ 28 pixels, but they are zero-padded to 32 „ 32 pixels and\n",
      "normalized before being fed to the network. The rest of the network does not use\n",
      " an y padding, which is wh y the size keeps shrinking as the image progresses\n",
      "through the network.\n",
      "⁄\n",
      " The a verage pooling la yers are sligh tly more com plex than usual: each neuron\n",
      " com putes the mean of its in puts, then m ultiplies the result by a learnable coeffiƒ\n",
      " cien t (one per ma p) and adds a learnable bias term (again, one per ma p), then\n",
      " finally a pplies the activa tion function.\n",
      "⁄\n",
      " M ost neurons in C3 ma ps are connected to neurons in only three or four S2\n",
      " ma ps (instead of all six S2 ma ps). See table 1 (page 8) in the original pa per\n",
      "10\n",
      " \n",
      "for\n",
      "details.\n",
      "⁄\n",
      " The output la yer is a bit special: instead of com puting the ma trix m ultiplica tion\n",
      " of the in puts and the weigh t vector , each neuron outputs the square of the E ucliƒ\n",
      " dian distance between its in put vector and its weigh t vector . Each output measƒ\n",
      " ures how m uch the image belongs to a particular digit class. The cross en tropy \n",
      " cost function is now preferred, as it penalizes bad predictions m uch more, proƒ\n",
      " ducing larger gradien ts and con verging faster .\n",
      " CNN Architectures  |  449\n",
      "\n",
      "11\n",
      " —ImageN et Classifica tion with Deep Con volutional N eural N etworks, – A. Krizhevsky et al. (2012).\n",
      " Y ann LeCun ‡ s \n",
      "website\n",
      " \n",
      " (—LENET – section) fea tures grea t demos of LeN et-5 classif ying \n",
      "digits.\n",
      "AlexNet\n",
      "The\n",
      " \n",
      " Al exN e t\n",
      " CNN architecture\n",
      "11\n",
      "  won the 2012 ImageN et ILSVRC challenge by a\n",
      " large margin: it achieved 17% top-5 error ra te while the second best achieved only\n",
      " 26%! I t was developed by Alex Krizhevsky (hence the name), Ilya Sutskever , and\n",
      " Geoffrey Hin ton. I t is quite similar to LeN et-5, only m uch larger and deeper , and it\n",
      " was the first to stack con volutional la yers directly on top of each other , instead of\n",
      " stacking a pooling la yer on top of each con volutional la yer . \n",
      " T able 14-2\n",
      " \n",
      " presen ts this\n",
      "architecture.\n",
      " T a b l e 14-2. Al exN e t a r c h i t e c t u r e\n",
      "Layer\n",
      "Type\n",
      "Maps\n",
      "Size\n",
      " Kernel   size\n",
      "Stride\n",
      "Padding\n",
      "Activation\n",
      "Out\n",
      " Fully   Connected\n",
      "–\n",
      "1,000\n",
      "–\n",
      "–\n",
      "–\n",
      "Softmax\n",
      "F9\n",
      " Fully   Connected\n",
      "–\n",
      "4,096\n",
      "–\n",
      "–\n",
      "–\n",
      "ReLU\n",
      "F8\n",
      " Fully   Connected\n",
      "–\n",
      "4,096\n",
      "–\n",
      "–\n",
      "–\n",
      "ReLU\n",
      "C7\n",
      "Convolution\n",
      "256\n",
      " 13   ‡   13\n",
      " 3   ‡   3\n",
      "1\n",
      "SAME\n",
      "ReLU\n",
      "C6\n",
      "Convolution\n",
      "384\n",
      " 13   ‡   13\n",
      " 3   ‡   3\n",
      "1\n",
      "SAME\n",
      "ReLU\n",
      "C5\n",
      "Convolution\n",
      "384\n",
      " 13   ‡   13\n",
      " 3   ‡   3\n",
      "1\n",
      "SAME\n",
      "ReLU\n",
      "S4\n",
      " Max   Pooling\n",
      "256\n",
      " 13   ‡   13\n",
      " 3   ‡   3\n",
      "2\n",
      "VALID\n",
      "–\n",
      "C3\n",
      "Convolution\n",
      "256\n",
      " 27   ‡   27\n",
      " 5   ‡   5\n",
      "1\n",
      "SAME\n",
      "ReLU\n",
      "S2\n",
      " Max   Pooling\n",
      "96\n",
      " 27   ‡   27\n",
      " 3   ‡   3\n",
      "2\n",
      "VALID\n",
      "–\n",
      "C1\n",
      "Convolution\n",
      "96\n",
      " 55   ‡   55\n",
      " 11   ‡   11\n",
      "4\n",
      "VALID\n",
      "ReLU\n",
      "In\n",
      "Input\n",
      " 3   (RGB)\n",
      " 227   ‡   227\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      " T o reduce overfitting, the a uthors used two regulariza tion techniques: first they\n",
      " a pplied dropout (in troduced in \n",
      " Cha pter 11\n",
      " ) with a 50% dropout ra te during training\n",
      " to the outputs of la yers F8 and F9. Second, they performed \n",
      " d a t a a u g m en t a t i o n\n",
      "   by ranƒ\n",
      " domly shifting the training images by various offsets, flipping them horizon tally , and\n",
      " changing the ligh ting conditions.\n",
      "Data Augmentation\n",
      " Da ta a ugmen ta tion artificially increases the size of the training set by genera ting\n",
      " man y realistic varian ts of each training instance. This reduces overfitting, making this\n",
      " a regulariza tion technique. The genera ted instances should be as realistic as possible:\n",
      " 450  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " ideally , given an image from the a ugmen ted training set, a h uman should not be able\n",
      " to tell whether it was a ugmen ted or not. M oreover , sim ply adding white noise will not\n",
      " help; the modifica tions should be learnable (white noise is not).\n",
      " F or exam ple, you can sligh tly shift, rota te, and resize ever y picture in the training set\n",
      " by various amoun ts and add the resulting pictures to the training set (see\n",
      "Figure 14-12\n",
      " ). This forces the model to be more toleran t to varia tions in the position,\n",
      " orien ta tion, and size of the objects in the pictures. If you wan t the model to be more\n",
      " toleran t to differen t ligh ting conditions, you can similarly genera te man y images with\n",
      " various con trasts. In general, you can also flip the pictures horizon tally (except for\n",
      " text, and other non-symmetrical objects). By combining these transforma tions you\n",
      " can grea tly increase the size of your training set.\n",
      " F i g u r e 14-12. G en er a t i n g n e w t r a i n i n g i ns t a n c e s f r o m exi s t i n g o n e s\n",
      " AlexN et also uses a com petitive normaliza tion step immedia tely after the ReL U step\n",
      " of la yers C1 and C3, called \n",
      " l o c a l r e s p o ns e n o r m a l iz a t i o n\n",
      " . The most strongly activa ted\n",
      " neurons inhibit other neurons loca ted a t the same position in neighboring fea ture\n",
      " ma ps (such com petitive activa tion has been obser ved in biological neurons). This\n",
      " encourages differen t fea ture ma ps to specialize, pushing them a part and forcing them\n",
      " CNN Architectures  |  451\n",
      "\n",
      "12\n",
      " — Going Deeper with Con volutions, – C. Szegedy et al. (2015).\n",
      "13\n",
      "In the 2010 movie \n",
      " I n c e p t i o n\n",
      " , the characters keep going deeper and deeper in to m ultiple la yers of dreams,\n",
      "hence the name of these modules.\n",
      " to explore a wider range of fea tures, ultima tely im proving generaliza tion. \n",
      " Equa tion\n",
      "14-2\n",
      "  shows how to a pply LRN.\n",
      " Eq u a t i o n 14-2. L o c a l r e s p o ns e n o r m a l iz a t i o n\n",
      "b\n",
      "i\n",
      "=\n",
      "a\n",
      "i\n",
      "k\n",
      "+\n",
      "‰\n",
      "“\n",
      "j\n",
      "=\n",
      "j\n",
      "low\n",
      "j\n",
      "high\n",
      "a\n",
      "j\n",
      "2\n",
      "”\n",
      "Ÿ\n",
      "with\n",
      "j\n",
      "high\n",
      " = min\n",
      "i\n",
      "+\n",
      "r\n",
      "2\n",
      ",\n",
      "f\n",
      "n\n",
      " ” 1\n",
      "j\n",
      "low\n",
      " = max\n",
      "0,\n",
      "i\n",
      "”\n",
      "r\n",
      "2\n",
      "⁄\n",
      "b\n",
      "i\n",
      " \n",
      " is the normalized output of the neuron loca ted in fea ture ma p \n",
      "i\n",
      " , a t some row \n",
      "u\n",
      "and column \n",
      "v\n",
      " \n",
      " (note tha t in this equa tion we consider only neurons loca ted a t this\n",
      "row and column, so \n",
      "u\n",
      " and \n",
      "v\n",
      " are not shown).\n",
      "⁄\n",
      "a\n",
      "i\n",
      "  is the activa tion of tha t neuron after the ReL U step , but before normaliza tion.\n",
      "⁄\n",
      "k\n",
      ", \n",
      "‰\n",
      ", \n",
      "Ÿ\n",
      ", and \n",
      "r\n",
      "  are h yperparameters. \n",
      "k\n",
      " is called the \n",
      " b i as\n",
      ", and \n",
      "r\n",
      " is called the \n",
      " d e p t h\n",
      " r a d i u s\n",
      ".\n",
      "⁄\n",
      "f\n",
      "n\n",
      "  is the n umber of fea ture ma ps.\n",
      " F or exam ple, if \n",
      "r\n",
      " \n",
      " = 2 and a neuron has a strong activa tion, it will inhibit the activa tion\n",
      " of the neurons loca ted in the fea ture ma ps immedia tely above and below its own.\n",
      " In AlexN et, the h yperparameters are set as follows: \n",
      "r\n",
      " \n",
      "= 2, \n",
      "‰\n",
      " \n",
      "= 0.00002, \n",
      "Ÿ\n",
      " \n",
      "= 0.75, and \n",
      "k\n",
      " = 1. This step can be im plemen ted using the \n",
      "tf.nn.local_response_normaliza\n",
      "tion()\n",
      "  function (which you can wra p in a \n",
      "Lambda\n",
      "  la yer if you wan t to use it in a\n",
      " K eras model).\n",
      " A varian t of AlexN et called \n",
      " ZF N e t\n",
      " \n",
      " was developed by M a tthew Zeiler and Rob F ergus\n",
      " and won the 2013 ILSVRC challenge. I t is essen tially AlexN et with a few tweaked \n",
      " h yperparameters (n umber of fea ture ma ps, kernel size, stride, etc.).\n",
      "GoogLeNet\n",
      "The \n",
      " GoogLeN et architecture\n",
      " was developed by Christian Szegedy et al. from Google\n",
      "Research,\n",
      "12\n",
      "  and it won the ILSVRC 2014 challenge by pushing the top-5 error ra te\n",
      " below 7%. This grea t performance came in large part from the fact tha t the network\n",
      " was m uch deeper than previous CNN s (see \n",
      "Figure 14-14\n",
      "). This was made possible by\n",
      "sub-networks called \n",
      " i n c e p t i o n m o d u l e s\n",
      ",\n",
      "13\n",
      "  which allow GoogLeN et to use parameters\n",
      " 452  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " m uch more efficien tly than previous architectures: GoogLeN et actually has 10 times\n",
      " fewer parameters than AlexN et (roughly 6 million instead of 60 million).\n",
      "Figure 14-13\n",
      "  shows the architecture of an inception module. The nota tion —3 „ 3 +\n",
      " 1(S)– means tha t the la yer uses a 3 „ 3 kernel, stride 1, and SAME padding. The in put\n",
      " signal is first copied and fed to four differen t la yers. All con volutional la yers use the\n",
      " ReL U activa tion function. N ote tha t the second set of con volutional la yers uses differƒ\n",
      " en t kernel sizes (1 „ 1, 3 „ 3, and 5 „ 5), allowing them to ca pture pa tterns a t differen t\n",
      " scales. Also note tha t ever y single la yer uses a stride of 1 and SAME padding (even\n",
      " the max pooling la yer), so their outputs all ha ve the same heigh t and width as their\n",
      " in puts. This makes it possible to conca tena te all the outputs along the depth dimenƒ\n",
      "sion in the final \n",
      " d e p t h c o n c a t l a y er\n",
      "  (i.e., stack the fea ture ma ps from all four top conƒ\n",
      " volutional la yers). This conca tena tion la yer can be im plemen ted in T ensorFlow using\n",
      "the \n",
      "tf.concat()\n",
      "  opera tion, with \n",
      "axis=3\n",
      " (axis 3 is the depth).\n",
      " F i g u r e 14-13. I n c e p t i o n m o d u l e\n",
      " Y ou ma y wonder wh y inception modules ha ve con volutional la yers with 1 „ 1 kerƒ\n",
      " nels. Surely these la yers cannot ca pture an y fea tures since they look a t only one pixel\n",
      " a t a time? In fact, these la yers ser ve three purposes:\n",
      "⁄\n",
      " First, although they cannot ca pture spa tial pa tterns, they can ca pture pa tterns\n",
      "along the depth dimension.\n",
      "⁄\n",
      " Second, they are configured to output fewer fea ture ma ps than their in puts, so\n",
      " they ser ve \n",
      "as \n",
      " b o tt l en e c k l a y er s\n",
      " , meaning they reduce dimensionality . This cuts the\n",
      " com puta tional cost and the n umber of parameters, speeding up training and\n",
      " im proving generaliza tion.\n",
      "⁄\n",
      " Lastly , each pair of con volutional la yers ([1 „ 1, 3 „ 3] and [1 „ 1, 5 „ 5]) acts like\n",
      " a single, powerful con volutional la yer , ca pable of ca pturing more com plex pa tƒ\n",
      " terns. Indeed, instead of sweeping a sim ple linear classifier across the image (as a\n",
      " CNN Architectures  |  453\n",
      "\n",
      " single con volutional la yer does), this pair of con volutional la yers sweeps a two-\n",
      " la yer neural network across the image.\n",
      " In short, you can think of the whole inception module as a con volutional la yer on\n",
      " steroids, able to output fea ture ma ps tha t ca pture com plex pa tterns a t various scales.\n",
      " The n umber of con volutional kernels for each con volutional la yer\n",
      " is a h yperparameter . U nfortuna tely , this means tha t you ha ve six\n",
      " more h yperparameters to tweak for ever y inception la yer you add.\n",
      " N ow let ‡ s look a t the architecture of the GoogLeN et CNN (see \n",
      "Figure 14-14\n",
      "). The\n",
      " n umber of fea ture ma ps output by each con volutional la yer and each pooling la yer is\n",
      " shown before the kernel size. The architecture is so deep tha t it has to be represen ted\n",
      " in three columns, but GoogLeN et is actually one tall stack, including nine inception\n",
      " modules (the boxes with the spinning tops). The six n umbers in the inception modƒ\n",
      " ules represen t the n umber of fea ture ma ps output by each con volutional la yer in the\n",
      "module (in the same order as in \n",
      "Figure 14-13\n",
      " ). N ote tha t all the con volutional la yers\n",
      " use the ReL U activa tion function.\n",
      " 454  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " F i g u r e 14-14. G o ogL eN e t a r c h i t e c t u r e\n",
      " Let ‡ s go through this network:\n",
      "⁄\n",
      " The first two la yers divide the image ‡ s heigh t and width by 4 (so its area is divided\n",
      " by 16), to reduce the com puta tional load. The first la yer uses a large kernel size,\n",
      " so tha t m uch of the informa tion is still preser ved.\n",
      "⁄\n",
      " Then the local response normaliza tion la yer ensures tha t the previous la yers learn\n",
      " a wide variety of fea tures (as discussed earlier).\n",
      "⁄\n",
      " T wo con volutional la yers follow , where the first acts like a \n",
      " b o tt l en e c k l a y er\n",
      ". As\n",
      " explained earlier , you can think of this pair as a single smarter con volutional\n",
      " la yer .\n",
      "⁄\n",
      " Again, a local response normaliza tion la yer ensures tha t the previous la yers ca pƒ\n",
      " ture a wide variety of pa tterns.\n",
      " CNN Architectures  |  455\n",
      "\n",
      "14\n",
      " —V er y Deep Con volutional N etworks for Large-Scale Image Recognition, – K. Simon yan and A. Zisserman\n",
      "(2015).\n",
      "⁄\n",
      " N ext a max pooling la yer reduces the image heigh t and width by 2, again to speed\n",
      " up com puta tions.\n",
      "⁄\n",
      " Then comes the tall stack of nine inception modules, in terlea ved with a couple\n",
      " max pooling la yers to reduce dimensionality and speed up the net.\n",
      "⁄\n",
      " N ext, the global a verage pooling la yer sim ply outputs the mean of each fea ture\n",
      " ma p: this drops an y remaining spa tial informa tion, which is fine since there was\n",
      " not m uch spa tial informa tion left a t tha t poin t. Indeed, GoogLeN et in put images\n",
      " are typically expected to be 224 „ 224 pixels, so after 5 max pooling la yers, each\n",
      " dividing the heigh t and width by 2, the fea ture ma ps are down to 7 „ 7. M oreƒ\n",
      " over , it is a classifica tion task, not localiza tion, so it does not ma tter where the\n",
      " object is. Thanks to the dimensionality reduction brough t by this la yer , there is\n",
      " no need to ha ve several fully connected la yers a t the top of the CNN (like in\n",
      " AlexN et), and this considerably reduces the n umber of parameters in the netƒ\n",
      "work and limits the risk of overfitting.\n",
      "⁄\n",
      " The last la yers are self-explana tor y : dropout for regulariza tion, then a fully conƒ\n",
      " nected la yer with 1,000 units, since there are a 1,000 classes, and a softmax actiƒ\n",
      " va tion function to output estima ted class probabilities.\n",
      " This diagram is sligh tly sim plified: the original GoogLeN et architecture also included\n",
      " two a uxiliar y classifiers plugged on top of the third and sixth inception modules.\n",
      " They were both com posed of one a verage pooling la yer , one con volutional la yer , two\n",
      " fully connected la yers, and a softmax activa tion la yer . During training, their loss\n",
      " (scaled down by 70%) was added to the overall loss. The goal was to figh t the vanishƒ\n",
      " ing gradien ts problem and regularize the network. H owever , it was la ter shown tha t\n",
      " their effect was rela tively minor .\n",
      " Several varian ts of the GoogLeN et architecture were la ter proposed by Google\n",
      " researchers, including Inception-v3 and Inception-v4, using sligh tly differen t incepƒ\n",
      "tion modules, and reaching even better performance.\n",
      "VGGNet\n",
      "The runner up in the ILSVRC 2014 challenge was\n",
      " \n",
      " V GGN et\n",
      "14\n",
      ", developed by K. Simonƒ\n",
      " yan and A. Zisserman. I t had a ver y sim ple and classical architecture, with 2 or 3 conƒ\n",
      " volutional la yers, a pooling la yer , then again 2 or 3 con volutional la yers, a pooling\n",
      " la yer , and so on (with a total of just 16 con volutional la yers), plus a final dense netƒ\n",
      " work with 2 hidden la yers and the output la yer . I t used only 3 „ 3 filters, but man y\n",
      "filters.\n",
      " 456  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "15\n",
      " —Deep Residual Learning for Image Recognition, – K. H e (2015).\n",
      "ResNet\n",
      "The ILSVRC 2015 challenge was won using a \n",
      " R e s i d u a l N e tw o r k\n",
      " \n",
      "(or \n",
      " R e sN e t\n",
      "), develƒ\n",
      " oped by Kaiming H e et al.,\n",
      "15\n",
      "  which delivered an astounding top-5 error ra te under\n",
      " 3.6%, using an extremely deep CNN com posed of 152 la yers. I t confirmed the general\n",
      " trend: models are getting deeper and deeper , with fewer and fewer parameters. The\n",
      "key to being able to train such a deep network is to use \n",
      " s k i p c o n n e c t i o ns\n",
      " \n",
      "(also called\n",
      " s h o r t cu t c o n n e c t i o ns\n",
      " ): the signal feeding in to a la yer is also added to the output of a\n",
      " la yer loca ted a bit higher up the stack. Let ‡ s see wh y this is useful.\n",
      "When training a neural network, the goal is to make it model a target function \n",
      "h\n",
      "(\n",
      "x\n",
      ").\n",
      " If you add the in put \n",
      "x\n",
      " to the output of the network (i.e., you add a skip connection),\n",
      "then the network will be forced to model \n",
      "f\n",
      "(\n",
      "x\n",
      ") = \n",
      "h\n",
      "(\n",
      "x\n",
      ") − \n",
      "x\n",
      "  ra ther than \n",
      "h\n",
      "(\n",
      "x\n",
      "). This is\n",
      "called \n",
      " r e s i d u a l l e a r n i n g\n",
      " (see \n",
      "Figure 14-15\n",
      ").\n",
      " F i g u r e 14-15. R e s i d u a l l e a r n i n g\n",
      " When you initialize a regular neural network, its weigh ts are close to zero , so the netƒ\n",
      " work just outputs values close to zero . If you add a skip connection, the resulting netƒ\n",
      " work just outputs a copy of its in puts; in other words, it initially models the iden tity\n",
      " function. If the target function is fairly close to the iden tity function (which is often\n",
      " the case), this will speed up training considerably .\n",
      " M oreover , if you add man y skip connections, the network can start making progress\n",
      " even if several la yers ha ve not started learning yet (see \n",
      "Figure 14-16\n",
      "). Thanks to skip\n",
      " connections, the signal can easily make its wa y across the whole network. The deep\n",
      "residual network can be seen as a stack of \n",
      " r e s i d u a l u n i ts\n",
      ", \n",
      "where each residual unit is a\n",
      "small neural network with a skip connection.\n",
      " CNN Architectures  |  457\n",
      "\n",
      " F i g u r e 14-16. R e g u l a r d e e p n eu r a l n e tw o r k \n",
      "(le“)\n",
      "  a n d d e e p r e s i d u a l n e tw o r k (r i gh t)\n",
      " N ow let ‡ s look a t ResN et ‡ s architecture (see \n",
      "Figure 14-17\n",
      " ). I t is actually surprisingly\n",
      " sim ple. I t starts and ends exactly like GoogLeN et (except without a dropout la yer),\n",
      " and in between is just a ver y deep stack of sim ple residual units. Each residual unit is\n",
      " com posed of two con volutional la yers (and no pooling la yer!), with \n",
      " Ba tch N ormalizaƒ\n",
      " tion (BN) and ReL U activa tion, using 3 „ 3 kernels and preser ving spa tial dimensions\n",
      "(stride 1, SAME padding).\n",
      " F i g u r e 14-17. R e sN e t a r c h i t e c t u r e\n",
      " N ote tha t the n umber of fea ture ma ps is doubled ever y few residual units, a t the same\n",
      " time as their heigh t and width are halved (using a con volutional la yer with stride 2).\n",
      " When this ha ppens the in puts cannot be added directly to the outputs of the residual\n",
      " unit since they don ‡ t ha ve the same sha pe (for exam ple, this problem affects the skip\n",
      " 458  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "16\n",
      " —Inception-v4, Inception-ResN et and the Im pact of Residual Connections on Learning, – C. Szegedy et al.\n",
      "(2016).\n",
      "17\n",
      " — Xception: Deep Learning with Depth wise Separable Con volutions, – Fran‹ois Chollet (2016)\n",
      " connection represen ted by the dashed arrow in \n",
      "Figure 14-17\n",
      " ). T o solve this problem,\n",
      " the in puts are passed through a 1 „ 1 con volutional la yer with stride 2 and the righ t\n",
      " n umber of output fea ture ma ps (see \n",
      "Figure 14-18\n",
      ").\n",
      " F i g u r e 14-18. S k i p c o n n e c t i o n w h en c h a n g i n g f e a t u r e m a p s iz e a n d d e p t h\n",
      " ResN et-34 is the ResN et with 34 la yers (only coun ting the con volutional la yers and\n",
      " the fully connected la yer) con taining three residual units tha t output 64 fea ture ma ps,\n",
      " 4 R U s with 128 ma ps, 6 R U s with 256 ma ps, and 3 R U s with 512 ma ps. W e will im pleƒ\n",
      " men t this architecture la ter in this cha pter .\n",
      " ResN ets deeper than tha t, such as ResN et-152, use sligh tly differen t residual units.\n",
      " Instead of two 3 „ 3 con volutional la yers with (sa y) 256 fea ture ma ps, they use three\n",
      " con volutional la yers: first a 1 „ 1 con volutional la yer with just 64 fea ture ma ps (4\n",
      " times less), which acts as a bottleneck la yer (as discussed already), then a 3 „ 3 la yer\n",
      " with 64 fea ture ma ps, and finally another 1 „ 1 con volutional la yer with 256 fea ture\n",
      " ma ps (4 times 64) tha t restores the original depth. ResN et-152 con tains three such\n",
      " R U s tha t output 256 ma ps, then 8 R U s with 512 ma ps, a whopping 36 R U s with 1,024\n",
      " ma ps, and finally 3 R U s with 2,048 ma ps.\n",
      " Google ‡ s \n",
      "Inception-v4\n",
      "16\n",
      " architecture merged the ideas of GoogLeƒ\n",
      " N et and ResN et and achieved close to 3% top-5 error ra te on\n",
      " ImageN et classifica tion.\n",
      "Xception\n",
      " Another varian t of the GoogLeN et architecture is also worth noting: \n",
      "Xception\n",
      "17\n",
      "(which stands for \n",
      " E xt r em e I n c e p t i o n\n",
      ") was proposed in 2016 by Fran‹ois Chollet (the\n",
      " CNN Architectures  |  459\n",
      "\n",
      "18\n",
      " This name can sometimes be ambiguous, since spa tially separable con volutions are often called — separable\n",
      " con volutions – as well.\n",
      " a uthor of K eras), and it significan tly outperformed Inception-v3 on a h uge vision task\n",
      " (350 million images and 17,000 classes). J ust like Inception-v4, it also merges the\n",
      " ideas of GoogLeN et and ResN et, but it replaces the inception modules with a special\n",
      " type of la yer called a \n",
      " d e p t h w i s e s e p a r a b l e c o n v o l u t i o n\n",
      " \n",
      "(or \n",
      " s e p a r a b l e c o n v o l u t i o n\n",
      " \n",
      "for\n",
      "short\n",
      "18\n",
      " ). These la yers had been used before in some CNN architectures, but they were\n",
      " not as cen tral as in the Xception architecture. While a regular con volutional la yer\n",
      " uses filters tha t tr y to sim ultaneously ca pture spa tial pa tterns (e.g., an oval) and cross-\n",
      " channel pa tterns (e.g., mouth + nose + eyes = face), a separable con volutional la yer\n",
      " makes the strong assum ption tha t spa tial pa tterns and cross-channel pa tterns can be\n",
      " modeled separa tely (see \n",
      "Figure 14-19\n",
      " ). Th us, it is com posed of two parts: the first part\n",
      " a pplies a single spa tial filter for each in put fea ture ma p , then the second part looks\n",
      " exclusively for cross-channel pa tterns›it is just a regular con volutional la yer with 1 „\n",
      "1 filters.\n",
      " F i g u r e 14-19. D e p t h w i s e S e p a r a b l e C o n v o l u t i o n a l L a y er\n",
      " Since separable con volutional la yers only ha ve one spa tial filter per in put channel,\n",
      " you should a void using them after la yers tha t ha ve too few channels, such as the in put\n",
      " la yer (gran ted, tha t ‡ s wha t \n",
      "Figure 14-19\n",
      "  represen ts, but it is just for illustra tion purƒ\n",
      " poses). F or this reason, the Xception architecture starts with 2 regular con volutional\n",
      " la yers, but then the rest of the architecture uses only separable con volutions (34 in\n",
      " 460  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "19\n",
      " — Crafting GBD-N et for Object Detection, – X. Zeng et al. (2016).\n",
      "20\n",
      " — Squeeze-and-Excita tion N etworks, – J ie H u et al. (2017)\n",
      " all), plus a few max pooling la yers and the usual final la yers (a global a verage pooling\n",
      " la yer , and a dense output la yer).\n",
      " Y ou migh t wonder wh y Xception is considered a varian t of GoogLeN et, since it conƒ\n",
      " tains no inception module a t all? W ell, as we discussed earlier , an Inception module\n",
      " con tains con volutional la yers with 1 „ 1 filters: these look exclusively for cross-\n",
      " channel pa tterns. H owever , the con volution la yers tha t sit on top of them are regular\n",
      " con volutional la yers tha t look both for spa tial and cross-channel pa tterns. So you can\n",
      " think of an Inception module as an in termedia te between a regular con volutional\n",
      " la yer (which considers spa tial pa tterns and cross-channel pa tterns join tly) and a sepaƒ\n",
      " rable con volutional la yer (which considers them separa tely). In practice, it seems tha t\n",
      " separable con volutions generally perform better .\n",
      " Separable con volutions use less parameters, less memor y and less\n",
      " com puta tions than regular con volutional la yers, and in general\n",
      " they even perform better , so you should consider using them by\n",
      " defa ult (except after la yers with few channels).\n",
      " The ILSVRC 2016 challenge was won by the CUImage team from the Chinese U niƒ\n",
      " versity of H ong K ong. They used an ensemble of man y differen t techniques, includƒ\n",
      " ing a sophistica ted object-detection system called \n",
      " GBD-N et\n",
      "19\n",
      ", to achieve a top-5 error\n",
      " ra te below 3%. Although this result is unquestionably im pressive, the com plexity of\n",
      " the solution con trasted with the sim plicity of ResN ets. M oreover , one year la ter\n",
      " another fairly sim ple architecture performed even better , as we will see now .\n",
      "SENet\n",
      "The winning architecture in the ILSVRC 2017 challenge was the \n",
      "Squeeze-and-\n",
      " Excita tion N etwork\n",
      "   (SEN et)\n",
      "20\n",
      ". This architecture extends existing architectures such as\n",
      " inception networks or ResN ets, and boosts their performance. This allowed SEN et to\n",
      " win the com petition with an astonishing 2.25% top-5 error ra te! The extended verƒ\n",
      " sions of inception networks and ResN et are called \n",
      " S E-I n c e p t i o n\n",
      "   and \n",
      " S E-R e sN e t\n",
      "   respecƒ\n",
      " tively . The boost comes from the fact tha t a SEN et adds a small neural network, called\n",
      "a \n",
      " S E B l o c k\n",
      " , to ever y unit in the original architecture (i.e., ever y inception module or\n",
      " ever y residual unit), as shown in \n",
      "Figure 14-20\n",
      ".\n",
      " CNN Architectures  |  461\n",
      "\n",
      " F i g u r e 14-20. S E-I n c e p t i o n M o d u l e \n",
      "(le“)\n",
      "  a n d S E-R e sN e t U n i t (r i gh t)\n",
      " A SE Block analyzes the output of the unit it is a ttached to , focusing exclusively on\n",
      " the depth dimension (it does not look for an y spa tial pa ttern), and it learns which feaƒ\n",
      " tures are usually most active together . I t then uses this informa tion to recalibra te the\n",
      " fea ture ma ps, as shown in \n",
      "Figure 14-21\n",
      " . F or exam ple, a SE Block ma y learn tha t\n",
      " mouths, noses and eyes usually a ppear together in pictures: if you see a mouth and a\n",
      " nose, you should expect to see eyes as well. So if a SE Block sees a strong activa tion in\n",
      " the mouth and nose fea ture ma ps, but only mild activa tion in the eye fea ture ma p , it\n",
      " will boost the eye fea ture ma p (more accura tely , it will reduce irrelevan t fea ture\n",
      " ma ps). If the eyes were somewha t confused with something else, this fea ture ma p\n",
      " recalibra tion will help resolve the ambiguity .\n",
      " 462  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " F i g u r e 14-21. An S E B l o c k P er f o r ms F e a t u r e M a p R e c a l i b r a t i o n\n",
      " A SE Block is com posed of just 3 la yers: a global a verage pooling la yer , a hidden dense\n",
      " la yer using the ReL U activa tion function, and a dense output la yer using the sigmoid\n",
      " activa tion function (see \n",
      "Figure 14-22\n",
      "):\n",
      " F i g u r e 14-22. S E B l o c k Ar c h i t e c t u r e\n",
      " CNN Architectures  |  463\n",
      "\n",
      " As earlier , the global a verage pooling la yer com putes the mean activa tion for each feaƒ\n",
      " ture ma p: for exam ple, if its in put con tains 256 fea ture ma ps, it will output 256 n umƒ\n",
      " bers represen ting the overall level of response for each filter . The next la yer is where\n",
      " the — squeeze – ha ppens: this la yer has m uch less than 256 neurons, typically 16 times\n",
      " less than the n umber of fea ture ma ps (e.g., 16 neurons), so the 256 n umbers get comƒ\n",
      " pressed in to a small vector (e.g., 16 dimensional). This is a low-dimensional vector\n",
      " represen ta tion (i.e., an embedding) of the distribution of fea ture responses. This botƒ\n",
      " tleneck step forces the SE Block to learn a general represen ta tion of the fea ture comƒ\n",
      " bina tions (we will see this principle in action again when we discuss a utoencoders\n",
      "in \n",
      "???\n",
      " ). Finally , the output la yer takes the embedding and outputs a recalibra tion vecƒ\n",
      " tor con taining one n umber per fea ture ma p (e.g., 256), each between 0 and 1. The\n",
      " fea ture ma ps are then m ultiplied by this recalibra tion vector , so irrelevan t fea tures\n",
      " (with a low recalibra tion score) get scaled down while relevan t fea tures (with a recaliƒ\n",
      " bra tion score close to 1) are left alone.\n",
      "Implementing a ResNet-34 CNN Using Keras\n",
      " M ost CNN architectures described so far are fairly straigh tfor ward to im plemen t\n",
      " (although generally you would load a pretrained network instead, as we will see). T o\n",
      " illustra te the process, let ‡ s im plemen t a ResN et-34 from scra tch using K eras. First, let ‡ s\n",
      " crea te a \n",
      "ResidualUnit\n",
      "  la yer :\n",
      "DefaultConv2D\n",
      " \n",
      "=\n",
      " \n",
      "partial\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Conv2D\n",
      ",\n",
      " \n",
      "kernel_size\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "1\n",
      ",\n",
      "                        \n",
      "padding\n",
      "=\n",
      "\"SAME\"\n",
      ",\n",
      " \n",
      "use_bias\n",
      "=\n",
      "False\n",
      ")\n",
      "class\n",
      " \n",
      "ResidualUnit\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Layer\n",
      "):\n",
      "    \n",
      "def\n",
      " \n",
      "__init__\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "filters\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"relu\"\n",
      ",\n",
      " \n",
      "**\n",
      "kwargs\n",
      "):\n",
      "        \n",
      "super\n",
      "()\n",
      ".\n",
      "__init__\n",
      "(\n",
      "**\n",
      "kwargs\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "activation\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "activations\n",
      ".\n",
      "get\n",
      "(\n",
      "activation\n",
      ")\n",
      "        \n",
      "self\n",
      ".\n",
      "main_layers\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "            \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "strides\n",
      "),\n",
      "            \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "(),\n",
      "            \n",
      "self\n",
      ".\n",
      "activation\n",
      ",\n",
      "            \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      "),\n",
      "            \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "()]\n",
      "        \n",
      "self\n",
      ".\n",
      "skip_layers\n",
      " \n",
      "=\n",
      " \n",
      "[]\n",
      "        \n",
      "if\n",
      " \n",
      "strides\n",
      " \n",
      ">\n",
      " \n",
      "1\n",
      ":\n",
      "            \n",
      "self\n",
      ".\n",
      "skip_layers\n",
      " \n",
      "=\n",
      " \n",
      "[\n",
      "                \n",
      "DefaultConv2D\n",
      "(\n",
      "filters\n",
      ",\n",
      " \n",
      "kernel_size\n",
      "=\n",
      "1\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "strides\n",
      "),\n",
      "                \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "()]\n",
      "    \n",
      "def\n",
      " \n",
      "call\n",
      "(\n",
      "self\n",
      ",\n",
      " \n",
      "inputs\n",
      "):\n",
      "        \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "        \n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "self\n",
      ".\n",
      "main_layers\n",
      ":\n",
      "            \n",
      "Z\n",
      " \n",
      "=\n",
      " \n",
      "layer\n",
      "(\n",
      "Z\n",
      ")\n",
      "        \n",
      "skip_Z\n",
      " \n",
      "=\n",
      " \n",
      "inputs\n",
      "        \n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "self\n",
      ".\n",
      "skip_layers\n",
      ":\n",
      " 464  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "            \n",
      "skip_Z\n",
      " \n",
      "=\n",
      " \n",
      "layer\n",
      "(\n",
      "skip_Z\n",
      ")\n",
      "        \n",
      "return\n",
      " \n",
      "self\n",
      ".\n",
      "activation\n",
      "(\n",
      "Z\n",
      " \n",
      "+\n",
      " \n",
      "skip_Z\n",
      ")\n",
      " As you can see, this code ma tches \n",
      "Figure 14-18\n",
      "  pretty closely . In the constructor , we\n",
      " crea te all the la yers we will need: the main la yers are the ones on the righ t side of the\n",
      " diagram, and the skip la yers are the ones on the left (only needed if the stride is\n",
      " grea ter than 1). Then in the \n",
      "call()\n",
      "  method, we sim ply make the in puts go through\n",
      " the main la yers, and the skip la yers (if an y), then we add both outputs and we a pply\n",
      " the activa tion function.\n",
      " N ext, we can build the ResN et-34 sim ply using a \n",
      "Sequential\n",
      " model, since it is really\n",
      " just a long sequence of la yers (we can trea t each residual unit as a single la yer now\n",
      " tha t we ha ve the \n",
      "ResidualUnit\n",
      " class):\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Sequential\n",
      "()\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "DefaultConv2D\n",
      "(\n",
      "64\n",
      ",\n",
      " \n",
      "kernel_size\n",
      "=\n",
      "7\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "2\n",
      ",\n",
      "                        \n",
      "input_shape\n",
      "=\n",
      "[\n",
      "224\n",
      ",\n",
      " \n",
      "224\n",
      ",\n",
      " \n",
      "3\n",
      "]))\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "BatchNormalization\n",
      "())\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Activation\n",
      "(\n",
      "\"relu\"\n",
      "))\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "MaxPool2D\n",
      "(\n",
      "pool_size\n",
      "=\n",
      "3\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "2\n",
      ",\n",
      " \n",
      "padding\n",
      "=\n",
      "\"SAME\"\n",
      "))\n",
      "prev_filters\n",
      " \n",
      "=\n",
      " \n",
      "64\n",
      "for\n",
      " \n",
      "filters\n",
      " \n",
      "in\n",
      " \n",
      "[\n",
      "64\n",
      "]\n",
      " \n",
      "*\n",
      " \n",
      "3\n",
      " \n",
      "+\n",
      " \n",
      "[\n",
      "128\n",
      "]\n",
      " \n",
      "*\n",
      " \n",
      "4\n",
      " \n",
      "+\n",
      " \n",
      "[\n",
      "256\n",
      "]\n",
      " \n",
      "*\n",
      " \n",
      "6\n",
      " \n",
      "+\n",
      " \n",
      "[\n",
      "512\n",
      "]\n",
      " \n",
      "*\n",
      " \n",
      "3\n",
      ":\n",
      "    \n",
      "strides\n",
      " \n",
      "=\n",
      " \n",
      "1\n",
      " \n",
      "if\n",
      " \n",
      "filters\n",
      " \n",
      "==\n",
      " \n",
      "prev_filters\n",
      " \n",
      "else\n",
      " \n",
      "2\n",
      "    \n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "ResidualUnit\n",
      "(\n",
      "filters\n",
      ",\n",
      " \n",
      "strides\n",
      "=\n",
      "strides\n",
      "))\n",
      "    \n",
      "prev_filters\n",
      " \n",
      "=\n",
      " \n",
      "filters\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "GlobalAvgPool2D\n",
      "())\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Flatten\n",
      "())\n",
      "model\n",
      ".\n",
      "add\n",
      "(\n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "10\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      "))\n",
      " The only sligh tly tricky part in this code is the loop tha t adds the \n",
      "ResidualUnit\n",
      "   la yers\n",
      " to the model: as explained earlier , the first 3 R U s ha ve 64 filters, then the next 4 R U s\n",
      " ha ve 128 filters, and so on. W e then set the strides to 1 when the n umber of filters is\n",
      " the same as in the previous R U , or else we set it to 2. Then we add the \n",
      "ResidualUnit\n",
      ",\n",
      " and finally we upda te \n",
      "prev_filters\n",
      ".\n",
      " I t is quite amazing tha t in less than 40 lines of code, we can build the model tha t won\n",
      " the ILSVRC 2015 challenge! I t demonstra tes both the elegance of the ResN et model,\n",
      " and the expressiveness of the K eras API. Im plemen ting the other CNN architectures\n",
      " is not m uch harder . H owever , K eras comes with several of these architectures built in,\n",
      " so wh y not use them instead?\n",
      "Using Pretrained Models From Keras\n",
      " In general, you won ‡ t ha ve to im plemen t standard models like GoogLeN et or ResN et\n",
      " man ually , since pretrained networks are readily a vailable with a single line of code, in\n",
      "the \n",
      "keras.applications\n",
      "  package. F or exam ple:\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "applications\n",
      ".\n",
      "resnet50\n",
      ".\n",
      "ResNet50\n",
      "(\n",
      "weights\n",
      "=\n",
      "\"imagenet\"\n",
      ")\n",
      " Using Pretrained Models From Keras  |  465\n",
      "\n",
      "21\n",
      " In the ImageN et da taset, each image is associa ted to a word in the \n",
      " W ordN et da taset\n",
      ": the class ID is just a\n",
      " W ordN et ID .\n",
      " Tha t ‡ s all! This will crea te a ResN et-50 model and download weigh ts pretrained on\n",
      " the ImageN et da taset. T o use it, you first need to ensure tha t the images ha ve the righ t\n",
      " size. A ResN et-50 model expects 224 „ 224 images (other models ma y expect other\n",
      " sizes, such as 299 „ 299), so let ‡ s use T ensorFlow‡ s \n",
      "tf.image.resize()\n",
      " \n",
      "function to\n",
      " resize the images we loaded earlier :\n",
      "images_resized\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "image\n",
      ".\n",
      "resize\n",
      "(\n",
      "images\n",
      ",\n",
      " \n",
      "[\n",
      "224\n",
      ",\n",
      " \n",
      "224\n",
      "])\n",
      "The \n",
      "tf.image.resize()\n",
      " \n",
      " will not preser ve the aspect ra tio . If this is\n",
      " a problem, you can tr y cropping the images to the a ppropria te\n",
      " aspect ra tio before resizing. B oth opera tions can be done in one\n",
      "shot with \n",
      "tf.image.crop_and_resize()\n",
      ".\n",
      " The pretrained models assume tha t the images are preprocessed in a specific wa y . In\n",
      " some cases they ma y expect the in puts to be scaled from 0 to 1, or -1 to 1, and so on.\n",
      "Each model provides a \n",
      "preprocess_input()\n",
      "  function tha t you can use to preprocess\n",
      " your images. These functions assume tha t the pixel values range from 0 to 255, so we\n",
      " m ust m ultiply them by 255 (since earlier we scaled them to the 0−1 range):\n",
      "inputs\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "applications\n",
      ".\n",
      "resnet50\n",
      ".\n",
      "preprocess_input\n",
      "(\n",
      "images_resized\n",
      " \n",
      "*\n",
      " \n",
      "255\n",
      ")\n",
      " N ow we can use the pretrained model to make predictions:\n",
      "Y_proba\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "predict\n",
      "(\n",
      "inputs\n",
      ")\n",
      "As usual, the output \n",
      "Y_proba\n",
      " \n",
      " is a ma trix with one row per image and one column per\n",
      " class (in this case, there are 1,000 classes). If you wan t to displa y the top K predicƒ\n",
      " tions, including the class name and the estima ted probability of each predicted class,\n",
      "you can use the \n",
      "decode_predictions()\n",
      "  function. F or each image, it returns an arra y\n",
      " con taining the top K predictions, where each prediction is represen ted as an arra y\n",
      " con taining the class iden tifier\n",
      "21\n",
      ", its name and the corresponding confidence score:\n",
      "top_K\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "applications\n",
      ".\n",
      "resnet50\n",
      ".\n",
      "decode_predictions\n",
      "(\n",
      "Y_proba\n",
      ",\n",
      " \n",
      "top\n",
      "=\n",
      "3\n",
      ")\n",
      "for\n",
      " \n",
      "image_index\n",
      " \n",
      "in\n",
      " \n",
      "range\n",
      "(\n",
      "len\n",
      "(\n",
      "images\n",
      ")):\n",
      "    \n",
      "print\n",
      "(\n",
      "\"Image #{}\"\n",
      ".\n",
      "format\n",
      "(\n",
      "image_index\n",
      "))\n",
      "    \n",
      "for\n",
      " \n",
      "class_id\n",
      ",\n",
      " \n",
      "name\n",
      ",\n",
      " \n",
      "y_proba\n",
      " \n",
      "in\n",
      " \n",
      "top_K\n",
      "[\n",
      "image_index\n",
      "]:\n",
      "        \n",
      "print\n",
      "(\n",
      "\"  {} - {:12s} {:.2f}%\"\n",
      ".\n",
      "format\n",
      "(\n",
      "class_id\n",
      ",\n",
      " \n",
      "name\n",
      ",\n",
      " \n",
      "y_proba\n",
      " \n",
      "*\n",
      " \n",
      "100\n",
      "))\n",
      "    \n",
      "print\n",
      "()\n",
      "The output looks like this:\n",
      "Image #0\n",
      "  n03877845 - palace       42.87%\n",
      "  n02825657 - bell_cote    40.57%\n",
      "  n03781244 - monastery    14.56%\n",
      " 466  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "Image #1\n",
      "  n04522168 - vase         46.83%\n",
      "  n07930864 - cup          7.78%\n",
      "  n11939491 - daisy        4.87%\n",
      " The correct classes (monaster y and daisy) a ppear in the top 3 results for both images.\n",
      " Tha t ‡ s pretty good considering tha t the model had to choose among 1,000 classes.\n",
      " As you can see, it is ver y easy to crea te a pretty good image classifier using a preƒ\n",
      " trained model. Other vision models are a vailable in \n",
      "keras.applications\n",
      ", including\n",
      " several ResN et varian ts, GoogLeN et varian ts like InceptionV3 and Xception,\n",
      " V GGN et varian ts, M obileN et and M obileN etV2 (ligh tweigh t models for use in\n",
      " mobile a pplica tions), and more.\n",
      " But wha t if you wan t to use an image classifier for classes of images tha t are not part\n",
      " of ImageN et? In tha t case, you ma y still benefit from the pretrained models to perƒ\n",
      "form transfer learning.\n",
      "Pretrained Models for Transfer Learning\n",
      " If you wan t to build an image classifier , but you do not ha ve enough training da ta,\n",
      " then it is often a good idea to reuse the lower la yers of a pretrained model, as we disƒ\n",
      "cussed in \n",
      " Cha pter 11\n",
      " . F or exam ple, let ‡ s train a model to classif y pictures of flowers,\n",
      " reusing a pretrained Xception model. First, let ‡ s load the da taset using T ensorFlow\n",
      " Da tasets (see \n",
      " Cha pter 13\n",
      "):\n",
      "import\n",
      " \n",
      "tensorflow_datasets\n",
      " \n",
      "as\n",
      " \n",
      "tfds\n",
      "dataset\n",
      ",\n",
      " \n",
      "info\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "load\n",
      "(\n",
      "\"tf_flowers\"\n",
      ",\n",
      " \n",
      "as_supervised\n",
      "=\n",
      "True\n",
      ",\n",
      " \n",
      "with_info\n",
      "=\n",
      "True\n",
      ")\n",
      "dataset_size\n",
      " \n",
      "=\n",
      " \n",
      "info\n",
      ".\n",
      "splits\n",
      "[\n",
      "\"train\"\n",
      "]\n",
      ".\n",
      "num_examples\n",
      " \n",
      "# 3670\n",
      "class_names\n",
      " \n",
      "=\n",
      " \n",
      "info\n",
      ".\n",
      "features\n",
      "[\n",
      "\"label\"\n",
      "]\n",
      ".\n",
      "names\n",
      " \n",
      "# [\"dandelion\", \"daisy\", ...]\n",
      "n_classes\n",
      " \n",
      "=\n",
      " \n",
      "info\n",
      ".\n",
      "features\n",
      "[\n",
      "\"label\"\n",
      "]\n",
      ".\n",
      "num_classes\n",
      " \n",
      "# 5\n",
      " N ote tha t you can get informa tion about the da taset by setting \n",
      "with_info=True\n",
      " . H ere,\n",
      " we get the da taset size and the names of the classes. U nfortuna tely , there is only a\n",
      "\"train\"\n",
      "  da taset, no test set or valida tion set, so we need to split the training set. The\n",
      " TF Da tasets project provides an API for this. F or exam ple, let ‡ s take the first 10% of\n",
      " the da taset for testing, the next 15% for valida tion, and the remaining 75% for trainƒ\n",
      "ing:\n",
      "test_split\n",
      ",\n",
      " \n",
      "valid_split\n",
      ",\n",
      " \n",
      "train_split\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "Split\n",
      ".\n",
      "TRAIN\n",
      ".\n",
      "subsplit\n",
      "([\n",
      "10\n",
      ",\n",
      " \n",
      "15\n",
      ",\n",
      " \n",
      "75\n",
      "])\n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "load\n",
      "(\n",
      "\"tf_flowers\"\n",
      ",\n",
      " \n",
      "split\n",
      "=\n",
      "test_split\n",
      ",\n",
      " \n",
      "as_supervised\n",
      "=\n",
      "True\n",
      ")\n",
      "valid_set\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "load\n",
      "(\n",
      "\"tf_flowers\"\n",
      ",\n",
      " \n",
      "split\n",
      "=\n",
      "valid_split\n",
      ",\n",
      " \n",
      "as_supervised\n",
      "=\n",
      "True\n",
      ")\n",
      "train_set\n",
      " \n",
      "=\n",
      " \n",
      "tfds\n",
      ".\n",
      "load\n",
      "(\n",
      "\"tf_flowers\"\n",
      ",\n",
      " \n",
      "split\n",
      "=\n",
      "train_split\n",
      ",\n",
      " \n",
      "as_supervised\n",
      "=\n",
      "True\n",
      ")\n",
      " Pretrained Models for Transfer Learning  |  467\n",
      "\n",
      " N ext we m ust preprocess the images. The CNN expects 224 „ 224 images, so we need\n",
      " to resize them. W e also need to run the image through Xception ‡ s \n",
      "prepro\n",
      "cess_input()\n",
      " function:\n",
      "def\n",
      " \n",
      "preprocess\n",
      "(\n",
      "image\n",
      ",\n",
      " \n",
      "label\n",
      "):\n",
      "    \n",
      "resized_image\n",
      " \n",
      "=\n",
      " \n",
      "tf\n",
      ".\n",
      "image\n",
      ".\n",
      "resize\n",
      "(\n",
      "image\n",
      ",\n",
      " \n",
      "[\n",
      "224\n",
      ",\n",
      " \n",
      "224\n",
      "])\n",
      "    \n",
      "final_image\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "applications\n",
      ".\n",
      "xception\n",
      ".\n",
      "preprocess_input\n",
      "(\n",
      "resized_image\n",
      ")\n",
      "    \n",
      "return\n",
      " \n",
      "final_image\n",
      ",\n",
      " \n",
      "label\n",
      " Let ‡ s a pply this preprocessing function to all 3 da tasets, and let ‡ s also sh uffle & repea t\n",
      " the training set, and add ba tching & prefetching to all da tasets:\n",
      "batch_size\n",
      " \n",
      "=\n",
      " \n",
      "32\n",
      "train_set\n",
      " \n",
      "=\n",
      " \n",
      "train_set\n",
      ".\n",
      "shuffle\n",
      "(\n",
      "1000\n",
      ")\n",
      ".\n",
      "repeat\n",
      "()\n",
      "train_set\n",
      " \n",
      "=\n",
      " \n",
      "train_set\n",
      ".\n",
      "map\n",
      "(\n",
      "preprocess\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "batch_size\n",
      ")\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      "valid_set\n",
      " \n",
      "=\n",
      " \n",
      "valid_set\n",
      ".\n",
      "map\n",
      "(\n",
      "preprocess\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "batch_size\n",
      ")\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      "test_set\n",
      " \n",
      "=\n",
      " \n",
      "test_set\n",
      ".\n",
      "map\n",
      "(\n",
      "preprocess\n",
      ")\n",
      ".\n",
      "batch\n",
      "(\n",
      "batch_size\n",
      ")\n",
      ".\n",
      "prefetch\n",
      "(\n",
      "1\n",
      ")\n",
      " If you wan t to perform some da ta a ugmen ta tion, you can just change the preprocessƒ\n",
      " ing function for the training set, adding some random transforma tions to the training\n",
      " images. F or exam ple, use \n",
      "tf.image.random_crop()\n",
      " \n",
      "to randomly crop the images, use\n",
      "tf.image.random_flip_left_right()\n",
      "  to randomly flip the images horizon tally , and\n",
      " so on (see the notebook for an exam ple).\n",
      " N ext let ‡ s load an Xception model, pretrained on ImageN et. W e exclude the top of the\n",
      "network (by setting \n",
      "include_top=False\n",
      " ): this excludes the global a verage pooling\n",
      " la yer and the dense output la yer . W e then add our own global a verage pooling la yer ,\n",
      " based on the output of the base model, followed by a dense output la yer with 1 unit\n",
      " per class, using the softmax activa tion function. Finally , we crea te the K eras \n",
      "Model\n",
      ":\n",
      "base_model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "applications\n",
      ".\n",
      "xception\n",
      ".\n",
      "Xception\n",
      "(\n",
      "weights\n",
      "=\n",
      "\"imagenet\"\n",
      ",\n",
      "                                                  \n",
      "include_top\n",
      "=\n",
      "False\n",
      ")\n",
      "avg\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "GlobalAveragePooling2D\n",
      "()(\n",
      "base_model\n",
      ".\n",
      "output\n",
      ")\n",
      "output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "n_classes\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ")(\n",
      "avg\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "(\n",
      "inputs\n",
      "=\n",
      "base_model\n",
      ".\n",
      "input\n",
      ",\n",
      " \n",
      "outputs\n",
      "=\n",
      "output\n",
      ")\n",
      "As explained in \n",
      " Cha pter 11\n",
      " , it ‡ s usually a good idea to freeze the weigh ts of the preƒ\n",
      " trained la yers, a t least a t the beginning of training:\n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "base_model\n",
      ".\n",
      "layers\n",
      ":\n",
      "    \n",
      "layer\n",
      ".\n",
      "trainable\n",
      " \n",
      "=\n",
      " \n",
      "False\n",
      " Since our model uses the base model ‡ s la yers directly , ra ther than\n",
      "the \n",
      "base_model\n",
      " object itself, setting \n",
      "base_model.trainable=False\n",
      " would ha ve no effect.\n",
      " Finally , we can com pile the model and start training:\n",
      " 468  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.2\n",
      ",\n",
      " \n",
      "momentum\n",
      "=\n",
      "0.9\n",
      ",\n",
      " \n",
      "decay\n",
      "=\n",
      "0.01\n",
      ")\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "\"sparse_categorical_crossentropy\"\n",
      ",\n",
      " \n",
      "optimizer\n",
      "=\n",
      "optimizer\n",
      ",\n",
      "              \n",
      "metrics\n",
      "=\n",
      "[\n",
      "\"accuracy\"\n",
      "])\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "train_set\n",
      ",\n",
      "                    \n",
      "steps_per_epoch\n",
      "=\n",
      "int\n",
      "(\n",
      "0.75\n",
      " \n",
      "*\n",
      " \n",
      "dataset_size\n",
      " \n",
      "/\n",
      " \n",
      "batch_size\n",
      "),\n",
      "                    \n",
      "validation_data\n",
      "=\n",
      "valid_set\n",
      ",\n",
      "                    \n",
      "validation_steps\n",
      "=\n",
      "int\n",
      "(\n",
      "0.15\n",
      " \n",
      "*\n",
      " \n",
      "dataset_size\n",
      " \n",
      "/\n",
      " \n",
      "batch_size\n",
      "),\n",
      "                    \n",
      "epochs\n",
      "=\n",
      "5\n",
      ")\n",
      " This will be ver y slow , unless you ha ve a GPU . If you do not, then\n",
      " you should run this cha pter‡ s notebook in Colab , using a GPU runƒ\n",
      " time (it ‡ s free!). See the instructions a t \n",
      " h ttp s://g i t h u b .c o m/a ger o n/\n",
      " h a n d s o n-m l2\n",
      ".\n",
      " After training the model for a few epochs, its valida tion accuracy should reach about\n",
      " 75-80%, and stop making m uch progress. This means tha t the top la yers are now\n",
      " pretty well trained, so we are ready to unfreeze all la yers (or you could tr y unfreezing\n",
      " just the top ones), and con tin ue training (don ‡ t forget to com pile the model when you\n",
      " freeze or unfreeze la yers). This time we use a m uch lower learning ra te to a void damƒ\n",
      " aging the pretrained weigh ts:\n",
      "for\n",
      " \n",
      "layer\n",
      " \n",
      "in\n",
      " \n",
      "base_model\n",
      ".\n",
      "layers\n",
      ":\n",
      "    \n",
      "layer\n",
      ".\n",
      "trainable\n",
      " \n",
      "=\n",
      " \n",
      "True\n",
      "optimizer\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "optimizers\n",
      ".\n",
      "SGD\n",
      "(\n",
      "lr\n",
      "=\n",
      "0.01\n",
      ",\n",
      " \n",
      "momentum\n",
      "=\n",
      "0.9\n",
      ",\n",
      " \n",
      "decay\n",
      "=\n",
      "0.001\n",
      ")\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "...\n",
      ")\n",
      "history\n",
      " \n",
      "=\n",
      " \n",
      "model\n",
      ".\n",
      "fit\n",
      "(\n",
      "...\n",
      ")\n",
      " I t will take a while, but this model should reach around 95% accuracy on the test set.\n",
      " W ith tha t, you can start training amazing image classifiers! But there ‡ s more to comƒ\n",
      " puter vision than just classifica tion. F or exam ple, wha t if you also wan t to know \n",
      " w h er e\n",
      " the flower is in the picture? Let ‡ s look a t this now .\n",
      "Classi•cation\n",
      " and Localization\n",
      "Localizing an object in a picture can be expressed as a regression task, as discussed in\n",
      " Cha pter 10\n",
      " : to predict a bounding box around the object, a common a pproach is to\n",
      " predict the horizon tal and vertical coordina tes of the object ‡ s cen ter , as well as its\n",
      " heigh t and width. This means we ha ve 4 n umbers to predict. I t does not require m uch\n",
      " change to the model, we just need to add a second dense output la yer with 4 units\n",
      " (typically on top of the global a verage pooling la yer), and it can be trained using the\n",
      "MSE loss:\n",
      "base_model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "applications\n",
      ".\n",
      "xception\n",
      ".\n",
      "Xception\n",
      "(\n",
      "weights\n",
      "=\n",
      "\"imagenet\"\n",
      ",\n",
      "                                                  \n",
      "include_top\n",
      "=\n",
      "False\n",
      ")\n",
      "avg\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "GlobalAveragePooling2D\n",
      "()(\n",
      "base_model\n",
      ".\n",
      "output\n",
      ")\n",
      "class_output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "n_classes\n",
      ",\n",
      " \n",
      "activation\n",
      "=\n",
      "\"softmax\"\n",
      ")(\n",
      "avg\n",
      ")\n",
      "Classi•cation\n",
      "  and Localization  |  469\n",
      "\n",
      "22\n",
      " — Crowdsourcing in Com puter V ision, – A. K ovashka et al. (2016).\n",
      "loc_output\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "layers\n",
      ".\n",
      "Dense\n",
      "(\n",
      "4\n",
      ")(\n",
      "avg\n",
      ")\n",
      "model\n",
      " \n",
      "=\n",
      " \n",
      "keras\n",
      ".\n",
      "models\n",
      ".\n",
      "Model\n",
      "(\n",
      "inputs\n",
      "=\n",
      "base_model\n",
      ".\n",
      "input\n",
      ",\n",
      "                           \n",
      "outputs\n",
      "=\n",
      "[\n",
      "class_output\n",
      ",\n",
      " \n",
      "loc_output\n",
      "])\n",
      "model\n",
      ".\n",
      "compile\n",
      "(\n",
      "loss\n",
      "=\n",
      "[\n",
      "\"sparse_categorical_crossentropy\"\n",
      ",\n",
      " \n",
      "\"mse\"\n",
      "],\n",
      "              \n",
      "loss_weights\n",
      "=\n",
      "[\n",
      "0.8\n",
      ",\n",
      " \n",
      "0.2\n",
      "],\n",
      " \n",
      "# depends on what you care most about\n",
      "              \n",
      "optimizer\n",
      "=\n",
      "optimizer\n",
      ",\n",
      " \n",
      "metrics\n",
      "=\n",
      "[\n",
      "\"accuracy\"\n",
      "])\n",
      " But now we ha ve a problem: the flowers da taset does not ha ve bounding boxes\n",
      "around the flowers. So we need to add them ourselves. This is often one of the hardƒ\n",
      " est and most costly part of a M achine Learning project: getting the labels. I t ‡ s a good\n",
      " idea to spend time looking for the righ t tools. T o annota te images with bounding\n",
      " boxes, you ma y wan t to use an open source image labeling tool like V GG Image\n",
      " Annota tor , LabelImg, OpenLabeler or ImgLab , or perha ps a commercial tool like\n",
      " LabelB ox or Super visely . Y ou ma y also wan t to consider crowdsourcing pla tforms\n",
      " such as Amazon M echanical T urk or CrowdFlower if you ha ve a ver y large n umber of\n",
      " images to annota te. H owever , it is quite a lot of work to setup a crowdsourcing pla tƒ\n",
      " form, prepare the form to be sen t to the workers, to super vise them and ensure the\n",
      "quality of the bounding boxes they produce is good, so make sure it is worth the\n",
      " effort: if there are just a few thousand images to label, and you don ‡ t plan to do this\n",
      " frequen tly , it ma y be preferable to do it yourself. A driana K ovashka et al. wrote a ver y\n",
      "practical \n",
      " pa per\n",
      "22\n",
      "  about crowdsourcing in Com puter V ision, I recommend you check\n",
      "it out, even if you do not plan to use crowdsourcing.\n",
      " So let ‡ s suppose you obtained the bounding boxes for ever y image in the flowers da taƒ\n",
      "set (for now we will assume there is a single bounding box per image), you then need\n",
      " to crea te a da taset whose items will be ba tches of preprocessed images along with\n",
      "their class labels and their bounding boxes. Each item should be a tuple of the form:\n",
      "(images, (class_labels, bounding_boxes))\n",
      ". Then you are ready to train your\n",
      "model!\n",
      " The bounding boxes should be normalized so tha t the horizon tal\n",
      " and vertical coordina tes, as well as the heigh t and width all range\n",
      " from 0 to 1. Also , it is common to predict the square root of the\n",
      " heigh t and width ra ther than the heigh t and width directly : this\n",
      " wa y , a 10 pixel error for a large bounding box will not be penalized\n",
      " as m uch as a 10 pixel error for a small bounding box.\n",
      "The MSE often works fairly well as a cost function to train the model, but it is not a\n",
      " grea t metric to evalua te how well the model can predict bounding boxes. The most\n",
      " common metric for this is the In tersection over U nion (I oU): it is the area of overla p\n",
      "between the predicted bounding box and the target bounding box, divided by the\n",
      " 470  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "area of their union (see \n",
      "Figure 14-23\n",
      " ). In tf.keras, it is im plemen ted by the\n",
      "tf.keras.metrics.MeanIoU\n",
      " class.\n",
      " F i g u r e 14-23. I n t er s e c t i o n o v er U n i o n (I oU) M e t r i c f o r B o u n d i n g B o x e s\n",
      " Classif ying and localizing a single object is nice, but wha t if the images con tain m ultiƒ\n",
      " ple objects (as is often the case in the flowers da taset)?\n",
      "Object Detection\n",
      " The task of classif ying and localizing m ultiple objects in an image is called \n",
      " o b je c t\n",
      " d e t e c t i o n\n",
      " . U n til a few years ago , a common a pproach was to take a CNN tha t was\n",
      " trained to classif y and loca te a single object, then slide it across the image, as shown\n",
      "in \n",
      "Figure 14-24\n",
      " . In this exam ple, the image was chopped in to a 6 „ 8 grid, and we\n",
      "show a CNN (the thick black rectangle) sliding across all 3 „ 3 regions. When the\n",
      " CNN was looking a t the top left of the image, it detected part of the left-most rose,\n",
      " and then it detected tha t same rose again when it was first shifted one step to the\n",
      " righ t. A t the next step , it started detecting part of the top-most rose, and then it detecƒ\n",
      " ted it again once it was shifted one more step to the righ t. Y ou would then con tin ue to\n",
      " slide the CNN through the whole image, looking a t all 3 „ 3 regions. M oreover , since\n",
      " objects can ha ve var ying sizes, you would also slide the CNN across regions of differƒ\n",
      " en t sizes. F or exam ple, once you are done with the 3 „ 3 regions, you migh t wan t to\n",
      "slide the CNN across all 4 „ 4 regions as well.\n",
      " Object Detection  |  471\n",
      "\n",
      " F i g u r e 14-24. D e t e c t i n g M u l t i p l e O b je c ts b y S l i d i n g a CNN Acr o s s t h e I m a ge\n",
      " This technique is fairly straigh tfor ward, but as you can see it will detect the same\n",
      " object m ultiple times, a t sligh tly differen t positions. Some post-processing will then\n",
      " be needed to get rid of all the unnecessar y bounding boxes. A common a pproach for\n",
      "this is called \n",
      " n o n-m ax s u p p r e s s i o n\n",
      ":\n",
      "⁄\n",
      "First, you need to add an extra \n",
      " o b je c t n e s s\n",
      "  output to your CNN, to estima te the\n",
      " probability tha t a flower is indeed presen t in the image (alterna tively , you could\n",
      " add a — no-flower– class, but this usually does not work as well). I t m ust use the\n",
      " sigmoid activa tion function and you can train it using the \n",
      "\"binary_crossen\n",
      "tropy\"\n",
      " loss. Then just get rid of all the bounding boxes for which the objectness\n",
      " score is below some threshold: this will drop all the bounding boxes tha t don ‡ t\n",
      " actually con tain a flower .\n",
      "⁄\n",
      "Second, find the bounding box with the highest objectness score, and get rid of\n",
      " all the other bounding boxes tha t overla p a lot with it (e.g., with an I oU grea ter\n",
      " than 60%). F or exam ple, in \n",
      "Figure 14-24\n",
      ", the bounding box with the max objectƒ\n",
      "ness score is the thick bounding box over the top-most rose (the objectness score\n",
      " is represen ted by the thickness of the bounding boxes). The other bounding box\n",
      " over tha t same rose overla ps a lot with the max bounding box, so we will get rid\n",
      "of it.\n",
      " 472  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "23\n",
      " —F ully Con volutional N etworks for Seman tic Segmen ta tion, – J . Long, E. Shelhamer , T . Darrell (2015).\n",
      "24\n",
      " There is one small exception: a con volutional la yer using V ALID padding will com plain if the in put size is\n",
      "smaller than the kernel size.\n",
      "⁄\n",
      " Third, repea t step two un til there are no more bounding boxes to get rid of.\n",
      " This sim ple a pproach to object detection works pretty well, but it requires running\n",
      " the CNN man y times, so it is quite slow . F ortuna tely , there is a m uch faster wa y to\n",
      "slide a CNN across an image: using a \n",
      " F u l l y C o n v o l u t i o n a l N e tw o r k\n",
      ".\n",
      "Fully Convolutional Networks (FCNs)\n",
      " The idea of FCN s was first in troduced in a \n",
      " 2015 pa per\n",
      "23\n",
      "  by J ona than Long et al., for\n",
      " seman tic segmen ta tion (the task of classif ying ever y pixel in an image according to\n",
      " the class of the object it belongs to). They poin ted out tha t you could replace the\n",
      " dense la yers a t the top of a CNN by con volutional la yers. T o understand this, let ‡ s look\n",
      " a t an exam ple: suppose a dense la yer with 200 neurons sits on top of a con volutional\n",
      " la yer tha t outputs 100 fea ture ma ps, each of size 7 „ 7 (this is the fea ture ma p size, not\n",
      " the kernel size). Each neuron will com pute a weigh ted sum of all 100 „ 7 „ 7 activaƒ\n",
      " tions from the con volutional la yer (plus a bias term). N ow let ‡ s see wha t ha ppens if we\n",
      " replace the dense la yer with a con volution la yer using 200 filters, each 7 „ 7, and with\n",
      " V ALID padding. This la yer will output 200 fea ture ma ps, each 1 „ 1 (since the kernel\n",
      " is exactly the size of the in put fea ture ma ps and we are using V ALID padding). In\n",
      " other words, it will output 200 n umbers, just like the dense la yer did, and if you look\n",
      " closely a t the com puta tions performed by a con volutional la yer , you will notice tha t\n",
      " these n umbers will be precisely the same as the dense la yer produced. The only differƒ\n",
      " ence is tha t the dense la yer‡ s output was a tensor of sha pe [ba tch size, 200] while the\n",
      " con volutional la yer will output a tensor of sha pe [ba tch size, 1, 1, 200].\n",
      " T o con vert a dense la yer to a con volutional la yer , the n umber of filƒ\n",
      " ters in the con volutional la yer m ust be equal to the n umber of units\n",
      " in the dense la yer , the filter size m ust be equal to the size of the\n",
      " in put fea ture ma ps, and you m ust use V ALID padding. The stride\n",
      " ma y be set to 1 or more, as we will see shortly .\n",
      " Wh y is this im portan t? W ell, while a dense la yer expects a specific in put size (since it\n",
      " has one weigh t per in put fea ture), a con volutional la yer will ha ppily process images of\n",
      " an y size\n",
      "24\n",
      "  (however , it does expect its in puts to ha ve a specific n umber of channels,\n",
      " since each kernel con tains a differen t set of weigh ts for each in put channel). Since an\n",
      " FCN con tains only con volutional la yers (and pooling la yers, which ha ve the same\n",
      " property), it can be trained and executed on images of an y size!\n",
      " Object Detection  |  473\n",
      "\n",
      "25\n",
      " This assumes we used only SAME padding in the network: indeed, V ALID padding would reduce the size of\n",
      " the fea ture ma ps. M oreover , 448 can be nea tly divided by 2 several times un til we reach 7, without an y roundƒ\n",
      " ing error . If an y la yer uses a differen t stride than 1 or 2, then there ma y be some rounding error , so again the\n",
      " fea ture ma ps ma y end up being smaller .\n",
      " F or exam ple, suppose we already trained a CNN for flower classifica tion and localizaƒ\n",
      " tion. I t was trained on 224 „ 224 images and it outputs 10 n umbers: outputs 0 to 4 are\n",
      " sen t through the softmax activa tion function, and this gives the class probabilities\n",
      " (one per class); output 5 is sen t through the logistic activa tion function, and this gives\n",
      " the objectness score; outputs 6 to 9 do not use an y activa tion function, and they repƒ\n",
      " resen t the bounding box‡ s cen ter coordina tes, and its heigh t and width. W e can now\n",
      " con vert its dense la yers to con volutional la yers. In fact, we don ‡ t even need to retrain\n",
      " it, we can just copy the weigh ts from the dense la yers to the con volutional la yers!\n",
      " Alterna tively , we could ha ve con verted the CNN in to an FCN before training.\n",
      " N ow suppose the last con volutional la yer before the output la yer (also called the botƒ\n",
      " tleneck la yer) outputs 7 „ 7 fea ture ma ps when the network is fed a 224 „ 224 image\n",
      "(see the left side of \n",
      "Figure 14-25\n",
      " ). If we feed the FCN a 448 „ 448 image (see the righ t\n",
      "side of \n",
      "Figure 14-25\n",
      " ), the bottleneck la yer will now output 14 „ 14 fea ture ma ps.\n",
      "25\n",
      " Since the dense output la yer was replaced by a con volutional la yer using 10 filters of\n",
      " size 7 „ 7, V ALID padding and stride 1, the output will be com posed of 10 fea tures\n",
      " ma ps, each of size 8 „ 8 (since 14 - 7 + 1 = 8). In other words, the FCN will process\n",
      " the whole image only once and it will output an 8 „ 8 grid where each cell con tains 10\n",
      " n umbers (5 class probabilities, 1 objectness score and 4 bounding box coordina tes).\n",
      " I t ‡ s exactly like taking the original CNN and sliding it across the image using 8 steps\n",
      "per row and 8 steps per column: to visualize this, imagine chopping the original\n",
      " image in to a 14 „ 14 grid, then sliding a 7 „ 7 window across this grid: there will be 8\n",
      " „ 8 = 64 possible loca tions for the window , hence 8 „ 8 predictions. H owever , the\n",
      " FCN a pproach is \n",
      " m u c h\n",
      "  more efficien t, since the network only looks a t the image\n",
      "once. In fact, \n",
      " Y o u O n l y L o o k O n c e\n",
      " \n",
      " (Y OLO) is the name of a ver y popular object detecƒ\n",
      "tion architecture!\n",
      " 474  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "26\n",
      " —Y ou Only Look Once: U nified, Real-T ime Object Detection, – J . Redmon, S. Divvala, R. Girshick, A. F arhadi\n",
      "(2015).\n",
      "27\n",
      " —Y OLO9000: B etter , F aster , Stronger , – J . Redmon, A. F arhadi (2016).\n",
      "28\n",
      " —Y OLO v3: An Incremen tal Im provemen t, – J . Redmon, A. F arhadi (2018).\n",
      " F i g u r e 14-25. A F u l l y C o n v o l u t i o n a l N e tw o r k P r o c e s s i n g a S m a l l I m a ge \n",
      "(le“)\n",
      "  a n d a\n",
      " L a r ge O n e (r i gh t)\n",
      "You Only Look Once (YOLO)\n",
      " Y OLO is an extremely fast and accura te object detection architecture proposed by\n",
      " J oseph Redmon et al. in a \n",
      " 2015 pa per\n",
      "26\n",
      " , and subsequen tly im proved \n",
      "in 2016\n",
      "27\n",
      " (Y OLO v2) and \n",
      "in 2018\n",
      "28\n",
      " \n",
      " (Y OLO v3). I t is so fast tha t it can run in realtime on a video\n",
      "(check out this nice \n",
      "demo\n",
      ").\n",
      " Y OLO v3‡ s architecture is quite similar to the one we just discussed, but with a few\n",
      " im portan t differences:\n",
      " Object Detection  |  475\n",
      "\n",
      "⁄\n",
      "First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each\n",
      " bounding box comes with an objectness score. I t also outputs 20 class probabiliƒ\n",
      " ties per grid cell, as it was trained on the P ASCAL V O C da taset, which con tains\n",
      " 20 classes. Tha t ‡ s a total of 45 n umbers per grid cell (5 * 4 bounding box coordiƒ\n",
      " na tes, plus 5 objectness scores, plus 20 class probabilities).\n",
      "⁄\n",
      " Second, instead of predicting the absolute coordina tes of the bounding box cenƒ\n",
      " ters, Y OLO v3 predicts an offset rela tive to the coordina tes of the grid cell, where\n",
      " (0, 0) means the top left of tha t cell, and (1, 1) means the bottom righ t. F or each\n",
      " grid cell, Y OLO v3 is trained to predict only bounding boxes whose cen ter lies in\n",
      " tha t cell (but the bounding box itself generally extends well beyond the grid cell).\n",
      " Y OLO v3 a pplies the logistic activa tion function to the bounding box coordina tes\n",
      "to ensure they remain in the 0 to 1 range.\n",
      "⁄\n",
      " Third, before training the neural net, Y OLO v3 finds 5 represen ta tive bounding\n",
      "box dimensions, called \n",
      " a n c h o r b o x e s\n",
      " (or \n",
      " b o u n d i n g b o x p r i o r s\n",
      "): it does this by\n",
      " a pplying the K-M eans algorithm (see \n",
      "???\n",
      " ) to the heigh t and width of the training\n",
      " set bounding boxes. F or exam ple, if the training images con tain man y pedesƒ\n",
      " trians, then one of the anchor boxes will likely ha ve the dimensions of a typical\n",
      "pedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it\n",
      " actually predicts how m uch to rescale each of the anchor boxes. F or exam ple,\n",
      "suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network\n",
      " predicts, sa y , a vertical rescaling factor of 1.5 and a horizon tal rescaling of 0.9 (for\n",
      "one of the grid cells), this will result in a predicted bounding box of size 150 „ 45\n",
      " pixels. T o be more precise, for each grid cell and each anchor box, the network\n",
      " predicts the log of the vertical and horizon tal rescaling factors. H a ving these priƒ\n",
      " ors makes the network more likely to predict bounding boxes of the a ppropria te\n",
      " dimensions, and it also speeds up training since it will more quickly learn wha t\n",
      "reasonable bounding boxes look like.\n",
      "⁄\n",
      " F ourth, the network is trained using images of differen t scales: ever y few ba tches\n",
      "during training, the network randomly chooses a new image dimension (from\n",
      "330 „ 330 to 608 „ 608 pixels). This allows the network to learn to detect objects\n",
      " a t differen t scales. M oreover , it makes it possible to use Y OLO v3 a t differen t\n",
      " scales: the smaller scale will be less accura te but faster than the larger scale, so\n",
      " you can choose the righ t tradeoff for your use case.\n",
      " There are a few more innova tions you migh t be in terested in, such as the use of skip\n",
      " connections to recover some of the spa tial resolution tha t is lost in the CNN (we will\n",
      " discuss this shortly when we look a t seman tic segmen ta tion). M oreover , in the 2016\n",
      " pa per , the a uthors in troduce the Y OLO9000 model tha t uses hierarchical classificaƒ\n",
      " tion: the model predicts a probability for each node in a visual hierarch y called \n",
      " W o r d‡\n",
      " T r e e\n",
      " . This makes it possible for the network to predict with high confidence tha t an\n",
      " image represen ts, sa y , a dog, even though it is unsure wha t specific type of dog it is.\n",
      " 476  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " So I encourage you to go ahead and read all three pa pers: they are quite pleasan t to\n",
      " read, and it is an excellen t exam ple of how Deep Learning systems can be incremenƒ\n",
      " tally im proved.\n",
      "Mean Average Precision (mAP)\n",
      " A ver y common metric used in object detection tasks is the \n",
      " m e a n Av er a ge P r e ci s i o n\n",
      " (mAP). — M ean A verage – sounds a bit redundan t, doesn ‡ t it? T o understand this metƒ\n",
      " ric, let ‡ s go back to two classifica tion metrics we discussed in \n",
      " Cha pter 3\n",
      ": precision and\n",
      " recall. Remember the tradeoff: the higher the recall, the lower the precision. Y ou can\n",
      " visualize this in a Precision/Recall cur ve (see \n",
      "Figure 3-5\n",
      " ). T o summarize this cur ve\n",
      " in to a single n umber , we could com pute its Area U nder the Cur ve (A UC). But note\n",
      " tha t the Precision/Recall cur ve ma y con tain a few sections where precision actually\n",
      " goes up when recall increases, especially a t low recall values (you can see this a t the\n",
      "top left of \n",
      "Figure 3-5\n",
      " ). This is one of the motiva tions for the mAP metric.\n",
      " Suppose the classifier has a 90% precision a t 10% recall, but a 96% precision a t 20%\n",
      " recall: there ‡ s really no tradeoff here: it sim ply makes more sense to use the classifier\n",
      " a t 20% recall ra ther than a t 10% recall, as you will get both higher recall and higher\n",
      " precision. So instead of looking a t the precision \n",
      " a t\n",
      " \n",
      "10% recall, we should really be\n",
      " looking a t the \n",
      " m axi m u m\n",
      " \n",
      " precision tha t the classifier can offer with \n",
      " a t l e as t\n",
      "   10% recall.\n",
      " I t would be 96%, not 90%. So one wa y to get a fair idea of the model ‡ s performance is\n",
      " to com pute the maxim um precision you can get with a t least 0% recall, then 10%\n",
      " recall, 20%, and so on up to 100%, and then calcula te the mean of these maxim um\n",
      "precisions. This is called the \n",
      " Av er a ge P r e ci s i o n\n",
      " \n",
      " (AP) metric. N ow when there are more\n",
      " than 2 classes, we can com pute the AP for each class, and then com pute the mean AP\n",
      " (mAP). Tha t ‡ s it!\n",
      " H owever , in an object detection systems, there is an additional level of com plexity :\n",
      " wha t if the system detected the correct class, but a t the wrong loca tion (i.e., the\n",
      " bounding box is com pletely off )? Surely we should not coun t this as a positive predicƒ\n",
      " tion. So one a pproach is to define an IOU threshold: for exam ple, we ma y consider\n",
      " tha t a prediction is correct only if the IOU is grea ter than, sa y , 0.5, and the predicted\n",
      "class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%,\n",
      "or sometimes just AP\n",
      "50\n",
      " ). In some com petitions (such as the P ascal V O C challenge),\n",
      " this is wha t is done. In others (such as the CO CO com petition), the mAP is com puted\n",
      " for differen t IOU thresholds (0.50, 0.55, 0.60, Ñ, 0.95), and the final metric is the\n",
      " mean of all these mAP s (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Y es, tha t ‡ s a mean\n",
      " mean a verage.\n",
      " Several Y OLO im plemen ta tions built using T ensorFlow are a vailable on gith ub , some\n",
      " with pretrained weigh ts. A t the time of writing, they are based on T ensorFlow 1, but\n",
      " by the time you read this, TF 2 im plemen ta tions will certainly be a vailable. M oreover ,\n",
      " other object detection models are a vailable in the T ensorFlow M odels project, man y\n",
      " Object Detection  |  477\n",
      "\n",
      "29\n",
      " — SSD: Single Shot M ultiB ox Detector , – W ei Liu et al. (2015).\n",
      "30\n",
      " —F aster R -CNN: T owards Real-T ime Object Detection with Region Proposal N etworks, – Shaoqing Ren et al.\n",
      "(2015).\n",
      " with pretrained weigh ts, and some ha ve even been ported to TF H ub , making them\n",
      "extremely easy to use, such as \n",
      "SSD\n",
      "29\n",
      "   and \n",
      " F aster -RCNN\n",
      ".\n",
      "30\n",
      ", which are both quite popuƒ\n",
      " lar . SSD is also a — single shot – detection model, quite similar to Y OLO , while F aster R -\n",
      " CNN is more com plex: the image first goes through a CNN, and the output is passed\n",
      " to a Region Proposal N etwork (RPN) which proposes bounding boxes tha t are most\n",
      " likely to con tain an object, and a classifier is run for each bounding box, based on the\n",
      "cropped output of the CNN.\n",
      " The choice of detection system depends on man y factors: speed, accuracy , a vailable\n",
      " pretrained models, training time, com plexity , etc. The pa pers con tain tables of metƒ\n",
      " rics, but there is quite a lot of variability in the testing en vironmen ts, and the technolƒ\n",
      " ogies evolve so fast tha t it is difficulty to make a fair com parison tha t will be useful for\n",
      " most people and remain valid for more than a few mon ths.\n",
      " Grea t! So we can loca te objects by dra wing bounding boxes around them. But perƒ\n",
      " ha ps you migh t wan t to be a bit more precise. Let ‡ s see how to go down to the pixel\n",
      "level.\n",
      "Semantic Segmentation\n",
      "In \n",
      " s em a n t i c s e g m en t a t i o n\n",
      ", each pixel is classified according to the class of the object it\n",
      " belongs to (e.g., road, car , pedestrian, building, etc.), as shown in \n",
      "Figure 14-26\n",
      " . N ote\n",
      " tha t differen t objects of the same class are \n",
      " n o t\n",
      "   distinguished. F or exam ple, all the bicyƒ\n",
      " cles on the righ t side of the segmen ted image end up as one big lum p of pixels. The\n",
      " main difficulty in this task is tha t when images go through a regular CNN, they gradƒ\n",
      " ually lose their spa tial resolution (due to the la yers with strides grea ter than 1): so a\n",
      " regular CNN ma y end up knowing tha t there ‡ s a person in the image, somewhere in\n",
      " the bottom left of the image, but it will not be m uch more precise than tha t.\n",
      " 478  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "31\n",
      " This type of la yer is sometimes referred to as a \n",
      " d e c o n v o l u t i o n l a y er\n",
      ", but it does \n",
      " n o t\n",
      "  perform wha t ma thema tiƒ\n",
      " cians call a decon volution, so this name should be a voided.\n",
      " F i g u r e 14-26. S em a n t i c s e g m en t a t i o n\n",
      " J ust like for object detection, there are man y differen t a pproaches to tackle this probƒ\n",
      " lem, some quite com plex. H owever , a fairly sim ple solution was proposed in the 2015\n",
      " pa per by J ona than Long et al. we discussed earlier . They start by taking a pretrained\n",
      " CNN and turning in to an FCN, as discussed earlier . The CNN a pplies a stride of 32 to\n",
      " the in put image overall (i.e., if you add up all the strides grea ter than 1), meaning the\n",
      " last la yer outputs fea ture ma ps tha t are 32 times smaller than the in put image. This is\n",
      "clearly too coarse, so they add a single \n",
      " u p s a m p l i n g l a y er\n",
      "  tha t m ultiplies the resolution\n",
      " by 32. There are several solutions a vailable for upsam pling (increasing the size of an\n",
      " image), such as bilinear in terpola tion, but it only works reasonably well up to „4 or\n",
      "„8. Instead, they used a \n",
      " t r a ns p o s e d c o n v o l u t i o n a l l a y er\n",
      ":\n",
      "31\n",
      "  it is equivalen t to first\n",
      " stretching the image by inserting em pty rows and columns (full of zeros), then perƒ\n",
      " forming a regular con volution (see \n",
      "Figure 14-27\n",
      " ). Alterna tively , some people prefer to\n",
      " think of it as a regular con volutional la yer tha t uses fractional strides (e.g., 1/2 in\n",
      "Figure 14-27\n",
      "). The \n",
      " t r a ns p o s e d c o n v o l u t i o n a l l a y er\n",
      " can be initialized to perform someƒ\n",
      " thing close to linear in terpola tion, but since it is a trainable la yer , it will learn to do\n",
      "better during training.\n",
      " Semantic Segmentation  |  479\n",
      "\n",
      " F i g u r e 14-27. U p s a m p l i n g U s i n g a T r a ns p o s e C o n v o l u t i o n a l L a y er\n",
      " In a transposed con volution la yer , the stride defines how m uch the\n",
      " in put will be stretched, not the size of the filter steps, so the larger\n",
      " the stride, the larger the output (unlike for con volutional la yers or\n",
      " pooling la yers).\n",
      "TensorFlow Convolution Operations\n",
      " T ensorFlow also offers a few other kinds of con volutional la yers:\n",
      "⁄\n",
      "keras.layers.Conv1D\n",
      "  crea tes a con volutional la yer for 1D in puts, such as time\n",
      "series or text (sequences of letters or words), as we will see in \n",
      "???\n",
      ".\n",
      "⁄\n",
      "keras.layers.Conv3D\n",
      "  crea tes a con volutional la yer for 3D in puts, such as 3D\n",
      "PET scan.\n",
      "⁄\n",
      "Setting the \n",
      "dilation_rate\n",
      "  h yperparameter of an y con volutional la yer to a value\n",
      " of 2 or more crea tes an \n",
      " œ-t r o u s c o n v o l u t i o n a l l a y er\n",
      "  (— Õ trous – is French for —with\n",
      " holes –). This is equivalen t to using a regular con volutional la yer with a filter dilaƒ\n",
      " ted by inserting rows and columns of zeros (i.e., holes). F or exam ple, a 1 „ 3 filter\n",
      "equal to \n",
      "[[1,2,3]]\n",
      "  ma y be dila ted with a \n",
      " d i l a t i o n r a t e\n",
      " of 4, resulting in a \n",
      " d i l a t e d\n",
      "†lter\n",
      " \n",
      "[[1, 0, 0, 0, 2, 0, 0, 0, 3]]\n",
      " . This allows the con volutional la yer to\n",
      " 480  |  Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      " ha ve a larger receptive field a t no com puta tional price and using no extra paramƒ\n",
      "eters.\n",
      "⁄\n",
      "tf.nn.depthwise_conv2d()\n",
      " \n",
      " can be used to crea te a \n",
      " d e p t h w i s e c o n v o l u t i o n a l l a y er\n",
      " (but you need to crea te the variables yourself ). I t a pplies ever y filter to ever y\n",
      " individual in put channel independen tly . Th us, if there are \n",
      "f\n",
      "n\n",
      " filters and \n",
      "f\n",
      "n\n",
      " \n",
      " in put\n",
      "channels, then this will output \n",
      "f\n",
      "n\n",
      " „ \n",
      "f\n",
      "n\n",
      "  fea ture ma ps.\n",
      " This solution is oka y , but still too im precise. T o do better , the a uthors added skip conƒ\n",
      " nections from lower la yers: for exam ple, they upsam pled the output image by a factor\n",
      " of 2 (instead of 32), and they added the output of a lower la yer tha t had this double\n",
      " resolution. Then they upsam pled the result by a factor of 16, leading to a total upsamƒ\n",
      "pling factor of 32 (see \n",
      "Figure 14-28\n",
      " ). This recovered some of the spa tial resolution\n",
      " tha t was lost in earlier pooling la yers. In their best architecture, they used a second\n",
      " similar skip connection to recover even finer details from an even lower la yer : in\n",
      "short, the output of the original CNN goes through the following extra steps: upscale\n",
      " „2, add the output of a lower la yer (of the a ppropria te scale), upscale „2, add the outƒ\n",
      " put of an even lower la yer , and finally upscale „8. I t is even possible to scale up\n",
      "beyond the size of the original image: this can be used to increase the resolution of an\n",
      "image, which is a technique called \n",
      " s u p er -r e s o l u t i o n\n",
      ".\n",
      " F i g u r e 14-28. S k i p l a y er s r e c o v er s o m e s p a t i a l r e s o l u t i o n f r o m l o w er l a y er s\n",
      " Once again, man y gith ub repositories provide T ensorFlow im plemen ta tions of\n",
      " seman tic segmen ta tion (T ensorFlow 1 for now), and you will even find a pretrained\n",
      " i ns t a n c e s e g m en t a t i o n\n",
      "  model in the T ensorFlow M odels project. Instance segmen taƒ\n",
      " tion is similar to seman tic segmen ta tion, but instead of merging all objects of the\n",
      " same class in to one big lum p , each object is distinguished from the others (e.g., it\n",
      " iden tifies each individual bicycle). A t the presen t, they provide m ultiple im plemen taƒ\n",
      "tions of the \n",
      " M as k R -CNN\n",
      " architecture, which was proposed in a \n",
      " 2017 pa per\n",
      ": it\n",
      " extends the F aster R -CNN model by additionally producing a pixel-mask for each\n",
      "bounding box. So not only do you get a bounding box around each object, with a set\n",
      " of estima ted class probabilities, you also get a pixel mask tha t loca tes pixels in the\n",
      " bounding box tha t belong to the object.\n",
      " Semantic Segmentation  |  481\n",
      "\n",
      "32\n",
      " — M a trix Ca psules with EM Routing, – G. Hin ton, S. Sabour , N. Frosst (2018).\n",
      " As you can see, the field of Deep Com puter V ision is vast and moving fast, with all\n",
      " sorts of architectures popping out ever y year , all based on Con volutional N eural N etƒ\n",
      "works. The progress made in just a few years has been astounding, and researchers\n",
      "are now focusing on harder and harder problems, such as \n",
      " a d v er s a r i a l l e a r n i n g\n",
      "   (which\n",
      " a ttem pts to make the network more resistan t to images designed to fool it), explainaƒ\n",
      " bility (understanding wh y the network makes a specific classifica tion), realistic \n",
      " i m a ge\n",
      " gen er a t i o n\n",
      " \n",
      "(which we will come back to in \n",
      "???\n",
      "), \n",
      " s i n gl e-s h o t l e a r n i n g\n",
      " \n",
      " (a system tha t can\n",
      " recognize an object after it has seen it just once), and m uch more. Some even explore\n",
      " com pletely novel architectures, such as Geoffrey Hin ton ‡ s \n",
      " c a p s u l e n e tw o r ks\n",
      "32\n",
      " (I preƒ\n",
      " sen ted them in a couple \n",
      "videos\n",
      " , with the corresponding code in a notebook). N ow on\n",
      " to the next cha pter , where we will look a t how to process sequen tial da ta such as time\n",
      " series using Recurren t N eural N etworks and Con volutional N eural N etworks.\n",
      "Exercises\n",
      "1.\n",
      " Wha t are the advan tages of a CNN over a fully connected DNN for image classiƒ\n",
      " fica tion?\n",
      "2.\n",
      " Consider a CNN com posed of three con volutional la yers, each with 3 „ 3 kernels,\n",
      " a stride of 2, and SAME padding. The lowest la yer outputs 100 fea ture ma ps, the\n",
      " middle one outputs 200, and the top one outputs 400. The in put images are RGB\n",
      " images of 200 „ 300 pixels. Wha t is the total n umber of parameters in the CNN?\n",
      " If we are using 32-bit floa ts, a t least how m uch RAM will this network require\n",
      " when making a prediction for a single instance? Wha t about when training on a\n",
      " mini-ba tch of 50 images?\n",
      "3.\n",
      " If your GPU runs out of memor y while training a CNN, wha t are five things you\n",
      " could tr y to solve the problem?\n",
      "4.\n",
      " Wh y would you wan t to add a max pooling la yer ra ther than a con volutional\n",
      " la yer with the same stride?\n",
      "5.\n",
      " When would you wan t to add a \n",
      " l o c a l r e s p o ns e n o r m a l iz a t i o n\n",
      "  la yer?\n",
      "6.\n",
      " Can you name the main innova tions in AlexN et, com pared to LeN et-5? Wha t\n",
      " about the main innova tions in GoogLeN et, ResN et, SEN et and Xception?\n",
      "7.\n",
      " Wha t is a F ully Con volutional N etwork? H ow can you con vert a dense la yer in to\n",
      " a con volutional la yer?\n",
      "8.\n",
      " Wha t is the main technical difficulty of seman tic segmen ta tion?\n",
      "9.\n",
      " Build your own CNN from scra tch and tr y to achieve the highest possible accuƒ\n",
      " racy on MNIST .\n",
      " 482   |  Chapter  14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "10.\n",
      " U se transfer learning for large image classifica tion.\n",
      "a.\n",
      " Crea te a training set con taining a t least 100 images per class. F or exam ple, you\n",
      " could classif y your own pictures based on the loca tion (beach, moun tain, city ,\n",
      " etc.), or alterna tively you can just use an existing da taset (e.g., from T ensorƒ\n",
      " Flow Da tasets).\n",
      " b .\n",
      " Split it in to a training set, a valida tion set and a test set.\n",
      "c.\n",
      " Build the in put pipeline, including the a ppropria te preprocessing opera tions,\n",
      " and optionally add da ta a ugmen ta tion.\n",
      "d.\n",
      " Fine-tune a pretrained model on this da taset.\n",
      "11.\n",
      " Go through T ensorFlow‡ s \n",
      "DeepDream tutorial\n",
      " . I t is a fun wa y to familiarize yourƒ\n",
      " self with various wa ys of visualizing the pa tterns learned by a CNN, and to generƒ\n",
      " a te art using Deep Learning.\n",
      " Solutions to these exercises are a vailable in \n",
      "???\n",
      ".\n",
      " Exercises  |  483\n",
      "\n",
      "About the Author\n",
      " A ur − li e n G − r o n\n",
      " \n",
      " is a M achine Learning consultan t. A former Googler , he led the Y ouƒ\n",
      " T ube video classifica tion team from 2013 to 2016. H e was also a founder and CTO of\n",
      " W ifirst from 2002 to 2012, a leading W ireless ISP in France; and a founder and CTO\n",
      " of P olyconseil in 2001, the firm tha t now manages the electric car sharing ser vice\n",
      " A utolib ‡ .\n",
      " B efore this he worked as an engineer in a variety of domains: finance ( JP M organ and\n",
      " Soci•t• G•n•rale), defense (Canada ‡ s D OD), and healthcare (blood transfusion). H e\n",
      " published a few technical books (on C++, W iFi, and in ternet architectures), and was\n",
      " a Com puter Science lecturer in a French engineering school.\n",
      " A few fun facts: he ta ugh t his three children to coun t in binar y with their fingers (up\n",
      " to 1023), he studied microbiolog y and evolutionar y genetics before going in to softƒ\n",
      " ware engineering, and his parach ute didn ‡ t open on the second jum p .\n",
      "Colophon\n",
      "The animal on the cover of \n",
      " H a n d s-O n M a c h i n e L e a r n i n g w i t h S ci k i t-L e a r n a n d T en‡\n",
      " s o rF l o w\n",
      " is the fire salamander (\n",
      " S a l a m a n d r a s a l a m a n d r a\n",
      " ), an am phibian found across\n",
      " most of E urope. I ts black, glossy skin fea tures large yellow spots on the head and\n",
      "back, signaling the presence of alkaloid toxins. This is a possible source of this\n",
      " am phibian ‡ s common name: con tact with these toxins (which they can also spra y\n",
      " short distances) ca uses con vulsions and h yper ven tila tion. Either the painful poisons\n",
      " or the moistness of the salamander‡ s skin (or both) led to a misguided belief tha t these\n",
      " crea tures not only could sur vive being placed in fire but could extinguish it as well.\n",
      "Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\n",
      " the pools or other fresh wa ter bodies tha t facilita te their breeding. Though they spend\n",
      " most of their life on land, they give birth to their young in wa ter . They subsist mostly\n",
      "on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\n",
      " in length, and in ca ptivity , ma y live as long as 50 years.\n",
      " The fire salamander‡ s n umbers ha ve been reduced by destruction of their forest habiƒ\n",
      " ta t and ca pture for the pet trade, but the grea test threa t is the susceptibility of their\n",
      " moisture-permeable skin to pollutan ts and microbes. Since 2014, they ha ve become\n",
      " extinct in parts of the N etherlands and B elgium due to an in troduced fungus.\n",
      " M an y of the animals on O ‡Reilly covers are endangered; all of them are im portan t to\n",
      " the world. T o learn more about how you can help , go to \n",
      " a n i m a l s.o r ei l l y .c o m\n",
      ".\n",
      "The cover image is from \n",
      " W o o d ‹ s I l l u s t r a t e d N a t u r a l H i s t o r y\n",
      " . The cover fon ts are UR W\n",
      " T ypewriter and Guardian Sans. The text fon t is A dobe Minion Pro; the heading fon t\n",
      " is A dobe M yriad Condensed; and the code fon t is Dalton M aag ‡ s Ubun tu M ono .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example Code:\n",
    "from PyPDF2 import PdfFileReader , PdfFileWriter\n",
    "pdf_reader = PdfFileReader('HandsonMachine-Learning-with-Scikit-2E.pdf')\n",
    "if pdf_reader.isEncrypted: # to check whether the pdf is encrypted or not\n",
    "    pdf_reader.decrypt(\"swordfish\")\n",
    "for page in pdf_reader.pages:\n",
    "    print(page.extractText()) # to print the text data of a page from pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2989bf",
   "metadata": {},
   "source": [
    "## 5. What methods do you use to rotate a page?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "PyPDF2 Package provides 2 methods to rotate a page:\n",
    "\n",
    "1. `rotateClockWise()` -> For Clockwise rotation\n",
    "2. `rotateCounterClockWise()` -> For Counter Clockwise rotation\n",
    "\n",
    "The PyPDF2 package only allows you to rotate a page in increments of 90 degrees. You will receive an AssertionError otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2526e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('HandsonMachine-Learning-with-Scikit-2E.pdf', 'rb')\n",
    "file = PdfFileReader(file_object)\n",
    "\n",
    "# ROTATING A SINGLE PAGE\n",
    "page_1 = file.getPage(0)\n",
    "page_1.rotateClockwise(90)\n",
    "# Creating a new Pdf and adding the rotated pdf page\n",
    "\n",
    "pdfWriter = PdfFileWriter()\n",
    "pdfWriter.addPage(page_1)\n",
    "resultPdfFile = open('HandsonMachine-Learning-with-Scikit-2E.pdf', 'wb')\n",
    "pdfWriter.write(resultPdfFile)\n",
    "\n",
    "resultPdfFile.close()\n",
    "file_object.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b46fab1",
   "metadata": {},
   "source": [
    "## 6. What is the difference between a Run object and a Paragraph object?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "The structure of a document is represented by three different data types in `python-Docx`. \n",
    "\n",
    "At the highest level, a `Document` object represents the entire document. The Document object contains a list of `Paragraph` objects for the paragraphs in the document. (A new paragraph begins whenever the user presses `ENTER` or `RETURN` while typing in a Word document.) Each of these Paragraph objects contains a list of one or more `Run` objects.\n",
    "\n",
    "The text in a Word document is more than just a string. It has font, size, color, and other styling information associated with it. A style in Word is a collection of these attributes. A Run object is a contiguous run of text with the same style. A new Run object is needed whenever the text style changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880f471",
   "metadata": {},
   "source": [
    "## 7. How do you obtain a list of Paragraph objects for a Document object that’s stored in a variable named doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51f2fe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<docx.text.paragraph.Paragraph object at 0x7f76a79fa390>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa400>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa470>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa4a8>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa358>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa518>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa208>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa1d0>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa160>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa128>, <docx.text.paragraph.Paragraph object at 0x7f76a79fa320>, <docx.text.paragraph.Paragraph object at 0x7f76a6461748>, <docx.text.paragraph.Paragraph object at 0x7f76a64617f0>]\n",
      "1. In what modes should the PdfFileReader() and PdfFileWriter() File objects will be opened?\n",
      "2. From a PdfFileReader object, how do you get a Page object for page 5?\n",
      "3. What PdfFileReader variable stores the number of pages in the PDF document?\n",
      "4. If a PdfFileReader object’s PDF is encrypted with the password swordfish, what must you do before you can obtain Page objects from it?\n",
      "5. What methods do you use to rotate a page?\n",
      "6. What is the difference between a Run object and a Paragraph object?\n",
      "7. How do you obtain a list of Paragraph objects for a Document object that’s stored in a variable named doc?\n",
      "8. What type of object has bold, underline, italic, strike, and outline variables?\n",
      "9. What is the difference between False, True, and None for the bold variable?\n",
      "10. How do you create a Document object for a new Word document?\n",
      "11. How do you add a paragraph with the text 'Hello, there!' to a Document object stored in a variable named doc?\n",
      "12. What integers represent the levels of headings available in Word documents?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "doc = Document(\"Assignment_12.docx\")  # Path of the Docx file\n",
    "print(doc.paragraphs) # Prints the list of Paragraph objects for a Document\n",
    "for paragraph in doc.paragraphs:\n",
    "    print(paragraph.text) # Prints the text in the paragraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e696c59",
   "metadata": {},
   "source": [
    "## 8. What type of object has bold, underline, italic, strike, and outline variables?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "`Run` object has bold, underline, italic, strike, and outline variables. The text in a Word document is more than just a string. It has font, size, color, and other styling information associated with it.\n",
    "\n",
    "A style in Word is a collection of these attributes. A Run object is a contiguous run of text with the same style. A new Run object is needed whenever the text style changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff87713",
   "metadata": {},
   "source": [
    "## 9. What is the difference between False, True, and None for the bold variable?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "- Runs can be further styled using text attributes. Each attribute can be set to one of three values:\n",
    "  True (the attribute is always enabled, no matter what other styles are applied to the run),\n",
    "  False (the attribute is always disabled),\n",
    "  None (defaults to whatever the run’s style is set to)\n",
    "\n",
    "- True always makes the Run object bolded and False makes it always not bolded, no matter what the style’s bold setting is . None will make the Run object just use the style’s bold setting\n",
    "\n",
    "```python\n",
    "\n",
    "bold = True  # Style Set to Bold\n",
    "bold = False  # Style Not Set to Bold\n",
    "bold = None  # Style is Not Applicable\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a280546",
   "metadata": {},
   "source": [
    "## 10. How do you create a Document object for a new Word document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a1d1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "document = Document()\n",
    "document.add_paragraph(\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum\")\n",
    "document.save('assignment.docx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f5441",
   "metadata": {},
   "source": [
    "## 11. How do you add a paragraph with the text 'Hello, there!' to a Document object stored in a variable named doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "031fd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "doc = Document()\n",
    "doc.add_paragraph('Hello, there!')\n",
    "doc.save('there.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed7bb9",
   "metadata": {},
   "source": [
    "## 12. What integers represent the levels of headings available in Word documents?\n",
    "\n",
    "### Answer :\n",
    "\n",
    "The levels for a heading in a word document can be specified by using the `level` attribute inside the `add_heading` method. There are a total of 5 levels statring for 0 t0 4. where level 0 makes a headline with the horizontal line below the text, whereas the heading level 1 is the main heading. Similarly, the other headings are sub-heading with their's font-sizes in decreasing order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da8a261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "new_doc = docx.Document()\n",
    "\n",
    "new_doc.add_heading('HEADING - 1', level=1)\n",
    "new_doc.add_heading('HEADING - 2', level=2)\n",
    "new_doc.add_heading('HEADING - 3', level=3)\n",
    "new_doc.add_heading('HEADING - 4', level=4)\n",
    "new_doc.add_heading('HEADING - 5', level=5)\n",
    "\n",
    "new_doc.save('sample_test.docx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
